{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/model9/autoencoder_reconstruction_dim5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tNYnFR-6Xh",
        "outputId": "f02291b8-a7f0-4544-e0da-a82cf9110fc3"
      },
      "execution_count": 774,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.7)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 775,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 776,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 777,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "8763fa13-26be-43ec-a939-e8e9948dbfa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 778,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint1\"\n",
        "data_source=\"5\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_error_part1.csv\"\n",
        "fname_bad = \"415_SC_error_part2.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 779,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" \n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" \n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 780,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "ddc9522a-de0c-4a9a-d92d-86f10c1093b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 781,
      "metadata": {
        "id": "vUcMjZAGKvtt"
      },
      "outputs": [],
      "source": [
        "#df_good = pd.read_csv(path_good,usecols=None)\n",
        "#df_bad = pd.read_csv(path_bad,usecols=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJlTtED0Gv2H"
      },
      "execution_count": 781,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSAhqTvMGczk"
      },
      "execution_count": 781,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 781,
      "metadata": {
        "id": "ZYuDXKraLOt4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname_good_list=[\"415_SC_error_part1.csv\",\"415_SC_error_new_part1.csv\",\"415_SC_error_new_part3.csv\"]\n",
        "fname_bad_list=[\"415_SC_error_part2.csv\",\"415_SC_error_new_part2.csv\",\"415_SC_error_new_part4.csv\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1gbYSaDLG915"
      },
      "execution_count": 782,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_debug_=False"
      ],
      "metadata": {
        "id": "1oj-pm2rOl04"
      },
      "execution_count": 783,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_create(df,csv_list,path_name):\n",
        "    df2=df\n",
        "    for fname in csv_list:\n",
        "        pfname=path_name+fname\n",
        "        if _debug_:\n",
        "            print(pfname)\n",
        "        df1=pd.read_csv(pfname,usecols=None)\n",
        "        df2=pd.concat([df2,df1],axis=0,ignore_index=True)\n",
        "        if _debug_:\n",
        "            print(df2.tail(1))\n",
        "    return df2\n"
      ],
      "metadata": {
        "id": "Ky65Q_p8HVoR"
      },
      "execution_count": 784,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test1=pd.read_csv(path_good+fname_good_list[2])\n",
        "df_test1.tail(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "CfAbvK6LMiFt",
        "outputId": "e1d5ecd1-2274-4c9c-f0d3-aaab6944411b"
      },
      "execution_count": 785,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "655  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "\n",
              "            7         8         9        10        11        12        13  \\\n",
              "655  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "\n",
              "           14        15        16        17        18        19  \n",
              "655  0.216297  0.216297  0.216297  0.216297  0.216297  1.159305  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b61ad7d-f78a-477a-b0b0-bc04303882a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>1.159305</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b61ad7d-f78a-477a-b0b0-bc04303882a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b61ad7d-f78a-477a-b0b0-bc04303882a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b61ad7d-f78a-477a-b0b0-bc04303882a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 785
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_good_new=pd.DataFrame()\n",
        "df_all_good_new=df_create(df_all_good_new,fname_good_list,path_good)"
      ],
      "metadata": {
        "id": "0R3D5a1WKDy9"
      },
      "execution_count": 786,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_bad_new=pd.DataFrame()\n",
        "df_all_bad_new=df_create(df_all_bad_new,fname_bad_list,path_good)"
      ],
      "metadata": {
        "id": "mRujHxlML10D"
      },
      "execution_count": 787,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 788,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "8e79c90b-ab42-4626-f24a-c7badb6e8755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "2496  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "2497  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "\n",
              "             7         8         9        10        11        12        13  \\\n",
              "2496  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "2497  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "\n",
              "            14        15        16        17        18        19  \n",
              "2496  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  \n",
              "2497  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c65d041-b9ca-4e67-9512-4a608ec2ab8a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c65d041-b9ca-4e67-9512-4a608ec2ab8a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c65d041-b9ca-4e67-9512-4a608ec2ab8a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c65d041-b9ca-4e67-9512-4a608ec2ab8a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 788
        }
      ],
      "source": [
        "df_all_bad_new.tail(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 788,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 789,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "df_all=pd.concat([df_all_good_new,df_all_bad_new],axis=0,ignore_index=True)\n",
        "scaler=MinMaxScaler()\n",
        "scaler.fit(df_all)\n",
        "good_scaled=scaler.transform(df_all_good_new)\n",
        "bad_scaled=scaler.transform(df_all_bad_new)\n"
      ],
      "metadata": {
        "id": "ibrdNyqsspbR"
      },
      "execution_count": 790,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFHmudoYPU3A"
      },
      "execution_count": 790,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxLD8g1htekl"
      },
      "execution_count": 790,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_good_scaled=pd.DataFrame(good_scaled,columns=df_all.columns)\n",
        "df_bad_scaled=pd.DataFrame(bad_scaled,columns=df_all.columns)\n",
        "df_good_scaled[\"state\"]=0\n",
        "df_bad_scaled[\"state\"]=1"
      ],
      "metadata": {
        "id": "9FdSgm_ztqjZ"
      },
      "execution_count": 791,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 791,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 792,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3W5mi70VM6hL",
        "outputId": "34498e07-5fef-4c69-f742-dcb7b1abfc5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0     0.203312  0.000000  0.000000  0.149256  0.339488  0.516456  0.546188   \n",
              "1     0.000000  0.155851  0.185726  0.339488  0.516456  0.546188  0.556316   \n",
              "2     0.155851  0.312632  0.367803  0.516456  0.546188  0.556316  0.556316   \n",
              "3     0.312632  0.466332  0.537185  0.546188  0.556316  0.556316  0.556316   \n",
              "4     0.466332  0.609315  0.565642  0.556316  0.556316  0.556316  0.556316   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "2073  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2074  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2075  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2076  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2077  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "0     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "1     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "2     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "3     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "4     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "2073  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2074  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2075  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2076  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2077  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "0     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "1     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "2     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "3     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "4     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "2073  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2074  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2075  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2076  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2077  0.556271  0.556271  0.556271  0.556271  0.556271  0.565565      0  \n",
              "\n",
              "[2078 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e33b81b7-35d9-424c-aeee-69cdd5012fc7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.203312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149256</td>\n",
              "      <td>0.339488</td>\n",
              "      <td>0.516456</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155851</td>\n",
              "      <td>0.185726</td>\n",
              "      <td>0.339488</td>\n",
              "      <td>0.516456</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.155851</td>\n",
              "      <td>0.312632</td>\n",
              "      <td>0.367803</td>\n",
              "      <td>0.516456</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.312632</td>\n",
              "      <td>0.466332</td>\n",
              "      <td>0.537185</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.466332</td>\n",
              "      <td>0.609315</td>\n",
              "      <td>0.565642</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2073</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2074</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2075</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2076</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2077</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.565565</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2078 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e33b81b7-35d9-424c-aeee-69cdd5012fc7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e33b81b7-35d9-424c-aeee-69cdd5012fc7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e33b81b7-35d9-424c-aeee-69cdd5012fc7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 792
        }
      ],
      "source": [
        "df_good_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 793,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_scaled=pd.concat([df_good_scaled,df_bad_scaled],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 794,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "a0c92064-a8b0-4ddd-cce4-8b2fec88bc91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "2493  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2494  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2495  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2496  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2497  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "2493  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2494  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2495  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2496  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2497  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "2493  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2494  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2495  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2496  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2497  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa8c3f88-53f5-4413-bbdf-7896a2452326\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa8c3f88-53f5-4413-bbdf-7896a2452326')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa8c3f88-53f5-4413-bbdf-7896a2452326 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa8c3f88-53f5-4413-bbdf-7896a2452326');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 794
        }
      ],
      "source": [
        "df_all_scaled.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "nVvhP84S_F1y"
      },
      "execution_count": 795,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 796,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\"]\n",
        "X_NN=df_all_scaled[prediktorok][:]  # mindent megtanul\n",
        "X_NN=df_good_scaled[prediktorok][:] # csak a jókat tanulja\n",
        "y_NN=df_all_scaled[\"state\"][:]\n",
        "y_NN=df_good_scaled[\"state\"][:] # csak a jók érdekelnek minket\n",
        "X_NN_test=df_bad_scaled[prediktorok][:] #\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7 dimenzióra képez, a grafikus megjelenítés biztató\n",
        "_N1_=135\n",
        "_N2_=30\n",
        "_N3_=7\n",
        "_lr_=0.0001\n",
        "_batch_size_=32\n",
        "_drop1_=0.0\n",
        "_drop2_=0.0\n",
        "_epochs_=3500\n",
        "'''\n",
        "\n",
        "'''\n",
        "_N1_=135\n",
        "_N2_=30\n",
        "_N3_=2\n",
        "_lr_=0.0001\n",
        "_batch_size_=32\n",
        "_drop1_=0.0\n",
        "_drop2_=0.0\n",
        "_epochs_=3500\n",
        "'''\n",
        "_N1_=13\n",
        "_N2_=5\n",
        "_N3_=5\n",
        "_N4_=5\n",
        "_N5_=13\n",
        "\n",
        "_lr_=0.0001\n",
        "_batch_size_=32\n",
        "_drop1_=0.0\n",
        "_drop2_=0.0\n",
        "_epochs_=7000\n",
        "_comment_=\"3 réteg:  20] 13 3 13 [20 \"\n",
        "\n"
      ],
      "metadata": {
        "id": "XC5_bGE0iyi4"
      },
      "execution_count": 797,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"lr\": _lr_, \"batch_size\": _batch_size_,\"architecture\": \"AutoencoderNN\", \n",
        "          \"depth\": 2,\n",
        "          \"layer1\":_N1_,  \"layer2\":_N2_,\"layer3\":_N3_,\"layer4\":_N2_,\"layer5\":_N1_,\"layer_out\":20, \n",
        "          \"drop1\":_drop1_,\"drop2\":_drop2_,\n",
        "          \"epochs\":_epochs_,\n",
        "          \"comment\": _comment_\n",
        "\n",
        "          \n",
        "          \n",
        "          }\n",
        "\n",
        "wandb.init(project=\"pid_autoencoder\", entity=\"pid_status\",config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463,
          "referenced_widgets": [
            "93029c7ca9b0465fa69fc739ed5ca905",
            "047303e54b2845068fff2ac83aee34ef",
            "c32a22e0e8c6444b987989e31d6bc9a2",
            "da885c36df2e4253a72f7f2ff7a14c53",
            "b34ea2d50d9147568ea78231e69b447c",
            "03e0e1dfc792425bb5b361bd1b69ab36",
            "178478eafffe4c84a2ffc6ff6ddb191a",
            "bb7de7b4223a4de582b014c3179d7a91"
          ]
        },
        "id": "nOtKllcviuoj",
        "outputId": "630e308c-d2a7-4976-9fd6-6295cd35a58a"
      },
      "execution_count": 798,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:p12cjwfb) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93029c7ca9b0465fa69fc739ed5ca905"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/MAE</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/MAE</td><td>0.00676</td></tr><tr><td>epoch/epoch</td><td>999</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.00676</td></tr><tr><td>epoch/lr</td><td>0.0001</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">dashing-shape-47</strong>: <a href=\"https://wandb.ai/pid_status/pid_autoencoder/runs/p12cjwfb\" target=\"_blank\">https://wandb.ai/pid_status/pid_autoencoder/runs/p12cjwfb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221226_160421-p12cjwfb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:p12cjwfb). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221226_162922-2kxn5vjc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/pid_status/pid_autoencoder/runs/2kxn5vjc\" target=\"_blank\">graceful-feather-48</a></strong> to <a href=\"https://wandb.ai/pid_status/pid_autoencoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/pid_status/pid_autoencoder/runs/2kxn5vjc?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ffa1bf32970>"
            ]
          },
          "metadata": {},
          "execution_count": 798
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 799,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=20\n",
        "\n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "\n",
        "l1_out=Dense(_N1_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(input1) # kernel_initializer='lecun_normal'  # L1\n",
        "\n",
        "#l2_out=Dense(_N2_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(l1_out) #kernel_initializer='lecun_normal',  # L2\n",
        "\n",
        "l3_out=Dense(_N3_,activation=\"linear\",kernel_initializer='glorot_uniform',name=\"encoded\",kernel_regularizer=None)(l1_out) #kernel_initializer='lecun_normal',  # L3\n",
        "\n",
        "#l4_out=Dense(_N4_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(l3_out) #kernel_initializer='lecun_normal',  # L4\n",
        "\n",
        "l5_out=Dense(_N5_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(l3_out) #kernel_initializer='lecun_normal',  # L5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pred=Dense(input_size, activation=\"sigmoid\",)(l5_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=_lr_,) #\n",
        "\n",
        "model.compile(loss='MAE',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"MAE\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 800,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\"\n",
        "#model_file=\"model_PID__94_loss_0.116_vloss_0.115_acc_0.950_vacc_0.966.hdf5\"\n",
        "model_file=\"model_PID__4491_loss_0.115_vloss_0.679_acc_0.954_vacc_0.880.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 801,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 802,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False"
      ],
      "metadata": {
        "id": "UIxI3AS6Yw3S"
      },
      "execution_count": 803,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 804,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 805,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=len(X_NN)-1,shuffle=True,random_state=33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_NN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xru7s5BsOD3",
        "outputId": "e779a699-f82f-4c13-de41-47c913af10e0"
      },
      "execution_count": 806,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2078"
            ]
          },
          "metadata": {},
          "execution_count": 806
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "jJfOOTfGfDXi"
      },
      "execution_count": 807,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate_corrector(epoch,lr):\n",
        "    \n",
        "    if epoch > 4500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 4000:\n",
        "        lr = 0.0005\n",
        "        return lr\n",
        "    \n",
        "\n",
        "    if epoch > 3500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    \n",
        "    if epoch > 3000:\n",
        "        lr = 0.0005\n",
        "        return lr\n",
        "    \n",
        "\n",
        "    if epoch > 2500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "\n",
        "    if epoch > 2000:\n",
        "        lr = 0.0005\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 1500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 1000:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    return lr\n",
        "    "
      ],
      "metadata": {
        "id": "A-Kv8ORiEfub"
      },
      "execution_count": 808,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.keras import WandbMetricsLogger\n",
        "fname=\"./model_Encoder_\"\n",
        "callbacks = [\n",
        "        LearningRateScheduler(learning_rate_corrector,verbose=1),\n",
        "        WandbMetricsLogger(),       \n",
        "        #ModelCheckpoint(filepath=fname+\"_{epoch:04.0f}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{MAE:.3f}_vacc_{val_MAE:.3f}.hdf5\", \n",
        "        #                monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 809,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__learning__=True"
      ],
      "metadata": {
        "id": "O6ofy0moderd"
      },
      "execution_count": 810,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7Z3Z4q14D7eC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd5e1bb-c822-4f2d-d29b-fef82bc1bd39"
      },
      "execution_count": 811,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 13)                273       \n",
            "                                                                 \n",
            " encoded (Dense)             (None, 5)                 70        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 13)                78        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                280       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 701\n",
            "Trainable params: 701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 812,
      "metadata": {
        "id": "9Ol0mW6WRlkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0988cd8-2e91-4fca-bd65-b3ef5be406fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA streamkimeneten csak az utolsó 5000 sor látható.\u001b[0m\n",
            "Epoch 5751: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5751/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5752: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5752/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5753: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5753/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5754: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5754/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5755: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5755/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5756: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5756/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5757: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5757/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5758: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5758/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5759: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5759/7000\n",
            "65/65 [==============================] - 1s 8ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5760: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5760/7000\n",
            "65/65 [==============================] - 1s 8ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5761: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5761/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5762: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5762/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5763: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5763/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5764: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5764/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5765: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5765/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5766: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5766/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5767: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5767/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5768: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5768/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5769: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5769/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5770: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5770/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5771: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5771/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5772: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5772/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5773: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5773/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5774: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5774/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5775: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5775/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5776: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5776/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5777: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5777/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5778: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5778/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5779: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5779/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5780: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5780/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5781: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5781/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5782: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5782/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5783: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5783/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5784: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5784/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5785: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5785/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5786: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5786/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5787: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5787/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5788: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5788/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5789: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5789/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5790: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5790/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5791: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5791/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5792: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5792/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5793: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5793/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5794: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5794/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5795: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5795/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5796: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5796/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5797: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5797/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5798: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5798/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5799: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5799/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5800: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5800/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5801: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5801/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5802: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5802/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5803: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5803/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5804: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5804/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5805: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5805/7000\n",
            "65/65 [==============================] - 1s 10ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5806: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5806/7000\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5807: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5807/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5808: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5808/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5809: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5809/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5810: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5810/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5811: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5811/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5812: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5812/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5813: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5813/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5814: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5814/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5815: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5815/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5816: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5816/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5817: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5817/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5818: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5818/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5819: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5819/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5820: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5820/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5821: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5821/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5822: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5822/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5823: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5823/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5824: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5824/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5825: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5825/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5826: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5826/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5827: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5827/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5828: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5828/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5829: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5829/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5830: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5830/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5831: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5831/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5832: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5832/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5833: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5833/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5834: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5834/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5835: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5835/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5836: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5836/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5837: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5837/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5838: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5838/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5839: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5839/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5840: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5840/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5841: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5841/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5842: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5842/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5843: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5843/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5844: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5844/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5845: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5845/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5846: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5846/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5847: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5847/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5848: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5848/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5849: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5849/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5850: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5850/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5851: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5851/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5852: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5852/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5853: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5853/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5854: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5854/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5855: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5855/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5856: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5856/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5857: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5857/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5858: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5858/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5859: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5859/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5860: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5860/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5861: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5861/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5862: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5862/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5863: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5863/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5864: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5864/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5865: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5865/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5866: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5866/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5867: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5867/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5868: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5868/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5869: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5869/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5870: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5870/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5871: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5871/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5872: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5872/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5873: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5873/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5874: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5874/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5875: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5875/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5876: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5876/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5877: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5877/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5878: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5878/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5879: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5879/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5880: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5880/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5881: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5881/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5882: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5882/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5883: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5883/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5884: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5884/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5885: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5885/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5886: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5886/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5887: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5887/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5888: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5888/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5889: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5889/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5890: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5890/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5891: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5891/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5892: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5892/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5893: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5893/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5894: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5894/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5895: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5895/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5896: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5896/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5897: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5897/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5898: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5898/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5899: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5899/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5900: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5900/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5901: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5901/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5902: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5902/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5903: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5903/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5904: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5904/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5905: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5905/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5906: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5906/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5907: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5907/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5908: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5908/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5909: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5909/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5910: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5910/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5911: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5911/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5912: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5912/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5913: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5913/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5914: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5914/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5915: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5915/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5916: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5916/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5917: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5917/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5918: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5918/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5919: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5919/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5920: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5920/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5921: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5921/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5922: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5922/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5923: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5923/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5924: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5924/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5925: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5925/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5926: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5926/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5927: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5927/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5928: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5928/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5929: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5929/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5930: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5930/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5931: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5931/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5932: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5932/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5933: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5933/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5934: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5934/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5935: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5935/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5936: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5936/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5937: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5937/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5938: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5938/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5939: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5939/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5940: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5940/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5941: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5941/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5942: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5942/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5943: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5943/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5944: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5944/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5945: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5945/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5946: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5946/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5947: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5947/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5948: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5948/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5949: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5949/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5950: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5950/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5951: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5951/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5952: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5952/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5953: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5953/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5954: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5954/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5955: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5955/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5956: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5956/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5957: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5957/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5958: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5958/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5959: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5959/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5960: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5960/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5961: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5961/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5962: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5962/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5963: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5963/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5964: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5964/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5965: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5965/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5966: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5966/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5967: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5967/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5968: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5968/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5969: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5969/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5970: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5970/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5971: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5971/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5972: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5972/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5973: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5973/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5974: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5974/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5975: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5975/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5976: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5976/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5977: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5977/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5978: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5978/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5979: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5979/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5980: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5980/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5981: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5981/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5982: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5982/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5983: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5983/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5984: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5984/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5985: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5985/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5986: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5986/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5987: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5987/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5988: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5988/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5989: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5989/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5990: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5990/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5991: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5991/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5992: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5992/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5993: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5993/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5994: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5994/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5995: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5995/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5996: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5996/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5997: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5997/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5998: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5998/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5999: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5999/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6000: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6000/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6001: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6001/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6002: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6002/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6003: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6003/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6004: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6004/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6005: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6005/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6006: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6006/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6007: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6007/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6008: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6008/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6009: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6009/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6010: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6010/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6011: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6011/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6012: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6012/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6013: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6013/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6014: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6014/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6015: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6015/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6016: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6016/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6017: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6017/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6018: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6018/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6019: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6019/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6020: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6020/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6021: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6021/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6022: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6022/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6023: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6023/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6024: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6024/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6025: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6025/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6026: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6026/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6027: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6027/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6028: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6028/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6029: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6029/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6030: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6030/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6031: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6031/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6032: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6032/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6033: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6033/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6034: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6034/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6035: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6035/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6036: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6036/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6037: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6037/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6038: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6038/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6039: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6039/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6040: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6040/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6041: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6041/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6042: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6042/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6043: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6043/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6044: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6044/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6045: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6045/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6046: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6046/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6047: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6047/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6048: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6048/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6049: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6049/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6050: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6050/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6051: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6051/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6052: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6052/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6053: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6053/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6054: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6054/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6055: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6055/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6056: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6056/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6057: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6057/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6058: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6058/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6059: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6059/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6060: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6060/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6061: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6061/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6062: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6062/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6063: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6063/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6064: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6064/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6065: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6065/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6066: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6066/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6067: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6067/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6068: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6068/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6069: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6069/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6070: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6070/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6071: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6071/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6072: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6072/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6073: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6073/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6074: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6074/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6075: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6075/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6076: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6076/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6077: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6077/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6078: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6078/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6079: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6079/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6080: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6080/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6081: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6081/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6082: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6082/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6083: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6083/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6084: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6084/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6085: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6085/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6086: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6086/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6087: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6087/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6088: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6088/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6089: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6089/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6090: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6090/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6091: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6091/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6092: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6092/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6093: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6093/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6094: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6094/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6095: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6095/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6096: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6096/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6097: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6097/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6098: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6098/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6099: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6099/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6100: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6100/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6101: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6101/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6102: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6102/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6103: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6103/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6104: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6104/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6105: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6105/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6106: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6106/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6107: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6107/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6108: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6108/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6109: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6109/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6110: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6110/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6111: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6111/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6112: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6112/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6113: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6113/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6114: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6114/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6115: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6115/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6116: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6116/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6117: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6117/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6118: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6118/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6119: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6119/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6120: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6120/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6121: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6121/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6122: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6122/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6123: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6123/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6124: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6124/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6125: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6125/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6126: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6126/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6127: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6127/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6128: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6128/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6129: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6129/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6130: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6130/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6131: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6131/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6132: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6132/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6133: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6133/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6134: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6134/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6135: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6135/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6136: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6136/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6137: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6137/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6138: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6138/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6139: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6139/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6140: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6140/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6141: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6141/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6142: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6142/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6143: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6143/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6144: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6144/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6145: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6145/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6146: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6146/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6147: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6147/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6148: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6148/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6149: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6149/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6150: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6150/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6151: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6151/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6152: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6152/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6153: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6153/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6154: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6154/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6155: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6155/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6156: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6156/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6157: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6157/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6158: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6158/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6159: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6159/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6160: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6160/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6161: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6161/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6162: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6162/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6163: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6163/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6164: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6164/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6165: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6165/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6166: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6166/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6167: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6167/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6168: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6168/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6169: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6169/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6170: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6170/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6171: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6171/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6172: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6172/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6173: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6173/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6174: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6174/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6175: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6175/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6176: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6176/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6177: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6177/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6178: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6178/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6179: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6179/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6180: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6180/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6181: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6181/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6182: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6182/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6183: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6183/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6184: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6184/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6185: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6185/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6186: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6186/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6187: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6187/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6188: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6188/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6189: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6189/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6190: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6190/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6191: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6191/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6192: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6192/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6193: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6193/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6194: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6194/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6195: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6195/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6196: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6196/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6197: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6197/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6198: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6198/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6199: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6199/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6200: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6200/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6201: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6201/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6202: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6202/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6203: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6203/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6204: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6204/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6205: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6205/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6206: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6206/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6207: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6207/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6208: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6208/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6209: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6209/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6210: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6210/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6211: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6211/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6212: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6212/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6213: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6213/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6214: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6214/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6215: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6215/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6216: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6216/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6217: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6217/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6218: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6218/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6219: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6219/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6220: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6220/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6221: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6221/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6222: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6222/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6223: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6223/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6224: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6224/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6225: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6225/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6226: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6226/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6227: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6227/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6228: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6228/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6229: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6229/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6230: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6230/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6231: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6231/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6232: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6232/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6233: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6233/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6234: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6234/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6235: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6235/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6236: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6236/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6237: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6237/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6238: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6238/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6239: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6239/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6240: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6240/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6241: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6241/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6242: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6242/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6243: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6243/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6244: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6244/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6245: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6245/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6246: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6246/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6247: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6247/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6248: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6248/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6249: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6249/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6250: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6250/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6251: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6251/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6252: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6252/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6253: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6253/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6254: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6254/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6255: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6255/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6256: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6256/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6257: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6257/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6258: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6258/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6259: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6259/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6260: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6260/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6261: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6261/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6262: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6262/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6263: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6263/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6264: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6264/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6265: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6265/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6266: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6266/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6267: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6267/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6268: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6268/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6269: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6269/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6270: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6270/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6271: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6271/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6272: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6272/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6273: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6273/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6274: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6274/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6275: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6275/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6276: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6276/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6277: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6277/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6278: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6278/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6279: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6279/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6280: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6280/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6281: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6281/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6282: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6282/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6283: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6283/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6284: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6284/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6285: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6285/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6286: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6286/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6287: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6287/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6288: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6288/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6289: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6289/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6290: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6290/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6291: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6291/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6292: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6292/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6293: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6293/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6294: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6294/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6295: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6295/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6296: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6296/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6297: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6297/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6298: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6298/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6299: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6299/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6300: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6300/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6301: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6301/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6302: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6302/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6303: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6303/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6304: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6304/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6305: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6305/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6306: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6306/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6307: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6307/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6308: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6308/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6309: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6309/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6310: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6310/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6311: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6311/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6312: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6312/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6313: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6313/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6314: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6314/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6315: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6315/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6316: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6316/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6317: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6317/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6318: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6318/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6319: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6319/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6320: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6320/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6321: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6321/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6322: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6322/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6323: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6323/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6324: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6324/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6325: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6325/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6326: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6326/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6327: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6327/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6328: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6328/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6329: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6329/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6330: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6330/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6331: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6331/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6332: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6332/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6333: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6333/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6334: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6334/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6335: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6335/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6336: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6336/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6337: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6337/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6338: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6338/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6339: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6339/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6340: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6340/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6341: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6341/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6342: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6342/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6343: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6343/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6344: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6344/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6345: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6345/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6346: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6346/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6347: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6347/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6348: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6348/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6349: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6349/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6350: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6350/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6351: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6351/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6352: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6352/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6353: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6353/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6354: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6354/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6355: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6355/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6356: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6356/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6357: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6357/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6358: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6358/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6359: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6359/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6360: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6360/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6361: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6361/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6362: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6362/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6363: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6363/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6364: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6364/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6365: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6365/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6366: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6366/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6367: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6367/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6368: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6368/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6369: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6369/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6370: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6370/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6371: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6371/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6372: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6372/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6373: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6373/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6374: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6374/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6375: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6375/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6376: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6376/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6377: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6377/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6378: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6378/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6379: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6379/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6380: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6380/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6381: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6381/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6382: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6382/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6383: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6383/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6384: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6384/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6385: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6385/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6386: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6386/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6387: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6387/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6388: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6388/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6389: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6389/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6390: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6390/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6391: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6391/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6392: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6392/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6393: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6393/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6394: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6394/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6395: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6395/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6396: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6396/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6397: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6397/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6398: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6398/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6399: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6399/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6400: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6400/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6401: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6401/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6402: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6402/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6403: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6403/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6404: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6404/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6405: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6405/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6406: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6406/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6407: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6407/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6408: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6408/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6409: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6409/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6410: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6410/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6411: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6411/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6412: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6412/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6413: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6413/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6414: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6414/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6415: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6415/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6416: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6416/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6417: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6417/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6418: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6418/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6419: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6419/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6420: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6420/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6421: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6421/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6422: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6422/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6423: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6423/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6424: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6424/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6425: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6425/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6426: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6426/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6427: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6427/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6428: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6428/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6429: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6429/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6430: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6430/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6431: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6431/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6432: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6432/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6433: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6433/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6434: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6434/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6435: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6435/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6436: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6436/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6437: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6437/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6438: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6438/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6439: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6439/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6440: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6440/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6441: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6441/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6442: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6442/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6443: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6443/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6444: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6444/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6445: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6445/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6446: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6446/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6447: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6447/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6448: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6448/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6449: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6449/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6450: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6450/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6451: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6451/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6452: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6452/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6453: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6453/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6454: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6454/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6455: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6455/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6456: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6456/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6457: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6457/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6458: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6458/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6459: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6459/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6460: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6460/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6461: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6461/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6462: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6462/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6463: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6463/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6464: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6464/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6465: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6465/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6466: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6466/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6467: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6467/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6468: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6468/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6469: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6469/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6470: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6470/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6471: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6471/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6472: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6472/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6473: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6473/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6474: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6474/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6475: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6475/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6476: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6476/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6477: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6477/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6478: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6478/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6479: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6479/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6480: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6480/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6481: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6481/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6482: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6482/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6483: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6483/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6484: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6484/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6485: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6485/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6486: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6486/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6487: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6487/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6488: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6488/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6489: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6489/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6490: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6490/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6491: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6491/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6492: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6492/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6493: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6493/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6494: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6494/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6495: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6495/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6496: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6496/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6497: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6497/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6498: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6498/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6499: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6499/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6500: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6500/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6501: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6501/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6502: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6502/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6503: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6503/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6504: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6504/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6505: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6505/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6506: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6506/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6507: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6507/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6508: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6508/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6509: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6509/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6510: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6510/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6511: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6511/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6512: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6512/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6513: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6513/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6514: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6514/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6515: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6515/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6516: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6516/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6517: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6517/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6518: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6518/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6519: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6519/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6520: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6520/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6521: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6521/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6522: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6522/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6523: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6523/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6524: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6524/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6525: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6525/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6526: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6526/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6527: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6527/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6528: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6528/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6529: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6529/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6530: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6530/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6531: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6531/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6532: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6532/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6533: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6533/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6534: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6534/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6535: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6535/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6536: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6536/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6537: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6537/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6538: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6538/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6539: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6539/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6540: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6540/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6541: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6541/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6542: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6542/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6543: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6543/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6544: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6544/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6545: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6545/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6546: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6546/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6547: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6547/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6548: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6548/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6549: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6549/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6550: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6550/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6551: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6551/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6552: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6552/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6553: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6553/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6554: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6554/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6555: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6555/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6556: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6556/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6557: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6557/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6558: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6558/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6559: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6559/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6560: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6560/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6561: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6561/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6562: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6562/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6563: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6563/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6564: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6564/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6565: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6565/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6566: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6566/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6567: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6567/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6568: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6568/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6569: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6569/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6570: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6570/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6571: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6571/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6572: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6572/7000\n",
            "65/65 [==============================] - 0s 6ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6573: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6573/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6574: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6574/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6575: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6575/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6576: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6576/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6577: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6577/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6578: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6578/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6579: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6579/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6580: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6580/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6581: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6581/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6582: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6582/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6583: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6583/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6584: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6584/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6585: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6585/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6586: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6586/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6587: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6587/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6588: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6588/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6589: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6589/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6590: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6590/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6591: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6591/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6592: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6592/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6593: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6593/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6594: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6594/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6595: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6595/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6596: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6596/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6597: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6597/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6598: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6598/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6599: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6599/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6600: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6600/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6601: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6601/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6602: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6602/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6603: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6603/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6604: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6604/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6605: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6605/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6606: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6606/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6607: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6607/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6608: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6608/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6609: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6609/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6610: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6610/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6611: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6611/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6612: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6612/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6613: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6613/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6614: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6614/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6615: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6615/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6616: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6616/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6617: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6617/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6618: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6618/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6619: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6619/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6620: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6620/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6621: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6621/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6622: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6622/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6623: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6623/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6624: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6624/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6625: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6625/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6626: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6626/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6627: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6627/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6628: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6628/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6629: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6629/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6630: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6630/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6631: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6631/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6632: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6632/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6633: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6633/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6634: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6634/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6635: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6635/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6636: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6636/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6637: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6637/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6638: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6638/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6639: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6639/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6640: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6640/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6641: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6641/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6642: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6642/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6643: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6643/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6644: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6644/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6645: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6645/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6646: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6646/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6647: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6647/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6648: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6648/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6649: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6649/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6650: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6650/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6651: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6651/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6652: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6652/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6653: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6653/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6654: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6654/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6655: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6655/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6656: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6656/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6657: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6657/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6658: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6658/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6659: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6659/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6660: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6660/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6661: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6661/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6662: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6662/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6663: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6663/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6664: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6664/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6665: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6665/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6666: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6666/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6667: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6667/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6668: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6668/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6669: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6669/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6670: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6670/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6671: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6671/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6672: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6672/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6673: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6673/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6674: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6674/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6675: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6675/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6676: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6676/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6677: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6677/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6678: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6678/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6679: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6679/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6680: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6680/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6681: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6681/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6682: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6682/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6683: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6683/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6684: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6684/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6685: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6685/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6686: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6686/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6687: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6687/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6688: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6688/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6689: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6689/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6690: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6690/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0065 - MAE: 0.0065 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6691: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6691/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6692: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6692/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6693: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6693/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6694: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6694/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6695: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6695/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6696: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6696/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6697: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6697/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6698: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6698/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6699: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6699/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6700: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6700/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6701: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6701/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6702: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6702/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6703: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6703/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6704: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6704/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6705: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6705/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6706: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6706/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6707: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6707/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6708: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6708/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6709: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6709/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6710: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6710/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6711: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6711/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6712: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6712/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6713: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6713/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6714: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6714/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6715: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6715/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6716: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6716/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6717: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6717/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6718: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6718/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6719: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6719/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6720: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6720/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6721: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6721/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6722: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6722/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6723: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6723/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6724: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6724/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6725: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6725/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6726: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6726/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6727: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6727/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6728: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6728/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6729: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6729/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6730: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6730/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6731: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6731/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6732: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6732/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6733: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6733/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6734: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6734/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6735: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6735/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6736: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6736/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6737: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6737/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6738: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6738/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6739: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6739/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6740: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6740/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6741: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6741/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6742: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6742/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6743: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6743/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6744: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6744/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6745: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6745/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6746: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6746/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6747: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6747/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6748: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6748/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6749: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6749/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6750: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6750/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6751: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6751/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6752: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6752/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6753: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6753/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6754: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6754/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6755: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6755/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6756: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6756/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6757: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6757/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6758: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6758/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6759: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6759/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6760: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6760/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6761: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6761/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6762: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6762/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6763: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6763/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6764: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6764/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6765: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6765/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6766: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6766/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6767: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6767/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6768: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6768/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6769: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6769/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6770: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6770/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6771: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6771/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6772: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6772/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6773: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6773/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6774: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6774/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6775: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6775/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6776: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6776/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6777: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6777/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6778: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6778/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6779: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6779/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6780: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6780/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6781: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6781/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6782: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6782/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6783: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6783/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6784: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6784/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6785: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6785/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6786: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6786/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6787: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6787/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6788: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6788/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6789: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6789/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6790: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6790/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6791: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6791/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6792: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6792/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6793: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6793/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6794: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6794/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6795: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6795/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6796: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6796/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6797: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6797/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6798: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6798/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6799: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6799/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6800: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6800/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6801: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6801/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6802: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6802/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6803: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6803/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6804: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6804/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6805: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6805/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6806: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6806/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6807: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6807/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6808: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6808/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6809: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6809/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6810: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6810/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6811: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6811/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6812: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6812/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6813: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6813/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6814: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6814/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6815: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6815/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6816: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6816/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6817: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6817/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6818: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6818/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6819: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6819/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6820: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6820/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6821: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6821/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6822: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6822/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6823: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6823/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6824: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6824/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6825: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6825/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6826: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6826/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6827: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6827/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6828: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6828/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6829: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6829/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6830: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6830/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6831: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6831/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6832: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6832/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6833: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6833/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6834: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6834/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6835: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6835/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6836: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6836/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6837: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6837/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6838: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6838/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6839: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6839/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6840: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6840/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6841: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6841/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6842: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6842/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6843: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6843/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6844: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6844/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6845: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6845/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6846: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6846/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6847: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6847/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6848: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6848/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6849: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6849/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6850: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6850/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6851: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6851/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6852: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6852/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6853: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6853/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6854: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6854/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6855: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6855/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6856: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6856/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6857: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6857/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6858: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6858/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6859: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6859/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6860: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6860/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6861: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6861/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6862: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6862/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6863: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6863/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6864: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6864/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6865: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6865/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6866: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6866/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6867: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6867/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6868: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6868/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6869: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6869/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6870: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6870/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6871: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6871/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6872: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6872/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6873: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6873/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6874: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6874/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6875: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6875/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6876: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6876/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6877: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6877/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6878: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6878/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6879: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6879/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6880: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6880/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6881: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6881/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6882: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6882/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6883: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6883/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6884: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6884/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6885: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6885/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6886: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6886/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6887: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6887/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6888: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6888/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6889: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6889/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6890: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6890/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6891: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6891/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6892: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6892/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6893: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6893/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6894: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6894/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6895: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6895/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6896: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6896/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6897: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6897/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6898: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6898/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6899: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6899/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6900: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6900/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6901: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6901/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6902: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6902/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6903: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6903/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6904: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6904/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6905: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6905/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6906: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6906/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6907: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6907/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6908: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6908/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6909: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6909/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6910: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6910/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6911: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6911/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6912: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6912/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6913: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6913/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6914: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6914/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6915: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6915/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6916: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6916/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6917: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6917/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6918: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6918/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6919: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6919/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6920: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6920/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6921: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6921/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6922: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6922/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6923: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6923/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6924: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6924/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6925: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6925/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6926: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6926/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6927: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6927/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6928: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6928/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6929: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6929/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6930: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6930/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6931: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6931/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6932: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6932/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6933: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6933/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6934: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6934/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6935: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6935/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6936: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6936/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6937: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6937/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6938: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6938/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6939: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6939/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6940: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6940/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6941: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6941/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6942: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6942/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6943: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6943/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6944: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6944/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6945: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6945/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6946: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6946/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6947: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6947/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6948: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6948/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6949: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6949/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6950: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6950/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6951: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6951/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6952: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6952/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6953: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6953/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6954: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6954/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6955: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6955/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6956: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6956/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6957: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6957/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6958: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6958/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6959: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6959/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6960: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6960/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6961: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6961/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6962: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6962/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6963: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6963/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6964: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6964/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6965: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6965/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6966: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6966/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6967: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6967/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6968: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6968/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6969: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6969/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6970: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6970/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6971: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6971/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6972: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6972/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6973: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6973/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6974: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6974/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6975: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6975/7000\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6976: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6976/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6977: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6977/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6978: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6978/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6979: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6979/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6980: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6980/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6981: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6981/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6982: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6982/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6983: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6983/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6984: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6984/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6985: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6985/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6986: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6986/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6987: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6987/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6988: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6988/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6989: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6989/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6990: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6990/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6991: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6991/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6992: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6992/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6993: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6993/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6994: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6994/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6995: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6995/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6996: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6996/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6997: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6997/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6998: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6998/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6999: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 6999/7000\n",
            "65/65 [==============================] - 0s 2ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 7000: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 7000/7000\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0064 - MAE: 0.0064 - lr: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "if __learning__: \n",
        "    history = model.fit(X_train, X_train, epochs=_epochs_, batch_size=_batch_size_, verbose=1,callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"autoencoder_model_1_encoder.hdf5\")"
      ],
      "metadata": {
        "id": "EkZ9T-NH513_"
      },
      "execution_count": 813,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False\n",
        "model_file=\"model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5\"\n",
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "EGg1PjCJDTKF"
      },
      "execution_count": 814,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "JgzklVywoNmk"
      },
      "execution_count": 815,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 816,
      "metadata": {
        "id": "pwcWQ94IpDFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f654324-d165-437c-ced3-27604b7d365a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 125ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": 817,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": 818,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JlHV6_j9wUiE"
      },
      "execution_count": 818,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i1=model.get_layer(\"dense\")"
      ],
      "metadata": {
        "id": "rLxI7wmUDgID"
      },
      "execution_count": 819,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o1=model.get_layer(\"encoded\")"
      ],
      "metadata": {
        "id": "zeveHSkCDoma"
      },
      "execution_count": 820,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "to3v7cxmD60p"
      },
      "execution_count": 821,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states_fun = K.function([i1.input],[o1.output])\n",
        "     "
      ],
      "metadata": {
        "id": "4798J_V2D9QW"
      },
      "execution_count": 822,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(X):\n",
        "    \"\"\"Return the hidden state associated with an input at the given timestep.\n",
        "    \"\"\"\n",
        "    \n",
        "    hidden_states = hidden_states_fun(X.to_numpy())[0]\n",
        "    \n",
        "    return hidden_states\n",
        "     "
      ],
      "metadata": {
        "id": "4e-T3C50EG_a"
      },
      "execution_count": 823,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_embedding(X_train)"
      ],
      "metadata": {
        "id": "pSdDesJpqcEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c47bf29-5662-471f-9bf3-2f3d95795823"
      },
      "execution_count": 824,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.08869462, -0.22081181,  0.28651667,  1.0462981 , -0.36561552],\n",
              "       [ 0.11953825, -0.22882041,  0.28259578,  0.9413724 , -0.36279932],\n",
              "       [ 0.11923902, -0.22848496,  0.28246778,  0.9418397 , -0.3625873 ],\n",
              "       ...,\n",
              "       [ 0.1193065 , -0.22849736,  0.282449  ,  0.9415916 , -0.36256552],\n",
              "       [ 0.11929542, -0.2286523 ,  0.2825519 ,  0.9419663 , -0.36271277],\n",
              "       [ 0.11196038, -0.21636578,  0.27606192,  0.9439348 , -0.3525803 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 824
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hs=hidden_states_fun(X_train.to_numpy())[0]"
      ],
      "metadata": {
        "id": "WHyjbcplufFm"
      },
      "execution_count": 825,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hs)"
      ],
      "metadata": {
        "id": "7sIYdxIqrmdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68cdd79-503b-45fc-91fa-af3b4f9db857"
      },
      "execution_count": 826,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2077"
            ]
          },
          "metadata": {},
          "execution_count": 826
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hs[3].tolist()"
      ],
      "metadata": {
        "id": "flhNv2Thq5jP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ca58ce-5ca4-42e2-cd3e-787e7ee17730"
      },
      "execution_count": 827,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11883153021335602,\n",
              " -0.22766530513763428,\n",
              " 0.2820546627044678,\n",
              " 0.9416952729225159,\n",
              " -0.3619537949562073]"
            ]
          },
          "metadata": {},
          "execution_count": 827
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.iloc[1:2,:]"
      ],
      "metadata": {
        "id": "OkaLjsUxEi6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "37e7ac76-89a4-4195-e3ac-3358c5db63b4"
      },
      "execution_count": 828,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "957  0.641246  0.641665  0.575011  0.556494  0.556494  0.556494  0.556494   \n",
              "\n",
              "            7         8         9        10        11        12        13  \\\n",
              "957  0.556494  0.555976  0.556105  0.556235  0.556365  0.556494  0.555976   \n",
              "\n",
              "           14        15        16        17        18        19  \n",
              "957  0.556494  0.555976  0.555976  0.555976  0.556494  0.556494  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1acd5b29-c3bb-4c1a-bd63-415b16d6daa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>957</th>\n",
              "      <td>0.641246</td>\n",
              "      <td>0.641665</td>\n",
              "      <td>0.575011</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.555976</td>\n",
              "      <td>0.556105</td>\n",
              "      <td>0.556235</td>\n",
              "      <td>0.556365</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.555976</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.555976</td>\n",
              "      <td>0.555976</td>\n",
              "      <td>0.555976</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.556494</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1acd5b29-c3bb-4c1a-bd63-415b16d6daa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1acd5b29-c3bb-4c1a-bd63-415b16d6daa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1acd5b29-c3bb-4c1a-bd63-415b16d6daa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 828
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list=get_embedding(X_train)"
      ],
      "metadata": {
        "id": "Tnh4GKWNERZF"
      },
      "execution_count": 829,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding_list)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e5415f-e6e9-46bb-c2f3-495e0c4eb499"
      },
      "execution_count": 830,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2077"
            ]
          },
          "metadata": {},
          "execution_count": 830
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 830,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z69kCq3T-pMo"
      },
      "execution_count": 830,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 830,
      "metadata": {
        "id": "nZ0rmkNsBGnl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNIc1l6vF6Y4"
      },
      "execution_count": 830,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_changer(arr):\n",
        "    o=[\"r\" if i>0.5 else \"g\" for i in arr]\n",
        "    return o"
      ],
      "metadata": {
        "id": "YFJoZO8TG1ED"
      },
      "execution_count": 831,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtghT29KKGA4"
      },
      "execution_count": 831,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list[:][0]"
      ],
      "metadata": {
        "id": "H96EO2p_LE3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08eb7585-effe-4667-f26f-7c01e3235ede"
      },
      "execution_count": 832,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.08869462, -0.22081181,  0.28651667,  1.0462981 , -0.36561552],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 832
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list[:][-100]"
      ],
      "metadata": {
        "id": "e82a7m3sJxZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5450c0a5-3fe9-41ef-be5f-ba15965d2750"
      },
      "execution_count": 833,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.11840062, -0.22671628,  0.28225678,  0.9421247 , -0.36230364],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 833
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embedding(list_in, predicted, index2=1):\n",
        "    xkoordinata=[i[0] for i in list_in]\n",
        "    ykoordinata=[i[index2] for i in list_in]\n",
        "    \n",
        "    plot.figure(figsize=(12,6))\n",
        "    col_ch=color_changer(predicted)\n",
        "    plot.scatter(xkoordinata,ykoordinata,c=col_ch,marker=\".\",alpha=0.3)\n",
        "    plot.ylabel('értékek')\n",
        "    plot.xlabel('index')\n",
        "    plot\n",
        "    plot.show()"
      ],
      "metadata": {
        "id": "YMHy-wbZGeqq"
      },
      "execution_count": 834,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embedding_3d(list_in, predicted):\n",
        "    import plotly.express as px\n",
        "\n",
        "    xkoordinata=[i[0] for i in list_in]\n",
        "    ykoordinata=[i[1] for i in list_in]\n",
        "    zkoordinata=[i[2] for i in list_in]\n",
        "    zipped=list(zip(xkoordinata,ykoordinata,zkoordinata))\n",
        "    df_tmp=pd.DataFrame(zipped,columns=[\"x\",\"y\",\"z\"])\n",
        "    df_tmp[\"pred\"]=predicted.tolist()\n",
        "    fig = px.scatter_3d(df_tmp, x='x', y='y', z='z', color='pred', width=1200, height=1000,opacity=0.8)\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HuL1OutGHHEc"
      },
      "execution_count": 835,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.tolist()"
      ],
      "metadata": {
        "id": "DlADklPHoKbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_embedding_3d(embedding_list[:],y_train[:])"
      ],
      "metadata": {
        "id": "4aXpzheKE7bM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d722ae75-4ee4-455e-ed94-d72c70b87e3a"
      },
      "execution_count": 837,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"1258ff1e-a3ce-4d25-a7ae-fb9774540891\" class=\"plotly-graph-div\" style=\"height:1000px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1258ff1e-a3ce-4d25-a7ae-fb9774540891\")) {                    Plotly.newPlot(                        \"1258ff1e-a3ce-4d25-a7ae-fb9774540891\",                        [{\"hovertemplate\":\"x=%{x}<br>y=%{y}<br>z=%{z}<br>pred=%{marker.color}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"coloraxis\":\"coloraxis\",\"opacity\":0.8,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[0.08869461715221405,0.11953824758529663,0.11923901736736298,0.11883153021335602,0.11321164667606354,0.11931180208921432,0.12218652665615082,0.11939097940921783,0.11934047937393188,0.11930649727582932,0.11756229400634766,0.11910559237003326,0.1193387508392334,0.11908284574747086,0.11929310858249664,0.11594094336032867,0.11923468858003616,0.1192970871925354,0.11935118585824966,0.1026594489812851,0.13201913237571716,0.11929556727409363,0.11913487315177917,0.11930173635482788,0.12386620044708252,0.11928488314151764,0.11925894021987915,0.11916483938694,0.11920814216136932,0.11662867665290833,0.10435640811920166,0.11924642324447632,0.10562984645366669,0.11927970498800278,0.11915886402130127,0.1229642704129219,0.120640330016613,0.13566400110721588,0.11993342638015747,0.11602112650871277,0.14921516180038452,0.11929541826248169,0.11936089396476746,0.11956080794334412,0.09807582199573517,0.11831515282392502,0.11883627623319626,0.0674842968583107,0.13109248876571655,0.1226307824254036,0.1193186342716217,0.10246691107749939,0.11989352107048035,0.11928991973400116,0.10379739105701447,0.1199038028717041,0.15586909651756287,0.11930413544178009,0.12269097566604614,0.11929310858249664,0.12326555699110031,0.11807677149772644,0.11930210888385773,0.11880777776241302,0.11946515738964081,0.10549554973840714,0.1226108968257904,0.14656341075897217,0.13464978337287903,0.11930649727582932,0.11927256733179092,0.13220295310020447,0.12013483047485352,0.11931537836790085,0.10783651471138,0.11936362832784653,0.11929541826248169,0.11943655461072922,0.1638677716255188,0.11929541826248169,0.11931662261486053,0.11929541826248169,0.11875173449516296,0.12311635166406631,0.03420855104923248,0.12036076933145523,0.11938080191612244,0.1168828010559082,0.11818964779376984,0.12269915640354156,0.11929541826248169,0.12562523782253265,0.11880539357662201,0.1071595624089241,0.1192970871925354,0.11926543712615967,0.11907897889614105,0.11782404780387878,0.11801670491695404,0.11921468377113342,0.11929541826248169,0.11924424767494202,0.08631867915391922,0.11916384100914001,0.11704982072114944,0.12004311382770538,0.1191878393292427,0.11507824063301086,0.119778573513031,0.11929004639387131,0.11992610991001129,0.1193113625049591,0.1191973090171814,0.10790014266967773,0.08849713951349258,0.06889522075653076,0.12341687083244324,0.12128715962171555,0.12243017554283142,0.1192145049571991,0.11925894021987915,0.1194152981042862,0.10520075261592865,0.11948040127754211,0.17699642479419708,0.11931537836790085,0.12029719352722168,0.1192970871925354,0.119174063205719,0.11931213736534119,0.12017956376075745,0.12132342159748077,0.11924974620342255,0.11920222640037537,0.11931491643190384,0.1443975567817688,0.11838575452566147,0.13199342787265778,0.11501777917146683,0.11250130832195282,0.1193595826625824,0.11930649727582932,0.119778573513031,0.14064233005046844,0.11595945805311203,0.12812724709510803,0.11778455972671509,0.12368422746658325,0.11731292307376862,0.12276890873908997,0.11934136599302292,0.1192970871925354,0.1000179797410965,0.11929485201835632,0.11929541826248169,0.10797002911567688,0.1192970871925354,0.11930649727582932,0.13618649542331696,0.11909957230091095,0.23684009909629822,0.11910770833492279,0.11350216716527939,0.11925578117370605,0.14011940360069275,0.11943958699703217,0.10282573103904724,0.11931537836790085,0.11951911449432373,0.1163368821144104,0.10942386090755463,0.11930153518915176,0.12948155403137207,0.1192743182182312,0.13727571070194244,0.1184120625257492,0.12222696095705032,0.11915692687034607,0.12112200260162354,0.11936435848474503,0.1103857010602951,0.11931541562080383,0.11636990308761597,0.11926396191120148,0.11941277980804443,0.13722334802150726,0.14032301306724548,0.11934882402420044,0.1167043149471283,0.11942873895168304,0.11931537836790085,0.11881890892982483,0.11750026047229767,0.11913737654685974,0.12041322886943817,0.11920029670000076,0.11922015994787216,0.11927003413438797,0.11931537836790085,0.11929485201835632,0.11929287016391754,0.11928960680961609,0.11646632105112076,0.11931771039962769,0.12471406161785126,0.11929753422737122,0.08965401351451874,0.12732647359371185,0.119246706366539,0.11930649727582932,0.11926944553852081,0.1255260407924652,0.11243022978305817,0.11951391398906708,0.11930210888385773,0.1192893385887146,0.10041224211454391,0.10236676037311554,0.11930649727582932,0.13020002841949463,0.12295135855674744,0.1329076588153839,0.11929310858249664,0.11915931850671768,0.11927902698516846,0.11673115938901901,0.11314906924962997,0.11945575475692749,0.1193474531173706,0.1190664991736412,0.11924522370100021,0.12865036725997925,0.119285948574543,0.11917558312416077,0.11061146855354309,0.1217302531003952,0.11957942694425583,0.11924368143081665,0.11925894021987915,0.1192970871925354,0.11929541826248169,0.12076378613710403,0.11938071250915527,0.11926610767841339,0.11930210888385773,0.1192893385887146,0.11521971225738525,0.11917823553085327,0.11923594772815704,0.12158219516277313,0.11879712343215942,0.11921216547489166,0.12003111839294434,0.11929541826248169,0.11915495991706848,0.0846078097820282,0.11859488487243652,0.11928488314151764,0.1371103823184967,0.11906213313341141,0.11926838755607605,0.11698439717292786,0.08039692789316177,0.11925894021987915,0.09751839190721512,0.11919055879116058,0.11924929916858673,0.11942843347787857,0.12384878098964691,0.11880937218666077,0.11920852959156036,0.11918023228645325,0.11920487880706787,0.11960802227258682,0.11935822665691376,0.11936075240373611,0.1278393566608429,0.11931436508893967,0.0851505845785141,0.11925812065601349,0.11910244822502136,0.11924939602613449,0.13968870043754578,0.11929133534431458,0.11647270619869232,0.11464238166809082,0.1422593593597412,0.11930210888385773,0.11929541826248169,0.12319116294384003,0.14818798005580902,0.1357773393392563,0.11921577155590057,0.09199114143848419,0.11732155084609985,0.1192970871925354,0.12954850494861603,0.1192893385887146,0.11916410177946091,0.12490145862102509,0.12035984545946121,0.12359391897916794,0.11926748603582382,0.1193840503692627,0.11925894021987915,0.11927003413438797,0.11828812956809998,0.12055598199367523,0.1105910986661911,0.1828441470861435,0.1200089082121849,0.12664684653282166,0.11916507035493851,0.12111327052116394,0.11931537836790085,0.11930649727582932,-0.02037888765335083,0.11928488314151764,0.11906273663043976,0.11616171896457672,0.11824449896812439,0.1614353060722351,0.11935628950595856,0.24532067775726318,0.11924698948860168,0.1142498105764389,0.1195138543844223,0.11930981278419495,0.11945740133523941,0.11934466660022736,0.11934953182935715,0.1173800379037857,0.134933739900589,0.11917808651924133,0.1235552728176117,0.11924424767494202,0.2758994698524475,0.11924086511135101,0.11914605647325516,0.11922447383403778,0.11380733549594879,0.11926697194576263,0.11929310858249664,0.11930981278419495,0.1192970871925354,0.11920112371444702,0.11943350732326508,0.0950457975268364,0.08791492879390717,0.11913046985864639,0.11864803731441498,0.11974243819713593,0.11929787695407867,0.11955168843269348,0.11941887438297272,0.11926071345806122,0.11940723657608032,0.1192893385887146,0.11917194724082947,0.11937491595745087,0.11922445893287659,0.11938343942165375,0.11921827495098114,0.12007217109203339,0.11837698519229889,0.11929541826248169,0.122873455286026,0.1194387897849083,0.1192970871925354,0.13934247195720673,0.11919990926980972,0.11921640485525131,0.11934866011142731,0.12372341752052307,0.11877216398715973,0.11594309657812119,0.12277034670114517,0.119778573513031,0.11929803341627121,0.12399085611104965,0.11930210888385773,0.1191256195306778,0.12347544729709625,0.12206973880529404,0.11934973299503326,0.11925021559000015,0.12405125051736832,0.11967966705560684,0.09057493507862091,0.12489301711320877,0.11922430247068405,0.11895830929279327,0.12266819924116135,0.11734143644571304,0.11930981278419495,0.1192970871925354,0.1192970871925354,0.11924106627702713,0.1192765086889267,0.12032081186771393,0.11947771906852722,0.11930210888385773,0.12731970846652985,0.11920182406902313,0.029746249318122864,0.12011385709047318,0.11931537836790085,0.11931537836790085,0.11988623440265656,0.11750863492488861,0.11251053214073181,0.12086716294288635,0.1192970871925354,0.06438593566417694,0.11752471327781677,0.1192970871925354,0.11925910413265228,0.13798922300338745,0.12026268243789673,0.1181778609752655,0.12205930054187775,0.12077775597572327,0.11927003413438797,0.11381293088197708,0.11934268474578857,0.11935599148273468,0.10909725725650787,0.12062683701515198,0.1278422325849533,0.11922422051429749,0.1127438098192215,0.21589908003807068,0.13133497536182404,0.12270435690879822,0.11937664449214935,0.1193774938583374,0.11925868690013885,0.11526434123516083,0.11724996566772461,0.12299977242946625,0.12374578416347504,0.11929668486118317,0.1192646399140358,0.11919350177049637,0.11621931195259094,0.12131902575492859,0.11933404207229614,0.10868065059185028,0.11925491690635681,0.11929485201835632,0.12318867444992065,0.12613904476165771,0.11711227893829346,0.11218585073947906,0.11929310858249664,0.11929325014352798,0.11791566014289856,0.11918272823095322,0.11929541826248169,0.11947926133871078,0.08881403505802155,0.11921656876802444,0.11919739842414856,0.12552037835121155,0.09882852435112,0.11902805417776108,0.12234905362129211,0.11918194591999054,0.11824237555265427,0.16745293140411377,0.11669360101222992,0.10476788878440857,0.11929310858249664,0.05291786044836044,0.11928397417068481,0.11920084059238434,0.12123052775859833,0.1192474514245987,0.11925894021987915,0.1642231047153473,0.119162917137146,0.1203383058309555,0.10594608634710312,0.11490204930305481,0.11930649727582932,0.11739245057106018,0.11931537836790085,0.1192970871925354,0.07284465432167053,0.11928440630435944,0.14835511147975922,0.11929310858249664,0.179067924618721,0.06590235233306885,0.11928679049015045,0.12274811416864395,0.11933904886245728,0.12055501341819763,0.10286805033683777,0.11930981278419495,0.1192970871925354,0.11930210888385773,0.11930608749389648,0.1138724684715271,0.12144581228494644,0.11927397549152374,0.11922244727611542,0.10397373139858246,0.11338648200035095,0.11988008767366409,0.11934138089418411,0.11917764693498611,0.11763668060302734,0.12082873284816742,0.11929485201835632,0.11924602091312408,0.11919409036636353,0.11925894021987915,0.11856569349765778,0.08725447952747345,0.11925894021987915,0.12010551989078522,0.09948444366455078,0.11930649727582932,0.11920483410358429,0.1192701905965805,0.08648543804883957,0.005055472254753113,0.2230398952960968,0.11940599977970123,0.12068173289299011,0.1352907121181488,0.10981094092130661,0.11921484023332596,0.11873354017734528,0.12250500172376633,0.11931627988815308,0.11930649727582932,0.11920453608036041,0.11921431124210358,0.18562769889831543,0.1192970871925354,0.12208975106477737,0.11927149444818497,0.11348928511142731,0.12029853463172913,0.043495796620845795,0.11942492425441742,0.1192559003829956,0.11954017728567123,0.12240128964185715,0.11947421729564667,0.11993680894374847,0.11764320731163025,0.11705194413661957,0.11970800906419754,0.1192893385887146,0.11685127019882202,0.11915232241153717,0.1192350909113884,0.12099624425172806,0.11927962303161621,0.11927003413438797,0.1369795799255371,0.1192179024219513,0.11578883230686188,0.11739873886108398,0.11929310858249664,0.1192970871925354,0.11921779066324234,0.1190614178776741,0.11929541826248169,0.11927598714828491,0.11923427134752274,0.11673594266176224,0.11836244910955429,0.11906056106090546,0.11930252611637115,0.11937303841114044,0.12055370211601257,0.1124778538942337,0.1192978098988533,0.12044256925582886,0.11919121444225311,0.11929310858249664,0.12236911803483963,0.14281059801578522,0.11244387924671173,0.11930371820926666,0.09908647835254669,0.12178511917591095,0.18547239899635315,0.12823179364204407,0.11925476789474487,0.15558956563472748,0.0910935178399086,0.12088332325220108,0.11925505846738815,0.11727732419967651,0.11929310858249664,0.1400986611843109,0.11742758750915527,0.10934771597385406,0.11929541826248169,-0.004215806722640991,0.11929541826248169,0.1192290335893631,0.12132963538169861,0.1192970871925354,0.11878184974193573,0.12403706461191177,0.16251599788665771,0.11570967733860016,0.11927337944507599,0.12065638601779938,0.11944117397069931,0.12169738113880157,0.11593993008136749,0.11929440498352051,0.11921313405036926,0.11859139800071716,0.11929310858249664,0.11927003413438797,0.09528664499521255,0.10454761981964111,0.11914940178394318,0.11936965584754944,0.06667079776525497,0.11939948052167892,0.09582020342350006,0.1246139332652092,0.10686792433261871,0.11924310028553009,0.12022768706083298,0.11938004195690155,0.11978046596050262,0.11931551992893219,0.11951339244842529,0.12004498392343521,0.1220102310180664,0.1192970871925354,0.11930649727582932,0.11930210888385773,0.1247873604297638,0.11852477490901947,0.1195804551243782,0.10302494466304779,0.11930649727582932,0.12553858757019043,0.11531621217727661,0.1198086142539978,0.11931277066469193,0.1192789077758789,0.11835196614265442,0.11930981278419495,0.12052168697118759,0.119778573513031,0.11482067406177521,0.12394820898771286,0.11987501382827759,0.11935118585824966,0.11675722897052765,0.11391323804855347,0.11929678916931152,0.1192271038889885,0.21763986349105835,0.11923573166131973,0.11924105882644653,0.11930981278419495,0.11919960379600525,0.11887629330158234,0.11949893832206726,0.11925657093524933,0.11920890212059021,0.11930649727582932,0.1360476016998291,0.11926483362913132,0.10716424137353897,0.22734226286411285,0.08258847892284393,0.11925894021987915,0.11954586207866669,0.10964667797088623,0.11937325447797775,0.14014874398708344,0.1377083659172058,0.13216526806354523,0.1559360921382904,0.11914734542369843,0.11971811950206757,0.11928440630435944,0.11923711001873016,0.1160181313753128,0.13915583491325378,0.13675302267074585,0.11930649727582932,0.11911706626415253,0.11768591403961182,0.11112654954195023,0.11944462358951569,0.11916710436344147,0.11930210888385773,0.11079224944114685,0.11742737889289856,0.1222265213727951,0.12019698321819305,0.11928991973400116,0.1192893385887146,0.11913535743951797,0.11929541826248169,0.11997587978839874,0.12050564587116241,0.11935271322727203,0.11921463906764984,0.12180410325527191,0.11608762294054031,0.11831387877464294,0.10927002131938934,0.12169373035430908,0.119778573513031,0.07162082940340042,0.11906963586807251,0.11926890909671783,0.1141575276851654,0.1193482056260109,0.11562550812959671,0.11930649727582932,0.12106397747993469,0.08820793032646179,0.11765314638614655,0.11929149925708771,0.11908215284347534,0.12160234153270721,0.11930649727582932,0.11679185926914215,0.11792568862438202,0.11886100471019745,0.11939989775419235,0.1388036161661148,0.11928991973400116,0.1192970871925354,0.1191810667514801,0.11941320449113846,0.11597269773483276,0.12937688827514648,0.12055829167366028,0.1192970871925354,0.11742791533470154,0.11935941874980927,0.11927492916584015,0.11930981278419495,-0.10476167500019073,0.11930792033672333,0.11899356544017792,0.11892683058977127,0.11924133449792862,0.1502130925655365,0.11927909404039383,0.11925894021987915,0.1215786263346672,0.11929541826248169,0.12180793285369873,0.11920147389173508,0.12058401852846146,0.11943882703781128,0.11930981278419495,0.14456546306610107,0.10976098477840424,0.11929541826248169,0.1160404235124588,0.11930649727582932,0.11928440630435944,0.12064482271671295,0.11462995409965515,0.11938716471195221,0.01082831621170044,0.15229228138923645,0.11803801357746124,0.1192450225353241,0.11921773850917816,0.11917728930711746,0.1323333978652954,0.1192893385887146,0.11863452196121216,0.11853523552417755,0.126634880900383,0.12025051563978195,0.11921879649162292,0.1192377507686615,0.17167460918426514,0.0993293821811676,0.11949504911899567,0.11929541826248169,0.11739616096019745,0.11680106818675995,0.11927780508995056,0.12434686720371246,0.11925894021987915,0.11783452332019806,0.11929541826248169,0.11926904320716858,0.1194121241569519,0.11906220018863678,0.1200614720582962,0.11930649727582932,0.11333179473876953,0.11624333262443542,0.11930656433105469,0.12015140056610107,0.1193774938583374,0.11930981278419495,0.11818984895944595,0.11926442384719849,0.11928419768810272,0.11930178105831146,0.11916414648294449,0.11925894021987915,0.11927461624145508,0.09032467752695084,0.11930981278419495,0.11928991973400116,0.11930649727582932,0.0712118074297905,0.11940473318099976,0.11944709718227386,0.11899347603321075,0.11931291222572327,0.11931635439395905,0.11922703683376312,0.11575837433338165,0.11945866048336029,0.12222286313772202,0.11925894021987915,0.11923454701900482,0.12056629359722137,0.11930470168590546,0.1256687194108963,0.11918607354164124,0.11930981278419495,0.11908462643623352,0.1192970871925354,0.12739582359790802,0.1192893385887146,0.11948190629482269,0.10053105652332306,0.12014783918857574,0.1192970871925354,0.11638372391462326,0.11955537647008896,0.11911114305257797,0.1162395104765892,0.15065719187259674,0.11923159658908844,0.11150351911783218,0.09587734937667847,0.10627271980047226,0.11928216367959976,0.12004067003726959,0.1324021816253662,0.119778573513031,0.12212476879358292,0.1094067245721817,0.11943826079368591,0.11926986277103424,0.11540719121694565,0.11929541826248169,0.14649222791194916,0.1174342930316925,0.14143610000610352,0.1192285567522049,0.11922048032283783,0.12826628983020782,0.12004902958869934,0.11837814748287201,0.12913455069065094,0.12941300868988037,0.11930981278419495,0.1193200945854187,0.1239171102643013,0.11923856288194656,0.11838755011558533,0.11926430463790894,0.11929541826248169,0.13051515817642212,0.13364587724208832,0.11450711637735367,0.1188894510269165,0.11981643736362457,0.1192571222782135,0.12052661180496216,0.11925894021987915,0.11729741096496582,0.11944321542978287,0.14628708362579346,0.1512473225593567,0.1192970871925354,0.11936730146408081,-0.0054903775453567505,0.11929512023925781,0.1191333532333374,0.1221388429403305,0.09704287350177765,0.11948128789663315,0.11549828946590424,0.12276817858219147,0.1192970871925354,0.11919538676738739,0.13936471939086914,0.11925894021987915,0.11900828778743744,0.11316343396902084,0.11920539289712906,0.11928991973400116,0.1266305297613144,0.10719940811395645,0.12050070613622665,0.1191314160823822,0.14191408455371857,0.12744733691215515,0.11923708021640778,0.10033617913722992,0.11926138401031494,0.07271203398704529,0.13340088725090027,0.11941064894199371,0.11944909393787384,0.11929541826248169,0.11007879674434662,0.12404067814350128,0.11940132081508636,0.11977513134479523,0.1251165121793747,0.11921831965446472,0.03404528647661209,0.11606979370117188,0.12137085944414139,0.1304308921098709,0.20150966942310333,0.11913946270942688,0.1191595196723938,0.11931537836790085,0.0843784511089325,0.19126351177692413,0.11937182396650314,0.1136782169342041,0.11929767578840256,0.11818238347768784,0.10779497772455215,0.11706089973449707,0.11929541826248169,0.11648160964250565,0.12270981073379517,0.1200939267873764,0.1141766756772995,0.11931513249874115,0.11911404132843018,0.14126792550086975,0.11930981278419495,0.11923980712890625,0.11930891126394272,0.12230861186981201,0.12072324752807617,0.09637485444545746,0.08880921453237534,0.11978762596845627,0.11931359767913818,0.10055426508188248,0.1254354566335678,0.11930666863918304,0.12576349079608917,0.1192927360534668,0.11930877715349197,0.11915586143732071,0.12080676853656769,0.0693657174706459,0.11941388249397278,0.11925894021987915,0.0638924315571785,0.1192256510257721,0.12130486965179443,0.1192970871925354,0.11933235824108124,0.157882422208786,0.1192970871925354,0.11948645114898682,0.12621533870697021,0.11774250864982605,0.11930210888385773,0.11998777836561203,0.12169027328491211,0.11924511194229126,0.11308325827121735,0.12487588077783585,0.11167725175619125,0.08815035969018936,0.11922182887792587,0.1184139996767044,0.1190815418958664,0.11930981278419495,0.11929541826248169,0.12183073163032532,0.1271498203277588,0.11925894021987915,0.1192166656255722,0.11930649727582932,0.10419520735740662,0.0627945140004158,0.11928488314151764,0.1195959523320198,0.14171364903450012,0.11930649727582932,0.12164731323719025,0.10366534441709518,0.11891765892505646,0.11930210888385773,0.11928488314151764,0.11943623423576355,0.11940173804759979,0.11940192431211472,0.11903242021799088,0.11921998858451843,0.09818029403686523,0.11922580003738403,0.11926935613155365,0.11638294160366058,0.1634063422679901,0.12121254205703735,0.11933089792728424,0.09298667311668396,0.11897769570350647,0.1209651380777359,0.09338873624801636,0.11930649727582932,0.15212278068065643,0.11445984244346619,0.1196993961930275,0.11912551522254944,0.11924514919519424,0.1233249381184578,0.1339455246925354,0.12138242274522781,0.11932843923568726,0.12253254652023315,0.1279682219028473,0.11912725865840912,0.1050567775964737,0.11915510892868042,0.1193387359380722,0.11925975233316422,0.1230526715517044,0.1194184422492981,0.11930232495069504,0.1300322711467743,0.11692184209823608,0.11350731551647186,0.11458633840084076,0.11929267644882202,0.11931537836790085,0.11960740387439728,0.11923478543758392,0.11929485201835632,0.11938907206058502,0.11933927237987518,0.11620867252349854,0.1193574070930481,0.13331182301044464,0.11810760200023651,0.11864028871059418,0.11942151933908463,0.1193615198135376,0.11923812329769135,0.12255297601222992,0.1192876473069191,0.11930649727582932,0.11930649727582932,0.11600085347890854,0.1192970871925354,0.12326206266880035,0.11483024060726166,0.12348823249340057,0.11929485201835632,0.11927948147058487,0.11236085742712021,0.11929541826248169,0.12016087770462036,0.11764992773532867,0.09963497519493103,0.1192195862531662,0.10686202347278595,0.11940796673297882,0.14807924628257751,0.11840463429689407,0.11930649727582932,0.1192970871925354,0.11930981278419495,0.11925894021987915,0.10571098327636719,0.11919930577278137,0.08258485049009323,0.11808982491493225,0.11940529942512512,0.12071135640144348,0.12375189363956451,0.11572375893592834,0.12318143993616104,0.11789976805448532,0.1192970871925354,0.12912341952323914,0.117897629737854,0.12205195426940918,0.11941690742969513,0.11921504139900208,0.08117389678955078,0.11782336980104446,0.13552157580852509,0.11930649727582932,0.11928488314151764,0.1330258846282959,0.11926774680614471,0.11930210888385773,0.1193813905119896,0.1192970871925354,0.1192450225353241,0.11080605536699295,0.12084411084651947,0.13367842137813568,0.11929260939359665,0.11830449104309082,0.16949817538261414,0.12396829575300217,0.11927768588066101,0.1059238538146019,0.11542145907878876,0.11938506364822388,0.11473526060581207,0.13052870333194733,0.11927402019500732,0.13200436532497406,0.11814174056053162,0.11929528415203094,0.11930649727582932,0.11932149529457092,0.11846461892127991,0.1192970871925354,0.26673388481140137,0.1191362738609314,0.1192970871925354,0.13770586252212524,0.11499638855457306,0.09661675244569778,0.11929541826248169,0.1192738488316536,0.1192970871925354,0.10876540094614029,0.12652793526649475,0.11929541826248169,0.11930649727582932,0.11924716830253601,0.11961942911148071,0.11918024718761444,0.11930649727582932,0.11931763589382172,0.12077699601650238,0.11931498348712921,0.15583771467208862,0.119778573513031,0.119778573513031,0.11925894021987915,0.12047465145587921,0.16945716738700867,0.1194533109664917,0.11596749722957611,0.08856174349784851,0.11923618614673615,0.1193109005689621,0.11930109560489655,0.11916005611419678,0.13410262763500214,0.11930210888385773,0.1216564029455185,0.1183977872133255,0.1192970871925354,0.11932031810283661,0.11929310858249664,0.11798165738582611,0.022182121872901917,0.1459469199180603,0.11941531300544739,0.11700074374675751,0.12403557449579239,0.11924006044864655,0.11941487342119217,0.1594219207763672,0.09483238309621811,0.11930510401725769,0.11927388608455658,0.11906660348176956,0.1214088723063469,0.11915335059165955,0.11918677389621735,0.12024739384651184,0.11930649727582932,0.1192970871925354,0.119778573513031,0.13013477623462677,0.11940617859363556,0.12063069641590118,0.1229538843035698,0.11421991884708405,0.119778573513031,0.11943833529949188,0.04706535488367081,0.11906540393829346,0.11920194327831268,0.11930649727582932,0.14717283844947815,0.1135234534740448,0.1191631406545639,0.11929310858249664,0.1177990511059761,0.1192970871925354,0.08661631494760513,0.11711369454860687,0.11944741755723953,0.11944326013326645,0.11931857466697693,0.017518192529678345,0.1192970871925354,0.12226901948451996,0.11639675498008728,0.11926394701004028,0.12039896845817566,0.11925114691257477,0.11518041044473648,0.12013206630945206,0.12084095180034637,0.11929541826248169,0.11901257932186127,0.10252292454242706,0.11812786012887955,0.11926110088825226,0.11562012135982513,0.11929631233215332,0.1140410304069519,0.11920304596424103,0.12736204266548157,0.11933138966560364,-0.04832296073436737,0.11913818120956421,0.11705751717090607,0.11936330795288086,0.12967462837696075,0.11928991973400116,0.11922374367713928,0.11036516726016998,0.11509223282337189,0.1371491253376007,0.11929485201835632,0.11942614614963531,0.12071891874074936,0.11942749470472336,0.11913815140724182,0.10027239471673965,0.10146719217300415,0.11930649727582932,0.09668991714715958,0.10960222780704498,0.1192970871925354,0.11931618303060532,0.11926651000976562,0.11907072365283966,0.11928258836269379,0.16089004278182983,0.11929541826248169,0.1160634234547615,0.11941119283437729,0.1192970871925354,0.1499418169260025,0.11926938593387604,0.12633754312992096,0.1192970871925354,0.1192970871925354,0.09035605192184448,0.11930649727582932,-0.8262714743614197,0.11569440364837646,0.1197136789560318,0.11078185588121414,0.11930486559867859,0.11824657022953033,0.11486084759235382,0.11429952085018158,0.11946948617696762,0.1192619800567627,0.12292370200157166,0.1254805326461792,0.11919175088405609,0.11920331418514252,0.15365949273109436,0.11106669902801514,0.11929541826248169,0.12425415217876434,0.1192970871925354,0.1197812482714653,0.13059115409851074,0.1054760217666626,0.11927629262208939,0.112638920545578,0.1192970871925354,0.11925894021987915,0.15537485480308533,0.12045055627822876,0.11666843295097351,0.11930649727582932,0.11933604627847672,0.1192970871925354,0.11929485201835632,0.11996693164110184,0.11928921937942505,0.11924003064632416,0.11921393126249313,0.11990323662757874,0.11938562989234924,0.11929485201835632,0.12341786921024323,0.11911442875862122,0.1192387044429779,0.11938667297363281,0.11079107224941254,0.14024776220321655,0.1266484409570694,0.11930649727582932,0.113798126578331,0.11927618086338043,0.11926309764385223,0.12016819417476654,0.1193414181470871,0.11938804388046265,0.11775314807891846,0.1171674132347107,0.1131487786769867,0.11978046596050262,0.12013895809650421,0.12692636251449585,0.1332637369632721,0.11945468187332153,0.1192532405257225,0.119778573513031,0.1217534989118576,0.11916761845350266,0.11828350275754929,0.11929866671562195,0.1194126307964325,0.11930210888385773,0.11925281584262848,0.11930649727582932,0.11929541826248169,0.11932942271232605,0.11237714439630508,0.11974938213825226,0.13696105778217316,0.11923646926879883,0.11937995254993439,0.11935225129127502,0.1263328492641449,0.11158697307109833,0.11929228156805038,0.11647599935531616,0.1192970871925354,0.1193215548992157,0.11620976030826569,0.12634772062301636,0.11930649727582932,0.11663493514060974,0.11868628859519958,0.14188796281814575,0.11940497905015945,0.11648055911064148,0.11924205720424652,0.11929541826248169,0.10451425611972809,0.11930649727582932,0.16629742085933685,0.13645246624946594,0.11929541826248169,0.1192515417933464,0.11960694193840027,0.11925894021987915,0.11925894021987915,0.1174575537443161,0.11927713453769684,0.11930649727582932,0.11928991973400116,0.11708591878414154,0.11335432529449463,0.11928488314151764,0.12254168093204498,0.11930649727582932,0.12458232045173645,0.11934338510036469,0.11320973932743073,0.11929541826248169,0.11928488314151764,0.11940258741378784,0.11670371890068054,0.11945725232362747,0.13082176446914673,0.11026325076818466,0.12778763473033905,0.11941025406122208,0.12320537865161896,0.11924174427986145,0.11931604892015457,0.11930649727582932,0.11925894021987915,0.11929541826248169,0.11936633288860321,0.1192970871925354,0.11923769116401672,0.11938749998807907,0.1192970871925354,0.1192970871925354,0.13459467887878418,0.13613666594028473,0.11926393210887909,0.11942847073078156,0.11863761395215988,0.11925430595874786,0.11925046145915985,0.11925894021987915,0.11918417364358902,-0.05957755446434021,0.11928488314151764,0.11910457909107208,0.11722414195537567,0.15975508093833923,0.12018656730651855,0.1192970871925354,0.07139699906110764,0.11913935095071793,0.11923117935657501,0.12192045152187347,0.11940307170152664,0.119341179728508,0.11924290657043457,0.12724651396274567,0.1199684739112854,0.11683671921491623,0.1192970871925354,0.11758432537317276,0.1192970871925354,0.12223301827907562,0.11530255526304245,0.08976823836565018,0.119778573513031,0.1192970871925354,0.11929310858249664,0.11921558529138565,0.16359615325927734,0.11917725205421448,0.11927003413438797,0.11929969489574432,0.11925894021987915,0.12221026420593262,0.11911647021770477,0.11909030377864838,0.10827643424272537,0.11928240954875946,0.12109719216823578,0.11980485916137695,0.11912339925765991,0.11381513625383377,0.13354603946208954,0.11534811556339264,0.11927003413438797,0.11930210888385773,0.14631710946559906,0.11943143606185913,0.11937595903873444,0.11941619217395782,0.1161409467458725,0.11876435577869415,0.11940249800682068,0.11909835040569305,0.11911959201097488,0.11924665421247482,0.11929541826248169,0.11932366341352463,0.119272381067276,0.1192970871925354,0.1829245388507843,0.11756037175655365,0.11908505856990814,0.11922062933444977,0.12457206845283508,0.11929541826248169,0.16367107629776,0.1098017692565918,0.1192970871925354,0.11898528039455414,0.11924523115158081,0.08841248601675034,0.14332899451255798,0.11536972224712372,0.10817520320415497,0.11928991973400116,0.11841502785682678,0.1230500340461731,0.11930649727582932,0.11840251833200455,0.1192762553691864,0.11942439526319504,0.11503903567790985,0.11929541826248169,0.11929033696651459,0.1320359706878662,0.11928652226924896,0.11931884288787842,0.11462186276912689,0.041723884642124176,0.1238156333565712,0.11930981278419495,0.11243404448032379,0.1207449808716774,0.11929608881473541,0.1183645948767662,0.10522912442684174,0.11929541826248169,0.11929541826248169,0.12130200862884521,0.11928656697273254,0.11930649727582932,-0.5662801265716553,0.12296035885810852,0.11925134807825089,0.11937025934457779,0.11488497257232666,0.14087040722370148,0.11929310858249664,0.11919964849948883,0.08836393058300018,0.11927003413438797,0.10705921053886414,0.11933395266532898,0.11828183382749557,0.11933980882167816,0.12308772653341293,0.1202164739370346,0.11177951097488403,0.11985175311565399,0.11426222324371338,0.06641075015068054,0.1192970871925354,0.11921784281730652,0.12758684158325195,0.09676307439804077,0.11925320327281952,0.10785676538944244,0.11663792282342911,0.12808825075626373,0.1232391744852066,0.1181994378566742,0.11389079689979553,0.11925634741783142,0.1222306340932846,0.12876564264297485,0.12231534719467163,0.11930941045284271,0.1191377341747284,0.1272454708814621,0.11918869614601135,0.11651840060949326,0.1178567111492157,0.11918051540851593,0.12463858723640442,0.11825752258300781,0.11917826533317566,0.11938279867172241,0.12229051440954208,0.11793885380029678,0.12258117645978928,0.11827260255813599,0.119778573513031,0.1058146208524704,0.10157258808612823,0.11925633251667023,0.13232845067977905,0.11384104192256927,0.11931237578392029,0.11503060162067413,0.11934264004230499,0.11919437348842621,0.11931222677230835,0.12664300203323364,0.11635766923427582,0.11897958815097809,0.11936169862747192,0.08818557858467102,0.11931537836790085,0.11864884197711945,0.11115914583206177,0.1149812638759613,0.11918747425079346,0.12268185615539551,0.11989794671535492,0.1191662847995758,0.1424620896577835,0.11942629516124725,0.11925894021987915,0.04276028275489807,0.11966951191425323,0.119135782122612,0.11925894021987915,0.11998823285102844,0.11931905150413513,0.11924265325069427,0.11924333870410919,0.11930649727582932,0.1193070262670517,0.11740599572658539,0.11918678879737854,0.11929541826248169,0.11932816356420517,0.1187068298459053,0.1287526786327362,0.11927135288715363,0.11928488314151764,0.11932828277349472,0.11998866498470306,0.11709864437580109,0.12269867956638336,0.11929541826248169,0.11945408582687378,0.12004129588603973,0.1189965009689331,0.11925894021987915,0.11918801069259644,0.12070076167583466,0.08491311222314835,0.11923517286777496,0.059167951345443726,0.11941361427307129,0.11974722146987915,0.11917710304260254,0.12433119118213654,0.11609597504138947,0.09713029861450195,0.11793149262666702,0.1047159731388092,0.11930649727582932,0.20281386375427246,0.13000066578388214,0.11916463077068329,0.11930981278419495,0.11809547990560532,0.1192970871925354,0.11929541826248169,0.119778573513031,0.11925894021987915,0.11919800192117691,0.11929541826248169,0.14062616229057312,0.11923813819885254,0.11932739615440369,0.1191301941871643,0.11893399059772491,0.11938729882240295,0.117436483502388,0.11934373527765274,0.11940202116966248,0.13195857405662537,0.12125720828771591,0.11879575997591019,0.11934471130371094,0.11714357882738113,0.1192970871925354,0.11922779679298401,0.11930649727582932,0.12270742654800415,0.11923494935035706,0.1192970871925354,0.11327236145734787,0.119778573513031,0.11929485201835632,0.11947944015264511,0.11599156260490417,0.11919020116329193,0.12030932307243347,0.10049282759428024,0.11917867511510849,0.15276409685611725,0.11927530169487,0.11986292898654938,0.11831344664096832,0.018896102905273438,0.11918720602989197,0.11919332295656204,0.13396435976028442,0.11911126971244812,0.12164035439491272,0.15761248767375946,0.11932286620140076,0.11759177595376968,0.11921426653862,0.1195819303393364,0.11699417978525162,0.11921853572130203,0.11928488314151764,0.11913783848285675,0.119778573513031,0.11470699310302734,0.1192893385887146,0.11925635486841202,0.14690598845481873,0.11779181659221649,0.11970508098602295,0.09297360479831696,0.11925894021987915,0.1426597535610199,0.11941355466842651,0.11930649727582932,0.11929541826248169,0.11932028830051422,0.13726742565631866,0.12100699543952942,0.11936835944652557,0.11926086246967316,0.1763320118188858,0.13917355239391327,0.11930649727582932,0.1192893385887146,0.10060291737318039,0.1126803308725357,0.1192893385887146,0.12006735801696777,0.11931537836790085,0.0980454832315445,0.11928991973400116,0.11928991973400116,0.15378296375274658,0.11923211812973022,0.11925894021987915,0.12004111707210541,0.11900952458381653,0.11926786601543427,0.1259356290102005,0.11925894021987915,0.11927370727062225,0.1195145696401596,0.11485499888658524,0.11928440630435944,0.11927395313978195,0.08950628340244293,0.11922760307788849,0.13106995820999146,0.11929541826248169,0.11931779980659485,0.08943088352680206,0.11930210888385773,0.11757953464984894,0.12014402449131012,0.11942857503890991,0.11915324628353119,0.1022830605506897,0.11930210888385773,0.11930649727582932,-0.230009987950325,0.1459013968706131,0.11925894021987915,0.1192970871925354,0.11688032001256943,0.12164980918169022,0.1141289621591568,0.12002596259117126,0.11945267766714096,0.1192970871925354,0.14778880774974823,0.1216554343700409,0.11930649727582932,0.13362111151218414,0.1193370521068573,0.119778573513031,0.11930649727582932,0.11928608268499374,0.11136069893836975,0.11941415071487427,0.11935882270336151,0.11812195926904678,0.11619474738836288,0.08817975968122482,0.11837814748287201,0.11943940818309784,0.11808328330516815,0.11606958508491516,0.14751838147640228,0.0883379727602005,0.12163304537534714,0.11921227723360062,0.1192970871925354,0.11921082437038422,0.11933572590351105,0.11927346885204315,0.11532261967658997,0.11926852911710739,0.11937887966632843,0.11929500102996826,0.11926344037055969,0.11871477961540222,0.11933934688568115,0.11908352375030518,0.11928258836269379,0.12628833949565887,0.09618975222110748,0.11929485201835632,0.12390632182359695,0.08193731307983398,0.11912191659212112,0.11935111880302429,0.11929541826248169,0.1192970871925354,0.13681161403656006,0.11090856790542603,0.11902913451194763,0.11931424587965012,0.11876946687698364,0.11476762592792511,0.14997802674770355,0.11930088698863983,0.1247597336769104,0.11909528076648712,0.13787786662578583,0.11945779621601105,0.11866723001003265,0.11952835321426392,0.10039527714252472,0.11856356263160706,0.12095028162002563,0.11299625784158707,0.12908817827701569,0.1192970871925354,0.11929485201835632,0.11930035054683685,0.12097709625959396,0.08939285576343536,0.11636964231729507,0.11930210888385773,0.13373307883739471,0.1431364268064499,0.12004611641168594,0.11951231956481934,0.11936292052268982,0.11928457021713257,0.11913156509399414,0.145773783326149,0.11997721344232559,0.1192893385887146,0.1197429746389389,0.1193053275346756,0.11246642470359802,0.13788099586963654,0.11931537836790085,0.10263270139694214,0.08864608407020569,0.11944366991519928,0.1194516271352768,0.11892463266849518,0.11478801816701889,0.11946586519479752,0.11889046430587769,0.11823132634162903,0.11919861286878586,0.1192743107676506,0.09508052468299866,0.09734167158603668,0.1141151562333107,0.11928440630435944,0.12161333858966827,0.11717668175697327,0.11930981278419495,0.11922004818916321,0.11929310858249664,0.11937222629785538,0.11947067081928253,0.15273869037628174,0.11943811178207397,0.11931133270263672,0.11910222470760345,0.12838339805603027,0.11927804350852966,0.11835505068302155,0.042535677552223206,0.12291917204856873,0.1192893385887146,0.11930649727582932,0.12151691317558289,0.12206687033176422,0.11919978260993958,0.11927998811006546,0.11942920088768005,0.11929310858249664,0.12042775750160217,0.11925894021987915,0.11931537836790085,0.12590250372886658,0.11854992061853409,0.11926112323999405,0.11811099946498871,0.11928991973400116,0.12355820834636688,0.12365847826004028,0.11930909752845764,0.11930649727582932,0.14500689506530762,0.11400853842496872,0.11688443273305893,0.1192275881767273,0.11662719398736954,0.11894337087869644,0.11939999461174011,0.11929541826248169,0.11160001158714294,0.12104339897632599,0.11930210888385773,0.119158074259758,0.12230408191680908,0.1194971576333046,0.11236129701137543,0.11287617683410645,0.13456420600414276,0.1003003716468811,0.12336725741624832,0.11827738583087921,0.09344729781150818,0.11942286044359207,0.11928991973400116,0.11927050352096558,0.10847248136997223,0.10869473218917847,0.11897680908441544,0.13204523921012878,0.11330361664295197,0.11929541826248169,0.0965990275144577,0.11941225826740265,0.13108836114406586,0.1192893385887146,0.14243151247501373,0.12674686312675476,0.11930981278419495,0.11954863369464874,0.11940771341323853,0.1154618114233017,0.11929541826248169,0.11929310858249664,0.11787310987710953,0.11912259459495544,0.11923232674598694,0.11929310858249664,0.14037351310253143,0.11922882497310638,0.11934951692819595,0.11918698251247406,0.12116022408008575,0.11930649727582932,0.1143120750784874,0.11928936839103699,0.11929485201835632,0.11928252875804901,0.11929310858249664,0.11930210888385773,0.12097558379173279,0.11928991973400116,0.11799132823944092,0.11922109127044678,0.11763845384120941,0.12026999890804291,0.04986002296209335,0.11840061843395233,0.11767511814832687,0.12373755872249603,0.11909258365631104,0.11928991973400116,0.11930649727582932,0.11927003413438797,0.11919209361076355,0.11929485201835632,0.13293904066085815,0.11928074061870575,0.10342995822429657,0.1190968006849289,0.053959429264068604,0.11984200775623322,0.11936736106872559,0.11928991973400116,0.11934422701597214,0.11924609541893005,0.1192893385887146,0.11735731363296509,0.11942777037620544,0.11737601459026337,0.11752679944038391,0.13225246965885162,0.11938165128231049,0.11909783631563187,0.11937084048986435,0.11930055916309357,0.1178850531578064,0.11929405480623245,0.11942048370838165,0.11218467354774475,0.1194685846567154,0.1297604739665985,0.1191381961107254,0.11925894021987915,0.09022256731987,0.11925114691257477,0.14546068012714386,0.11930981278419495,0.11929654330015182,0.11940254271030426,0.11941400170326233,0.11925002932548523,0.11601405590772629,0.11928117275238037,0.10912458598613739,0.11930649727582932,0.10612543672323227,0.11702781915664673,0.15166127681732178,0.13750319182872772,0.14286063611507416,0.11495900899171829,0.2633366584777832,0.10698297619819641,0.11030948162078857,0.11929719150066376,0.11930649727582932,0.119778573513031,0.11930649727582932,0.1193922907114029,0.11913585662841797,0.11927665770053864,0.09782885015010834,0.11938796192407608,0.1181366890668869,0.11744007468223572,0.11930649727582932,0.11936582624912262,0.13649068772792816,0.10763664543628693,0.11778630316257477,0.119778573513031,0.12502604722976685,0.11368655413389206,0.11930981278419495,0.11828073859214783,0.11922991275787354,0.12091229856014252,0.1723899245262146,0.11805751919746399,0.1560440957546234,0.11920374631881714,0.12012524902820587,0.11244139075279236,0.11930649727582932,0.11922799050807953,0.11929541826248169,0.11923344433307648,0.04068666696548462,0.0985647439956665,0.1192970871925354,0.11920038610696793,0.10999121516942978,0.11930981278419495,0.11930649727582932,0.11929541826248169,0.11196038126945496],\"y\":[-0.22081181406974792,-0.22882041335105896,-0.22848495841026306,-0.22766530513763428,-0.21714931726455688,-0.2287171483039856,-0.23426297307014465,-0.22891077399253845,-0.22882002592086792,-0.22849735617637634,-0.23001566529273987,-0.22843295335769653,-0.22893667221069336,-0.228315532207489,-0.2286849021911621,-0.23239386081695557,-0.22854748368263245,-0.22862908244132996,-0.22882795333862305,-0.19932213425636292,-0.25055065751075745,-0.22862526774406433,-0.22862130403518677,-0.2290838658809662,-0.23633155226707458,-0.22879955172538757,-0.22916388511657715,-0.2285412847995758,-0.22886282205581665,-0.22338083386421204,-0.2036128044128418,-0.229264497756958,-0.21705546975135803,-0.2295154333114624,-0.22905969619750977,-0.23956912755966187,-0.23093131184577942,-0.23524802923202515,-0.22935447096824646,-0.22320079803466797,-0.261976420879364,-0.22865229845046997,-0.22871604561805725,-0.22160467505455017,-0.19294396042823792,-0.226828932762146,-0.22790634632110596,-0.21192508935928345,-0.22494423389434814,-0.23432844877243042,-0.22920092940330505,-0.199941486120224,-0.2279829978942871,-0.22872987389564514,-0.20330238342285156,-0.22955453395843506,-0.2086811363697052,-0.22881415486335754,-0.23427805304527283,-0.2286849021911621,-0.2353920340538025,-0.226688414812088,-0.22855940461158752,-0.22783595323562622,-0.22872498631477356,-0.20509815216064453,-0.2340056598186493,-0.22248032689094543,-0.22568067908287048,-0.22849735617637634,-0.22856831550598145,-0.2516660690307617,-0.23026013374328613,-0.22837349772453308,-0.2104893922805786,-0.22891223430633545,-0.22865229845046997,-0.22879070043563843,-0.24534451961517334,-0.22865229845046997,-0.22894182801246643,-0.22865229845046997,-0.227838397026062,-0.23619091510772705,-0.08088240027427673,-0.23031020164489746,-0.22905072569847107,-0.22507762908935547,-0.22656133770942688,-0.2346341907978058,-0.22865229845046997,-0.23202654719352722,-0.22707995772361755,-0.23217728734016418,-0.22862908244132996,-0.2286641001701355,-0.2283208966255188,-0.22674435377120972,-0.2255079746246338,-0.2367599904537201,-0.22865229845046997,-0.2284563183784485,-0.17172259092330933,-0.22841045260429382,-0.22507509589195251,-0.22024112939834595,-0.22868388891220093,-0.22183296084403992,-0.22188854217529297,-0.2289372682571411,-0.2293936312198639,-0.22932353615760803,-0.2288740873336792,-0.20885229110717773,-0.17694085836410522,-0.20458874106407166,-0.2349976897239685,-0.23211532831192017,-0.23416823148727417,-0.22849440574645996,-0.22916388511657715,-0.22900736331939697,-0.21239334344863892,-0.22878560423851013,-0.2913733422756195,-0.22837349772453308,-0.22897395491600037,-0.22862908244132996,-0.22886213660240173,-0.22862857580184937,-0.23038005828857422,-0.23264265060424805,-0.22874078154563904,-0.2288285791873932,-0.22916632890701294,-0.27255335450172424,-0.22819137573242188,-0.25099095702171326,-0.22449994087219238,-0.2171858847141266,-0.22862738370895386,-0.22849735617637634,-0.22188854217529297,-0.2505236566066742,-0.2233962118625641,-0.2438894808292389,-0.22951781749725342,-0.23637095093727112,-0.2240174114704132,-0.23406130075454712,-0.22888314723968506,-0.22862908244132996,-0.19572719931602478,-0.22866016626358032,-0.22865229845046997,-0.2085183560848236,-0.22862908244132996,-0.22849735617637634,-0.23066380620002747,-0.22850054502487183,-0.4395197331905365,-0.24864116311073303,-0.21905183792114258,-0.2288309931755066,-0.26430171728134155,-0.22891518473625183,-0.19999656081199646,-0.22837349772453308,-0.228736013174057,-0.21586421132087708,-0.21030259132385254,-0.22871440649032593,-0.24656853079795837,-0.2287062108516693,-0.23985221982002258,-0.22723513841629028,-0.2336585521697998,-0.22856006026268005,-0.2246338427066803,-0.22869357466697693,-0.20136108994483948,-0.22902020812034607,-0.22334063053131104,-0.22874858975410461,-0.22867387533187866,-0.2595829963684082,-0.2656756639480591,-0.22856542468070984,-0.22460472583770752,-0.22871768474578857,-0.22837349772453308,-0.22787511348724365,-0.23393121361732483,-0.2284718155860901,-0.2314661145210266,-0.22863826155662537,-0.22907531261444092,-0.22900891304016113,-0.22837349772453308,-0.22866016626358032,-0.22876328229904175,-0.22894224524497986,-0.22405779361724854,-0.22883829474449158,-0.23844441771507263,-0.2286951243877411,-0.1771794855594635,-0.23520374298095703,-0.22926214337348938,-0.22849735617637634,-0.22835302352905273,-0.23973122239112854,-0.21589666604995728,-0.2288610339164734,-0.22855940461158752,-0.22873759269714355,-0.19653016328811646,-0.19895264506340027,-0.22849735617637634,-0.23298895359039307,-0.23499161005020142,-0.24155300855636597,-0.2286849021911621,-0.22867348790168762,-0.22878000140190125,-0.22363510727882385,-0.21767252683639526,-0.22870764136314392,-0.2284005582332611,-0.2288799285888672,-0.22871029376983643,-0.24405047297477722,-0.2289203405380249,-0.22886237502098083,-0.21722570061683655,-0.23268744349479675,-0.23017022013664246,-0.22913673520088196,-0.22916388511657715,-0.22862908244132996,-0.22865229845046997,-0.23115545511245728,-0.228521466255188,-0.22869142889976501,-0.22855940461158752,-0.22873759269714355,-0.22093302011489868,-0.2282920777797699,-0.2288474142551422,-0.23216500878334045,-0.2345922291278839,-0.22878217697143555,-0.230802983045578,-0.22865229845046997,-0.22877293825149536,-0.16944009065628052,-0.22176247835159302,-0.22879955172538757,-0.23107850551605225,-0.22878414392471313,-0.22917956113815308,-0.2249564230442047,-0.21145275235176086,-0.22916388511657715,-0.22055351734161377,-0.22899287939071655,-0.2284555435180664,-0.22902530431747437,-0.2363806962966919,-0.2278538942337036,-0.22834116220474243,-0.22852274775505066,-0.23607662320137024,-0.229437917470932,-0.2288966178894043,-0.22872194647789001,-0.24268311262130737,-0.22883811593055725,-0.22764462232589722,-0.22916489839553833,-0.22847944498062134,-0.22858834266662598,-0.25127020478248596,-0.22847023606300354,-0.22378510236740112,-0.23268699645996094,-0.25295352935791016,-0.22855940461158752,-0.22865229845046997,-0.2362123429775238,-0.24762532114982605,-0.26016122102737427,-0.22889089584350586,-0.18139198422431946,-0.2533322870731354,-0.22862908244132996,-0.24592804908752441,-0.22873759269714355,-0.22855284810066223,-0.23894047737121582,-0.2367802858352661,-0.2362232208251953,-0.22869807481765747,-0.2286217212677002,-0.22916388511657715,-0.22900891304016113,-0.2272094190120697,-0.23064333200454712,-0.21252736449241638,-0.3395499289035797,-0.22988051176071167,-0.24119403958320618,-0.22927412390708923,-0.23172682523727417,-0.22837349772453308,-0.22849735617637634,-0.1990641951560974,-0.22879955172538757,-0.22901371121406555,-0.2284553349018097,-0.22666391730308533,-0.2310197651386261,-0.22885888814926147,-0.4525809586048126,-0.22902628779411316,-0.23481932282447815,-0.2290545105934143,-0.22845077514648438,-0.22829610109329224,-0.22870641946792603,-0.22866204380989075,-0.22493654489517212,-0.2559426426887512,-0.2287333607673645,-0.23563361167907715,-0.22926056385040283,-0.28963372111320496,-0.22921398282051086,-0.22935226559638977,-0.22895139455795288,-0.2184659242630005,-0.22889843583106995,-0.2286849021911621,-0.22845077514648438,-0.22862908244132996,-0.22878775000572205,-0.2287447452545166,-0.18655788898468018,-0.17429286241531372,-0.22851410508155823,-0.22783735394477844,-0.2292473018169403,-0.2289164662361145,-0.22872218489646912,-0.22884145379066467,-0.22884202003479004,-0.2288661003112793,-0.22873759269714355,-0.22859227657318115,-0.22852998971939087,-0.22826343774795532,-0.22889888286590576,-0.228805810213089,-0.23032847046852112,-0.23588082194328308,-0.22865229845046997,-0.23485639691352844,-0.22860407829284668,-0.22862908244132996,-0.23771440982818604,-0.22840818762779236,-0.22903847694396973,-0.22845956683158875,-0.2361334264278412,-0.2280886173248291,-0.22299504280090332,-0.22059869766235352,-0.22188854217529297,-0.22855836153030396,-0.23664602637290955,-0.22855940461158752,-0.22814545035362244,-0.23607242107391357,-0.173717200756073,-0.22874242067337036,-0.2285793423652649,-0.22799372673034668,-0.22968602180480957,-0.1787700653076172,-0.23787683248519897,-0.22881540656089783,-0.2329084575176239,-0.23523730039596558,-0.22673994302749634,-0.22845077514648438,-0.22862908244132996,-0.22862908244132996,-0.22888565063476562,-0.22887155413627625,-0.22278743982315063,-0.22899091243743896,-0.22855940461158752,-0.24280476570129395,-0.22911053895950317,-0.240247905254364,-0.23082712292671204,-0.22837349772453308,-0.22837349772453308,-0.23479938507080078,-0.22631987929344177,-0.21663028001785278,-0.23139578104019165,-0.22862908244132996,-0.19458577036857605,-0.22582122683525085,-0.22862908244132996,-0.22844472527503967,-0.2612238824367523,-0.2304285168647766,-0.2272396981716156,-0.23399975895881653,-0.23134279251098633,-0.22900891304016113,-0.21921613812446594,-0.22902610898017883,-0.22872009873390198,-0.20929262042045593,-0.23126006126403809,-0.24330276250839233,-0.22820600867271423,-0.2266729176044464,-0.20248091220855713,-0.23327374458312988,-0.22033283114433289,-0.22894954681396484,-0.228875070810318,-0.22912150621414185,-0.23865506052970886,-0.22583049535751343,-0.23549199104309082,-0.2367919683456421,-0.22882813215255737,-0.243482768535614,-0.22887861728668213,-0.22413748502731323,-0.23239058256149292,-0.22900572419166565,-0.20403695106506348,-0.2292787730693817,-0.22866016626358032,-0.23654112219810486,-0.24151486158370972,-0.22502043843269348,-0.21695908904075623,-0.2286849021911621,-0.2291666865348816,-0.22615915536880493,-0.22845634818077087,-0.22865229845046997,-0.22891542315483093,-0.17653784155845642,-0.22878962755203247,-0.22893065214157104,-0.2384280264377594,-0.20717588067054749,-0.2288593053817749,-0.2305808663368225,-0.22860771417617798,-0.22718459367752075,-0.3119629919528961,-0.23274296522140503,-0.2035556435585022,-0.2286849021911621,-0.11293955147266388,-0.22870421409606934,-0.22863879799842834,-0.231805682182312,-0.22886967658996582,-0.22916388511657715,-0.25476136803627014,-0.2284807562828064,-0.2303791046142578,-0.2066071629524231,-0.22241681814193726,-0.22849735617637634,-0.2252982258796692,-0.22837349772453308,-0.22862908244132996,-0.21901148557662964,-0.22880735993385315,-0.26097652316093445,-0.2286849021911621,-0.33237332105636597,-0.1368626058101654,-0.2284887731075287,-0.23566573858261108,-0.22976434230804443,-0.2304527461528778,-0.20202264189720154,-0.22845077514648438,-0.22862908244132996,-0.22855940461158752,-0.2284417748451233,-0.218202143907547,-0.23291000723838806,-0.2286764681339264,-0.2285308837890625,-0.2023169994354248,-0.22211605310440063,-0.23020154237747192,-0.22873952984809875,-0.2284052073955536,-0.22531437873840332,-0.2313232719898224,-0.22866016626358032,-0.22908931970596313,-0.22877192497253418,-0.22916388511657715,-0.23606771230697632,-0.1737438142299652,-0.22916388511657715,-0.23028355836868286,-0.21019211411476135,-0.22849735617637634,-0.228793203830719,-0.22872981429100037,-0.2272876799106598,-0.19948440790176392,-0.27319929003715515,-0.22971490025520325,-0.23048344254493713,-0.2565319240093231,-0.21913892030715942,-0.22903624176979065,-0.2281877100467682,-0.2341839075088501,-0.22900590300559998,-0.22849735617637634,-0.22859680652618408,-0.2289268672466278,-0.17113426327705383,-0.22862908244132996,-0.22917863726615906,-0.2284623682498932,-0.23328226804733276,-0.23008224368095398,-0.09882284700870514,-0.2281310260295868,-0.22923845052719116,-0.22921308875083923,-0.23526224493980408,-0.2285398244857788,-0.2300102710723877,-0.23432955145835876,-0.22475746273994446,-0.22995874285697937,-0.22873759269714355,-0.22495192289352417,-0.2285277247428894,-0.2286444902420044,-0.22471582889556885,-0.22863295674324036,-0.22900891304016113,-0.25918763875961304,-0.22861754894256592,-0.22295191884040833,-0.2250124216079712,-0.2286849021911621,-0.22862908244132996,-0.22876757383346558,-0.2284853756427765,-0.22865229845046997,-0.22909021377563477,-0.22867736220359802,-0.22401916980743408,-0.22712767124176025,-0.22846797108650208,-0.2293611764907837,-0.22852784395217896,-0.2311718761920929,-0.23124796152114868,-0.2286350131034851,-0.2306104302406311,-0.22847694158554077,-0.2286849021911621,-0.23422986268997192,-0.25339409708976746,-0.22067055106163025,-0.22889357805252075,-0.19410258531570435,-0.23393863439559937,-0.3432719111442566,-0.2445976436138153,-0.2288198471069336,-0.23229870200157166,-0.18005046248435974,-0.23223280906677246,-0.22887074947357178,-0.22180575132369995,-0.2286849021911621,-0.26795482635498047,-0.22495484352111816,-0.21165326237678528,-0.22865229845046997,-0.22311171889305115,-0.22865229845046997,-0.22880303859710693,-0.232221782207489,-0.22862908244132996,-0.22820627689361572,-0.23782524466514587,-0.3041298985481262,-0.2223598062992096,-0.2287723422050476,-0.23129460215568542,-0.22929462790489197,-0.2326684296131134,-0.22104686498641968,-0.22934725880622864,-0.2285233438014984,-0.2271253764629364,-0.2286849021911621,-0.22900891304016113,-0.23064672946929932,-0.22334033250808716,-0.22866472601890564,-0.2433978021144867,-0.2091367244720459,-0.22883257269859314,-0.230854332447052,-0.2356761395931244,-0.22962170839309692,-0.22851768136024475,-0.22662800550460815,-0.228806734085083,-0.22954416275024414,-0.2289922833442688,-0.22881346940994263,-0.22987636923789978,-0.22900062799453735,-0.22862908244132996,-0.22849735617637634,-0.22855940461158752,-0.2303808033466339,-0.2277211844921112,-0.2284316122531891,-0.2352311611175537,-0.22849735617637634,-0.24643748998641968,-0.22057795524597168,-0.22184845805168152,-0.2287362515926361,-0.22849997878074646,-0.22785943746566772,-0.22845077514648438,-0.22324591875076294,-0.22188854217529297,-0.22811028361320496,-0.2331022322177887,-0.2277144491672516,-0.22882795333862305,-0.22470426559448242,-0.21894726157188416,-0.2287408709526062,-0.22902145981788635,-0.39932432770729065,-0.22927045822143555,-0.22880804538726807,-0.22845077514648438,-0.22863078117370605,-0.22601673007011414,-0.2292434573173523,-0.22861692309379578,-0.22860044240951538,-0.22849735617637634,-0.2583855092525482,-0.22910842299461365,-0.2077522873878479,-0.4169662296772003,-0.1761900782585144,-0.22916388511657715,-0.22856834530830383,-0.2123083770275116,-0.22880196571350098,-0.23672828078269958,-0.2596205770969391,-0.2508377730846405,-0.23388922214508057,-0.22881478071212769,-0.22946995496749878,-0.22880735993385315,-0.2288074791431427,-0.22310474514961243,-0.23995128273963928,-0.25893452763557434,-0.22849735617637634,-0.2284826636314392,-0.23459193110466003,-0.21505451202392578,-0.22905108332633972,-0.228776752948761,-0.22855940461158752,-0.22798573970794678,-0.22527402639389038,-0.23456600308418274,-0.224993497133255,-0.22872987389564514,-0.22873759269714355,-0.22908863425254822,-0.22865229845046997,-0.22979021072387695,-0.2311769425868988,-0.22875872254371643,-0.2286105751991272,-0.2286880910396576,-0.2227436602115631,-0.22692888975143433,-0.21082139015197754,-0.23268923163414001,-0.22188854217529297,-0.17247799038887024,-0.2284259796142578,-0.22913116216659546,-0.22000962495803833,-0.22888964414596558,-0.2226797640323639,-0.22849735617637634,-0.23175892233848572,-0.17473143339157104,-0.22603195905685425,-0.22844555974006653,-0.22823792695999146,-0.23216462135314941,-0.22849735617637634,-0.2225683629512787,-0.22607910633087158,-0.22846299409866333,-0.22908353805541992,-0.26269492506980896,-0.22872987389564514,-0.22862908244132996,-0.2283802628517151,-0.2289429008960724,-0.22262439131736755,-0.24700233340263367,-0.23109161853790283,-0.22862908244132996,-0.2220969796180725,-0.22876346111297607,-0.22879165410995483,-0.22845077514648438,-0.27564796805381775,-0.22882333397865295,-0.23713165521621704,-0.22803521156311035,-0.22880819439888,-0.24661892652511597,-0.22891303896903992,-0.22916388511657715,-0.23293131589889526,-0.22865229845046997,-0.22991636395454407,-0.22901251912117004,-0.23128744959831238,-0.22870975732803345,-0.22845077514648438,-0.27280476689338684,-0.2165910303592682,-0.22865229845046997,-0.22346621751785278,-0.22849735617637634,-0.22880735993385315,-0.23131614923477173,-0.22081464529037476,-0.22881314158439636,-0.040611788630485535,-0.2867981791496277,-0.22564610838890076,-0.2285599708557129,-0.22892647981643677,-0.22886672616004944,-0.24127855896949768,-0.22873759269714355,-0.227568119764328,-0.22738173604011536,-0.24374407529830933,-0.22987595200538635,-0.2288147509098053,-0.22849005460739136,-0.3197100758552551,-0.20565268397331238,-0.22902074456214905,-0.22865229845046997,-0.22559267282485962,-0.22389519214630127,-0.2289217710494995,-0.2363225221633911,-0.22916388511657715,-0.2248077690601349,-0.22865229845046997,-0.22876128554344177,-0.2287197709083557,-0.22912031412124634,-0.23011156916618347,-0.22849735617637634,-0.21860605478286743,-0.22324582934379578,-0.22864478826522827,-0.23076695203781128,-0.228875070810318,-0.22845077514648438,-0.23374301195144653,-0.22877341508865356,-0.2286376953125,-0.2288544476032257,-0.2287428379058838,-0.22916388511657715,-0.2288585603237152,-0.197879821062088,-0.22845077514648438,-0.22872987389564514,-0.22849735617637634,-0.22225263714790344,-0.22905489802360535,-0.2287222146987915,-0.2288677990436554,-0.22913119196891785,-0.22925376892089844,-0.2286299765110016,-0.22226548194885254,-0.22919881343841553,-0.23366904258728027,-0.22916388511657715,-0.22856083512306213,-0.23166751861572266,-0.22879359126091003,-0.24513202905654907,-0.22868826985359192,-0.22845077514648438,-0.22887489199638367,-0.22862908244132996,-0.2429153025150299,-0.22873759269714355,-0.22905698418617249,-0.19710096716880798,-0.2362273931503296,-0.22862908244132996,-0.22400492429733276,-0.22844600677490234,-0.2288711965084076,-0.22298142313957214,-0.23953500390052795,-0.2286890745162964,-0.21538695693016052,-0.2235853374004364,-0.20648568868637085,-0.22880223393440247,-0.23002883791923523,-0.1867198944091797,-0.22188854217529297,-0.23453569412231445,-0.2118198275566101,-0.22895261645317078,-0.22863256931304932,-0.22858622670173645,-0.22865229845046997,-0.27495837211608887,-0.22557497024536133,-0.26691216230392456,-0.2286020815372467,-0.2287522256374359,-0.2359861135482788,-0.23000624775886536,-0.22729846835136414,-0.24529409408569336,-0.2504732310771942,-0.22845077514648438,-0.22861111164093018,-0.23128512501716614,-0.2290649116039276,-0.2273913025856018,-0.22845041751861572,-0.22865229845046997,-0.24794697761535645,-0.2415429651737213,-0.22039231657981873,-0.22748205065727234,-0.22900262475013733,-0.22873294353485107,-0.2307814061641693,-0.22916388511657715,-0.22527259588241577,-0.22111564874649048,-0.27447640895843506,-0.2829623520374298,-0.22862908244132996,-0.22899538278579712,-0.16950565576553345,-0.22862428426742554,-0.22825869917869568,-0.23354655504226685,-0.19028761982917786,-0.22893443703651428,-0.2336292862892151,-0.27533289790153503,-0.22862908244132996,-0.22854506969451904,-0.2632318437099457,-0.22916388511657715,-0.22807931900024414,-0.22656574845314026,-0.22901949286460876,-0.22872987389564514,-0.22724559903144836,-0.21322205662727356,-0.23073330521583557,-0.22834745049476624,-0.24084585905075073,-0.24344787001609802,-0.2290748655796051,-0.20772239565849304,-0.22888872027397156,-0.21726229786872864,-0.25423598289489746,-0.22906559705734253,-0.22906577587127686,-0.22865229845046997,-0.2305242121219635,-0.23669186234474182,-0.2287854552268982,-0.22989913821220398,-0.23971474170684814,-0.22948938608169556,-0.1772536039352417,-0.22322112321853638,-0.233666330575943,-0.2481156587600708,-0.31890976428985596,-0.22887545824050903,-0.22579282522201538,-0.22837349772453308,-0.18952694535255432,-0.32548749446868896,-0.22872373461723328,-0.2195035219192505,-0.22877338528633118,-0.2275211215019226,-0.21813535690307617,-0.22483128309249878,-0.22865229845046997,-0.22424864768981934,-0.23653319478034973,-0.2297269105911255,-0.21867114305496216,-0.22870826721191406,-0.22864371538162231,-0.26646703481674194,-0.22845077514648438,-0.2286558747291565,-0.2285984456539154,-0.23349839448928833,-0.23171529173851013,-0.18976542353630066,-0.17576798796653748,-0.22966057062149048,-0.22884684801101685,-0.2127065658569336,-0.23902666568756104,-0.22857195138931274,-0.23238477110862732,-0.2286854088306427,-0.22881785035133362,-0.22909212112426758,-0.23161396384239197,-0.22657889127731323,-0.22887665033340454,-0.22916388511657715,-0.13422462344169617,-0.2284250259399414,-0.2320399284362793,-0.22862908244132996,-0.22877582907676697,-0.2440480887889862,-0.22862908244132996,-0.22875988483428955,-0.232814222574234,-0.22707599401474,-0.22855940461158752,-0.22981920838356018,-0.23278898000717163,-0.22835856676101685,-0.2183937132358551,-0.23066860437393188,-0.2213226556777954,-0.17495602369308472,-0.22882387042045593,-0.2274770438671112,-0.2285269796848297,-0.22845077514648438,-0.22865229845046997,-0.23315507173538208,-0.24217912554740906,-0.22916388511657715,-0.22891953587532043,-0.22849735617637634,-0.20593148469924927,-0.24847587943077087,-0.22879955172538757,-0.2294941246509552,-0.2212306261062622,-0.22849735617637634,-0.23472312092781067,-0.2450508177280426,-0.22799968719482422,-0.22855940461158752,-0.22879955172538757,-0.22880986332893372,-0.22897976636886597,-0.22917023301124573,-0.22822096943855286,-0.22868451476097107,-0.239906907081604,-0.2282869815826416,-0.2285347580909729,-0.2233475148677826,-0.24485549330711365,-0.22891157865524292,-0.2291230857372284,-0.1830214560031891,-0.22830379009246826,-0.23189261555671692,-0.18406397104263306,-0.22849735617637634,-0.28589537739753723,-0.23998209834098816,-0.2286832630634308,-0.2287755012512207,-0.22926145792007446,-0.2361801266670227,-0.2544684410095215,-0.23528128862380981,-0.22927013039588928,-0.23468253016471863,-0.24164393544197083,-0.22833576798439026,-0.20486119389533997,-0.22882863879203796,-0.22913971543312073,-0.22873961925506592,-0.2352447211742401,-0.22883901000022888,-0.22876077890396118,-0.21848267316818237,-0.2221989631652832,-0.2223421335220337,-0.22374612092971802,-0.22866395115852356,-0.22837349772453308,-0.2287575602531433,-0.22884416580200195,-0.22866016626358032,-0.2287035584449768,-0.2285582423210144,-0.22354722023010254,-0.22854828834533691,-0.2449386715888977,-0.22616469860076904,-0.22802573442459106,-0.2285653054714203,-0.22894138097763062,-0.22884133458137512,-0.233936607837677,-0.22861340641975403,-0.22849735617637634,-0.22849735617637634,-0.22351214289665222,-0.22862908244132996,-0.23623904585838318,-0.22091460227966309,-0.22459006309509277,-0.22866016626358032,-0.22865527868270874,-0.217995285987854,-0.22865229845046997,-0.2299840748310089,-0.22600680589675903,-0.2188890278339386,-0.22869253158569336,-0.19941836595535278,-0.22891294956207275,-0.2786102592945099,-0.2307739555835724,-0.22849735617637634,-0.22862908244132996,-0.22845077514648438,-0.22916388511657715,-0.20521831512451172,-0.2287808060646057,-0.21792039275169373,-0.22716781497001648,-0.22877871990203857,-0.23117569088935852,-0.23630639910697937,-0.2225373089313507,-0.23617509007453918,-0.22628623247146606,-0.22862908244132996,-0.24573469161987305,-0.225990891456604,-0.23816826939582825,-0.22888794541358948,-0.22863635420799255,-0.2057703137397766,-0.22611311078071594,-0.2492503821849823,-0.22849735617637634,-0.22879955172538757,-0.24495062232017517,-0.22887954115867615,-0.22855940461158752,-0.22885054349899292,-0.22862908244132996,-0.22863176465034485,-0.212855726480484,-0.23197141289710999,-0.2532848119735718,-0.22880610823631287,-0.2357722520828247,-0.3158218264579773,-0.2370828092098236,-0.22887247800827026,-0.20557352900505066,-0.22122108936309814,-0.2288110852241516,-0.22045797109603882,-0.24781322479248047,-0.22890964150428772,-0.25077277421951294,-0.22677457332611084,-0.22887572646141052,-0.22849735617637634,-0.22898143529891968,-0.2270127832889557,-0.22862908244132996,-0.49142006039619446,-0.22850802540779114,-0.22862908244132996,-0.26081597805023193,-0.22955337166786194,-0.18929225206375122,-0.22865229845046997,-0.2292676866054535,-0.22862908244132996,-0.21053671836853027,-0.24190807342529297,-0.22865229845046997,-0.22849735617637634,-0.228971928358078,-0.22939783334732056,-0.2287912666797638,-0.22849735617637634,-0.2288484275341034,-0.23130977153778076,-0.22885242104530334,-0.2922501862049103,-0.22188854217529297,-0.22188854217529297,-0.22916388511657715,-0.23066109418869019,-0.32170024514198303,-0.22914472222328186,-0.21522045135498047,-0.2534842789173126,-0.22837811708450317,-0.22869381308555603,-0.2291092872619629,-0.22851234674453735,-0.24261584877967834,-0.22855940461158752,-0.23291143774986267,-0.2267550528049469,-0.22862908244132996,-0.22905877232551575,-0.2286849021911621,-0.22642230987548828,-0.18982341885566711,-0.274787575006485,-0.22840842604637146,-0.22420337796211243,-0.2359672486782074,-0.22945654392242432,-0.22900497913360596,-0.298424631357193,-0.22668150067329407,-0.22881150245666504,-0.22926759719848633,-0.22922667860984802,-0.2323530614376068,-0.22870349884033203,-0.22861579060554504,-0.2299589216709137,-0.22849735617637634,-0.22862908244132996,-0.22188854217529297,-0.2471495270729065,-0.2292957305908203,-0.23069429397583008,-0.23459240794181824,-0.2235087752342224,-0.22188854217529297,-0.22899362444877625,-0.21295681595802307,-0.22918561100959778,-0.2286781668663025,-0.22849735617637634,-0.222844660282135,-0.2238999605178833,-0.22848102450370789,-0.2286849021911621,-0.22600871324539185,-0.22862908244132996,-0.2083374261856079,-0.23390236496925354,-0.22873428463935852,-0.22845250368118286,-0.22869905829429626,-0.29270437359809875,-0.22862908244132996,-0.2269744873046875,-0.22349494695663452,-0.22887524962425232,-0.2307508885860443,-0.22927233576774597,-0.2231108546257019,-0.23058047890663147,-0.22962021827697754,-0.22865229845046997,-0.22925806045532227,-0.2299008071422577,-0.22619915008544922,-0.22880148887634277,-0.22224920988082886,-0.22894158959388733,-0.22006425261497498,-0.2285020649433136,-0.24448928236961365,-0.2286970615386963,-0.25017300248146057,-0.22364172339439392,-0.24213162064552307,-0.22859454154968262,-0.24656713008880615,-0.22872987389564514,-0.22843602299690247,-0.21390977501869202,-0.22238653898239136,-0.25921016931533813,-0.22866016626358032,-0.22896888852119446,-0.23141872882843018,-0.2289348840713501,-0.22901210188865662,-0.2331598997116089,-0.22523045539855957,-0.22849735617637634,-0.2179030478000641,-0.2072703242301941,-0.22862908244132996,-0.2285507619380951,-0.2287517488002777,-0.22893092036247253,-0.22870737314224243,-0.30028098821640015,-0.22865229845046997,-0.2232648730278015,-0.22922596335411072,-0.22862908244132996,-0.2815292477607727,-0.2291821837425232,-0.22543850541114807,-0.22862908244132996,-0.22862908244132996,-0.1786496937274933,-0.22849735617637634,-0.48887166380882263,-0.22636502981185913,-0.22939422726631165,-0.21022331714630127,-0.22862741351127625,-0.2270243763923645,-0.22073760628700256,-0.22867798805236816,-0.22910556197166443,-0.22907501459121704,-0.23471492528915405,-0.23979845643043518,-0.22937127947807312,-0.22905215620994568,-0.24517104029655457,-0.23771244287490845,-0.22865229845046997,-0.23754507303237915,-0.22862908244132996,-0.22945904731750488,-0.24914726614952087,-0.2056582272052765,-0.22931477427482605,-0.21779999136924744,-0.22862908244132996,-0.22916388511657715,-0.23873960971832275,-0.2336418330669403,-0.22339406609535217,-0.22849735617637634,-0.22867852449417114,-0.22862908244132996,-0.22866016626358032,-0.23043376207351685,-0.22886312007904053,-0.22908082604408264,-0.22907355427742004,-0.22272846102714539,-0.22895747423171997,-0.22866016626358032,-0.23660284280776978,-0.22875913977622986,-0.22920650243759155,-0.22878757119178772,-0.21405130624771118,-0.26616477966308594,-0.23328042030334473,-0.22849735617637634,-0.2187354862689972,-0.2290627658367157,-0.22869637608528137,-0.23081937432289124,-0.22908490896224976,-0.22839802503585815,-0.22547605633735657,-0.22490134835243225,-0.23639702796936035,-0.22954416275024414,-0.22959661483764648,-0.2425774335861206,-0.25330039858818054,-0.22871103882789612,-0.22877717018127441,-0.22188854217529297,-0.23374396562576294,-0.2287331521511078,-0.2269718050956726,-0.22865992784500122,-0.22910666465759277,-0.22855940461158752,-0.22873547673225403,-0.22849735617637634,-0.22865229845046997,-0.22868072986602783,-0.21569272875785828,-0.22935795783996582,-0.2594469487667084,-0.22896835207939148,-0.22933551669120789,-0.22874999046325684,-0.2398027777671814,-0.21528801321983337,-0.2286539077758789,-0.2162429392337799,-0.22862908244132996,-0.22924688458442688,-0.22347435355186462,-0.24076882004737854,-0.22849735617637634,-0.2333102524280548,-0.22829341888427734,-0.23672053217887878,-0.2287781834602356,-0.2239525318145752,-0.22871047258377075,-0.22865229845046997,-0.23242762684822083,-0.22849735617637634,-0.3102768063545227,-0.26573988795280457,-0.22865229845046997,-0.229271799325943,-0.22863727807998657,-0.22916388511657715,-0.22916388511657715,-0.22570568323135376,-0.22859516739845276,-0.22849735617637634,-0.22872987389564514,-0.224904865026474,-0.22779580950737,-0.22879955172538757,-0.22710281610488892,-0.22849735617637634,-0.23717832565307617,-0.22967123985290527,-0.19612860679626465,-0.22865229845046997,-0.22879955172538757,-0.22889313101768494,-0.22390025854110718,-0.22914260625839233,-0.25046950578689575,-0.20882916450500488,-0.24016177654266357,-0.22892341017723083,-0.23330935835838318,-0.22852995991706848,-0.22914165258407593,-0.22849735617637634,-0.22916388511657715,-0.22865229845046997,-0.22893348336219788,-0.22862908244132996,-0.22880128026008606,-0.22890964150428772,-0.22862908244132996,-0.22862908244132996,-0.255881667137146,-0.2573690116405487,-0.22920089960098267,-0.22850194573402405,-0.22714751958847046,-0.22860586643218994,-0.2287292778491974,-0.22916388511657715,-0.22846919298171997,-0.2114945948123932,-0.22879955172538757,-0.22890883684158325,-0.22532948851585388,-0.240338534116745,-0.23036673665046692,-0.22862908244132996,-0.2197602391242981,-0.22867733240127563,-0.22875380516052246,-0.23364314436912537,-0.2288631796836853,-0.2289040982723236,-0.2287779152393341,-0.25948336720466614,-0.23003992438316345,-0.2238931953907013,-0.22862908244132996,-0.2348749339580536,-0.22862908244132996,-0.23390865325927734,-0.22149303555488586,-0.19657480716705322,-0.22188854217529297,-0.22862908244132996,-0.2286849021911621,-0.2290363311767578,-0.30436086654663086,-0.2288191318511963,-0.22900891304016113,-0.22894632816314697,-0.22916388511657715,-0.2314591407775879,-0.2288576364517212,-0.22871515154838562,-0.20971089601516724,-0.2288026213645935,-0.2321597933769226,-0.2514020502567291,-0.22840142250061035,-0.22028914093971252,-0.2265866994857788,-0.24623018503189087,-0.22900891304016113,-0.22855940461158752,-0.24790436029434204,-0.22898226976394653,-0.22886380553245544,-0.22864142060279846,-0.22329434752464294,-0.22761180996894836,-0.22857975959777832,-0.22889700531959534,-0.22896400094032288,-0.22914311289787292,-0.22865229845046997,-0.22913113236427307,-0.22842571139335632,-0.22862908244132996,-0.2037510871887207,-0.22787290811538696,-0.22863948345184326,-0.22912093997001648,-0.23326149582862854,-0.22865229845046997,-0.24491742253303528,-0.2232973575592041,-0.22862908244132996,-0.22824084758758545,-0.22824716567993164,-0.17601439356803894,-0.2532075345516205,-0.23003315925598145,-0.21298697590827942,-0.22872987389564514,-0.22728821635246277,-0.2354038655757904,-0.22849735617637634,-0.22723400592803955,-0.22890520095825195,-0.22888517379760742,-0.21761712431907654,-0.22865229845046997,-0.22869616746902466,-0.2505466938018799,-0.22858086228370667,-0.22952395677566528,-0.22071340680122375,-0.2166861593723297,-0.23710757493972778,-0.22845077514648438,-0.21994197368621826,-0.23118561506271362,-0.22854521870613098,-0.22693410515785217,-0.22517552971839905,-0.22865229845046997,-0.22865229845046997,-0.23205411434173584,-0.22882887721061707,-0.22849735617637634,-0.4592075049877167,-0.23492297530174255,-0.22922742366790771,-0.2290520966053009,-0.22167649865150452,-0.218440443277359,-0.2286849021911621,-0.22876688838005066,-0.23688089847564697,-0.22900891304016113,-0.20778587460517883,-0.22878149151802063,-0.22696453332901,-0.22902223467826843,-0.2352047860622406,-0.23038935661315918,-0.21592769026756287,-0.22931277751922607,-0.22041887044906616,-0.21173334121704102,-0.22862908244132996,-0.2284579575061798,-0.22724151611328125,-0.2146499752998352,-0.22872686386108398,-0.20719483494758606,-0.2311493158340454,-0.24369487166404724,-0.23581430315971375,-0.22656044363975525,-0.21924525499343872,-0.2291576862335205,-0.23412302136421204,-0.24548646807670593,-0.23388448357582092,-0.22864988446235657,-0.22898557782173157,-0.24194014072418213,-0.22873520851135254,-0.22441083192825317,-0.22638720273971558,-0.2288055121898651,-0.23828533291816711,-0.2274278700351715,-0.22886401414871216,-0.2290121614933014,-0.23403263092041016,-0.22588953375816345,-0.2372264862060547,-0.2269572913646698,-0.22188854217529297,-0.20577076077461243,-0.19909223914146423,-0.22876399755477905,-0.2510209381580353,-0.21910586953163147,-0.22882309556007385,-0.2214236855506897,-0.22882050275802612,-0.22881931066513062,-0.2288302481174469,-0.24107950925827026,-0.22279343008995056,-0.23492926359176636,-0.22915339469909668,-0.17528924345970154,-0.22837349772453308,-0.22787940502166748,-0.2148585021495819,-0.22215545177459717,-0.22858858108520508,-0.23494356870651245,-0.2298647165298462,-0.22844967246055603,-0.2686508297920227,-0.22875621914863586,-0.22916388511657715,-0.25935909152030945,-0.22904247045516968,-0.22843608260154724,-0.22916388511657715,-0.22998422384262085,-0.22939035296440125,-0.22845324873924255,-0.22868657112121582,-0.22849735617637634,-0.22896814346313477,-0.22514724731445312,-0.22881963849067688,-0.22865229845046997,-0.22882521152496338,-0.22758281230926514,-0.2446269989013672,-0.22889438271522522,-0.22879955172538757,-0.2291705310344696,-0.22978517413139343,-0.22575420141220093,-0.23448890447616577,-0.22865229845046997,-0.2290126383304596,-0.23102828860282898,-0.2280431091785431,-0.22916388511657715,-0.2294107973575592,-0.23163187503814697,-0.1697760820388794,-0.22884970903396606,-0.16508710384368896,-0.22886577248573303,-0.2294151484966278,-0.2291199266910553,-0.2367827594280243,-0.2237890660762787,-0.19066190719604492,-0.22627544403076172,-0.20394915342330933,-0.22849735617637634,-0.3731379806995392,-0.24005016684532166,-0.22850272059440613,-0.22845077514648438,-0.2264426350593567,-0.22862908244132996,-0.22865229845046997,-0.22188854217529297,-0.22916388511657715,-0.2285815179347992,-0.22865229845046997,-0.26544108986854553,-0.2285550832748413,-0.22863036394119263,-0.228738933801651,-0.2287331521511078,-0.22899147868156433,-0.2253647744655609,-0.22882601618766785,-0.22887516021728516,-0.25017049908638,-0.23261064291000366,-0.22825869917869568,-0.2288038730621338,-0.22529691457748413,-0.22862908244132996,-0.22902464866638184,-0.22849735617637634,-0.22062420845031738,-0.22906851768493652,-0.22862908244132996,-0.21275749802589417,-0.22188854217529297,-0.22866016626358032,-0.2289661169052124,-0.22323575615882874,-0.22897416353225708,-0.23036858439445496,-0.23310965299606323,-0.2288699746131897,-0.28676697611808777,-0.22867989540100098,-0.22950702905654907,-0.22685709595680237,-0.054243966937065125,-0.22876104712486267,-0.22864830493927002,-0.25408050417900085,-0.22834697365760803,-0.2316581904888153,-0.23896977305412292,-0.22860679030418396,-0.2254711389541626,-0.2289150059223175,-0.22877678275108337,-0.22145748138427734,-0.22893944382667542,-0.22879955172538757,-0.22867748141288757,-0.22188854217529297,-0.22071951627731323,-0.22873759269714355,-0.22883465886116028,-0.27630171179771423,-0.22594985365867615,-0.22908109426498413,-0.19856667518615723,-0.22916388511657715,-0.26964271068573,-0.22899454832077026,-0.22849735617637634,-0.22865229845046997,-0.22869673371315002,-0.2589016258716583,-0.2320079505443573,-0.22875452041625977,-0.22853955626487732,-0.32765302062034607,-0.262766569852829,-0.22849735617637634,-0.22873759269714355,-0.19321677088737488,-0.22566279768943787,-0.22873759269714355,-0.23038709163665771,-0.22837349772453308,-0.20419132709503174,-0.22872987389564514,-0.22872987389564514,-0.23276719450950623,-0.22848078608512878,-0.22916388511657715,-0.23000958561897278,-0.23680919408798218,-0.22862064838409424,-0.23304161429405212,-0.22916388511657715,-0.2285463809967041,-0.2291463315486908,-0.2218572497367859,-0.22880735993385315,-0.22862380743026733,-0.22656306624412537,-0.22872689366340637,-0.24860495328903198,-0.22865229845046997,-0.22881197929382324,-0.21883586049079895,-0.22855940461158752,-0.2262134850025177,-0.23032331466674805,-0.2282225489616394,-0.22825849056243896,-0.19988897442817688,-0.22855940461158752,-0.22849735617637634,-0.351972371339798,-0.27438923716545105,-0.22916388511657715,-0.22862908244132996,-0.22235196828842163,-0.2331359088420868,-0.2189556360244751,-0.22998046875,-0.22946229577064514,-0.22862908244132996,-0.22298890352249146,-0.23307347297668457,-0.22849735617637634,-0.2536737620830536,-0.22886428236961365,-0.22188854217529297,-0.22849735617637634,-0.22859281301498413,-0.22040030360221863,-0.22885662317276,-0.22881397604942322,-0.2263799011707306,-0.22283479571342468,-0.17526742815971375,-0.22674769163131714,-0.22877103090286255,-0.22637298703193665,-0.22554859519004822,-0.2767610549926758,-0.21039500832557678,-0.23213660717010498,-0.22912350296974182,-0.22862908244132996,-0.22880229353904724,-0.22889477014541626,-0.22877150774002075,-0.22185304760932922,-0.22872436046600342,-0.2287949025630951,-0.22864317893981934,-0.22869908809661865,-0.22792276740074158,-0.22873133420944214,-0.22865426540374756,-0.22863319516181946,-0.23527073860168457,-0.2207508385181427,-0.22866016626358032,-0.2288864254951477,-0.16405802965164185,-0.2281913459300995,-0.22854995727539062,-0.22865229845046997,-0.22862908244132996,-0.25852304697036743,-0.2185283601284027,-0.22885870933532715,-0.22880098223686218,-0.22835946083068848,-0.220364511013031,-0.2620764672756195,-0.22858178615570068,-0.23839077353477478,-0.2287192940711975,-0.26105597615242004,-0.22867530584335327,-0.22779661417007446,-0.22894591093063354,-0.2425176203250885,-0.22746309638023376,-0.23149117827415466,-0.21484243869781494,-0.2454967200756073,-0.22862908244132996,-0.22866016626358032,-0.2288368046283722,-0.22393298149108887,-0.2216450273990631,-0.22391992807388306,-0.22855940461158752,-0.24306803941726685,-0.2322787344455719,-0.23020154237747192,-0.22873753309249878,-0.22865670919418335,-0.22880709171295166,-0.22847208380699158,-0.27441659569740295,-0.2296624779701233,-0.22873759269714355,-0.22943797707557678,-0.22867849469184875,-0.21780428290367126,-0.26208093762397766,-0.22837349772453308,-0.20016974210739136,-0.2179393470287323,-0.22896257042884827,-0.22876036167144775,-0.2281889021396637,-0.2242106795310974,-0.22871771454811096,-0.22039452195167542,-0.22706347703933716,-0.22924083471298218,-0.22886234521865845,-0.18685659766197205,-0.1912766993045807,-0.2232559621334076,-0.22880735993385315,-0.22178438305854797,-0.22504004836082458,-0.22845077514648438,-0.2291335165500641,-0.2286849021911621,-0.228582501411438,-0.22874990105628967,-0.2856006920337677,-0.22904208302497864,-0.228610098361969,-0.22892722487449646,-0.24475184082984924,-0.22886905074119568,-0.22708100080490112,-0.09457001090049744,-0.2356899082660675,-0.22873759269714355,-0.22849735617637634,-0.23266318440437317,-0.23445910215377808,-0.2287311851978302,-0.2285746932029724,-0.2288581132888794,-0.2286849021911621,-0.23067781329154968,-0.22916388511657715,-0.22837349772453308,-0.23262551426887512,-0.22719112038612366,-0.22849151492118835,-0.22542110085487366,-0.22872987389564514,-0.23678293824195862,-0.19621708989143372,-0.22895348072052002,-0.22849735617637634,-0.23754048347473145,-0.21976721286773682,-0.23480045795440674,-0.22882360219955444,-0.2239381968975067,-0.22907811403274536,-0.2289201021194458,-0.22865229845046997,-0.21528080105781555,-0.23230284452438354,-0.22855940461158752,-0.2286263108253479,-0.23366907238960266,-0.2297271490097046,-0.21633172035217285,-0.21762609481811523,-0.24308118224143982,-0.1966131031513214,-0.23042809963226318,-0.22681540250778198,-0.20085641741752625,-0.22979339957237244,-0.22872987389564514,-0.22878071665763855,-0.2098773717880249,-0.2181590497493744,-0.22812411189079285,-0.25270289182662964,-0.21963787078857422,-0.22865229845046997,-0.18979975581169128,-0.2288891077041626,-0.24125215411186218,-0.22873759269714355,-0.2685159742832184,-0.2418111264705658,-0.22845077514648438,-0.22866111993789673,-0.22879162430763245,-0.22612684965133667,-0.22865229845046997,-0.2286849021911621,-0.2282300889492035,-0.22805386781692505,-0.22950157523155212,-0.2286849021911621,-0.2653551697731018,-0.22888976335525513,-0.22877353429794312,-0.22839626669883728,-0.235878586769104,-0.22849735617637634,-0.21969881653785706,-0.22893193364143372,-0.22866016626358032,-0.22864902019500732,-0.2286849021911621,-0.22855940461158752,-0.2317483127117157,-0.22872987389564514,-0.2262914478778839,-0.22869673371315002,-0.22470667958259583,-0.2308318018913269,-0.10809352993965149,-0.2267162799835205,-0.23301565647125244,-0.2285987138748169,-0.2288263738155365,-0.22872987389564514,-0.22849735617637634,-0.22900891304016113,-0.2287844717502594,-0.22866016626358032,-0.25444459915161133,-0.2287878394126892,-0.2010345458984375,-0.22855252027511597,-0.11652997136116028,-0.22991353273391724,-0.22914007306098938,-0.22872987389564514,-0.22874507308006287,-0.2283882200717926,-0.22873759269714355,-0.22548189759254456,-0.2287985384464264,-0.22547826170921326,-0.22636577486991882,-0.2510862648487091,-0.22897639870643616,-0.22860535979270935,-0.22884780168533325,-0.229042649269104,-0.22038710117340088,-0.2286611795425415,-0.22910815477371216,-0.2150852084159851,-0.22911271452903748,-0.24731147289276123,-0.22865065932273865,-0.22916388511657715,-0.1780983805656433,-0.22927233576774597,-0.2740216553211212,-0.22845077514648438,-0.2290971875190735,-0.2288680374622345,-0.22886961698532104,-0.2284833788871765,-0.23245570063591003,-0.22855731844902039,-0.21121957898139954,-0.22849735617637634,-0.20585989952087402,-0.22161850333213806,-0.28477609157562256,-0.260292649269104,-0.26936495304107666,-0.2206079661846161,-0.3034973740577698,-0.2428080439567566,-0.21338140964508057,-0.22866395115852356,-0.22849735617637634,-0.22188854217529297,-0.22849735617637634,-0.22861379384994507,-0.22842785716056824,-0.22880640625953674,-0.19200417399406433,-0.22876131534576416,-0.23139473795890808,-0.2254728376865387,-0.22849735617637634,-0.22887319326400757,-0.2424222230911255,-0.231871098279953,-0.22648781538009644,-0.22188854217529297,-0.23798292875289917,-0.26151344180107117,-0.22845077514648438,-0.22688955068588257,-0.22837665677070618,-0.2311069667339325,-0.3284125328063965,-0.22638654708862305,-0.2923913598060608,-0.2285497486591339,-0.21434590220451355,-0.22177761793136597,-0.22849735617637634,-0.2286595106124878,-0.22865229845046997,-0.2287197709083557,-0.18585750460624695,-0.23135751485824585,-0.22862908244132996,-0.22878295183181763,-0.21280956268310547,-0.22845077514648438,-0.22849735617637634,-0.22865229845046997,-0.216365784406662],\"z\":[0.2865166664123535,0.28259578347206116,0.282467782497406,0.2820546627044678,0.280590295791626,0.2825690805912018,0.28391072154045105,0.28269004821777344,0.2826535403728485,0.28244900703430176,0.2821351885795593,0.28246116638183594,0.2824460566043854,0.28240084648132324,0.2825735807418823,0.28674808144569397,0.2825048267841339,0.28253650665283203,0.28264299035072327,0.2720532715320587,0.29235297441482544,0.28252604603767395,0.2825908064842224,0.2828339636325836,0.28667712211608887,0.2826497256755829,0.2828916609287262,0.28251785039901733,0.2827117145061493,0.2832392156124115,0.27016085386276245,0.28295987844467163,0.2795277237892151,0.2831103801727295,0.2827361524105072,0.28698715567588806,0.28422170877456665,0.28169623017311096,0.2829737067222595,0.2812725305557251,0.2930153012275696,0.28255191445350647,0.2825642228126526,0.27771806716918945,0.2664201557636261,0.28216734528541565,0.2823282778263092,0.28787708282470703,0.277579665184021,0.2846549451351166,0.2829042077064514,0.2691495418548584,0.2861379384994507,0.2826034426689148,0.26970162987709045,0.2834746837615967,0.2608846426010132,0.28265684843063354,0.2851710915565491,0.2825735807418823,0.28479164838790894,0.28212812542915344,0.28249022364616394,0.2823382019996643,0.28254345059394836,0.2723589837551117,0.285489946603775,0.27218762040138245,0.2769978940486908,0.28244900703430176,0.282511830329895,0.29331833124160767,0.28347721695899963,0.2823667526245117,0.2698766887187958,0.28270789980888367,0.28255191445350647,0.28259435296058655,0.27922847867012024,0.28255191445350647,0.2827299237251282,0.28255191445350647,0.28247031569480896,0.283768892288208,0.21962660551071167,0.28397563099861145,0.28278160095214844,0.28095200657844543,0.28276222944259644,0.2849664092063904,0.28255191445350647,0.2824569344520569,0.2829897701740265,0.2875736951828003,0.28253650665283203,0.28255465626716614,0.2824293076992035,0.2815167307853699,0.28284752368927,0.2876463234424591,0.28255191445350647,0.2824532091617584,0.25774243474006653,0.2824297845363617,0.28057751059532166,0.2782250642776489,0.2826123833656311,0.2788335084915161,0.27806007862091064,0.28272297978401184,0.2828776240348816,0.2829749882221222,0.28271859884262085,0.27397191524505615,0.25589174032211304,0.28361472487449646,0.2866400480270386,0.28369659185409546,0.284892737865448,0.2824823558330536,0.2828916609287262,0.2827345132827759,0.2767823338508606,0.28258848190307617,0.30362799763679504,0.2823667526245117,0.28611406683921814,0.28253650665283203,0.2827238142490387,0.2825492024421692,0.2833571434020996,0.2832711637020111,0.28263336420059204,0.2819020450115204,0.28286150097846985,0.3008219301700592,0.2812928855419159,0.29199719429016113,0.27596303820610046,0.2775505781173706,0.28252333402633667,0.28244900703430176,0.27806007862091064,0.2895529866218567,0.2814202606678009,0.28964555263519287,0.2776506543159485,0.2851095199584961,0.2824709117412567,0.28595754504203796,0.28269314765930176,0.28253650665283203,0.26809582114219666,0.2825571298599243,0.28255191445350647,0.2749687433242798,0.28253650665283203,0.28244900703430176,0.2791346311569214,0.28250813484191895,0.37679433822631836,0.2913759648799896,0.2770332098007202,0.2826685607433319,0.2981794476509094,0.28267720341682434,0.27032607793807983,0.2823667526245117,0.282545804977417,0.27569901943206787,0.277145117521286,0.2826763093471527,0.2902526259422302,0.28259700536727905,0.28487908840179443,0.28246498107910156,0.2858194410800934,0.28254154324531555,0.2800361216068268,0.2831485867500305,0.27152547240257263,0.28278547525405884,0.28064587712287903,0.28261518478393555,0.2825334668159485,0.29572704434394836,0.2988404333591461,0.283687025308609,0.2803513705730438,0.282537043094635,0.2823667526245117,0.28220462799072266,0.2873383164405823,0.2824932038784027,0.2829555869102478,0.2825675308704376,0.28285297751426697,0.2827887535095215,0.2823667526245117,0.2825571298599243,0.28261083364486694,0.2827284634113312,0.2802167236804962,0.2826601564884186,0.2868666350841522,0.28257110714912415,0.2608923316001892,0.2847942113876343,0.28296616673469543,0.28244900703430176,0.2823738753795624,0.2877173125743866,0.2796914279460907,0.2826084494590759,0.28249022364616394,0.2826085686683655,0.26793381571769714,0.27102887630462646,0.28244900703430176,0.2818455100059509,0.28505265712738037,0.2864464223384857,0.2825735807418823,0.28262078762054443,0.2826331853866577,0.28183063864707947,0.2778347432613373,0.2825523316860199,0.28265446424484253,0.2819274663925171,0.2826155424118042,0.28982552886009216,0.28273653984069824,0.2827296853065491,0.27825844287872314,0.2843245267868042,0.2839241623878479,0.28288233280181885,0.2828916609287262,0.28253650665283203,0.28255191445350647,0.2831328809261322,0.2824460566043854,0.2823678255081177,0.28249022364616394,0.2826085686683655,0.2811104357242584,0.2823663055896759,0.2826879024505615,0.28429874777793884,0.2866622805595398,0.28265461325645447,0.28247523307800293,0.28255191445350647,0.28269100189208984,0.25701481103897095,0.2782624363899231,0.2826497256755829,0.27909794449806213,0.28271540999412537,0.28289926052093506,0.2814197540283203,0.2840801179409027,0.2828916609287262,0.28417861461639404,0.28280746936798096,0.28243473172187805,0.2827523350715637,0.2867327630519867,0.2821822464466095,0.2823924720287323,0.2825169563293457,0.28736424446105957,0.2825028598308563,0.2826911509037018,0.2825738191604614,0.2903566360473633,0.2826487421989441,0.2908042371273041,0.28288665413856506,0.28250381350517273,0.2825371325016022,0.29030364751815796,0.28243136405944824,0.2818431258201599,0.286479115486145,0.29070329666137695,0.28249022364616394,0.28255191445350647,0.2839089632034302,0.2856962978839874,0.28844815492630005,0.28273805975914,0.2634103000164032,0.29409104585647583,0.28253650665283203,0.2911149859428406,0.2826085686683655,0.2825391888618469,0.28680792450904846,0.28750500082969666,0.28470924496650696,0.2825947403907776,0.282499760389328,0.2828916609287262,0.2827887535095215,0.28209617733955383,0.28350022435188293,0.27525147795677185,0.3296838402748108,0.28309518098831177,0.2884518802165985,0.2829861342906952,0.28369393944740295,0.2823667526245117,0.28244900703430176,0.3039033114910126,0.2826497256755829,0.28285321593284607,0.2832358181476593,0.282054603099823,0.27262988686561584,0.2826646864414215,0.38162997364997864,0.2827984094619751,0.28712788224220276,0.2827307879924774,0.2824181020259857,0.28339648246765137,0.2825717031955719,0.2825342118740082,0.28186658024787903,0.29456228017807007,0.28264960646629333,0.28601476550102234,0.2829591929912567,0.27236902713775635,0.2829221189022064,0.28164032101631165,0.2827567756175995,0.27979326248168945,0.2827160358428955,0.2825735807418823,0.2824181020259857,0.28253650665283203,0.2826644778251648,0.2825838625431061,0.2648831307888031,0.2592688202857971,0.2825213372707367,0.2821529805660248,0.2831213176250458,0.28271540999412537,0.2828715443611145,0.2826427221298218,0.2826770842075348,0.28265833854675293,0.2826085686683655,0.2825596034526825,0.2824537456035614,0.2823241651058197,0.2826816737651825,0.2819964289665222,0.2824350595474243,0.2873742878437042,0.28255191445350647,0.2852816581726074,0.28246527910232544,0.28253650665283203,0.28181713819503784,0.2824309766292572,0.28282463550567627,0.28241291642189026,0.2853386700153351,0.2830750346183777,0.2793837785720825,0.2776864469051361,0.27806007862091064,0.2824920415878296,0.28542718291282654,0.28249022364616394,0.2822704017162323,0.2846739590167999,0.25039568543434143,0.2825984060764313,0.2825135886669159,0.28126341104507446,0.28316232562065125,0.26304230093955994,0.28688281774520874,0.28267955780029297,0.28327128291130066,0.28392931818962097,0.28139975666999817,0.2824181020259857,0.28253650665283203,0.28253650665283203,0.28271499276161194,0.2827018201351166,0.2784949839115143,0.2827216386795044,0.28249022364616394,0.2888602912425995,0.28287187218666077,0.2847655415534973,0.2837710678577423,0.2823667526245117,0.2823667526245117,0.2864474952220917,0.28091320395469666,0.28057411313056946,0.2833045721054077,0.28253650665283203,0.2800065279006958,0.2813212275505066,0.28253650665283203,0.28242337703704834,0.2974059581756592,0.2827255129814148,0.2816215753555298,0.2841973304748535,0.2830928862094879,0.2827887535095215,0.278858482837677,0.28276386857032776,0.28357431292533875,0.27912598848342896,0.2836569845676422,0.29012277722358704,0.2822831869125366,0.28295862674713135,0.24341750144958496,0.28162604570388794,0.27757105231285095,0.2827109396457672,0.2826736569404602,0.2828560769557953,0.28848618268966675,0.28148460388183594,0.28658103942871094,0.28486108779907227,0.28266578912734985,0.2880413234233856,0.28272488713264465,0.28056079149246216,0.2834908962249756,0.2827639579772949,0.27448639273643494,0.2829672396183014,0.2825571298599243,0.28481730818748474,0.28606361150741577,0.2800901234149933,0.2763490676879883,0.2825735807418823,0.2828793525695801,0.28265348076820374,0.2824728488922119,0.28255191445350647,0.2826401889324188,0.25957560539245605,0.28266048431396484,0.28275617957115173,0.28738075494766235,0.2763895094394684,0.28268760442733765,0.28333595395088196,0.282535582780838,0.28190410137176514,0.318545937538147,0.2861836850643158,0.2719535231590271,0.2825735807418823,0.23431594669818878,0.2825932800769806,0.2825866639614105,0.2841973602771759,0.2826922833919525,0.2828916609287262,0.28390932083129883,0.2825000584125519,0.28242990374565125,0.269509494304657,0.2791211009025574,0.28244900703430176,0.2814643681049347,0.2823667526245117,0.28253650665283203,0.28987741470336914,0.28265488147735596,0.2921978831291199,0.2825735807418823,0.3259468078613281,0.24103958904743195,0.28245678544044495,0.28475937247276306,0.2832479774951935,0.28343865275382996,0.2660127580165863,0.2824181020259857,0.28253650665283203,0.28249022364616394,0.2824213206768036,0.2802697718143463,0.28359949588775635,0.28257817029953003,0.28250935673713684,0.2706175744533539,0.28039899468421936,0.2825625538825989,0.2825959026813507,0.28244253993034363,0.28126078844070435,0.2840026319026947,0.2825571298599243,0.28284740447998047,0.282667338848114,0.2828916609287262,0.28753089904785156,0.25889694690704346,0.2828916609287262,0.2833409011363983,0.28041350841522217,0.28244900703430176,0.28266698122024536,0.28260454535484314,0.2905120849609375,0.29752472043037415,0.2778828740119934,0.28319093585014343,0.2843657433986664,0.29442816972732544,0.2790282666683197,0.2828231453895569,0.28226208686828613,0.2845757007598877,0.2827245891094208,0.28244900703430176,0.282544881105423,0.28276270627975464,0.2337549328804016,0.28253650665283203,0.2810501158237457,0.2824341654777527,0.28669455647468567,0.2831364572048187,0.22342485189437866,0.284211665391922,0.2829357981681824,0.28283533453941345,0.2839139699935913,0.28243210911750793,0.2831968367099762,0.28671693801879883,0.2807113826274872,0.28383326530456543,0.2826085686683655,0.2818959057331085,0.28251728415489197,0.2825824022293091,0.2801288664340973,0.28254780173301697,0.2827887535095215,0.2959150969982147,0.2825740575790405,0.2794326841831207,0.28225165605545044,0.2825735807418823,0.28253650665283203,0.282647967338562,0.28250521421432495,0.28255191445350647,0.28283628821372986,0.28237107396125793,0.28122565150260925,0.2820054292678833,0.28253209590911865,0.2830021381378174,0.28301313519477844,0.28342393040657043,0.2863796651363373,0.28253546357154846,0.28362134099006653,0.28248322010040283,0.2825735807418823,0.2843703031539917,0.2908190190792084,0.2796887457370758,0.2827056348323822,0.2671109735965729,0.2839498817920685,0.3313974440097809,0.28686293959617615,0.282430499792099,0.2750096619129181,0.26206719875335693,0.2846742570400238,0.2827018201351166,0.28003427386283875,0.2825735807418823,0.29081299901008606,0.28228917717933655,0.27464479207992554,0.28255191445350647,0.31236904859542847,0.28255191445350647,0.2826675474643707,0.2841333746910095,0.28253650665283203,0.2843649685382843,0.2844938635826111,0.3144937753677368,0.2807671129703522,0.28262627124786377,0.28228962421417236,0.2828253507614136,0.284365177154541,0.2786829173564911,0.28299668431282043,0.28249216079711914,0.2817586064338684,0.2825735807418823,0.2827887535095215,0.2898731231689453,0.2837233543395996,0.28260666131973267,0.2867544889450073,0.28648456931114197,0.2826470136642456,0.2894754707813263,0.28397080302238464,0.28569379448890686,0.2824940085411072,0.28124672174453735,0.2826228141784668,0.2829633355140686,0.28249016404151917,0.28259071707725525,0.2835138738155365,0.2811123728752136,0.28253650665283203,0.28244900703430176,0.28249022364616394,0.28219425678253174,0.2821803689002991,0.28400692343711853,0.2910587787628174,0.28244900703430176,0.28792738914489746,0.281028151512146,0.27824866771698,0.2825988233089447,0.28246310353279114,0.281930148601532,0.2824181020259857,0.27854207158088684,0.27806007862091064,0.28313010931015015,0.2835271656513214,0.28682631254196167,0.28264299035072327,0.2814171612262726,0.2813063859939575,0.2826116979122162,0.28281170129776,0.35693737864494324,0.2829677164554596,0.28266292810440063,0.2824181020259857,0.2825574278831482,0.28159260749816895,0.2835370898246765,0.28252407908439636,0.2825615108013153,0.28244900703430176,0.29354891180992126,0.28285646438598633,0.27308452129364014,0.362241268157959,0.26668497920036316,0.2828916609287262,0.2841050922870636,0.27523860335350037,0.2826297879219055,0.2811545133590698,0.29729965329170227,0.2916654348373413,0.27544745802879333,0.28270572423934937,0.2826552391052246,0.28265488147735596,0.2826766073703766,0.2798469364643097,0.28450942039489746,0.29584646224975586,0.28244900703430176,0.28250396251678467,0.28674978017807007,0.2757425606250763,0.2826935946941376,0.28236129879951477,0.28249022364616394,0.2843717634677887,0.28171440958976746,0.28438225388526917,0.28129515051841736,0.2826034426689148,0.2826085686683655,0.2828867435455322,0.28255191445350647,0.2830680012702942,0.2838509678840637,0.2826124429702759,0.28257760405540466,0.2818029224872589,0.280562162399292,0.28143903613090515,0.2775254547595978,0.2843268811702728,0.27806007862091064,0.2625347077846527,0.2824518084526062,0.28287357091903687,0.27893227338790894,0.28268834948539734,0.2803555428981781,0.28244900703430176,0.2840271592140198,0.2594628632068634,0.2815752923488617,0.28242895007133484,0.2823432683944702,0.2843300700187683,0.28244900703430176,0.28051602840423584,0.2820531725883484,0.2825563848018646,0.2827965021133423,0.29779699444770813,0.2826034426689148,0.28253650665283203,0.2824104428291321,0.2826869487762451,0.28047919273376465,0.28838881850242615,0.2829545736312866,0.28253650665283203,0.2801714241504669,0.2825987935066223,0.2826472818851471,0.2824181020259857,0.2978331446647644,0.28264760971069336,0.2881004810333252,0.28213852643966675,0.28266608715057373,0.285106360912323,0.2827150821685791,0.2828916609287262,0.28415966033935547,0.28255191445350647,0.2830388844013214,0.2828141152858734,0.28345295786857605,0.282536119222641,0.2824181020259857,0.30138909816741943,0.27803948521614075,0.28255191445350647,0.2795935571193695,0.28244900703430176,0.28265488147735596,0.2840642035007477,0.2795967757701874,0.28263112902641296,0.2022823542356491,0.3056786358356476,0.28319135308265686,0.28267860412597656,0.2827502489089966,0.28272685408592224,0.2854177951812744,0.2826085686683655,0.28188079595565796,0.28196412324905396,0.2881324291229248,0.283183753490448,0.28268423676490784,0.2824799120426178,0.3223424255847931,0.27481314539909363,0.2827208340167999,0.28255191445350647,0.28188616037368774,0.2812446355819702,0.28271979093551636,0.2864271402359009,0.2828916609287262,0.2817654311656952,0.28255191445350647,0.28264009952545166,0.2825601398944855,0.28123024106025696,0.283718079328537,0.28244900703430176,0.2786339223384857,0.280887633562088,0.2825441062450409,0.2826719582080841,0.2826736569404602,0.2824181020259857,0.2863481640815735,0.2826404273509979,0.28253310918807983,0.28222817182540894,0.2826419472694397,0.2828916609287262,0.28268754482269287,0.2714211344718933,0.2824181020259857,0.2826034426689148,0.28244900703430176,0.2918034791946411,0.2827657461166382,0.2825484871864319,0.28279909491539,0.28285399079322815,0.2829286456108093,0.2825753092765808,0.28105977177619934,0.2828647792339325,0.28481003642082214,0.2828916609287262,0.2825179994106293,0.2835691571235657,0.28265079855918884,0.2871859073638916,0.2826061248779297,0.2824181020259857,0.28275182843208313,0.28253650665283203,0.2892444431781769,0.2826085686683655,0.28276491165161133,0.2681160569190979,0.2871916890144348,0.28253650665283203,0.280413955450058,0.28435617685317993,0.2827450931072235,0.2812369167804718,0.27989545464515686,0.28260737657546997,0.27627041935920715,0.28607240319252014,0.2735847532749176,0.28265610337257385,0.28332096338272095,0.2552835941314697,0.27806007862091064,0.28466010093688965,0.27794384956359863,0.28268617391586304,0.2825588285923004,0.28392094373703003,0.28255191445350647,0.30460330843925476,0.28084510564804077,0.299188494682312,0.2825362980365753,0.2820814251899719,0.2843053936958313,0.28386732935905457,0.2818653881549835,0.29004591703414917,0.2882833778858185,0.2824181020259857,0.28252553939819336,0.28359273076057434,0.2828493118286133,0.2820430099964142,0.28242769837379456,0.28255191445350647,0.29202955961227417,0.2881219685077667,0.27923595905303955,0.28265127539634705,0.2821030914783478,0.28261786699295044,0.2833639085292816,0.2828916609287262,0.2804521322250366,0.27824610471725464,0.30506783723831177,0.3108264207839966,0.28253650665283203,0.28274115920066833,0.2800822854042053,0.2825387120246887,0.28235456347465515,0.2847115099430084,0.26604869961738586,0.28268423676490784,0.28637880086898804,0.3023671805858612,0.28253650665283203,0.2825111150741577,0.29840588569641113,0.2828916609287262,0.28278276324272156,0.2831421196460724,0.28281712532043457,0.2826034426689148,0.2800515592098236,0.2759065628051758,0.2831544876098633,0.28239697217941284,0.2827669084072113,0.2874458432197571,0.2828395366668701,0.2749720811843872,0.28271764516830444,0.2888997793197632,0.2896021604537964,0.28276774287223816,0.2827152907848358,0.28255191445350647,0.28594642877578735,0.28662270307540894,0.2834419906139374,0.2827017903327942,0.28551068902015686,0.2831186056137085,0.2743794322013855,0.27949920296669006,0.2844565510749817,0.29089078307151794,0.3133695423603058,0.2827593982219696,0.2813641428947449,0.2823667526245117,0.26687344908714294,0.3189917802810669,0.2825755774974823,0.2772635221481323,0.28262361884117126,0.28271615505218506,0.27931851148605347,0.28197336196899414,0.28255191445350647,0.28083711862564087,0.28633707761764526,0.28343501687049866,0.2800633907318115,0.2825860381126404,0.28259655833244324,0.29931843280792236,0.2824181020259857,0.2825659513473511,0.2825101613998413,0.2856568992137909,0.28414344787597656,0.2659308910369873,0.26002663373947144,0.28276678919792175,0.2826758325099945,0.2781503200531006,0.2884558141231537,0.2820936143398285,0.2838161885738373,0.28257107734680176,0.28263938426971436,0.28287428617477417,0.284313827753067,0.2947305738925934,0.2826518714427948,0.2828916609287262,0.23817555606365204,0.282432496547699,0.2838086485862732,0.28253650665283203,0.28261908888816833,0.2801693379878998,0.28253650665283203,0.2825543284416199,0.28397536277770996,0.2819943130016327,0.28249022364616394,0.28332704305648804,0.2844146490097046,0.282401442527771,0.2767505943775177,0.2820112109184265,0.2800832986831665,0.2596149444580078,0.2826792001724243,0.28199222683906555,0.28255486488342285,0.2824181020259857,0.28255191445350647,0.2845778465270996,0.28861692547798157,0.2828916609287262,0.28273043036460876,0.28244900703430176,0.2749515175819397,0.30792590975761414,0.2826497256755829,0.2821672856807709,0.27276530861854553,0.28244900703430176,0.2848665118217468,0.2957308888435364,0.2822311818599701,0.28249022364616394,0.2826497256755829,0.2826218605041504,0.28273409605026245,0.2828490138053894,0.28226619958877563,0.2825774550437927,0.2941584289073944,0.2823543846607208,0.2824788987636566,0.2812877893447876,0.27907007932662964,0.28264299035072327,0.2828362286090851,0.26310423016548157,0.28243356943130493,0.28341126441955566,0.26306167244911194,0.28244900703430176,0.3067363500595093,0.290977418422699,0.2856546938419342,0.28269249200820923,0.2829611897468567,0.28507745265960693,0.2936139702796936,0.28519517183303833,0.28293362259864807,0.28422272205352783,0.28778037428855896,0.28240251541137695,0.27062293887138367,0.2827117443084717,0.28283968567848206,0.2826138734817505,0.28675612807273865,0.28264257311820984,0.2826341688632965,0.2741880416870117,0.2792953550815582,0.28004759550094604,0.2810436487197876,0.2825455069541931,0.2823667526245117,0.2825891971588135,0.28269320726394653,0.2825571298599243,0.28255993127822876,0.2824800908565521,0.28018590807914734,0.28245246410369873,0.288081556558609,0.28188395500183105,0.28252214193344116,0.2824524939060211,0.2827111482620239,0.28269514441490173,0.2847836911678314,0.28252893686294556,0.28244900703430176,0.28244900703430176,0.28195032477378845,0.28253650665283203,0.28491631150245667,0.2800610065460205,0.27924486994743347,0.2825571298599243,0.28255245089530945,0.27913010120391846,0.28255191445350647,0.2831456661224365,0.28172028064727783,0.28249210119247437,0.28258275985717773,0.26874852180480957,0.2826778292655945,0.303670734167099,0.2829029858112335,0.28244900703430176,0.28253650665283203,0.2824181020259857,0.2828916609287262,0.27265334129333496,0.2826700508594513,0.28689390420913696,0.282259076833725,0.28259706497192383,0.2831288278102875,0.28582751750946045,0.28107139468193054,0.2838411331176758,0.2810094952583313,0.28253650665283203,0.2905157506465912,0.2812218964099884,0.28771159052848816,0.28265872597694397,0.282564640045166,0.28111132979393005,0.28198352456092834,0.28990960121154785,0.28244900703430176,0.2826497256755829,0.28849634528160095,0.282715380191803,0.28249022364616394,0.2826496660709381,0.28253650665283203,0.28255051374435425,0.2786470651626587,0.283814936876297,0.2933846414089203,0.28263917565345764,0.2872924506664276,0.3211125135421753,0.2854575514793396,0.28270408511161804,0.27278244495391846,0.2815481126308441,0.28259941935539246,0.2788451910018921,0.2928546369075775,0.2827270030975342,0.29169291257858276,0.2837904691696167,0.28268566727638245,0.28244900703430176,0.2827647030353546,0.2823815941810608,0.28253650665283203,0.39514532685279846,0.28251543641090393,0.28253650665283203,0.29641395807266235,0.2841933071613312,0.2663339376449585,0.28255191445350647,0.28295063972473145,0.28253650665283203,0.27455219626426697,0.2878236472606659,0.28255191445350647,0.28244900703430176,0.28300368785858154,0.2826403081417084,0.28267544507980347,0.28244900703430176,0.28267601132392883,0.283771276473999,0.2826765775680542,0.3096291422843933,0.27806007862091064,0.27806007862091064,0.2828916609287262,0.2834663391113281,0.3306193947792053,0.28261899948120117,0.2754494845867157,0.2931264340877533,0.28239163756370544,0.28258463740348816,0.2828465402126312,0.2825077772140503,0.28750696778297424,0.28249022364616394,0.28481003642082214,0.28314176201820374,0.28253650665283203,0.2831903100013733,0.2825735807418823,0.28343990445137024,0.28835296630859375,0.3014344573020935,0.28320106863975525,0.2819004952907562,0.28453582525253296,0.2830897271633148,0.28273847699165344,0.31297817826271057,0.28785383701324463,0.282638818025589,0.2829524576663971,0.2829892635345459,0.2843220829963684,0.2826277017593384,0.28255781531333923,0.28327322006225586,0.28244900703430176,0.28253650665283203,0.27806007862091064,0.2911858856678009,0.28292936086654663,0.28344330191612244,0.2854732573032379,0.2802864909172058,0.27806007862091064,0.28272590041160583,0.2935788929462433,0.28110912442207336,0.28259339928627014,0.28244900703430176,0.2722456753253937,0.28161248564720154,0.28249141573905945,0.2825735807418823,0.2821211814880371,0.28253650665283203,0.27903589606285095,0.28668689727783203,0.28256210684776306,0.28350743651390076,0.2825799882411957,0.30759894847869873,0.28253650665283203,0.2816586494445801,0.28234636783599854,0.28270524740219116,0.2833746671676636,0.28296369314193726,0.2797418534755707,0.28208789229393005,0.2829074263572693,0.28255191445350647,0.281315416097641,0.28744184970855713,0.28210198879241943,0.2826552987098694,0.28040480613708496,0.28270241618156433,0.2785499393939972,0.28249257802963257,0.28621906042099,0.2827149033546448,0.2874581515789032,0.27985355257987976,0.2892148494720459,0.2824886739253998,0.2904343903064728,0.2826034426689148,0.2824336886405945,0.27496063709259033,0.2802485227584839,0.2960260212421417,0.2825571298599243,0.28271088004112244,0.28326666355133057,0.28269150853157043,0.2828451693058014,0.29049763083457947,0.2854587435722351,0.28244900703430176,0.2821715772151947,0.2762865424156189,0.28253650665283203,0.28247958421707153,0.28262099623680115,0.2811152935028076,0.28258922696113586,0.31445345282554626,0.28255191445350647,0.27950596809387207,0.28288301825523376,0.28253650665283203,0.3054833710193634,0.2828966975212097,0.2789396345615387,0.28253650665283203,0.28253650665283203,0.26155325770378113,0.28244900703430176,0.3762674033641815,0.2814444899559021,0.2837338149547577,0.27705350518226624,0.282539039850235,0.2817413806915283,0.27918165922164917,0.2867666482925415,0.28279775381088257,0.2828367352485657,0.28685012459754944,0.28620484471321106,0.2823544442653656,0.2828376293182373,0.2818675935268402,0.2902897894382477,0.28255191445350647,0.2857441008090973,0.28253650665283203,0.282927006483078,0.2922167181968689,0.2711191177368164,0.2829848825931549,0.2774246335029602,0.28253650665283203,0.2828916609287262,0.2787213623523712,0.2843555808067322,0.28240031003952026,0.28244900703430176,0.2825538218021393,0.28253650665283203,0.2825571298599243,0.2830232083797455,0.282697468996048,0.2828396260738373,0.28284183144569397,0.2801324129104614,0.2827097475528717,0.2825571298599243,0.2839016318321228,0.2826762795448303,0.2829286754131317,0.28260520100593567,0.27570641040802,0.29877927899360657,0.2841191291809082,0.28244900703430176,0.2783784866333008,0.28282248973846436,0.28260624408721924,0.2838023602962494,0.28262776136398315,0.28235673904418945,0.28240805864334106,0.28147852420806885,0.2887303829193115,0.2829633355140686,0.284891813993454,0.28633689880371094,0.29199960827827454,0.2825424075126648,0.2826476991176605,0.27806007862091064,0.2832147479057312,0.28263166546821594,0.2814439535140991,0.2825673520565033,0.28280311822891235,0.28249022364616394,0.28261053562164307,0.28244900703430176,0.28255191445350647,0.2825578451156616,0.2794254720211029,0.28306788206100464,0.2952755093574524,0.2827723026275635,0.2829599678516388,0.28259408473968506,0.29054081439971924,0.2768462896347046,0.2825571596622467,0.2755247950553894,0.28253650665283203,0.2829194962978363,0.2795793414115906,0.28822991251945496,0.28244900703430176,0.28559795022010803,0.2825073003768921,0.2807044982910156,0.2826007008552551,0.2798967659473419,0.28259727358818054,0.28255191445350647,0.2885916531085968,0.28244900703430176,0.3192599415779114,0.2969270646572113,0.28255191445350647,0.28296616673469543,0.2835797667503357,0.2828916609287262,0.2828916609287262,0.2812289893627167,0.2825220823287964,0.28244900703430176,0.2826034426689148,0.280644029378891,0.2844754457473755,0.2826497256755829,0.2805744409561157,0.28244900703430176,0.28734275698661804,0.2831893563270569,0.26544928550720215,0.28255191445350647,0.2826497256755829,0.2826646566390991,0.2812078297138214,0.282799631357193,0.2879537045955658,0.2762175500392914,0.28655242919921875,0.2845642566680908,0.28333279490470886,0.28247198462486267,0.28285646438598633,0.28244900703430176,0.2828916609287262,0.28255191445350647,0.28270870447158813,0.28253650665283203,0.2826625406742096,0.28267842531204224,0.28253650665283203,0.28253650665283203,0.29444873332977295,0.29634442925453186,0.28291991353034973,0.2824171483516693,0.2828149199485779,0.28254541754722595,0.28261712193489075,0.2828916609287262,0.28247663378715515,0.3211357295513153,0.2826497256755829,0.28276631236076355,0.28197842836380005,0.2784149944782257,0.2826901078224182,0.28253650665283203,0.28942832350730896,0.28260672092437744,0.2826347053050995,0.2837562561035156,0.28265705704689026,0.28270041942596436,0.28264686465263367,0.29366087913513184,0.28257083892822266,0.2818604111671448,0.28253650665283203,0.28794556856155396,0.28253650665283203,0.28370028734207153,0.2808116376399994,0.271064430475235,0.27806007862091064,0.28253650665283203,0.2825735807418823,0.28282609581947327,0.3171248733997345,0.28269311785697937,0.2827887535095215,0.28274545073509216,0.2828916609287262,0.28823450207710266,0.28273195028305054,0.2826630175113678,0.27461519837379456,0.28265711665153503,0.2844756543636322,0.29349759221076965,0.2824572026729584,0.27769961953163147,0.2774569094181061,0.29258787631988525,0.2827887535095215,0.28249022364616394,0.2855394780635834,0.28270047903060913,0.28266122937202454,0.2825140655040741,0.28098541498184204,0.28257614374160767,0.2833101749420166,0.28276896476745605,0.28154265880584717,0.2828807532787323,0.28255191445350647,0.28285786509513855,0.282418429851532,0.28253650665283203,0.2523685395717621,0.27874505519866943,0.28259459137916565,0.28287890553474426,0.28383591771125793,0.28255191445350647,0.2790546715259552,0.2823199927806854,0.28253650665283203,0.2823847830295563,0.28246426582336426,0.2571912705898285,0.290577232837677,0.28718847036361694,0.2825765609741211,0.2826034426689148,0.282800555229187,0.28467056155204773,0.28244900703430176,0.2827700674533844,0.28271719813346863,0.2826526165008545,0.27747073769569397,0.28255191445350647,0.28258445858955383,0.29235512018203735,0.28252241015434265,0.2834045886993408,0.2792036533355713,0.2969481348991394,0.28537777066230774,0.2824181020259857,0.28001368045806885,0.283681184053421,0.2824968695640564,0.2822584807872772,0.2841748595237732,0.28255191445350647,0.28255191445350647,0.2837485373020172,0.2826739251613617,0.28244900703430176,0.37024855613708496,0.2863301932811737,0.28293803334236145,0.28278544545173645,0.2783827483654022,0.2712743878364563,0.2825735807418823,0.2826557159423828,0.2949010133743286,0.2827887535095215,0.2732640504837036,0.2826065123081207,0.28171882033348083,0.2827688157558441,0.28505730628967285,0.28341737389564514,0.2768346071243286,0.28425243496894836,0.2792823910713196,0.28612059354782104,0.28253650665283203,0.28244656324386597,0.27953866124153137,0.2801593542098999,0.2826121747493744,0.2793043255805969,0.28480640053749084,0.28928375244140625,0.2845652401447296,0.2819218933582306,0.27852344512939453,0.28289172053337097,0.2837907671928406,0.28920963406562805,0.28559672832489014,0.2825474143028259,0.2828128933906555,0.2895559072494507,0.28262069821357727,0.2805466949939728,0.281662255525589,0.2818305194377899,0.28689277172088623,0.28183263540267944,0.2827185392379761,0.28265121579170227,0.28558745980262756,0.2819216549396515,0.285789430141449,0.28144899010658264,0.27806007862091064,0.27172204852104187,0.2682948410511017,0.2826363742351532,0.28819388151168823,0.27819401025772095,0.2826704680919647,0.2789948582649231,0.28264105319976807,0.28268083930015564,0.2826721668243408,0.2880476415157318,0.2814774811267853,0.2867087423801422,0.28285107016563416,0.26014941930770874,0.2823667526245117,0.28259554505348206,0.2763936221599579,0.2796247601509094,0.28254517912864685,0.2833177149295807,0.2830270826816559,0.2824631929397583,0.29941973090171814,0.28257128596305847,0.2828916609287262,0.29454532265663147,0.2827005088329315,0.28246957063674927,0.2828916609287262,0.28317010402679443,0.2830134332180023,0.2824462056159973,0.28258535265922546,0.28244900703430176,0.28274646401405334,0.2813662588596344,0.2826985716819763,0.28255191445350647,0.28266194462776184,0.28221362829208374,0.29047632217407227,0.28271880745887756,0.2826497256755829,0.28286299109458923,0.2830408215522766,0.2819150388240814,0.284757524728775,0.28255191445350647,0.2827253043651581,0.28253453969955444,0.2824682593345642,0.2828916609287262,0.28307265043258667,0.2850029766559601,0.25747716426849365,0.28268319368362427,0.25894758105278015,0.2826467454433441,0.2828880250453949,0.2829017639160156,0.2870704233646393,0.27972522377967834,0.2651662230491638,0.28178009390830994,0.27070581912994385,0.28244900703430176,0.34675806760787964,0.28879299759864807,0.28249499201774597,0.2824181020259857,0.2810561954975128,0.28253650665283203,0.28255191445350647,0.27806007862091064,0.2828916609287262,0.28254827857017517,0.28255191445350647,0.29866617918014526,0.28251132369041443,0.2825257480144501,0.2826577425003052,0.2812541127204895,0.28272590041160583,0.2820051312446594,0.2826341986656189,0.2826691269874573,0.29113292694091797,0.2839416563510895,0.2819441258907318,0.28263163566589355,0.28189870715141296,0.28253650665283203,0.28280964493751526,0.28244900703430176,0.27774640917778015,0.28284257650375366,0.28253650665283203,0.27543070912361145,0.27806007862091064,0.2825571298599243,0.282705157995224,0.28210315108299255,0.2827928066253662,0.2835349142551422,0.2898506820201874,0.28270620107650757,0.3064579665660858,0.2825792729854584,0.28334999084472656,0.2821524441242218,0.20926712453365326,0.2826597988605499,0.28173843026161194,0.2934233844280243,0.28240421414375305,0.2831653952598572,0.27777692675590515,0.28252172470092773,0.28141486644744873,0.28274911642074585,0.28483957052230835,0.28012707829475403,0.28275659680366516,0.2826497256755829,0.2826167941093445,0.27806007862091064,0.2811427712440491,0.2826085686683655,0.28267326951026917,0.30379506945610046,0.28244251012802124,0.2826477289199829,0.272006094455719,0.2828916609287262,0.2997286021709442,0.2827319800853729,0.28244900703430176,0.28255191445350647,0.2825605869293213,0.2978229820728302,0.2834223210811615,0.2825830578804016,0.28250205516815186,0.32457658648490906,0.29852351546287537,0.28244900703430176,0.2826085686683655,0.271312415599823,0.2842443585395813,0.2826085686683655,0.28205394744873047,0.2823667526245117,0.27412229776382446,0.2826034426689148,0.2826034426689148,0.2756977677345276,0.28246206045150757,0.2828916609287262,0.2831410765647888,0.2878096401691437,0.28254419565200806,0.28298112750053406,0.2828916609287262,0.28249379992485046,0.28280261158943176,0.27846649289131165,0.28265488147735596,0.28254377841949463,0.2893441617488861,0.2826339602470398,0.29168400168418884,0.28255191445350647,0.28257253766059875,0.2853637635707855,0.28249022364616394,0.28145208954811096,0.28297674655914307,0.2830773591995239,0.28234946727752686,0.2691636383533478,0.28249022364616394,0.28244900703430176,0.33028918504714966,0.304856538772583,0.2828916609287262,0.28253650665283203,0.2806969881057739,0.284824401140213,0.280886709690094,0.2830057442188263,0.28301364183425903,0.28253650665283203,0.27214738726615906,0.2833196818828583,0.28244900703430176,0.29406577348709106,0.28266817331314087,0.27806007862091064,0.28244900703430176,0.2825108468532562,0.27967846393585205,0.2826460003852844,0.2826378643512726,0.2822902798652649,0.2816520035266876,0.25976917147636414,0.28172382712364197,0.2825833857059479,0.28230175375938416,0.28093206882476807,0.3046313524246216,0.2814069390296936,0.2848546802997589,0.28251174092292786,0.28253650665283203,0.28267544507980347,0.2826962471008301,0.2826295495033264,0.27898868918418884,0.282601922750473,0.2826108932495117,0.28255200386047363,0.2825758457183838,0.28278011083602905,0.28273680806159973,0.2826260030269623,0.2825608253479004,0.28448769450187683,0.28452545404434204,0.2825571298599243,0.2814796566963196,0.2552483379840851,0.2823009788990021,0.2824687957763672,0.28255191445350647,0.28253650665283203,0.2959345579147339,0.2782845199108124,0.2827575206756592,0.28264039754867554,0.2825181782245636,0.28144189715385437,0.2914743423461914,0.2825104594230652,0.2871151566505432,0.28264859318733215,0.29591792821884155,0.2825016379356384,0.2813890874385834,0.2826494872570038,0.29503053426742554,0.28196442127227783,0.28381288051605225,0.27888956665992737,0.2901303470134735,0.28253650665283203,0.2825571298599243,0.28266385197639465,0.27895253896713257,0.2859223783016205,0.2794223427772522,0.28249022364616394,0.2859411835670471,0.27812719345092773,0.28356724977493286,0.2842572331428528,0.28252947330474854,0.28265613317489624,0.2825038433074951,0.3024201989173889,0.28370925784111023,0.2826085686683655,0.2825286090373993,0.2825621962547302,0.2773262560367584,0.2968139052391052,0.2823667526245117,0.2692296802997589,0.28520795702934265,0.2826981544494629,0.2825610339641571,0.28236180543899536,0.280651330947876,0.28254538774490356,0.2772885262966156,0.28117960691452026,0.28295642137527466,0.28269797563552856,0.26402273774147034,0.26547208428382874,0.2817493975162506,0.28265488147735596,0.27938392758369446,0.28085461258888245,0.2824181020259857,0.28288570046424866,0.2825735807418823,0.2824784517288208,0.2825576663017273,0.30936571955680847,0.2827625572681427,0.28262582421302795,0.28278684616088867,0.28936734795570374,0.28269821405410767,0.28179559111595154,0.22764845192432404,0.2840316891670227,0.2826085686683655,0.28244900703430176,0.28364214301109314,0.28368374705314636,0.28264427185058594,0.28250226378440857,0.2826213836669922,0.2825735807418823,0.2835709750652313,0.2828916609287262,0.2823667526245117,0.2824222445487976,0.2824361324310303,0.28246715664863586,0.2812097370624542,0.2826034426689148,0.28397247195243835,0.26560431718826294,0.28273263573646545,0.28244900703430176,0.2811192572116852,0.2797921895980835,0.2861882448196411,0.2826766073703766,0.2804408073425293,0.28291091322898865,0.28268569707870483,0.28255191445350647,0.2769449055194855,0.2838050425052643,0.28249022364616394,0.2820435166358948,0.2832867503166199,0.28246745467185974,0.27806612849235535,0.2776190936565399,0.2873114347457886,0.2679840624332428,0.2824302017688751,0.28222015500068665,0.27952051162719727,0.283231645822525,0.2826034426689148,0.2826502025127411,0.2752419710159302,0.2837589681148529,0.28229641914367676,0.2872372269630432,0.2785045802593231,0.28255191445350647,0.2648182511329651,0.28265848755836487,0.2867155075073242,0.2826085686683655,0.299620658159256,0.28770098090171814,0.2824181020259857,0.2824796736240387,0.2825949192047119,0.2810406982898712,0.28255191445350647,0.2825735807418823,0.2829669117927551,0.28221210837364197,0.2831268608570099,0.2825735807418823,0.29858583211898804,0.28272324800491333,0.2826027572154999,0.2824069857597351,0.28501027822494507,0.28244900703430176,0.28093472123146057,0.28271806240081787,0.2825571298599243,0.282539427280426,0.2825735807418823,0.28249022364616394,0.28357210755348206,0.2826034426689148,0.28174740076065063,0.2825961410999298,0.2834762632846832,0.2833363711833954,0.2309110015630722,0.2822567820549011,0.2850724458694458,0.28146305680274963,0.28272801637649536,0.2826034426689148,0.28244900703430176,0.2827887535095215,0.2826676368713379,0.2825571298599243,0.28927651047706604,0.2826388478279114,0.27083829045295715,0.28253939747810364,0.23177234828472137,0.2832309901714325,0.28284579515457153,0.2826034426689148,0.28259706497192383,0.2824110984802246,0.2826085686683655,0.2813223898410797,0.2837791442871094,0.28086405992507935,0.2815629243850708,0.2921972870826721,0.2827262282371521,0.2825948894023895,0.28264376521110535,0.2827926576137543,0.2794460952281952,0.2825521230697632,0.2828204035758972,0.2815007269382477,0.28280261158943176,0.2903069853782654,0.2826126217842102,0.2828916609287262,0.2612845003604889,0.28296369314193726,0.3018411695957184,0.2824181020259857,0.2828410863876343,0.2826547920703888,0.28266555070877075,0.2824554443359375,0.2854348421096802,0.282490998506546,0.274603933095932,0.28244900703430176,0.2731827199459076,0.27996692061424255,0.3075614869594574,0.29538190364837646,0.3013070523738861,0.28084927797317505,0.28269702196121216,0.29335954785346985,0.2759925127029419,0.2825608253479004,0.28244900703430176,0.27806007862091064,0.28244900703430176,0.28221365809440613,0.2824573218822479,0.28265687823295593,0.26677465438842773,0.2825884222984314,0.284166544675827,0.28118956089019775,0.28244900703430176,0.2826642394065857,0.2890600264072418,0.28633078932762146,0.28068867325782776,0.27806007862091064,0.2867518961429596,0.3027779459953308,0.2824181020259857,0.2825544476509094,0.28240612149238586,0.283652126789093,0.3283874988555908,0.2815224826335907,0.3095133900642395,0.2825232148170471,0.2761426866054535,0.2785269618034363,0.28244900703430176,0.28256866335868835,0.28255191445350647,0.28261786699295044,0.281807541847229,0.28966739773750305,0.28253650665283203,0.28266942501068115,0.2753641903400421,0.2824181020259857,0.28244900703430176,0.28255191445350647,0.27606192231178284],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"pred\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"height\":1000,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1258ff1e-a3ce-4d25-a7ae-fb9774540891');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70M96KLSdCHW"
      },
      "execution_count": 837,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e72-bHKzdZve"
      },
      "execution_count": 837,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list[0]"
      ],
      "metadata": {
        "id": "M1t90K4auzlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac216664-291f-4072-ec15-ee363148588b"
      },
      "execution_count": 838,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.08869462, -0.22081181,  0.28651667,  1.0462981 , -0.36561552],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 838
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_embedding(embedding_list[:],y_train[:],index2=1)"
      ],
      "metadata": {
        "id": "BO3xdrHVGXhL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "0577803d-6fde-43cd-dd32-27f175d48821"
      },
      "execution_count": 839,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtkAAAFzCAYAAADrIhWLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5ScV4Hn/e+tHDvnpNBq5dC2Jcu2HMDZxsYJzIKXMIcZmPAuc9gDL7yH+WOH2Z1hGGb3DMzu7AI7gAdmgbXxGBsHjIyNJUs2ylbOrc65u7qqK9d9/5BoLCu1pOquVuv38dHpqq7neepXbfucX1/d515jrUVERERERPLHUegAIiIiIiKzjUq2iIiIiEieqWSLiIiIiOSZSraIiIiISJ6pZIuIiIiI5JlKtoiIiIhInrkKHSDfKioq7Ny5cwsdQ0RERERmua1btw5YayvP9tqsK9lz585ly5YthY4hIiIiIrOcMabtXK9puoiIiIiISJ6pZIuIiIiI5JlKtoiIiIhInqlki4iIiIjkmUq2iIiIiEieqWSLiIiIiOSZSraIiIiISJ6pZIuIiIiI5JlKtoiIiIhInqlki4iIiIjk2azbVl1ERERmNmst7ZF24uk4TcVN+N3+QkcSyTuVbBEREZlWe/r38Otjv8ZhHJQHyvnQ0g/hcqiSyOyi6SIiIiIyrU6MnKDUV0pTcRMj8RHG0+OFjiSSdyrZIiIiMq2ay5oZTY7SHmmnKlRF0B0sdCSRvNPfzYiIiMi0WlSxiGJfMfF0nLpwHU6Hs9CRRPJOJVtERESmXU2optARRKaUpouIiIiIiOSZSraIiIiISJ6pZIuIiIiI5JlKtoiIiBTcaGKUPX176Ih0FDqKSF7oxkcREREpqHg6zjP7nyGejpO1We5vuZ/5pfMLHUvksmgkW0RERAoqkoyQSCdoKGog4ArQPdZd6Egil00j2SIiIlJQJb4SSvwldEQ6MMYwr3ReoSOJXDaVbBERESkor8vLw4sfpjfaS9gbpsxfVuhIIpdNJVtEREQKzufyMadkTqFjiOSN5mSLiIiIiOSZSraIiIiISJ6pZIuIiIiI5JlKtoiIiIhInqlki4iIiIjkmUq2iIiIiEieqWSLiIiIiOSZSraIiIiISJ6pZIuIiIiI5JlKtoiIiIhInqlki4iIiIjkmUq2iIiIXBGstVhrCx1DZFJchQ4gIiIiciGD44O8eOhF4pk4t825jYUVCwsdSeS8NJItIiIiM97GExvJ2Rxl/jJea3uNdDZd6Egi56WSLSIiIjOey+kik8uQyWUwxmCMKXQkkfNSyRYREZEZb13jOsr8ZWRyGe5tvheXQzNeZWbTf6EiIiIy4xX7inl4ycOFjiEyaRrJFhERERHJM5VsEREREZE8U8kWEREREckzlWwRERG5YuVsrtARRM5KNz6KiIjIFWlb9zZ+2/lbygPl3LvgXkKeUKEjiUzQSLaIiIhccUYSI2zu2Ex1sJrh+DA7e3YWOpLIaQpSso0xZcaYV4wxh059LT3HcS8ZY0aMMc9Pd0YRERGZuRzGgcEwnh5nLDWGQZvTyMxSqJHsLwPrrbUtwPpTz8/m74CPT1sqERERuSIUeYtYXr2c9cfWc2ToCMPxYc3PlhmlUCX7IeAHpx7/ADjr6vLW2vXA2HSFEhERkStHKpPilqZbuKf5HtoibYwmRgsdSWRCoUp2tbW2+9TjHqC6QDlERETkClXqLyWajjKUGMLj9OBz+QodSWTClK0uYoz5FVBzlpe+8u4n1lprjLGX+V6fAT4D0NTUdDmXEhERkSvEyuqVAAzHh1letRy/21/gRCK/N2Ul21p757leM8b0GmNqrbXdxphaoO8y3+vbwLcBVq9efVmFXURERK4MLoeLa2uvLXQMkbMq1HSRnwOfPPX4k8CzBcohIiIis0gikyCTyxQ6hkjBNqP5GvBTY8yngTbgcQBjzGrgj621f3jq+RvAYiBkjOkAPm2tfblAmUVERGQG29S+iR29Owi4Ajyw8AHKA+WFjiRXsYKUbGvtIHDHWb6/BfjDdz2/ZTpziYiIyJUpkoywo2cHdeE6huPDbO/ezp3N55y5KjLltOOjiIiIXPHcDjcuh4toKko8EyfgCRQ6klzlVLJFRETkiud3+7l3wb34XD5S2RR7evfwZvub2qBGCkYlW0RERGaFxuJGWmtacRon5YFytnVvozPSWehYcpUq1I2PIiIiInlnsWRtls6xTnqiPSQyiUJHkquURrJFRERk1phTPId4Js7GExsZS42xr39foSPJVUoj2SIiIjJruJ1uaoI1fHjph3E5XXRFu7DWYowpdDS5ymgkW0RERGaV5VXL6Yn10BnpZHnVchVsKQiNZIuIiMiscl3ddTQUN2CtpTpUXeg4cpVSyRYREZFZxRhDTaim0DHkKqfpIiIiIiIieaaSLSIiIiKSZyrZIiIiIiJ5ppItIiIiIpJnKtkiIiIiInmmki0iIiIikmdawk9ERERmvZzNsal9E4cGDzG/dD7rmtbhdDgLHUtmMY1ki4iIyKzXEelge892Qp4Qb3W+xaGhQ4WOJLOcRrJFRERk1rPWkrM5dvTs4PDQYTBQG6ql2Fdc6GgyS2kkW0RERGa9hqIGakO1HBk+wtLKpQRdwZNlW2SKaCRbREREZj2nw8md8++kP9ZP2BtmJDFCyBMqdCyZxTSSLSIiIleFUn8pH1j4AaqCVdwy5xZaylsKHUlmMY1ki4iIyFVjTskc5pTMKXQMuQpoJFtEREREJM9UskVERERE8kwlW0REREQkz1SyRURERETyTCVbRERERCTPVLJFRERERPJMJVtEREREJM9UskVERERE8kwlW0REREQkz1SyRURERETyTCVbRERERCTPVLJFRERERPJMJVtERETkXeLpOD3RHhKZRKGjyBXMVegAIiIiIlMlloqxt38vbqebZZXLcDvd5z0+mory9L6niafjhDwhHln8CEFPcJrSymyiki0iIiKzkrWWFw+9yGB8kEwuw3B8mPfPe/95z+ke6yaWitFY1Eh7pJ2eaA/NZc3TlFhmE5VsERERmZWyNstAfIDaUC3JTJLuaPcFzwl7wwD0x/pPey5ysTQnW0RERPImnU3zdufbrD+6fqKoForL4WJ51XI6xzoZjA+yqnrVBc+pCdXwQMsD1BfVk0gleO7gcwX/HHJl0ki2iIiI5M2Wri1s79lOwBWgbbSNf7/y3+NxegqWZ13jOhaVL8LpcFLmL5vUOU0lTfxg5w/Y17+PVDbFOz3v8F/v/a84zOwZmxyKD7FvYB/F3mKWVCzB6XAWOtKso5ItIiIieTMcHybkCVHiLaFzrJNkJlnQkm2MoTJYedbXeqI9bOvaRrGvmDX1ayZyWmvpiHSQI0ckGWFzx2Z29+5mZc3K6Yw+ZZKZJM/uf5aczRFPx8nkMrTWtBY61qwze34lExERkYK7pvYaUtkUHWMdLK1cSsgTKnSks0pmkjx/8HkG4gPs7N3Jlq4tE68ZY7i7+W4ODh6kPdJONBXlHzb9A9lc9rLec1v3Nv73tv/Ni4dfJJlJXu5HuGTxTJxEJkFVsIoib5Gmw0wRjWSLiIhI3tSGa3lixROksimKvEUYYwod6axS2RTpbJrKQCVYiCQjE6/lbI6bGm/ilsZb+Nd3/pWRxAgHBw/icXr4pw/+0yW938D4AJs7NlMTrOHY8DH2BvdyTe01+fo4F6XIW8TckrkcGzmGy+FiWdWyguSY7VSyRUREJK/8bj9+tz+v10xmkuzt30vO5lhaufSyrx/yhFhRvYJ3et/B4/Rwbe21wMmpIq8efZWDgwfJZrMMJYbIkQML393+Xf7unr8j5L340XlrLcDEvG6Lvaz8l8NhHNzdfDdD8SF8Lp9WUJkiKtkiIiIy4712/DWODB/BgYOOSAcPLX7osq5njOHmpptprWnF4/RMzMdOZBIcGjpEQ1HDye9t+/05GTJ8fcPX+eodX73o96sIVLCmbg3bu7fTVNzE0sqll5X/cjkdznPOVZf8UMkWERGRGa97rJvqYDVO46Qn2oO1Ni9TUd47Z9zr8lLqL6U72k06l2Zx2WL2Du2deP3JXU/yhXVfoMhXdFHvY4xhTf0a1tSvuezMcmXQjY8iIiIy47XWttIb66Ur2kVrTeuUzfV2GAcPLHyA1XWref/c9/PMR5+h1FuKEycBZ4BIMsKGExum5L1ldtFItoiIiMx4rTWtNBY1krM5KgIVU/peIU+I6+qum3j+3+79b3zxl18kZ3MEXAGe2fcMdzffjcupGiXnppFsERERuSKUB8qpDFZO+4olH1vxMe5ZcA+lvlKCniDbe7fzdufb05pBrjwq2SIiInLF2dGzg+9u/S4/P/BzxtPjU/pebqebP7r2jyj2FVNXVEfQFeSlwy/l7fpD8SHaRtqIp+N5u6YUnkq2iIiIXFGG48Nsat9Emb+M7rFu3ul9Z8rfc039Gm5ovIHusW46xjp4ve113jj+xmVftyfaw0/3/JQXD73Iz/b/jEQmkYe0MhOoZIuIiMiMEUvF6Ih0EEvFznvc79aZnqr1psfT47zT+w6HBw+Tszn8bj8fW/Exyv3lLCpfxHhqnB/u+uHE+teX6sToCVzGRX1RPWPJMYbiQ3n6BFJomrEvIiIiM0IkGeHpvU+TzCbxuXw8tuSxs26UUuov5abGm9jSuYX6cD0rqlfkNUfO5nj+4PMMjQ+RzqW5seFGVtevZmXVSuqK6njx0IvEM3EODh7k9nm385EVH7nk96oOVpPIJuiIdOBz+Sj2Fufxk0ghqWSLiIjIjNAb7SWRSdBQ1EBHpIO+WN85dyNsrWmltaZ1SnLE03EGxwdpKGogmorSHmlndf1qQt4QNzbcyLP7nsXtcBPPxPnq61/l0aWP4na6L+m95pTM4ZHFjzCSGKG+qJ6gJ5jnTyOFoukiIiIiMiMU+4qxWHqiPRPP88lay3B8+II3SgbcAeaUzKF9tJ3h+DBLKpZMvHbLnFvAQCqbImMzHB85zrfe+hb9sf5LzlVfVM+yqmWU+Eou+Roy82gkW0RERGaEqmAVjyx+hO5oN3Xhuryuh22t5bXjr7FvYB8uh4sHFj5AXbjurMcaY7i7+W56oj14nd7Tth+/ru461jWuY8OJDThw4HF52Na9jaVVS7l3wb2nvd9ochSv04vf7Z/4fs7mGE+P43f5cTqceft8MvOoZIuIiMiMURuupTZcm/frRlNRDgweoCHcwEhyhB3dO85ZsgFcDhcNRQ0AHB0+yoYTGwh5QtzdfDd/f/ff87GffWxi2b1fHv0ljyx5ZOJcay3rj63n0OAh3E43Dy58kOpQNalsil8c/AW9sV7K/eU8uOhBfC5f3j+rzAwFmS5ijCkzxrxijDl06mvpWY5pNcZsMsbsMcbsMsZc+l0FIiIikjeD44OXtAqGtfayV+O4VF6XF6/Ty2B8kEgyQnmgfFLnJTIJfrb3Z2xu38yPdv2Iv3njbyjxl/BAywNgwOv0Ek/FeenwS1hryeQyjCZHOTR4iPpwPW6Hm129uwDojHTSNdZFfbie/vF+ToyemMqPLAVWqJHsLwPrrbVfM8Z8+dTzL73nmHHgE9baQ8aYOmCrMeZla+3IdIcVERGRk7Z0buHtrpO7Ha5rXMeqmlWTOq97rJuXj7xMzua4a/5dNBY3TmXMM3icHh5c9CDv9L5D2BOedG5rLYeHD0MOBsYHeLP9Tcr95aysWUnYE2YsMcZ4bpwntz9J2BNmYflCllctx+P0MJQYIpqKUuo7OZboc/mwWMaSY2RzWY1iz3KFuvHxIeAHpx7/AHj4vQdYaw9aaw+detwF9AGV7z1OREREpoe1lm3d26gL1VETrGFr99ZJn/vq8VfxOr0E3UHWH1tfkBHtikAF75/3flbXr570aiB+t5/W6lb6xvsYS42RyWXY3rOdmlANyyqXMZ47eRNlihTf2fodyvxl/Nv+f8PmLCF3iBvqb6C19uQqKLXhWm6fdzthb5jb5t5GY9H0/qIxFbK5bKEjzFiFGsmuttZ2n3rcA1Sf72BjzPWABzhyjtc/A3wGoKmpKY8xRURE5HeMMVSHqumOdmOtZW7J3Emf6zRO0tk0TuPEYRwYY6YuaJ49sfIJin3FfHvrt2koasDr9NI33sfn136eV9tenTgulomxoW0DfeN9LKxYyGhylBXVK3A5fl+3llYuZWnl0kJ8jAmJTIJkJkmRt+iS/z2ks2leOfoKJ0ZOML9sPrfPu/20zylTWLKNMb8Cas7y0lfe/cRaa40x5/x11hhTC/wL8Elrbe5sx1hrvw18G2D16tWFmewlIiJyFbi7+W529+3GYFhevXzS5905/05ePfYq2VyWe+bfM4UJLyxnc+zv389wYphFFYsuuIpJ2BvmQ0s/RCQZObl0XzZDV6SLZCBJubecweQgcHL3yZ/s+Qk3N91MNBnF7/aTzqXx4p2OjzUp/bF+njvwHIlsgoXlC7l93u04zMVPbGgbbePo8FGaipo4NHiIheULL+qXrqvBlJVsa+2d53rNGNNrjKm11nafKtF95ziuCPgF8BVr7eYpiioiIiKT5Hf7WVO/5qLPqwhU8Piyx6cg0cXb27+XXx/7NUF3kP0D+/nYio9NLLM3mhjl9bbXSWfT3DLnFqqCVQC4nW4eWvwQmzs2MxAboCfaw66eXcwpmcNg7+DEtfvifaQyKX7b9Vs+1fopQp5QQT7juezu240xhoZwAwcHD3Jd7XWU+s9Yf+KCnMYJ9uR64RZ78rmcplBzsn8OfPLU408Cz773AGOMB3gGeNJa+9Q0ZhMREZFZbGB8gLAnTGWwklQ2RSwdm3jt18d+zUBsgHg6zouHXmQ8NU40FQWgJlTDw4sf5vqG69ndv5ttPdtI2/QZ1//V0V8RcAW4puaaaftMk1XsKyaWjjGSGMHj9OB1Xdoo+5ySOVxbdy2JbII1dWuoL6rPc9IrX6Emz3wN+Kkx5tNAG/A4gDFmNfDH1to/PPW9W4FyY8ynTp33KWvtjgLkFRERkVkgmUmSSCc4MHiA6kQ1zWXNE6t/wMn5yn63H4/Tw/6B/Ty560kAbmi4YWIb96biJgZiA3RHu3EbN37jJ27jv7+GTfBW51ts6tjEQ4sfyvtnSGQSvHr0Vbqj3VxXd91FbS+/snoluVyOocQQK6tXEnAHLimDwzi4qfEmbmq86ZLOvxoUZCTbWjtorb3DWttirb3TWjt06vtbThVsrLU/tNa6rbWt7/qjgi0iIjJJOZsjkUkUOsaM8vrx1zk2coyaUA1hb5j7W+4/befFW5puYSQxQv94P363H6/TS8doB9/e+u2JrdN9Lh/X1l6L2+Ema7O01rXi5PTpEnsG9vD97d8nnT1zpPty7e7dTdtoG6W+Ut5sf/Oi1ix3OVysrl/N3c13UxM6261zki+Fmi4iIiIiU2gsOcZPdv+E723/Hr8+9mtyZ187YNZIZ9PEUrELHrenbw+vt73O+mPr2d23+7TXsrksewb2kLM5wp4wc4rnsLljM52RTnI2xytHXwFOluzm0mbK/eVUBCqoClbxl7f95RnvtbNnJwcGD+TnA76LxWIwEyuDFGqDHzk/rbUiIiIyC+3t38tYaoz6cD37BvaxvGo5lcHZud3EwPgAzx14jmQ2yfKq5axrXHfWpelyNkfnWCfHR47jMR6OcpSusa6JjXH6x/s5PHSYpuImuqPdzC2ZS7m/HKdxsqBiAfF0HGvtyRsHixtYUb2CocQQo/FRRhIjOHGS5ffrRh+LHOPpvU9T7C2mNlxLJBkh5AnhcXou6/Mur1pOT7SHnmgPNzTcMOndK2V6qWSLiIjMQj6Xb+KmPoOZ9OYrV6Lt3dsxGGpDtbzT+w4rq1dS5C064zhrLWWBMhaULiDkCZHJZUhlUxOve5weDIax5BipTIpSXymfWf0ZXjnyCslMktvn3Q7ApvZN7OzdyYGBAywoX0Cpt5S+8T6qg9V0xbpOe88XDr5AMpNkXuk8sjZL0B3k4cUPE/aGL/nz+t1+Hlz04CWfL9NDJVtERGQWWlq5lGgqSm+sl7ua76LEV1LoSHmVzWXZ3bebocQQqWyK8cw4rqQLl8N1zpFip8PJh5Z+iG+NfYtYKsb75r2PMn8Z3WPdlAfKKfOXcVfzXezu282C8gUsqliE0+HkU62fwmLxOD0MjA+wvWc7C8sXcnDwIGW+MgKeAEcHj1IRrDijZL/d/TadkU6K/cV8ed2XiaVjHB85zorqFdPxY5ICUskWERGZhdxON+ua1hU6xpTZ3bebN9reIOQJEc/GWVS+iPH0OLfX3Y7P5TvneQvLF/LN+75JIpNgPD3OU3ufIpqKUhOq4dElj7KgbAE+p4+N7Rvpj/Wzpn4Nb3e+TXe0m1ubbqUyWElnpJP9/ftxGRfVwWpubLyRDa4N9I/3s6dvz2lTRgBGYiNYY3mr8y0WVSyaGGXP5rIcHzlONpdlXum8ib9tyOQyHB06StZmaS5rvuzpJVIYKtkiIiJyxRlODBP0BCkPlNMR6aC1pnXSc5ONMfjdfl49+irPHniWkDtE2BtmbcNaGosaeenISwTdQbrGuvjnbf/MsZFjpLNpNhzfwBfXfRFjDaOJUY6PHqc8UM6+gX04jROvy0vYG2YkOXLa+8WIUZIrwe/yM79kPjZ38kbFjSc2sqt3FxhoLmvmvgX3AfBG2xvs6dsDBo6NHOP+lvvz+8OTaaGSLSIiIlecpZVLOTx0mI5IB3NL5l70roVjyTHWH1tP91g3IU+ISDLCWHKMnM2RzWXxOr2knWk6xzrZ07cHp+Nkid7fv58jI0fY3bebjM2cXHM6PkQyk2RgfOCcOx92jnfyjc3f4Fubv8XiqsX80XV/RMgbojZci9vh5vjIcXI2h8M4aBtpAwOD44MMxYe4b8F9Z72RU2Y2lWwRERHJq3Q2TXukHZfDRUNRAw6T/xWDq4JVPLHiCeKZOCW+kot+j7HkGKPJUQyGRCbBovJFVAWr8Lq8rGtax8YTG/G7/bSUtvDSoZfAnJzTPRAfYCQxgtfpJZlKcnjkMC3lLdw5/04slv5YP4OJwXO+b5IkXSNdPLnzSf7hvn9gS9cWDIYllUsmPkNlsJIXdr6Az+kj5A3xjY3f4NjoMd7X9D4+tPxDU/LzlPxTyRYREZG8euXoKxwbPkbO5ljbsJbr66+fkvfxu/343f5LOjeZTeI0TuaVzqMv1sf75r6P2nAtcHKJvCUVSzDG8NSep1hRvQKvy0s0FSWZTZLIJPA4PVQEKmgMN1ITruH146/jc/m4f8H9dEQ6iKQj53zv/lQ/S1xLWFu/lppQDcPxYVrKWiZeX1i+kNbqVmrDtWxo28CP9/wYv9vPWx1v4XV7p2QXSck/lWwRERHJm3Q2TdtIG03FTSQyCQ4PHZ6ykn05XA4XzaXNlPnLiKQirKlfc9rrv9sF8ta5t/L68dcZSAywomoFYXeY5VXL6Yh04DAOFpQt4Ic7f4jT4STgCtA22sZjSx7je7u+d973L/YU852t38HldJHKpnjuwHM0FjfSWtPKwvKFzC+bz0ji5A2TDuOgNlRLx1gHhwcPT9nPRPJLJVtERETyxu1001TSxPHh4xMj2TNNMpPk7c63GU4O0zbaxsOLH2Z+6fwzjjs0eIi3Ot+i1F9KkbeIqkAVndFO+sf7qQ3X4nP4cDvdhD1hdvXtYmnlUlwOF2FP+IyNad7NgYOuaBc/3v1jnMbJ7fNu57lDz1EbqOXI0BE+0foJHlvyGGOpMe6afxf/4YX/wLGRY9SF6rh17q1T/eORPFHJFhERkby6e/7dp83Jnmm6o930RntZ17iO9kg7TcVNEyPXv3No8BBf3/h1BsYH6I32csucW0jbNIPjg7TWtNIx2sGC8gVYLAvKF9A+1k5DuIHG4kZ29+zGYzzEbfys758jx86enayqWsVQagiOnRxZry2qpS3SRjwdx+10U+Yvo8xfxo8//GMODBygKljFnJI50/EjkjxQyRYREZG8cjvdZx0Znin8Lj8Wy2hilGwuS8ATOOOYttE2sjbLvJJ5DIwP0BHpoMRfgtftJZVJ4XF6WFO/hj19e9g+tJ2moiZ6Yj24jItEJoHL5YL0uTNkyDCcGGZR1SIq/BVYaxmIDeBz+c7YnbPIW3TGdJZ3S2QSjCXHKPGVzOqdPa80KtkiIiJyVakOVXPX/Ls4MnyE1tpWGosazzhmXsk8aoO1HBk5Ql2ojnWN61jbsJan9jzFi4dfJJlNcnzkOBWBCm6bexubOjbRM9bDWGqMw0OHMbkLL7nXM9bDh5d/mD9Z8yds6drCT/b8hEQmwV//5q/58xv+nDklc4imotSGak+7wTOTy9A+2o4xhqA7yC8O/YJ4Ok5lsJIPLvqgNq+ZIYy1ttAZ8mr16tV2y5YthY4hIiIiV7ieaA99sT5qgjVUhapIZBJ87sXP4Xa4OTB4gIHYALF0jAp/BQ7j4ODQQbxOL5lchgp/BXsH92I5f8+6Z+49LKpaRFNRE293vs2C8gX0xnqp9FfSWNKIwVDqL+XRJY/icXroi/Xxv7b8L7b1bKM6WE1zSTMhb4iGogbaI+18cNEHZ+QUndnKGLPVWrv6bK9pJFtERETkLGpCNdSEaiaee51ellQs4bW21+ge657Yjv3I8BFqQ7VUBaowxpDOpYmmo7hwkT7fnBHg9ROvM79sPm92vMlocpR4b5wibxGesIeAO0C5v5zOsU4iyQgVgQre7nybE5ETRBIRhmJD9EdPbv3ud/lxGAdBd3CqfywySSrZIiIiIudgraVrrIuczVFfVM+frvlTrqm5ho0nNvLsgWcp8ZYQ9oQxGDBQHiinOlhNb6yXrkjXBa+fyCUYTg6TyqaoDZ3c/bEiUMGCsgVs69pGTbiGhqIGwp4wAAF3gEwmQywdw+f0UeIv4aamm7DW0lLectE7X8rUUckWEREROYe3Ot9ia9dWAJZVLuN9897H++a9j1vn3kqJv4T/887/YUH5Am5uupnh+DAPL3qY8ew4X9vwNcp8ZfQmes97/TJvGXv797K8ajk3N91M0B3ku9u/y4b2DXidXsLDYdbWr2VP/x6urb2WGxtupCvSRTR9chR9aeVSrkAqjUcAACAASURBVKu9jqBHI9gzjUq2iIiIzFg5m2MsOYbf7S/IDX37+/dTG6rF6XCyf3A/t829DWMMkWSE+1vuZ3H5Yvb078HpcHJNzTW0VLTw5M4naSpqon+sn2gqSiwXO+f155bM5cFFD3J/y/1s7tzM+qPr6Yv2UeovZSQ5gtPhxOf2sbljM9lclngmzpySOSyrXMbO3p24nW62dm3l5jk3a7v1GUYlW0RERGakbC7Ly0depm2kjYAnwEOLHqLEVzKtGeaWzmV3324AmsuaMcYwFB/iqb1Pkc1l8Tq93LPgHrwuL/XhenpjvQTcAWpCNUQzURZWLuTwwGHGsmNnvf6e3j3MLZlLIpugOlDNcGKYaCrKwPgAOXKE3CFKfCX0xnp59dirVAerea3tNWqCNVQEK9jTu4eusS7aI+18eNmHtbLIDKKSLSIiIjPSwPgAx0eO01jUSE+0hwMDB6Z9B8mbm26mPlyPtZZ5pfMA6I32nlxf2x1ge/d2akI13NtyL91j3YzER2goaqA32suc4jnUheowGDxOD9FUlN0Du0+7fpIkvzryK14+8jIGQ31RPVWBKnpiPcwtmcv8kvn0RHtYVLGI/vF+ygPlBFwBRhIj9I71ksqmKPWV0jPWQ9dYF3NL5k7rz0fObVIl2xhTZq0des/35llrj01NLBEREbna+d0nV8wYTgyTzCYnbv6bTi6Hi5byltO+VxGoIJFJ8MaJN/A4Pezu3w0Gjg4fxelwEnAH+ETrJwh7w6w/tp6wN8x4ehyvy8tt9bfxeufrp10vkongwkWOHMeHj9NY0khNsIbb5t5GQ1EDNzTcQHNZM8/sf4bOSCfXN1xPhb+CZw88y1hqjDJfGW6nm9H4KKlQCo9Lo9kzwWRHsp8zxtxnrY0AGGOWAj8Flk9ZMhEREbmqFXmL+EDLB9jbv5cVVStYXLm40JEAqAxWcnfz3YwkRmgubWZbzza2d2+nNlTLrXNvpWesh2JvMR9d8VH6Y/3s6NlB0BOk1FeKMQYXLjJkTrtmhgwOHHjdXlZUr2BR+SKqQ9WU+ktZWL6QoCfIR5Z9hLbhNrZ0b+Hg0EHqw/U0FjdyfPg4ewf28tM9P6UmVMM37/smdUV1BfrpyO9MtmT/NSeL9geARcCTwBNTlkpEREQEaCxupLH4zB0ZC21xxWKur7+erd1b6Y/1c0P9DWzq2MSu3l20lLVQ4ithV+8uin3FDIwP4MDB3JK5xNIx5hfP5+DowTOu6XV68Tg83DbnNv6g9Q9O3vTo8pHOpdndu5sN7RvY2bOThqIGfC4fh4YP8eElH2YwNshvu35Lc2kzx0eP86/v/CtfWPeFAvxU5N0mVbKttb8wxriBXwJh4BFr7Zn/dYiIiIjMQEPxId7qeAuvy8va+rWXveSd0+Hkvpb7qC+q59eeX1NfVM+qmlXcMucWrqu97uSqH91bWVG1gjJfGU/vf5pMNsMDLQ8wnho/a8meXzyf2+bdxp+u+VPcTjcAR4aO8L0d32PjiY0EPUHCnjDxTJybG2/G5/TRNdZFRbACp3GSzCaxWFxO3XI3E5z334Ix5lswsR+oAYqBI8D/Y4zBWvu5Kc4nIiIieXZi5ATHRo5RX1TPgrIFhY4z5XI2xwsHXyCTy5DKpUhmktzXct9lX9dhHKysXsl4epyjw0d5/7z3c3399RNL6VX4K+gb72PPwB6WVy5nWdUycjbHsqpljCRG2Nix8fQLGphXOo+3Ot+ioagBl8PFm+1vMpoYpSJQQf94P8WeYhw4yNosX1r3JcoD5YTcIXwuH+uPrefG+hv5+MqPX/Znk8t3oV91tlzguYiIiFxB+mP9PH/oeQKuAO/0vcMjix+hvqi+0LGmlLWWaDpKdbCaZCZJJBnJ27UdxsFNjTdxU+NNZ7x2b8u97OrdRdtIGx6Hh4ODBwm6g3xi1SdorW1lW8c24sQnjj8weIDaUC1f3/h1Qu4QdUV11AZr8Tq9J+dyGxflwXI+t/ZzZ/xy9MV1X+Rzaz+H2+nWetkzxHlLtrX2B797bIzxA03W2gNTnkpERESmRDQVxWAoD5QTj8SJJCPUM7tLttPh5KbGm9jYvhGncXJv872TPtday4nRE6SyKZqKm/C6vJM+N+QJsbpuNb/t/C0vHHyB2nAt7rCbymAld82/i4+u+ij/vPOfJ47PkOFHu37ESHKE2opaXjn8CmW+Mv7din/HwoqFLKlYwuq61fjd/rO+38Vkk6k32SX8HgS+AXiAecaYVuCr1toPTmU4ERERya/acC2l/lI6I50EPUEaihoKHWlarKxeyYKyBTiN86LK6I6eHWw4sQGHcVAXruOhxQ9d1Ejx68dfpzfWS9ATpDxQTlNRE+lsmspgJdc3Xs/3dn4POzEzF3b17OKuBXfx2onX6BvvI5FJ8ItDv+D7D3+fskDZOd8nloqxu283bqebZZXLVLhngMnOjP9PwPXAawDW2h3GmPlTlElERESmiM/l49EljzKaGCXsDeNz+QodadoE3IGLPufYyDEqA5UEPUE6xzpJZpLnHEk+m/ZIOyuqVpDNZmkfa+e+BfdRE6rB6XDy8ZUf5+32t08bze4a72J51XI2dW6iqbiJxuJGIskII8mR85bsFw+/yOD4IFmbZSg+xJ3z77zozyr5NdlfxdLW2tH3fC+X7zAiIiIy9TxOD5XByquqYF+q5tJmBuIDnBg9QV247qJ/ZiurVjIcH6apuInPXvdZ7my+E6fDCZws/V+6+Uv4Xb8v7RbL5o7N/Me1/xGf00dfrI/VdavP+zcO2VyWgfEBakI1VAYq6Yn2XNqHlbya7Ej2HmPMxwCnMaYF+Bzw5tTFEhERETm/seQY+wf243P5WFK5BJcj/0vXraxeSUWgglQ2RUNRA8aYizr/urrraCppImdzVAerJ77fF+ujN9pLRaCCz6/9PF/b+DVyp8Yvn97/NHfMu4MnH32SWCrG0sqleJzn3sXR6XDSWtPKtu5tANw257ZL+KSSb8Zae+GDjAkAXwHuPvWtX3JyTnZyCrNdktWrV9stW7QIioiIyGyWzWX56Z6fMpYaI5VNcW3ttWdd4aOQoqko1lrC3tO3gx8cH+T/7v2/GAw5m+PRJY/y0L8+xFvdb00c48FD5CsRvC4vOZvjyNAREpkE80vnn3WNb2stg/FBnMZJqb90yj+bnGSM2WqtXX221yY7XeSj1tqvWGvXnPrzFeAv8xdRREREZPJS2RQjiRFqgjWU+8tn3BSJ/f37+Zed/8IPd/2QXb27TnttODGMtZa68MmtzyPJCB9cfPpaEilSbG7bDMC2rm38j7f/B59/6fN86t8+Rfto+xnvZ4yhIlChgj2DTLZkP2aMmdhG3Rjzj0Dl1EQSEREROT+fy8fC8oV0jHUwkhhhZfXKQkc6zaaOTVQGKqkOVvNWx1u8e+ZAVbAKn8tHZ6QTj9NDVbCKW5puwYnztGv81Ya/AmDvwF529e7C7XBzYvQE39n2HSYzE0EKa7KTlx4Dfm6MyQH3AiPW2k9PXSwRERGRczPG8P5572d51XI8Ts+MG8EtD5wcXXcYB6X+0tPmchd5i/jwsg8zFB+i1FdK2BtmTcMaPrHqE3xv5/cmjnu74212de9iQekCYpkYAKX+UrK5LBaL4eLmh8v0Ou9ItjGmzBhTBviBPwT+X2AM+MtT3xcREREpCIdxUB2qnnEFG+COeXewpGIJLWUt3NN8zxmvhzwhmoqbJuZrZ3NZ1tavpSZQM3HMWGaML/7yi9zYeCN/fN0fU+wrpqmoiceXPa5dHa8A573x0RhzDLCAedfX37HW2hm3VrZufBQREZErTftoO784+AuOjxznb9/824nvB51Bdv3JLuaXzyeZSeIwDtxOdwGTyrtd8o2P1tp51tr57/n6uz8zrmCLiIiIXIlKfCW4nW5cDhcex++X60tkE/yXN/4LcHLbdBXsK8ek/q7BGBMwxvyFMebbp563GGMemNpoIiIiIleHsDfMY0sf454F9/CRJR/BeeqfUl8pG9s3ksqmCh1RLtI5S7Yx5gFjTOjU0+8BKeB3C1B2Av95irOJiIiIXDVKfCXc3HQzf3bDnzG3ZC5F3iLSuTSxVIwD/QcKHe+CeqI97Onbw1B8qNBRZoTzjWQfBf7nqcfN1tqvA2kAa+046JZWERERkYuRyqZIZs69l58xhrUNa3n68acpD5TjMA6G48Pc+6N7aRtqm8akF6d7rJuf7fsZb7S9wc/2/YxIMlLoSAV3zpJtrd0L/H+nnqaMMX5O3vyIMaYZmHG7PYqIiFwJ0tk064+u5wc7f8COnh2FjiPT5PjIcb6/4/t8f+f3OTBw/pHpVbWraK1pJZ6JE8/G6Y528+CPH5yx62P3xfpwGif1RfVkchlGEiOFjlRwF7rx8XdbCv0n4CWg0RjzI2A98KWpjSYiIjI7HRg4wP6B/QRdQd5sf5O+WF+hI8k0eKPtDYq9xVT4K/hN228uWJjvnHcnqWwKi8WBg4ODBzkydGSa0l6c+qJ6jDF0jHXgd/kp95cXOlLBTWozGmvtL40xW4EbODlN5M+ttQNTmkxERGSWytgMxhjcTjfWWnI2V+hIMg1CnhBD8SGcDidBT/C0DWrO5g+u/QO+tvFrtI22kSNHyB3i1WOvsqB8wTQlnryKQAWPL3uckcQIlYFKgp5goSMV3GRXF1lvrR201v7CWvu8tXbAGLN+qsOJiIjMRosrFlMfrqcr2sWqmlVUB6sLHUmmwR3z76AuXEdloJJ7F9x7weM9Tg/PfOQZ5hXPo7m0mZaKFk6MnpiGpJemxFfC3JK5KtinnHck2xjjAwJAhTGmlN/f7FgE1E9xNhERkVnJ5/Lx0OKHyNmcdu67ihR5i7iv5b6LOqe1tpVPX/dpdvftZiwxxvHR4xwePDwjR7PldBf6P/uzwFZg8amvv/vzLPCPUxtNRERkdlPBlsn49DWfpsRbgtvlJpfL8Tcb/obB8cFCx5ILuNCNj/8ALAD+83t2fFxlrVXJFhEREZli1aFqbmy8EadxMjA+wO6+3Tx/4PlCx5ILuOCv0NbaLPDoNGQRERERkbO4Y94ddEW72NGzg0NDh/j7TX/PQExrUMxkk/17qvXGmMfMhW6DFREREZG8qyuqo8RTQjwTx4mTjkgHLx56sdCx5DwmW7I/C/wUSBpjIsaYMWOMtvIRERERmSZLq5bicXrwur34XD4t/QicGDnBK0deYXff7hm3Uc+k1skGioEngHnW2q8aY5qA2qmLJSIiIiLv9tnrPsvOnp30x/qZWzqXu5rvmtR5qWyKgwMHMcbQUt6Cx+mZ4qTTYzg+zAuHXiDgDrB/YD8+l48FZTNn1ZVzlmxjzM3AplNzsv87kANuB74KjAFPA2umI6SIiIjMXpFkhJ09O/E4PayqWYXP5St0pBlpQfkCfvjoD9nXv49fHv0lf/HqX3DH/Dv42IqPnXdjm/XH1nN06CgWS9dY16TL+Uw3nh4nR45SfynxTJxIcmZNsjjfdJEc8E+nHq+11v4ZkACw1g4Ds+PXIBERESkYay0vHHqB/QP72da9jY0nNhY60oxWFaqiN9bL4aHDVAQq+PmBn7NvYN95z2kfaac+XE99uH5Gb2ZzsaqCVdSF6+iMdOJz+2gubS50pNOccyTbWvumMWb81NO0McYJWABjTCUnS7iIiIjIJcvaLCOJEWqCNSSzSfpifYWONONZLAYzsc76heYiL69ezo6eHWDhurrrpiPitHA73Ty48EFGk6ME3UG8Lm+hI53mvHOyrbU7Tj38JvAMUGWM+S/Ah4C/mOJsIiIiMsu5HC6uqbmGrd1bcRgHd86/s9CRZrx7F9zL3r69HBs5xl3Nd7Gkcsl5j7+x4UbmlczDGEN1sHqaUk4Pp8NJmb+s0DHOykz2TkxjzGLgDk5urb7eWnv+v5s4/7XKgJ8Ac4HjwOOnpqC8+5g5nCz2DsANfMta+z8vdO3Vq1fbLVu2XGo0ERERmWbWWkYSI7gcLsLecKHjXDGiySi/afsN/eP9rK5bzbKqZYWOdNUxxmy11q4+22uT3s/VWrvfWvvfrbX/eDkF+5Qvc7KotwDrTz1/r27gRmttK7AW+LIxpu4y31dERERmGGMMpf5SFeyLtKd/D0eGjtAR6eC72747q+ZbzwaTLtl59hDwg1OPfwA8/N4DrLUpa23y1FMvhcsqIiIiMuNkbZYjI0c4MXqCvlgfLx9+ecatFX01K1RxrbbWdp963AOcdYKQMabRGLMLaAf+1lrbdY7jPmOM2WKM2dLf3z81iUVERERmkFXVq3AYBzmbY3nVcqKpKJlcptCx8m4oPsSWzi0nlyG8gn6JmOxmNBfNGPMroOYsL33l3U+stdYYc9afmLW2HVh5aprIvxljnrLW9p7luG8D34aTc7IvO7yIiIjIDBf0BPmzNX/G93d8n1eOvoLbuPE6vXz62k/PuJU2LlUsFeOZfc+QszmS2ST3NN9DS3lLoWNNypSNZFtr77TWLj/Ln2eBXmNMLcCpr+ddr+fUCPZu4JapyisiIiJypWkua8br9NJQ1EBrTSsbOzZyZPhIoWPlTTQVJZVNUROqIeAKXFFLPBZqusjPgU+eevxJ4Nn3HmCMaTDG+E89LgVuBg5MW0IRERGRK0BNuIZMNkMkFSHgCkysnz0blPnLqAxU0hHpIGMzNJfNrA1nzmfKpotcwNeAnxpjPg20AY8DGGNWA39srf1DYAnw96emkhjgG9badwqUV0RERGRGenzp42SyGdpG27ix4UYaihoKHSlv3E43H1z8QQbGBwh5QhR5iwodadImvU72lULrZIuIiMjV6MTICV4+8jKZXIabm25mRfWKQkea9fKyTraIiIiIzFybOjbRF+1jR88OvvnWNxlNjBY60lVNJVtERERkNrCwZ2APXqeXrM3yTq9m2RaSSraIiIjILHDL3Fuo9Ffic/moDFQyGB+8otaVnm1UskVERERmgbpwHZ9d/VmS2STRVJSjw0fZ0bOj0LGuWoVaXURERERE8mxu6VyWVC6hPlxPLBXj2Mgxrqm95pKuNZIYYWB8gDJ/GWX+sjwnnf1UskVERERmCb/LT124jn39+9g/sJ+W8hZ6oj3UhM62Cfe5jSZGeWrvU6SzaZwOJ48teYzyQPkUpZ6dNF1EREREZJYwxnBT4030j/dTGayksaiRlw6/dNFzswfjg2RyGRqKGsjaLAPjA1OUePbSSLaIiIjILJHJZXj5yMsMx4dJ59IUeYqoKbq4UWw4udOi0zjpiHTgMA4qAhVTkHZ2U8kWERERmSWSmSSRRIRb59zKpo5N9Iz38PFVH8cYc97z4uk4LocLt9MNQImvhA8t/dDEnGxNFbl4KtkiIiIis0TAHaC5rJnDQ4dZUb2Cu+ffTVNJ03nP2dGzg80dm3E73Nzfcj+14VoASv2llPpLpyP2rKSSLSIiIjJLGGO4c/6drKpehcM4cDvdHBs+xpHhI9SEalhauRSH+f0tealsis3tm6kJ1RBLx9jcsZlHljxSwE8we6hki4iIiMwiDuMg5Anx7IFn6Yn2sH9gP2vr1rKvfx9ep5eW8paJY53GidflJZKMEM/EqQ5VFzD57KLVRURERERmmbbRNkYSI5T5yxhNjIIBj9PDWGrstOOcDicfWPgBSgOltJS3cFPjTQVKPPtoJFtERERklgl5QmRzWay1FPuK6Yv1Ya1lLDVGLBUj6AlOHFsVrOLBhQ8WMO3spJFsERERkVmmsaiRO+bfQX1RPV+48QtcU3sNAXeAff37eP7g8+RsrtARZz2NZIuIiIjMMsYYllYuZWnlUgB29e6iJlRDwB2gM9pJKpvC5/IVOOXsppFsERERkVluedVyhhJDdEQ6aClrwev0FjrSrKeRbBEREZFZbnHlYiqDlaRzaaqCVRfcnEYun0q2iIiIyFWgPFDOeHqcXx75JSPxEdY2rGVe6bxCx5q1NF1ERERE5CqxtWsrbSNtGGN45cgrxNPxQkeatVSyRURERK4SyUwSt8MNFnpjvfTEegodadZSyRYRERG5SqyuX43T4eTlIy8TS8d46dBLdIx2FDrWrKQ52SIiIiJXiRJfCfc030MsHaOxqJH+WD9to200FDcUOtqso5ItIiIichUp9hUT9ATpiHSQszkailSwp4JKtoiIiMhVxO/28+jiR+mMdFLkK6IuXFfoSLOSSraIiIjIVSbsDbO4cnGhY8xqKtkiIjKl0tk0O3t2MpYaY0X1CioCFYWOJCIy5bS6iIiITKmt3Vt5q/Mtjo8c57kDz5HKpgodSURkyqlki4jIlBoaH6LIW0S5v5xEJkEykyx0JBGRKaeSLSIiU6q1tpVEJkHHWAdLKpYQ8oQKHUlEZMppTraIiEypunAdT6x8glQ2RbG3GGNMoSOJiEw5lWwREZlyAXeAgDtQ6BgiItNG00VERERERPJMJVtEREREJM9UskVERERE8kwlW0REREQkz1SyRURERETyTCVbRERERCTPVLJFRERERPJMJVtEREREJM+0GY2IiIiInCGVTfHbzt8yFB/imppraChuKHSkK4pGskVERETkDNu7t7OzZyejiVFeOPwC4+nxQke6oqhki4iIiMgZYukYfrefsDdMJpchlU0VOtIVRSVbRERERM6wqnoVBkN3tJsVVSso9hYXOtIVRXOyRUREROQM5YFynlj5BKlsiqAnWOg4VxyVbBERERE5K7fTjdvpLnSMK5Kmi4iIiIiI5JlKtoiIiIhInqlki4iIiIjkmUq2iIiIiEieqWSLiIiIiOSZSraIiIiISJ6pZIuIiIiI5JlKtoiIiIhInqlki4iIiIjkmUq2iIiIiEieFaRkG2PKjDGvGGMOnfpaep5ji4wxHcaYf5zOjCIiIiIil6pQI9lfBtZba1uA9aeen8tfAb+ZllQiIiL/f3t3H1tXfd9x/P1N/BAc7AaSkDjkCQYtNFCR4jEaxEjT8NBJPOxB3apOCxobf6Dtn44/mJD2x/iHFVXbqm7SGExKt0pjQ2PASgUkBTWlMJKNhwiaNDyUYGMIITFOcDBJ/N0fPnROuI5Ncu49vs77JVk+x+fY/kRf2fdzj3+5R9Kk9Q328Vz/c7z7wbtVR5lyqirZ1wPri+31wA21ToqIi4EFwGMNyiVJkqRJ6N/Xz4PbH2Rz32Ye2PYAAx8OVB1pSqmqZC/IzP5i+21Gi/QRImIG8G3g1kYGkyRJ0sT2HNjDDGbQ3dnN4ZHDvP/h+1VHmlJa6vWFI2IDsLDGodvH7mRmRkTWOO8W4JHM7I2Iib7XzcDNAEuXLj2+wJIkSZq07s5uWma20DfYx+y22czrmFd1pCklMmv12zp/04jtwOrM7I+IbuDJzPzcUed8H7gcGAFOBdqAv8/MY63fpqenJ7ds2VKn5JIkSfrYvuF9DHw4wNyOuXS0dlQdp+Ei4n8ys6fWsbpdyZ7AQ8A64M7i/YNHn5CZ3/h4OyJuBHomKtiSJElqnM72TjrbO6uOMSVVtSb7TuDKiNgBrC32iYieiLinokySJElSKSpZLlJPLheRJElSIxxruYh3fJQkSZJKZsmWJEmSSmbJliRJkkpmyZYkSZJKVtVL+EmSJGka2r57Oz958yfMmTWHq86+6qR9iT+vZEuSJKkUQweHeOIXTzCnfQ57hvaw+a3NVUeqjCVbkiRJKpklW5IkSaXoaO1g9bLVDAwPcHrH6fQsqvkS0icF12RLkiSpNOfNP4/z5p9XdYzKeSVbkiRJKpklW5IkSSqZJVuSJEkqmSVbkiRJKpklW5IkSSqZJVuSJEkqmSVbkiRJKpklW5IkSSqZJVuSJEkqmSVbkiRJdffR4Y8YPjRcdYyG8bbqkiRJqqvX977Ohtc2MJIjXLHsipPituteyZYkSVJdPfXmU3S1dzG/Yz6bdm4iM6uOVHdeyZYkSVJdndp6KruHdtMyo4XZbbOJiKoj1Z0lW5IkSXW15uw1PP3m04zkCJcuvrTqOA1hyZYkSVJddbV3cfU5V1cdo6Fcky1JkiSVzJItSZIklcySLUmSJJXMNdmSJElquH3D+9j5/k462ztZ0rVk2r3iiCVbkiRJDTV8aJgHtj3ABwc/4NDIIa4868ppd4Mal4tIkiSpofZ/tJ+hj4ZY3LmYrrYu+vf3Vx2pdF7JliRJUkN1tXcxr2MevYO9AJx92tkVJyqfJVuq2KGRQwwdHGJ262xmzphZdRxJkuqudWYr137uWt754B1mt85mbsfcqiOVzpItVejAwQM8/POH2XNgD/M65nHtZ6+lvaW96liSJNVde0s7Sz+ztOoYdeOabKlCvYO97B7azZmdZ7Jr/y769vVVHUmSJJXAki1VaFbLLEZyhMHhQZLklJZTqo4kSZJK4HIRqUKLuxbz5eVf5o2BN7i4+2K6O7urjiRJkkpgyZYqFBGsOGMFK85YUXUUSZJUIpeLSJIkSSWzZEuSJEkls2RLkiRJJbNkS5IkSSWzZEuSJEkls2RLkiRJJbNkS5IkSSWzZEuSJEkls2RLkiRJJbNkS5IkSSWzZEuSJEkls2RLkiRJJbNkS5IkSSWzZEuSJEkla6k6wHQwODzI1l1bOaXlFC444wLaZrZVHUmSJEkVsmSfoJEc4ZEdj7BveB8HDx9kcHiQ1ctXVx1LkiRp2slMIqLqGJNiyT5Bh0YOsffAXhZ1LuLAwQPs+mBX1ZEkSZKmlZEcYdMbm9i2exvL5yxnzVlraJ3ZWnWsY3JN9glqm9nGhQsupG9fHwPDA3yx+4tVR5IkSZpW+gb72LprKwtPXciOPTt4fe/rVUeakFeyS3DZkss4f975tM5spau9q+o4kiRJ00pEEASHRw7/cn+q80p2CSKCuR1zLdiSJEl1sKhzESu7VzIwPMCK+Ss467Szqo40oUquZEfE6cB9wHLgF8DXMnNvjfMOA1uL3Z2ZeV2jMkqSJGlqmBEzWLVkFauWxmiKCAAAB1xJREFUrKo6yqRVdSX7NmBjZp4LbCz2azmQmRcVbxZsSZIkNYWqSvb1wPpiez1wQ0U5JEmSpNJVVbIXZGZ/sf02sGCc82ZFxJaIeCYiLOKSJElqCnVbkx0RG4CFNQ7dPnYnMzMicpwvsywz+yLibOBHEbE1M1+t8b1uBm4GWLp06QkmlyRJkk5M3Up2Zq4d71hEvBMR3ZnZHxHdQM07uGRmX/H+tYh4ElgJfKJkZ+bdwN0APT094xV2SZIkqSGqWi7yELCu2F4HPHj0CRFxWkS0F9vzgMuAlxuWUJIkSTpOVZXsO4ErI2IHsLbYJyJ6IuKe4pzzgS0R8QLwBHBnZlqyJUmSNOVV8jrZmfke8JUaH98C/FGx/VPgwgZHkyRJkk6Yd3yUJEmSSmbJliRJkkpmyZYkSZJKZsmWJEmSShaZ0+tlpSPiXeCNqnN8SvOA3VWH0HFxds3JuTUvZ9ecnFvzcnbHtiwz59c6MO1KdjOKiC2Z2VN1Dn16zq45Obfm5eyak3NrXs7u+LlcRJIkSSqZJVuSJEkqmSV7ari76gA6bs6uOTm35uXsmpNza17O7ji5JluSJEkqmVeyJUmSpJJZsisQEadHxOMRsaN4f9o4530rIl6KiJ9FxHciIhqdVUf6FLNbGhGPFbN7OSKWNzapxprs3IpzuyKiNyK+28iMqm0ys4uIiyLi6eL35YsR8btVZBVExDURsT0iXomI22ocb4+I+4rj/+3vxqlhEnP7ZvFY9mJEbIyIZVXkbDaW7GrcBmzMzHOBjcX+ESJiFXAZ8AXgAuBXgSsaGVI1TTi7wveAuzLzfOASYFeD8qm2yc4N4A7gxw1JpcmYzOyGgD/IzBXANcDfRMScBmYUEBEzgb8Dvgp8Hvh6RHz+qNNuAvZm5jnAXwN/1diUOtok5/Yc0JOZXwDuB77V2JTNyZJdjeuB9cX2euCGGuckMAtoA9qBVuCdhqTTsUw4u+KXU0tmPg6Qmfszc6hxEVXDZH7miIiLgQXAYw3KpYlNOLvM/Hlm7ii232L0SW3Nm0Oori4BXsnM1zLzI+BfGZ3fWGPneT/wFf9KW7kJ55aZT4x5HHsGWNzgjE3Jkl2NBZnZX2y/zeiD+hEy82ngCaC/eHs0M3/WuIgax4SzAz4LDETEf0TEcxFxV3GlQNWZcG4RMQP4NnBrI4NpQpP5mfuliLiE0YsTr9Y7mD7hTODNMfu9xcdqnpOZh4D3gbkNSafxTGZuY90E/LCuiaaJlqoDTFcRsQFYWOPQ7WN3MjMj4hMv8RIR5wDn8//PFh+PiMszc1PpYXWEE50doz9XlwMrgZ3AfcCNwL3lJtVYJcztFuCRzOz1wlpjlTC7j79ON/DPwLrMHCk3paSI+H2gB5evToolu04yc+14xyLinYjozsz+4kGh1nrd3wSeycz9xef8EPgSYMmusxJm1ws8n5mvFZ/zn8ClWLLrqoS5fQm4PCJuAU4F2iJif2Yea/22SlDC7IiILuAHwO2Z+UydourY+oAlY/YXFx+rdU5vRLQAnwHea0w8jWMycyMi1jL6xPeKzBxuULam5nKRajwErCu21wEP1jhnJ3BFRLRERCujzxpdLlK9ycxuMzAnIj5eE7oGeLkB2TS+CeeWmd/IzKWZuZzRJSPfs2BPCRPOLiLagAcYndn9DcymI20Gzo2Is4qZ/B6j8xtr7Dx/B/hResOOqk04t4hYCfwDcF1m+h/5J8mSXY07gSsjYgewttgnInoi4p7inPsZXVO4FXgBeCEzH64irI4w4ewy8zCjJW1jRGwFAvjHivJq1GR+5jQ1TWZ2XwN+HbgxIp4v3i6qJu7Jq1hj/SfAo4xeFPq3zHwpIv4yIq4rTrsXmBsRrwDf5Niv9KMGmOTc7mL0L3z/Xvx8Hf3kSTV4x0dJkiSpZF7JliRJkkpmyZYkSZJKZsmWJEmSSmbJliRJkkpmyZYkSZJKZsmWpGkmIn76Kc9fHRH/Va88knQysmRL0jSTmauqziBJJztLtiRNMxGxv3i/OiKejIj7I2JbRHw/IqI4dk3xsf8FfmvM586OiH+KiGcj4rmIuL74+N9GxF8U21dHxI8jwscQSRpHS9UBJEl1tRJYAbwFPAVcFhFbGL0L6RrgFeC+Meffzuitrv8wIuYAz0bEBuDPgc0RsQn4DvAbmTnSwH+HJDUVr0JI0vT2bGb2FoX4eWA5cB7wembuyNHb/v7LmPOvAm6LiOeBJ4FZwNLMHAL+GHgc+G5mvtrAf4MkNR2vZEvS9DY8ZvswE//eD+C3M3N7jWMXAu8Bi0rKJknTlleyJenksw1YHhG/Uux/fcyxR4E/HbN2e2XxfhnwZ4wuP/lqRPxaA/NKUtOxZEvSSSYzPwRuBn5Q/MfHXWMO3wG0Ai9GxEvAHUXhvhe4NTPfAm4C7omIWQ2OLklNI0aX40mSJEkqi1eyJUmSpJJZsiVJkqSSWbIlSZKkklmyJUmSpJJZsiVJkqSSWbIlSZKkklmyJUmSpJJZsiVJkqSS/R/wO1NO7cTw1gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rekonstrukciós hiba számítása\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "X_NN_test_predicted=model.predict(X_NN_test)\n",
        "y_MAE=[]\n",
        "for i in range(len(X_NN_test)):\n",
        "    y_MAE_calc=mean_absolute_error(X_NN_test_predicted.tolist()[i],X_NN_test.to_numpy().tolist()[i])\n",
        "    y_MAE.append(y_MAE_calc)\n"
      ],
      "metadata": {
        "id": "WMRd5eGU9ASA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4b5d75-8afd-4cf0-f266-1d957ffc2bfc"
      },
      "execution_count": 840,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def color_changer(arr):\n",
        "    o=[\"r\" if i>0.5 else \"g\" for i in arr]\n",
        "    return o"
      ],
      "metadata": {
        "id": "8p1FltMDj8fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotgraf(list_in, colors=\"r\"):\n",
        "    xkoordinata=[i for i in range(len(list_in))]\n",
        "    plot.figure(figsize=(12,6))\n",
        "    #col_ch=color_changer(predicted)\n",
        "    plot.scatter(xkoordinata,list_in,c=colors,marker=\".\",alpha=0.3)\n",
        "    plot.ylabel('értékek')\n",
        "    plot.xlabel('index')\n",
        "    plot\n",
        "    plot.show()"
      ],
      "metadata": {
        "id": "8BRkLSxxg20U"
      },
      "execution_count": 848,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotgraf(y_MAE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "yq9zhnuYgu-b",
        "outputId": "e1a93cff-f2c4-4ca1-93fd-d1fc972af9b5"
      },
      "execution_count": 849,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_NN_test_list=X_NN_test.to_numpy().tolist()"
      ],
      "metadata": {
        "id": "xQ934dPgnJtv"
      },
      "execution_count": 843,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_NN_test_list_0=[i[0] for i in X_NN_test_list]"
      ],
      "metadata": {
        "id": "nGsVbGwCoBEs"
      },
      "execution_count": 844,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_NN_test_list_0"
      ],
      "metadata": {
        "id": "R-8R1JD3otTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotgraf(X_NN_test_list_0, colors=y_MAE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "2QeLvfJbo2cX",
        "outputId": "78512603-a01f-4033-bc3f-0ba71e3fdd88"
      },
      "execution_count": 851,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotgraf(X_NN_test_predicted.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "ydCU_YgKmyFx",
        "outputId": "f0846420-69d7-421a-c129-aab1611ccc4d"
      },
      "execution_count": 847,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-847-d329a93cf218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplotgraf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_NN_test_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-841-d9d1a3bb872c>\u001b[0m in \u001b[0;36mplotgraf\u001b[0;34m(list_in)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#col_ch=color_changer(predicted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxkoordinata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'értékek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecated_parameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2810\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 2811\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   2812\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4391\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sknuF_FpeKQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnDglVlGeQkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hF3GTeAIeAX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_df=pd.DataFrame({\"x\":X_NN_test, \"y\":y_NN})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "id": "4ENvDCA-0U1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEfwptIxv0pjnRsTm10jMC",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93029c7ca9b0465fa69fc739ed5ca905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_047303e54b2845068fff2ac83aee34ef",
              "IPY_MODEL_c32a22e0e8c6444b987989e31d6bc9a2"
            ],
            "layout": "IPY_MODEL_da885c36df2e4253a72f7f2ff7a14c53"
          }
        },
        "047303e54b2845068fff2ac83aee34ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b34ea2d50d9147568ea78231e69b447c",
            "placeholder": "​",
            "style": "IPY_MODEL_03e0e1dfc792425bb5b361bd1b69ab36",
            "value": "0.001 MB of 0.047 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "c32a22e0e8c6444b987989e31d6bc9a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_178478eafffe4c84a2ffc6ff6ddb191a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb7de7b4223a4de582b014c3179d7a91",
            "value": 0.011937334062949678
          }
        },
        "da885c36df2e4253a72f7f2ff7a14c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b34ea2d50d9147568ea78231e69b447c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e0e1dfc792425bb5b361bd1b69ab36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "178478eafffe4c84a2ffc6ff6ddb191a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7de7b4223a4de582b014c3179d7a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}