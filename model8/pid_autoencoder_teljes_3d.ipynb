{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/model8/pid_autoencoder_teljes_3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tNYnFR-6Xh",
        "outputId": "a6df9c3f-3954-4f7b-f7bf-34d961175f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.7-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.12.1-py2.py3-none-any.whl (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 61.9 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.12.0-py2.py3-none-any.whl (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 54.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 47.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 69.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 40.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 60.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 65.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 67.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 66.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 61.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 57.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 55.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 58.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=58bce35864decfb847055dea7eb5cc4b457d24c8a5a3647395d70236bfc22563\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "bcc472a1-e3b9-426c-a409-654d17a44df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint1\"\n",
        "data_source=\"5\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_error_part1.csv\"\n",
        "fname_bad = \"415_SC_error_part2.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" \n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" \n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "9856c3c1-cd91-4b95-da05-b6365731abae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "vUcMjZAGKvtt"
      },
      "outputs": [],
      "source": [
        "#df_good = pd.read_csv(path_good,usecols=None)\n",
        "#df_bad = pd.read_csv(path_bad,usecols=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJlTtED0Gv2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSAhqTvMGczk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYuDXKraLOt4",
        "outputId": "630d0e8e-5f7b-48cd-dd29-ce0535fab8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(df_good.isnull().values.any())\n",
        "print(df_bad.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname_good_list=[\"415_SC_error_part1.csv\",\"415_SC_error_new_part1.csv\",\"415_SC_error_new_part3.csv\"]\n",
        "fname_bad_list=[\"415_SC_error_part2.csv\",\"415_SC_error_new_part2.csv\",\"415_SC_error_new_part4.csv\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1gbYSaDLG915"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_debug_=False"
      ],
      "metadata": {
        "id": "1oj-pm2rOl04"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_create(df,csv_list,path_name):\n",
        "    df2=df\n",
        "    for fname in csv_list:\n",
        "        pfname=path_name+fname\n",
        "        if _debug_:\n",
        "            print(pfname)\n",
        "        df1=pd.read_csv(pfname,usecols=None)\n",
        "        df2=pd.concat([df2,df1],axis=0,ignore_index=True)\n",
        "        if _debug_:\n",
        "            print(df2.tail(1))\n",
        "    return df2\n"
      ],
      "metadata": {
        "id": "Ky65Q_p8HVoR"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test1=pd.read_csv(path_good+fname_good_list[2])\n",
        "df_test1.tail(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "CfAbvK6LMiFt",
        "outputId": "c1664bbe-cd6d-4014-c250-1be9c7071b18"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "655  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "\n",
              "            7         8         9        10        11        12        13  \\\n",
              "655  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "\n",
              "           14        15        16        17        18        19  \n",
              "655  0.216297  0.216297  0.216297  0.216297  0.216297  1.159305  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-046c9c5e-f4c4-41ef-ab03-f5dbd61f6dea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>1.159305</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-046c9c5e-f4c4-41ef-ab03-f5dbd61f6dea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-046c9c5e-f4c4-41ef-ab03-f5dbd61f6dea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-046c9c5e-f4c4-41ef-ab03-f5dbd61f6dea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_good_new=pd.DataFrame()\n",
        "df_all_good_new=df_create(df_all_good_new,fname_good_list,path_good)"
      ],
      "metadata": {
        "id": "0R3D5a1WKDy9"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_bad_new=pd.DataFrame()\n",
        "df_all_bad_new=df_create(df_all_bad_new,fname_bad_list,path_good)"
      ],
      "metadata": {
        "id": "mRujHxlML10D"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "dc4f9c76-35ff-4333-c430-996de18e2f60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "2496  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "2497  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "\n",
              "             7         8         9        10        11        12        13  \\\n",
              "2496  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "2497  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185   \n",
              "\n",
              "            14        15        16        17        18        19  \n",
              "2496  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  \n",
              "2497  0.239185  0.239185  0.239185  0.239185  0.239185  0.239185  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0db76829-584d-413c-a7e3-0cba3f88d6e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.239185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0db76829-584d-413c-a7e3-0cba3f88d6e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0db76829-584d-413c-a7e3-0cba3f88d6e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0db76829-584d-413c-a7e3-0cba3f88d6e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ],
      "source": [
        "df_all_bad_new.tail(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "df_all=pd.concat([df_all_good_new,df_all_bad_new],axis=0,ignore_index=True)\n",
        "scaler=MinMaxScaler()\n",
        "scaler.fit(df_all)\n",
        "good_scaled=scaler.transform(df_all_good_new)\n",
        "bad_scaled=scaler.transform(df_all_bad_new)\n"
      ],
      "metadata": {
        "id": "ibrdNyqsspbR"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFHmudoYPU3A"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxLD8g1htekl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_good_scaled=pd.DataFrame(good_scaled,columns=df_all.columns)\n",
        "df_bad_scaled=pd.DataFrame(bad_scaled,columns=df_all.columns)\n",
        "df_good_scaled[\"state\"]=0\n",
        "df_bad_scaled[\"state\"]=1"
      ],
      "metadata": {
        "id": "9FdSgm_ztqjZ"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3W5mi70VM6hL",
        "outputId": "234178b2-1c36-4777-ec08-b95e21a42073"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0     0.203312  0.000000  0.000000  0.149256  0.339488  0.516456  0.546188   \n",
              "1     0.000000  0.155851  0.185726  0.339488  0.516456  0.546188  0.556316   \n",
              "2     0.155851  0.312632  0.367803  0.516456  0.546188  0.556316  0.556316   \n",
              "3     0.312632  0.466332  0.537185  0.546188  0.556316  0.556316  0.556316   \n",
              "4     0.466332  0.609315  0.565642  0.556316  0.556316  0.556316  0.556316   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "2073  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2074  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2075  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2076  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "2077  0.641484  0.641484  0.575293  0.556271  0.556271  0.556271  0.556271   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "0     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "1     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "2     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "3     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "4     0.556316  0.556316  0.556316  ...  0.556316  0.556316  0.556316   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "2073  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2074  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2075  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2076  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "2077  0.556271  0.556271  0.556271  ...  0.556271  0.556271  0.556271   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "0     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "1     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "2     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "3     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "4     0.556316  0.556316  0.556316  0.556316  0.556316  0.556316      0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "2073  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2074  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2075  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2076  0.556271  0.556271  0.556271  0.556271  0.556271  0.556271      0  \n",
              "2077  0.556271  0.556271  0.556271  0.556271  0.556271  0.565565      0  \n",
              "\n",
              "[2078 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-402af0e7-408a-4c47-96db-3759f7a358b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.203312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149256</td>\n",
              "      <td>0.339488</td>\n",
              "      <td>0.516456</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155851</td>\n",
              "      <td>0.185726</td>\n",
              "      <td>0.339488</td>\n",
              "      <td>0.516456</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.155851</td>\n",
              "      <td>0.312632</td>\n",
              "      <td>0.367803</td>\n",
              "      <td>0.516456</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.312632</td>\n",
              "      <td>0.466332</td>\n",
              "      <td>0.537185</td>\n",
              "      <td>0.546188</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.466332</td>\n",
              "      <td>0.609315</td>\n",
              "      <td>0.565642</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0.556316</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2073</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2074</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2075</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2076</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2077</th>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.641484</td>\n",
              "      <td>0.575293</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.556271</td>\n",
              "      <td>0.565565</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2078 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-402af0e7-408a-4c47-96db-3759f7a358b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-402af0e7-408a-4c47-96db-3759f7a358b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-402af0e7-408a-4c47-96db-3759f7a358b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ],
      "source": [
        "df_good_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_scaled=pd.concat([df_good_scaled,df_bad_scaled],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "a74d243d-033c-4585-b311-e570914198e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "2493  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2494  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2495  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2496  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "2497  0.641667  0.641667  0.575509  0.556497  0.556497  0.556497  0.556497   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "2493  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2494  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2495  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2496  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "2497  0.556497  0.556497  0.556497  ...  0.556497  0.556497  0.556497   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "2493  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2494  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2495  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2496  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "2497  0.556497  0.556497  0.556497  0.556497  0.556497  0.556497      1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e3e46cf-4f55-47fe-a189-9e63ccf942fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.575509</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>0.556497</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e3e46cf-4f55-47fe-a189-9e63ccf942fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1e3e46cf-4f55-47fe-a189-9e63ccf942fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1e3e46cf-4f55-47fe-a189-9e63ccf942fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ],
      "source": [
        "df_all_scaled.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "nVvhP84S_F1y"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7 dimenzióra képez, a grafikus megjelenítés biztató\n",
        "_N1_=135\n",
        "_N2_=30\n",
        "_N3_=7\n",
        "_lr_=0.0001\n",
        "_batch_size_=32\n",
        "_drop1_=0.0\n",
        "_drop2_=0.0\n",
        "_epochs_=3500\n",
        "'''\n",
        "\n",
        "'''\n",
        "_N1_=135\n",
        "_N2_=30\n",
        "_N3_=2\n",
        "_lr_=0.0001\n",
        "_batch_size_=32\n",
        "_drop1_=0.0\n",
        "_drop2_=0.0\n",
        "_epochs_=3500\n",
        "'''\n",
        "_N1_=13\n",
        "_N2_=5\n",
        "_N3_=3\n",
        "_N4_=5\n",
        "_N5_=13\n",
        "\n",
        "_lr_=0.0001\n",
        "_batch_size_=32\n",
        "_drop1_=0.0\n",
        "_drop2_=0.0\n",
        "_epochs_=5000\n",
        "_comment_=\"3 réteg:  20] 13 2 13 [20 \"\n",
        "\n"
      ],
      "metadata": {
        "id": "XC5_bGE0iyi4"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"lr\": _lr_, \"batch_size\": _batch_size_,\"architecture\": \"AutoencoderNN\", \n",
        "          \"depth\": 2,\n",
        "          \"layer1\":_N1_,  \"layer2\":_N2_,\"layer3\":_N3_,\"layer4\":_N2_,\"layer5\":_N1_,\"layer_out\":20, \n",
        "          \"drop1\":_drop1_,\"drop2\":_drop2_,\n",
        "          \"epochs\":_epochs_,\n",
        "          \"comment\": _comment_\n",
        "\n",
        "          \n",
        "          \n",
        "          }\n",
        "\n",
        "wandb.init(project=\"pid_autoencoder\", entity=\"pid_status\",config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "d576aee882a24fcb9a970824188530fb"
          ]
        },
        "id": "nOtKllcviuoj",
        "outputId": "686d1086-9606-48e1-d69d-3b28334aff4b"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:sl8olbno) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d576aee882a24fcb9a970824188530fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/MAE</td><td>████▇▇▇▇▆▆▅▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████▁▁▁▁████▁▁▁▁████▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>████▇▇▇▇▆▆▅▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████▁▁▁▁████▁▁▁▁████▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/MAE</td><td>0.01788</td></tr><tr><td>epoch/epoch</td><td>4999</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.01788</td></tr><tr><td>epoch/lr</td><td>0.0001</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">lunar-wind-39</strong>: <a href=\"https://wandb.ai/pid_status/pid_autoencoder/runs/sl8olbno\" target=\"_blank\">https://wandb.ai/pid_status/pid_autoencoder/runs/sl8olbno</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221225_181745-sl8olbno/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:sl8olbno). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221225_214736-3k5sclxx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/pid_status/pid_autoencoder/runs/3k5sclxx\" target=\"_blank\">flowing-leaf-40</a></strong> to <a href=\"https://wandb.ai/pid_status/pid_autoencoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/pid_status/pid_autoencoder/runs/3k5sclxx?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc2f9cf1880>"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=20\n",
        "\n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "\n",
        "l1_out=Dense(_N1_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(input1) # kernel_initializer='lecun_normal'  # L1\n",
        "\n",
        "#l2_out=Dense(_N2_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(l1_out) #kernel_initializer='lecun_normal',  # L2\n",
        "\n",
        "l3_out=Dense(_N3_,activation=\"linear\",kernel_initializer='glorot_uniform',name=\"encoded\",kernel_regularizer=None)(l1_out) #kernel_initializer='lecun_normal',  # L3\n",
        "\n",
        "#l4_out=Dense(_N4_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(l3_out) #kernel_initializer='lecun_normal',  # L4\n",
        "\n",
        "l5_out=Dense(_N5_,activation=\"relu\",kernel_initializer='glorot_uniform',kernel_regularizer=None)(l3_out) #kernel_initializer='lecun_normal',  # L5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pred=Dense(input_size, activation=\"sigmoid\",)(l5_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=_lr_,) #\n",
        "\n",
        "model.compile(loss='MAE',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"MAE\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\"]\n",
        "X_NN=df_all_scaled[prediktorok][:]  # \n",
        "y_NN=df_all_scaled[\"state\"][:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\"\n",
        "#model_file=\"model_PID__94_loss_0.116_vloss_0.115_acc_0.950_vacc_0.966.hdf5\"\n",
        "model_file=\"model_PID__4491_loss_0.115_vloss_0.679_acc_0.954_vacc_0.880.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False"
      ],
      "metadata": {
        "id": "UIxI3AS6Yw3S"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=len(X_NN)-1,shuffle=True,random_state=33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_NN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xru7s5BsOD3",
        "outputId": "aefaf1cd-38af-4423-f071-ae59bbd1e014"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4576"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "jJfOOTfGfDXi"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate_corrector(epoch,lr):\n",
        "    \n",
        "    if epoch > 4500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 4000:\n",
        "        lr = 0.0005\n",
        "        return lr\n",
        "    \n",
        "\n",
        "    if epoch > 3500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    \n",
        "    if epoch > 3000:\n",
        "        lr = 0.0005\n",
        "        return lr\n",
        "    \n",
        "\n",
        "    if epoch > 2500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "\n",
        "    if epoch > 2000:\n",
        "        lr = 0.0005\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 1500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 1000:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 500:\n",
        "        lr = 0.0001\n",
        "        return lr\n",
        "    return lr\n",
        "    "
      ],
      "metadata": {
        "id": "A-Kv8ORiEfub"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.keras import WandbMetricsLogger\n",
        "fname=\"./model_Encoder_\"\n",
        "callbacks = [\n",
        "        LearningRateScheduler(learning_rate_corrector,verbose=1),\n",
        "        WandbMetricsLogger(),       \n",
        "        #ModelCheckpoint(filepath=fname+\"_{epoch:04.0f}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{MAE:.3f}_vacc_{val_MAE:.3f}.hdf5\", \n",
        "        #                monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__learning__=True"
      ],
      "metadata": {
        "id": "O6ofy0moderd"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7Z3Z4q14D7eC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d529f28-f258-4d00-da5f-314c3f8d2085"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 13)                273       \n",
            "                                                                 \n",
            " encoded (Dense)             (None, 3)                 42        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 13)                52        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                280       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 647\n",
            "Trainable params: 647\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "9Ol0mW6WRlkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb31d3b-d062-4bdd-c974-e2a64b5e906d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA streamkimeneten csak az utolsó 5000 sor látható.\u001b[0m\n",
            "Epoch 3751: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3751/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3752: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3752/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3753: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3753/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3754: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3754/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3755: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3755/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3756: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3756/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3757: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3757/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3758: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3758/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3759: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3759/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3760: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3760/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3761: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3761/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3762: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3762/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3763: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3763/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3764: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3764/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3765: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3765/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3766: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3766/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3767: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3767/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3768: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3768/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3769: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3769/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3770: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3770/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3771: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3771/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3772: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3772/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3773: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3773/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3774: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3774/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3775: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3775/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3776: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3776/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3777: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3777/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3778: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3778/5000\n",
            "143/143 [==============================] - 0s 2ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3779: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3779/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3780: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3780/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3781: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3781/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3782: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3782/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3783: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3783/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3784: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3784/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3785: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3785/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3786: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3786/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3787: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3787/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3788: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3788/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3789: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3789/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3790: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3790/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3791: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3791/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3792: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3792/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3793: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3793/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3794: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3794/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3795: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3795/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3796: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3796/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3797: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3797/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3798: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3798/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3799: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3799/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3800: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3800/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3801: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3801/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3802: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3802/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3803: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3803/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3804: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3804/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3805: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3805/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3806: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3806/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3807: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3807/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3808: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3808/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3809: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3809/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3810: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3810/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3811: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3811/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3812: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3812/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3813: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3813/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3814: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3814/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3815: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3815/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3816: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3816/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3817: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3817/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3818: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3818/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3819: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3819/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3820: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3820/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3821: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3821/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3822: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3822/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3823: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3823/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3824: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3824/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3825: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3825/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3826: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3826/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3827: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3827/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3828: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3828/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3829: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3829/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3830: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3830/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3831: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3831/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3832: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3832/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3833: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3833/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3834: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3834/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3835: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3835/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3836: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3836/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3837: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3837/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3838: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3838/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3839: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3839/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3840: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3840/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3841: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3841/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3842: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3842/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3843: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3843/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3844: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3844/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3845: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3845/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3846: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3846/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3847: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3847/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3848: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3848/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3849: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3849/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3850: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3850/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3851: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3851/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3852: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3852/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3853: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3853/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3854: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3854/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3855: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3855/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3856: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3856/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3857: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3857/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3858: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3858/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3859: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3859/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3860: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3860/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3861: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3861/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3862: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3862/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3863: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3863/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3864: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3864/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3865: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3865/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3866: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3866/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3867: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3867/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3868: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3868/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3869: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3869/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3870: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3870/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3871: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3871/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3872: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3872/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3873: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3873/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3874: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3874/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3875: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3875/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3876: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3876/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3877: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3877/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3878: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3878/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3879: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3879/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3880: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3880/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3881: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3881/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3882: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3882/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3883: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3883/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3884: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3884/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3885: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3885/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3886: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3886/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3887: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3887/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3888: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3888/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3889: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3889/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3890: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3890/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3891: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3891/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3892: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3892/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3893: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3893/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3894: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3894/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3895: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3895/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3896: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3896/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3897: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3897/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3898: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3898/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3899: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3899/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3900: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3900/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3901: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3901/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3902: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3902/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3903: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3903/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3904: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3904/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3905: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3905/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3906: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3906/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3907: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3907/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3908: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3908/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3909: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3909/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3910: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3910/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3911: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3911/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3912: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3912/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3913: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3913/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3914: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3914/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3915: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3915/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3916: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3916/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3917: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3917/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3918: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3918/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3919: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3919/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3920: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3920/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3921: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3921/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3922: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3922/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3923: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3923/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3924: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3924/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3925: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3925/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3926: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3926/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3927: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3927/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3928: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3928/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3929: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3929/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3930: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3930/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3931: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3931/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3932: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3932/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3933: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3933/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3934: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3934/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3935: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3935/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3936: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3936/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3937: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3937/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3938: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3938/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3939: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3939/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3940: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3940/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3941: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3941/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3942: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3942/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3943: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3943/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3944: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3944/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3945: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3945/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3946: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3946/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3947: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3947/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3948: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3948/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3949: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3949/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3950: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3950/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3951: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3951/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3952: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3952/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3953: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3953/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3954: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3954/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3955: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3955/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3956: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3956/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3957: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3957/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3958: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3958/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3959: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3959/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3960: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3960/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3961: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3961/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3962: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3962/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3963: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3963/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3964: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3964/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3965: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3965/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3966: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3966/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3967: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3967/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3968: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3968/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3969: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3969/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3970: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3970/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3971: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3971/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3972: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3972/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3973: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3973/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3974: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3974/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3975: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3975/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3976: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3976/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3977: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3977/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3978: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3978/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3979: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3979/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3980: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3980/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3981: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3981/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3982: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3982/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3983: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3983/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3984: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3984/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3985: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3985/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3986: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3986/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3987: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3987/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3988: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3988/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3989: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3989/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3990: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3990/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3991: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3991/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3992: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3992/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3993: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3993/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3994: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3994/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3995: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3995/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3996: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3996/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3997: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3997/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3998: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3998/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3999: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 3999/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4000: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4000/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4001: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4001/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4002: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4002/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4003: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4003/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4004: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4004/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4005: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4005/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4006: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4006/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4007: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4007/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4008: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4008/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4009: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4009/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4010: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4010/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4011: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4011/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4012: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4012/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4013: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4013/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4014: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4014/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4015: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4015/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4016: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4016/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4017: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4017/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4018: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4018/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4019: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4019/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4020: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4020/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4021: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4021/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4022: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4022/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4023: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4023/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4024: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4024/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4025: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4025/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4026: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4026/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4027: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4027/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4028: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4028/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4029: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4029/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4030: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4030/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4031: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4031/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4032: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4032/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4033: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4033/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4034: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4034/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4035: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4035/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4036: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4036/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4037: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4037/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4038: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4038/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4039: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4039/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4040: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4040/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4041: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4041/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4042: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4042/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4043: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4043/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4044: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4044/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4045: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4045/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4046: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4046/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4047: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4047/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4048: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4048/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4049: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4049/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4050: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4050/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4051: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4051/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4052: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4052/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4053: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4053/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4054: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4054/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4055: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4055/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4056: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4056/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4057: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4057/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4058: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4058/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4059: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4059/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4060: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4060/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4061: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4061/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4062: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4062/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4063: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4063/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4064: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4064/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4065: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4065/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4066: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4066/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4067: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4067/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4068: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4068/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4069: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4069/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4070: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4070/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4071: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4071/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4072: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4072/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4073: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4073/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4074: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4074/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4075: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4075/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4076: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4076/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4077: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4077/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4078: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4078/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4079: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4079/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4080: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4080/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4081: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4081/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4082: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4082/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4083: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4083/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4084: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4084/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4085: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4085/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4086: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4086/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4087: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4087/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4088: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4088/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4089: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4089/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4090: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4090/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4091: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4091/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4092: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4092/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4093: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4093/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4094: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4094/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4095: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4095/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4096: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4096/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4097: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4097/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4098: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4098/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4099: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4099/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4100: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4100/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4101: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4101/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4102: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4102/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4103: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4103/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4104: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4104/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4105: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4105/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4106: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4106/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4107: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4107/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4108: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4108/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4109: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4109/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4110: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4110/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4111: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4111/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4112: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4112/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4113: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4113/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4114: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4114/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4115: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4115/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4116: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4116/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4117: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4117/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4118: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4118/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4119: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4119/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4120: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4120/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4121: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4121/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4122: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4122/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4123: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4123/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4124: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4124/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4125: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4125/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4126: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4126/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4127: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4127/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4128: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4128/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4129: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4129/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4130: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4130/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4131: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4131/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4132: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4132/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4133: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4133/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4134: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4134/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4135: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4135/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4136: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4136/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4137: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4137/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4138: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4138/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4139: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4139/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4140: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4140/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4141: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4141/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4142: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4142/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4143: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4143/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4144: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4144/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4145: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4145/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4146: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4146/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4147: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4147/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4148: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4148/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4149: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4149/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4150: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4150/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4151: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4151/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4152: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4152/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4153: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4153/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4154: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4154/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4155: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4155/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4156: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4156/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4157: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4157/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4158: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4158/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4159: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4159/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4160: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4160/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4161: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4161/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4162: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4162/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4163: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4163/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4164: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4164/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4165: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4165/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4166: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4166/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4167: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4167/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4168: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4168/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4169: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4169/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4170: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4170/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4171: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4171/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4172: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4172/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4173: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4173/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4174: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4174/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4175: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4175/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4176: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4176/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4177: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4177/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4178: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4178/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4179: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4179/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4180: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4180/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4181: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4181/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4182: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4182/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4183: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4183/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4184: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4184/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4185: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4185/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4186: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4186/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4187: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4187/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4188: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4188/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4189: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4189/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4190: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4190/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4191: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4191/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4192: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4192/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4193: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4193/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4194: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4194/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4195: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4195/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4196: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4196/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4197: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4197/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4198: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4198/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4199: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4199/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4200: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4200/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4201: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4201/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4202: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4202/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4203: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4203/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4204: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4204/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4205: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4205/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4206: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4206/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4207: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4207/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4208: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4208/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4209: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4209/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4210: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4210/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4211: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4211/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4212: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4212/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4213: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4213/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4214: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4214/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4215: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4215/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4216: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4216/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4217: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4217/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4218: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4218/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4219: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4219/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4220: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4220/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4221: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4221/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4222: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4222/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4223: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4223/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4224: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4224/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4225: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4225/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4226: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4226/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4227: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4227/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4228: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4228/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4229: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4229/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4230: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4230/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4231: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4231/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4232: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4232/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4233: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4233/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4234: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4234/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4235: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4235/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4236: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4236/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4237: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4237/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4238: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4238/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4239: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4239/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4240: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4240/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4241: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4241/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4242: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4242/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4243: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4243/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4244: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4244/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4245: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4245/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4246: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4246/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4247: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4247/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4248: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4248/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4249: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4249/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4250: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4250/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4251: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4251/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4252: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4252/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4253: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4253/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4254: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4254/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4255: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4255/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4256: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4256/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4257: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4257/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4258: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4258/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4259: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4259/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4260: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4260/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4261: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4261/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4262: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4262/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4263: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4263/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4264: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4264/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4265: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4265/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4266: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4266/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4267: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4267/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4268: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4268/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4269: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4269/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4270: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4270/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4271: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4271/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4272: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4272/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4273: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4273/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4274: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4274/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4275: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4275/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4276: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4276/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4277: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4277/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4278: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4278/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4279: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4279/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4280: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4280/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4281: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4281/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4282: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4282/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4283: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4283/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4284: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4284/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4285: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4285/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4286: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4286/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4287: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4287/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4288: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4288/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4289: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4289/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4290: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4290/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4291: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4291/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4292: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4292/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4293: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4293/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4294: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4294/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4295: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4295/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4296: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4296/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4297: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4297/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4298: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4298/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4299: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4299/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4300: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4300/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4301: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4301/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4302: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4302/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4303: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4303/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4304: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4304/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4305: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4305/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4306: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4306/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4307: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4307/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4308: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4308/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4309: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4309/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4310: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4310/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4311: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4311/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4312: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4312/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4313: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4313/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4314: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4314/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4315: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4315/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4316: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4316/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4317: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4317/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4318: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4318/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4319: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4319/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4320: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4320/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4321: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4321/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4322: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4322/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4323: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4323/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4324: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4324/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4325: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4325/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4326: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4326/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4327: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4327/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4328: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4328/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4329: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4329/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4330: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4330/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4331: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4331/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4332: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4332/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4333: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4333/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4334: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4334/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4335: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4335/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4336: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4336/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4337: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4337/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4338: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4338/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4339: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4339/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4340: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4340/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4341: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4341/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4342: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4342/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4343: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4343/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4344: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4344/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4345: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4345/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4346: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4346/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4347: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4347/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4348: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4348/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4349: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4349/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4350: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4350/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4351: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4351/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4352: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4352/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4353: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4353/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4354: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4354/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4355: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4355/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4356: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4356/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4357: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4357/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4358: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4358/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4359: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4359/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4360: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4360/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4361: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4361/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4362: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4362/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4363: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4363/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4364: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4364/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4365: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4365/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4366: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4366/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4367: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4367/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4368: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4368/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4369: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4369/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4370: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4370/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4371: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4371/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4372: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4372/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4373: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4373/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4374: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4374/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4375: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4375/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4376: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4376/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4377: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4377/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4378: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4378/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4379: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4379/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4380: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4380/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4381: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4381/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4382: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4382/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4383: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4383/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4384: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4384/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4385: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4385/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4386: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4386/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4387: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4387/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4388: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4388/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4389: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4389/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4390: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4390/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4391: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4391/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4392: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4392/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4393: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4393/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4394: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4394/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4395: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4395/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4396: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4396/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4397: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4397/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4398: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4398/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4399: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4399/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4400: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4400/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4401: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4401/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4402: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4402/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4403: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4403/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4404: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4404/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4405: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4405/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4406: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4406/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4407: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4407/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4408: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4408/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4409: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4409/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4410: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4410/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4411: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4411/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4412: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4412/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4413: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4413/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4414: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4414/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4415: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4415/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4416: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4416/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4417: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4417/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4418: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4418/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4419: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4419/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4420: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4420/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4421: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4421/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4422: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4422/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4423: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4423/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4424: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4424/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4425: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4425/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4426: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4426/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4427: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4427/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4428: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4428/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4429: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4429/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4430: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4430/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4431: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4431/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4432: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4432/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4433: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4433/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4434: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4434/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4435: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4435/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4436: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4436/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4437: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4437/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4438: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4438/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4439: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4439/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4440: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4440/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4441: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4441/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4442: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4442/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4443: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4443/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4444: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4444/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4445: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4445/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4446: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4446/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4447: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4447/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4448: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4448/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4449: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4449/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4450: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4450/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4451: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4451/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4452: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4452/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4453: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4453/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4454: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4454/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4455: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4455/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4456: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4456/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4457: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4457/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4458: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4458/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4459: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4459/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4460: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4460/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4461: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4461/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4462: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4462/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4463: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4463/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4464: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4464/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4465: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4465/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4466: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4466/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4467: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4467/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4468: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4468/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4469: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4469/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4470: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4470/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4471: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4471/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4472: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4472/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4473: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4473/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4474: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4474/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4475: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4475/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4476: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4476/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4477: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4477/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4478: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4478/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4479: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4479/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4480: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4480/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4481: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4481/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4482: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4482/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4483: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4483/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4484: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4484/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4485: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4485/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4486: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4486/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4487: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4487/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4488: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4488/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4489: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4489/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4490: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4490/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4491: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4491/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4492: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4492/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4493: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4493/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4494: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4494/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4495: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4495/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4496: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4496/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4497: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4497/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4498: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4498/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4499: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4499/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4500: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4500/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0205 - MAE: 0.0205 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4501: LearningRateScheduler setting learning rate to 0.0005.\n",
            "Epoch 4501/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 5.0000e-04\n",
            "\n",
            "Epoch 4502: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4502/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4503: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4503/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4504: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4504/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4505: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4505/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4506: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4506/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4507: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4507/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4508: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4508/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4509: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4509/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4510: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4510/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4511: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4511/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4512: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4512/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4513: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4513/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4514: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4514/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4515: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4515/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4516: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4516/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4517: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4517/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4518: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4518/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4519: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4519/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4520: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4520/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4521: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4521/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4522: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4522/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4523: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4523/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4524: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4524/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4525: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4525/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4526: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4526/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4527: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4527/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4528: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4528/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4529: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4529/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4530: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4530/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4531: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4531/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4532: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4532/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4533: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4533/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4534: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4534/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4535: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4535/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4536: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4536/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4537: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4537/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4538: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4538/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4539: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4539/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4540: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4540/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4541: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4541/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4542: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4542/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4543: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4543/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4544: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4544/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4545: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4545/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4546: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4546/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4547: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4547/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4548: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4548/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4549: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4549/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4550: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4550/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4551: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4551/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4552: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4552/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4553: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4553/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4554: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4554/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4555: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4555/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4556: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4556/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4557: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4557/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4558: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4558/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4559: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4559/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4560: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4560/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4561: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4561/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4562: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4562/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4563: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4563/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4564: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4564/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4565: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4565/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4566: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4566/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4567: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4567/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4568: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4568/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4569: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4569/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4570: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4570/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4571: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4571/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4572: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4572/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4573: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4573/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4574: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4574/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4575: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4575/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4576: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4576/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4577: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4577/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4578: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4578/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4579: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4579/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4580: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4580/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4581: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4581/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4582: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4582/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4583: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4583/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4584: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4584/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4585: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4585/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4586: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4586/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4587: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4587/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4588: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4588/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4589: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4589/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4590: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4590/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4591: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4591/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4592: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4592/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4593: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4593/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4594: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4594/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4595: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4595/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4596: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4596/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4597: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4597/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4598: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4598/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4599: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4599/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4600: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4600/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4601: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4601/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4602: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4602/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4603: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4603/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4604: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4604/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4605: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4605/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4606: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4606/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4607: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4607/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4608: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4608/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4609: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4609/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4610: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4610/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4611: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4611/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4612: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4612/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4613: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4613/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4614: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4614/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4615: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4615/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4616: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4616/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4617: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4617/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4618: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4618/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4619: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4619/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4620: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4620/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4621: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4621/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4622: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4622/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4623: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4623/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4624: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4624/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4625: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4625/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4626: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4626/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4627: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4627/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4628: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4628/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4629: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4629/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4630: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4630/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4631: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4631/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4632: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4632/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4633: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4633/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4634: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4634/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4635: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4635/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4636: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4636/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4637: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4637/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4638: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4638/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4639: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4639/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4640: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4640/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4641: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4641/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4642: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4642/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4643: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4643/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4644: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4644/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4645: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4645/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4646: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4646/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4647: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4647/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4648: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4648/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4649: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4649/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4650: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4650/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4651: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4651/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4652: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4652/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4653: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4653/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4654: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4654/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4655: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4655/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4656: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4656/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4657: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4657/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4658: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4658/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4659: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4659/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4660: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4660/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4661: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4661/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4662: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4662/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4663: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4663/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4664: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4664/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4665: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4665/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4666: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4666/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4667: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4667/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4668: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4668/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4669: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4669/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4670: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4670/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4671: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4671/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4672: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4672/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4673: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4673/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4674: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4674/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4675: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4675/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4676: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4676/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4677: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4677/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4678: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4678/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4679: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4679/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4680: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4680/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4681: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4681/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4682: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4682/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4683: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4683/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4684: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4684/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4685: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4685/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4686: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4686/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4687: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4687/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4688: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4688/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4689: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4689/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4690: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4690/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4691: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4691/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4692: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4692/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4693: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4693/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4694: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4694/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4695: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4695/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4696: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4696/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4697: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4697/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4698: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4698/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4699: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4699/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4700: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4700/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4701: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4701/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4702: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4702/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4703: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4703/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4704: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4704/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4705: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4705/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4706: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4706/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4707: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4707/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4708: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4708/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4709: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4709/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4710: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4710/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4711: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4711/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4712: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4712/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4713: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4713/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4714: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4714/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4715: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4715/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4716: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4716/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4717: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4717/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4718: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4718/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4719: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4719/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4720: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4720/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4721: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4721/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4722: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4722/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4723: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4723/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4724: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4724/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4725: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4725/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4726: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4726/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4727: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4727/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4728: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4728/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4729: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4729/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4730: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4730/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4731: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4731/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4732: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4732/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4733: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4733/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4734: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4734/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4735: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4735/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4736: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4736/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4737: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4737/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4738: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4738/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4739: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4739/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4740: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4740/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4741: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4741/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4742: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4742/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4743: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4743/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4744: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4744/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4745: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4745/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4746: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4746/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4747: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4747/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4748: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4748/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4749: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4749/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4750: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4750/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4751: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4751/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4752: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4752/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4753: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4753/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4754: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4754/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4755: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4755/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4756: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4756/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4757: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4757/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4758: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4758/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4759: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4759/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4760: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4760/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4761: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4761/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4762: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4762/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4763: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4763/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4764: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4764/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4765: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4765/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4766: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4766/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4767: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4767/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4768: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4768/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4769: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4769/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4770: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4770/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4771: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4771/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4772: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4772/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4773: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4773/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4774: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4774/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4775: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4775/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4776: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4776/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4777: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4777/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4778: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4778/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4779: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4779/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4780: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4780/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4781: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4781/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4782: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4782/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4783: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4783/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4784: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4784/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4785: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4785/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4786: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4786/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4787: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4787/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4788: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4788/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4789: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4789/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4790: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4790/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4791: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4791/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4792: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4792/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4793: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4793/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4794: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4794/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4795: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4795/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4796: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4796/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4797: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4797/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4798: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4798/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4799: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4799/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4800: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4800/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4801: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4801/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4802: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4802/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4803: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4803/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4804: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4804/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4805: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4805/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4806: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4806/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4807: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4807/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4808: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4808/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4809: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4809/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4810: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4810/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4811: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4811/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4812: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4812/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4813: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4813/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4814: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4814/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4815: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4815/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4816: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4816/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4817: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4817/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4818: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4818/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4819: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4819/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4820: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4820/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4821: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4821/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4822: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4822/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4823: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4823/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4824: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4824/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4825: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4825/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4826: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4826/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4827: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4827/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4828: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4828/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4829: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4829/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4830: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4830/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4831: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4831/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4832: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4832/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4833: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4833/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4834: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4834/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4835: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4835/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4836: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4836/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4837: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4837/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4838: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4838/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4839: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4839/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4840: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4840/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4841: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4841/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4842: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4842/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4843: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4843/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4844: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4844/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4845: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4845/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4846: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4846/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4847: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4847/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4848: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4848/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4849: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4849/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4850: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4850/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4851: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4851/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4852: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4852/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4853: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4853/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4854: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4854/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4855: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4855/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4856: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4856/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4857: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4857/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4858: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4858/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4859: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4859/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4860: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4860/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4861: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4861/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4862: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4862/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4863: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4863/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4864: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4864/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4865: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4865/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4866: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4866/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4867: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4867/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4868: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4868/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4869: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4869/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4870: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4870/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4871: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4871/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4872: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4872/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4873: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4873/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4874: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4874/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4875: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4875/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4876: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4876/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4877: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4877/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4878: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4878/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4879: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4879/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4880: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4880/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4881: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4881/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4882: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4882/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4883: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4883/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4884: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4884/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4885: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4885/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4886: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4886/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4887: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4887/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4888: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4888/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4889: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4889/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4890: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4890/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4891: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4891/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4892: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4892/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4893: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4893/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4894: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4894/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4895: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4895/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4896: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4896/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4897: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4897/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4898: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4898/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4899: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4899/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4900: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4900/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4901: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4901/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4902: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4902/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4903: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4903/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4904: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4904/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4905: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4905/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4906: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4906/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4907: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4907/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4908: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4908/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4909: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4909/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4910: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4910/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4911: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4911/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4912: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4912/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4913: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4913/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4914: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4914/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4915: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4915/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4916: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4916/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4917: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4917/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4918: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4918/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4919: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4919/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4920: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4920/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4921: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4921/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4922: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4922/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4923: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4923/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4924: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4924/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4925: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4925/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4926: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4926/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4927: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4927/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4928: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4928/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4929: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4929/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4930: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4930/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4931: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4931/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4932: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4932/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4933: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4933/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4934: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4934/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4935: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4935/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4936: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4936/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4937: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4937/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4938: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4938/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4939: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4939/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4940: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4940/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4941: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4941/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4942: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4942/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4943: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4943/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4944: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4944/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4945: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4945/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4946: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4946/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4947: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4947/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4948: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4948/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4949: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4949/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4950: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4950/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4951: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4951/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4952: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4952/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4953: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4953/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4954: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4954/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4955: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4955/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4956: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4956/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4957: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4957/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4958: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4958/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4959: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4959/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4960: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4960/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4961: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4961/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4962: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4962/5000\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4963: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4963/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4964: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4964/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4965: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4965/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4966: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4966/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4967: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4967/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4968: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4968/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4969: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4969/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4970: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4970/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4971: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4971/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4972: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4972/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4973: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4973/5000\n",
            "143/143 [==============================] - 1s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4974: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4974/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4975: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4975/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4976: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4976/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4977: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4977/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4978: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4978/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4979: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4979/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4980: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4980/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4981: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4981/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4982: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4982/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4983: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4983/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4984: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4984/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4985: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4985/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4986: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4986/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4987: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4987/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4988: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4988/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4989: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4989/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4990: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4990/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4991: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4991/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4992: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4992/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4993: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4993/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4994: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4994/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4995: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4995/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4996: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4996/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4997: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4997/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4998: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4998/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4999: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 4999/5000\n",
            "143/143 [==============================] - 0s 3ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5000: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 5000/5000\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0204 - MAE: 0.0204 - lr: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "if __learning__: \n",
        "    history = model.fit(X_train, X_train, epochs=_epochs_, batch_size=_batch_size_, verbose=1,callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"autoencoder_model_1_encoder.hdf5\")"
      ],
      "metadata": {
        "id": "EkZ9T-NH513_"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False\n",
        "model_file=\"model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5\"\n",
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "EGg1PjCJDTKF"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "JgzklVywoNmk"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "pwcWQ94IpDFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5d0051-f62b-428f-f674-7fc65b1d9634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 90ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JlHV6_j9wUiE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i1=model.get_layer(\"dense\")"
      ],
      "metadata": {
        "id": "rLxI7wmUDgID"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o1=model.get_layer(\"encoded\")"
      ],
      "metadata": {
        "id": "zeveHSkCDoma"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "to3v7cxmD60p"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states_fun = K.function([i1.input],[o1.output])\n",
        "     "
      ],
      "metadata": {
        "id": "4798J_V2D9QW"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(X):\n",
        "    \"\"\"Return the hidden state associated with an input at the given timestep.\n",
        "    \"\"\"\n",
        "    \n",
        "    hidden_states = hidden_states_fun(X.to_numpy())[0]\n",
        "    \n",
        "    return hidden_states\n",
        "     "
      ],
      "metadata": {
        "id": "4e-T3C50EG_a"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_embedding(X_train)"
      ],
      "metadata": {
        "id": "pSdDesJpqcEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03be09a-59bf-457d-b304-93aa9bc2eb98"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.0869117 , -2.3722944 , -0.21100315],\n",
              "       [ 0.9855263 , -1.2680924 , -0.33857045],\n",
              "       [ 0.002409  , -1.3145576 , -0.37738058],\n",
              "       ...,\n",
              "       [ 0.8566735 , -1.6068366 , -0.22196949],\n",
              "       [ 0.91630685, -1.6906996 , -0.25152162],\n",
              "       [ 0.6884663 , -2.2734547 , -0.26194426]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hs=hidden_states_fun(X_train.to_numpy())[0]"
      ],
      "metadata": {
        "id": "WHyjbcplufFm"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hs)"
      ],
      "metadata": {
        "id": "7sIYdxIqrmdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bce013-60b9-4af7-f5c0-24b31ef84913"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4575"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hs[3].tolist()"
      ],
      "metadata": {
        "id": "flhNv2Thq5jP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05764f81-f896-4f8d-c080-7809b1c640b9"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9725261926651001, -1.7199878692626953, -0.25989794731140137]"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.iloc[1:2,:]"
      ],
      "metadata": {
        "id": "OkaLjsUxEi6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "56ebd7a3-94f6-475f-e28c-fa71fe16cf71"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0        1         2         3         4         5         6  \\\n",
              "482  0.64153  0.64153  0.575347  0.556328  0.556328  0.556328  0.556328   \n",
              "\n",
              "            7         8         9        10        11        12       13  \\\n",
              "482  0.556328  0.556328  0.556328  0.556328  0.556328  0.556328  0.48241   \n",
              "\n",
              "           14        15        16        17        18       19  \n",
              "482  0.418058  0.428683  0.401072  0.505245  0.515328  0.37134  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e01b3160-4f18-4f5f-bc84-ed1b23a812ac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>0.64153</td>\n",
              "      <td>0.64153</td>\n",
              "      <td>0.575347</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.556328</td>\n",
              "      <td>0.48241</td>\n",
              "      <td>0.418058</td>\n",
              "      <td>0.428683</td>\n",
              "      <td>0.401072</td>\n",
              "      <td>0.505245</td>\n",
              "      <td>0.515328</td>\n",
              "      <td>0.37134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e01b3160-4f18-4f5f-bc84-ed1b23a812ac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e01b3160-4f18-4f5f-bc84-ed1b23a812ac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e01b3160-4f18-4f5f-bc84-ed1b23a812ac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list=get_embedding(X_train)"
      ],
      "metadata": {
        "id": "Tnh4GKWNERZF"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedding_list)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15191575-bf70-4875-845a-4f837b5d84d4"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4575"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z69kCq3T-pMo"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "nZ0rmkNsBGnl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNIc1l6vF6Y4"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_changer(arr):\n",
        "    o=[\"r\" if i>0.5 else \"g\" for i in arr]\n",
        "    return o"
      ],
      "metadata": {
        "id": "YFJoZO8TG1ED"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtghT29KKGA4"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list[:][0]"
      ],
      "metadata": {
        "id": "H96EO2p_LE3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9dcc1e-0696-4a37-fb17-a9556367cc77"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.0869117 , -2.3722944 , -0.21100315], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list[:][-100]"
      ],
      "metadata": {
        "id": "e82a7m3sJxZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28da6942-cf5d-45a2-ce6b-4470e3c01e79"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.91644686, -1.6921346 , -0.25180054], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embedding(list_in, predicted, index2=1):\n",
        "    xkoordinata=[i[0] for i in list_in]\n",
        "    ykoordinata=[i[index2] for i in list_in]\n",
        "    \n",
        "    plot.figure(figsize=(12,6))\n",
        "    col_ch=color_changer(predicted)\n",
        "    plot.scatter(xkoordinata,ykoordinata,c=col_ch,marker=\".\",alpha=0.3)\n",
        "    plot.ylabel('értékek')\n",
        "    plot.xlabel('index')\n",
        "    plot\n",
        "    plot.show()"
      ],
      "metadata": {
        "id": "YMHy-wbZGeqq"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embedding_3d(list_in, predicted):\n",
        "    import plotly.express as px\n",
        "    xkoordinata=[i[0] for i in list_in]\n",
        "    ykoordinata=[i[1] for i in list_in]\n",
        "    zkoordinata=[i[2] for i in list_in]\n",
        "    zipped=list(zip(xkoordinata,ykoordinata,zkoordinata))\n",
        "    df_tmp=pd.DataFrame(zipped,columns=[\"x\",\"y\",\"z\"])\n",
        "    df_tmp[\"pred\"]=predicted.tolist()\n",
        "    fig = px.scatter_3d(df_tmp, x='x', y='y', z='z', color='pred', width=1200, height=1000,)\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HuL1OutGHHEc"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.tolist()"
      ],
      "metadata": {
        "id": "DlADklPHoKbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_embedding_3d(embedding_list[:],y_train[:])"
      ],
      "metadata": {
        "id": "4aXpzheKE7bM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7da1136d-2afe-42c8-9cfd-69fc1e11243d"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"35f0d94c-6794-4a74-bfd1-6222ee4536eb\" class=\"plotly-graph-div\" style=\"height:1000px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"35f0d94c-6794-4a74-bfd1-6222ee4536eb\")) {                    Plotly.newPlot(                        \"35f0d94c-6794-4a74-bfd1-6222ee4536eb\",                        [{\"hovertemplate\":\"x=%{x}<br>y=%{y}<br>z=%{z}<br>pred=%{marker.color}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":[1,1,1,0,1,1,1,0,0,0,1,0,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,0,1,1,1,1,0,1,0,1,0,0,0,0,1,1,1,1,0,0,1,0,1,0,1,0,1,0,0,0,1,1,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,1,0,0,1,1,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,0,0,0,1,0,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,1,0,1,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,1,1,1,1,1,0,1,1,0,1,0,0,1,1,0,0,0,1,1,0,0,0,1,0,1,0,0,1,1,0,1,0,0,0,1,1,0,0,0,1,1,0,1,1,1,0,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,0,0,1,0,0,0,1,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,0,1,0,0,0,0,1,0,1,1,1,1,0,0,1,0,1,1,1,0,0,1,1,0,0,1,0,0,1,0,1,1,0,0,0,0,0,1,1,0,1,1,0,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,1,1,1,0,0,1,1,1,0,0,0,0,1,1,1,1,0,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,1,1,1,0,1,0,1,0,0,1,1,1,1,0,1,0,1,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,1,0,1,0,0,1,0,0,1,0,1,0,0,1,0,0,0,0,1,0,1,0,1,1,0,0,1,0,1,1,1,0,0,1,1,1,1,1,0,1,0,0,1,0,1,1,1,0,1,1,0,0,1,0,0,1,0,1,0,1,0,0,0,1,0,0,1,1,1,1,1,0,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1,1,0,1,1,1,1,1,0,1,0,0,1,0,0,0,1,1,0,0,1,0,0,1,0,1,1,1,0,0,1,1,0,0,0,1,1,1,1,1,0,1,1,1,0,0,0,0,1,0,0,0,1,1,1,1,0,1,1,0,1,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,0,1,0,0,0,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1,0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,1,0,0,1,1,0,0,0,1,0,0,0,0,1,1,0,1,0,1,1,1,1,0,0,0,1,1,1,1,1,0,1,0,1,0,0,0,0,1,1,1,0,1,1,0,0,0,1,1,0,1,1,1,0,0,1,1,1,0,1,1,0,1,0,1,1,1,0,0,1,1,1,1,0,0,0,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,1,1,0,1,0,1,0,1,0,0,1,1,0,0,1,1,1,0,1,0,0,1,0,1,0,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,1,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,1,1,0,1,0,1,0,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,0,1,0,0,0,1,0,0,1,1,0,1,1,1,1,0,0,1,0,1,0,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,0,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,1,1,0,0,1,1,1,0,1,1,1,0,1,1,0,1,0,1,1,0,1,0,1,1,0,0,1,1,1,0,1,1,1,1,0,1,1,1,1,1,0,0,0,1,0,0,1,0,1,1,0,0,0,1,1,0,1,1,1,1,1,1,1,0,1,0,1,0,0,1,1,1,1,1,1,1,0,0,1,1,0,1,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,0,1,0,1,1,1,0,0,0,0,1,0,1,1,0,1,0,0,1,0,1,0,1,0,1,0,0,0,1,0,1,1,1,1,0,0,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,0,0,1,0,1,1,1,1,1,0,0,1,0,1,0,1,0,1,1,1,1,0,0,1,1,0,1,0,0,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,1,0,1,0,0,1,1,0,0,1,1,0,1,0,1,1,1,1,0,1,0,1,0,0,0,0,1,1,0,1,1,0,1,1,1,1,0,0,1,1,0,1,0,1,0,0,1,0,0,1,0,0,1,0,1,0,1,1,1,0,1,1,0,0,1,1,1,0,0,1,0,0,0,0,1,1,0,1,1,1,1,0,1,1,1,1,1,0,1,1,0,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,1,1,0,0,1,0,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,0,0,0,1,0,1,0,0,1,1,1,1,1,1,1,1,0,0,1,0,1,0,0,1,0,1,0,0,0,1,0,0,0,1,0,0,1,1,1,1,0,0,0,1,0,0,1,0,0,0,1,0,1,1,1,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,0,1,1,1,0,1,0,1,1,0,0,0,1,1,1,0,1,1,0,1,0,0,0,1,0,0,1,1,1,1,1,0,0,1,0,1,0,1,1,0,1,0,0,0,0,1,0,0,1,1,1,1,1,1,0,1,0,1,0,1,0,0,1,0,0,1,1,1,1,1,1,1,1,0,0,0,0,1,1,0,1,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,0,1,1,1,1,1,0,1,0,1,1,1,1,0,1,0,0,1,0,1,0,1,0,1,0,0,0,0,1,0,0,0,1,0,1,1,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,0,0,1,0,1,1,1,0,1,0,0,1,0,0,0,0,1,1,1,0,1,1,1,0,0,0,1,0,0,1,1,1,0,1,0,0,0,0,1,0,1,1,0,0,1,1,0,1,0,0,1,1,0,1,1,1,0,0,1,1,0,0,1,1,0,0,1,0,0,1,1,0,1,1,1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,1,0,1,1,0,0,1,1,1,1,0,0,0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,0,1,1,1,1,1,0,1,0,1,0,1,0,1,1,0,1,0,1,1,0,0,1,0,1,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,0,1,1,0,1,0,0,1,1,0,1,1,1,1,0,1,1,1,1,0,0,1,1,0,1,0,1,1,0,0,1,0,1,1,1,1,1,0,1,0,1,0,0,1,1,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,0,0,0,1,1,1,0,0,1,0,1,1,0,0,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,1,1,0,1,1,0,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,0,0,1,0,0,0,1,1,1,0,1,1,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,1,1,1,1,1,1,0,1,0,1,1,1,0,0,0,1,1,0,0,1,1,0,1,1,1,1,1,0,0,0,0,0,1,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0,1,1,0,0,1,0,1,1,1,1,1,1,1,0,1,1,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,0,1,0,0,1,0,1,0,1,0,1,0,1,0,0,0,0,1,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,1,1,0,1,0,1,0,0,1,0,1,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,1,1,1,1,0,1,1,1,1,0,1,0,0,1,1,0,1,0,0,1,1,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,1,1,1,0,1,0,0,1,1,0,1,0,0,0,1,1,1,1,1,1,0,0,1,0,1,0,0,1,0,1,0,0,1,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,0,0,1,0,0,0,1,0,0,1,0,1,1,1,0,0,0,0,1,0,1,0,0,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,1,0,1,0,0,1,0,1,0,1,1,0,1,0,0,1,1,1,1,1,0,1,0,0,1,0,0,1,1,0,1,1,1,1,0,0,0,0,0,1,1,1,1,0,1,1,0,1,1,1,1,0,0,0,1,0,1,0,1,1,0,0,0,0,0,0,1,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,0,1,0,0,1,1,1,1,1,1,1,0,1,0,1,1,0,1,0,1,1,0,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,1,1,1,0,1,0,1,0,1,1,0,1,0,0,1,1,0,1,1,1,0,0,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,0,1,1,0,0,1,0,0,0,0,1,1,0,1,1,1,0,0,0,0,0,1,1,0,1,1,0,1,1,1,1,1,1,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,0,1,0,1,1,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,1,1,0,1,1,0,0,1,0,1,0,0,0,0,1,0,1,0,0,1,0,0,1,1,1,0,1,0,1,0,1,1,1,0,1,0,1,0,0,1,0,0,1,1,1,1,0,1,1,0,1,1,1,1,0,0,1,1,0,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,0,1,1,0,1,1,0,1,0,1,0,1,1,0,0,1,1,0,1,1,1,1,0,1,1,1,0,1,1,0,1,0,1,1,1,0,1,0,1,0,1,1,0,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,1,1,1,1,0,1,0,1,0,0,1,0,1,1,1,0,0,1,0,0,1,0,0,1,0,0,1,1,1,1,0,0,0,1,0,1,0,1,1,0,1,1,1,1,1,0,1,1,0,0,0,1,1,0,0,0,1,1,1,0,1,1,0,1,0,0,1,0,1,1,1,0,1,1,1,0,0,1,0,1,1,0,1,0,1,0,0,1,1,1,1,1,1,0,1,1,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0,1,0,0,1,1,1,1,1,0,1,1,0,1,0,1,1,1,1,0,1,1,1,0,1,0,1,0,1,1,1,1,0,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,0,1,0,1,1,0,1,1,0,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,1,1,1,1,0,1,1,0,0,1,1,1,0,1,1,1,1,0,0,0,1,0,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,0,0,1,0,1,1,0,0,0,0,1,0,0,1,0,1,1,0,0,0,1,1,1,1,1,1,1,0,1,0,1,1,0,0,1,1,0,0,1,1,0,1,1,1,1,0,1,0,1,0,0,0,1,0,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,0,0,1,0,1,1,1,1,1,0,0,0,1,0,1,0,1,1,0,1,0,1,1,1,0,0,1,0,0,1,0,1,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,1,0,0,1,1,0,1,0,0,0,0,1,1,1,0,0,1,1,0,1,1,1,1,0,0,0,1,1,0,1,1,0,0,1,1,0,1,1,1,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,1,1,0,1,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,0,1,1,0,0,1,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,0,1,1,1,0,0,0,1,1,0,0,0,1,1,0,0,1,1,1,1,0,1,0,0,0,1,1,0,1,0,0,1,1,1,1,0,1,0,1,1,0,1,0,0,0,1,0,1,0,1,0,1,1,0,1,0,0,1,1,1,0,0,1,0,1,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,0,1,0,1,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,1,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,1,1,0,0,1,0,1,0,1,1,0,0,1,0,0,1,0,0,1,0,1,1,0,0,1,1,1,0,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,0,1,0,0,1,0,0,1,0,1,1,1,1,0,1,1,1,1,1,1,1,0,1,0,1,0,1,1,1,0,1,1,0,0,1,0,0,1,1,1,0,0,0,1,0,1,1,1,0,1,0,1,0,0,1,0,1,1,0,0,1,1,0,1,1,0,0,1,0,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,0,0,1,0,1,1,1,1,0,0,0,0,1,1,0,0,1,0,1,1,0,0,1,1,1,0,0,1,1,0,1,1,1,0,1,0,1,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,0,0,0,1,0,0,1,1,0,1,1,1,0,1,1,0,1,1,0,0,1,0,0,1,0,1,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,1,0,1,1,0,1,1,0,1,1,1,1,0,1,1,0,1,0,0,1,1,1,1,1,0,1,1,0,1,0,0,0,0,0,1,0,1,0,1,0,0,0,1,1,0,1,0,1,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,1,1,0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,1,1,1,0,1,1,1,0,1,1,0,0,0,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,1,0,0,0,0,1,1,1,1,1,1,0,1,1,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,1,1,1,0,1,0,0,0,1,0,1,0,1,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,1,1,1,0,1,0,0,1,1,0,1,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,1,0,1,0,0,0,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,1,1,0,1,0,1,0,0,1,1,0,1,1,1,0,0,1,0,1,1,1,0,0,0,0,1,1,1,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,1,1,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,1,0,0,1,0,0,0,0,1,0,1,0,1,1,0,0,1,0,0,1,0,0,1,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,0,1,1,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,1,1,1,1,0,0,0,0,1,0,0,1,1,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,1,1,1,0,1,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,1,0,1,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,0,1,0,0,1,1,1,0,1,1,0,1,0,1,1,0,0,1,1,1,1,1,1,0,1,0,1,0,1,1,1,1,0,1,1,1,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,1,1,0,0,1,1,1,1,1,0,0,0,0,1,0,1,1,0,1,0,1,1,0,0,0,0,0,1,1,0,0,1,1,1,0,1,0,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,1,0,1,0,0,0,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,0,0,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,0,1,0,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,0,0,0,1,0,1,0,0,1,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,1,0,1,0,0,1,1,1,0,0,1,0,1,1,1,1,0,0,1,0,0,0,0,1,0,0,1,0,1,0,1,1,1,0,1,0,1,1,0,1,1,0,0,1,0,1,1,0,0,0,1,0,0,0,0,0,1,0,1,0,1,1,0,1],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[1.086911678314209,0.9855263233184814,0.0024090036749839783,0.9725261926651001,0.29547712206840515,1.045695185661316,1.070296287536621,1.1734919548034668,0.9162395000457764,0.8967214226722717,0.9169166088104248,0.9162395000457764,0.30834877490997314,1.5165945291519165,0.9470655918121338,0.9453338384628296,0.8441975712776184,1.0031651258468628,0.9158578515052795,0.4398447275161743,0.8967214226722717,1.1406275033950806,0.5680407881736755,1.150985598564148,0.8181584477424622,0.9162395000457764,0.7975503206253052,0.9165684580802917,1.5597162246704102,1.0061652660369873,0.5372647047042847,1.006729006767273,0.9162395000457764,0.8154187798500061,1.291003942489624,1.2337901592254639,0.7731859683990479,0.8690691590309143,1.0509204864501953,0.9162184596061707,0.482419490814209,1.074400782585144,0.8508367538452148,0.45364153385162354,1.1098076105117798,0.5512934923171997,0.7559946179389954,0.9181323647499084,0.8114360570907593,0.9147552847862244,0.6163944005966187,0.7804027795791626,0.9337247610092163,1.1562540531158447,0.9063957929611206,1.0211398601531982,0.9171406030654907,0.4978814125061035,0.9155537486076355,0.9178615212440491,0.9178964495658875,0.9160817861557007,0.9463106393814087,0.9186185598373413,0.9057782888412476,1.7820780277252197,0.9285663366317749,0.9600915908813477,0.7915910482406616,0.878788948059082,0.8573319315910339,1.7775403261184692,0.7554707527160645,1.1592389345169067,0.917788028717041,0.915499210357666,0.9593976736068726,1.0182271003723145,0.32964378595352173,0.8247727155685425,0.9165538549423218,0.7927957773208618,0.8906394243240356,0.9158578515052795,0.6024481654167175,1.0414294004440308,0.927324652671814,0.20201930403709412,1.7520049810409546,1.176177740097046,0.9413578510284424,0.569811999797821,1.3161042928695679,0.7800996899604797,1.1317219734191895,0.9523918628692627,0.917788028717041,0.8003332614898682,0.719289243221283,1.190274953842163,0.9674252271652222,0.7744998931884766,1.0494880676269531,0.8667990565299988,0.9146220684051514,0.6727966070175171,1.0776506662368774,0.7234728336334229,0.970866322517395,1.1019257307052612,1.1906380653381348,0.930680513381958,0.9153650999069214,0.9588559865951538,0.9166065454483032,0.589836835861206,0.914964497089386,0.8572061061859131,0.9064375162124634,0.917788028717041,1.5295236110687256,0.6951882243156433,0.9157769083976746,0.9158578515052795,0.9162130951881409,0.5926119685173035,0.7778626680374146,0.9734877347946167,0.8303463459014893,0.9162395000457764,0.9995315074920654,0.9165586233139038,1.1068836450576782,0.9460091590881348,0.9324241876602173,0.9539823532104492,0.16694262623786926,0.7487662434577942,0.909923791885376,0.9155814051628113,0.9435685873031616,0.9154697060585022,0.882524847984314,0.9919357299804688,0.11281400173902512,0.6833299398422241,0.9160374402999878,0.9158578515052795,0.8068153262138367,0.2212112843990326,0.9629249572753906,0.915499210357666,0.8799189925193787,0.9192503094673157,0.915499210357666,0.9187110662460327,0.9160254001617432,0.7249411344528198,0.8458161950111389,0.9176673889160156,0.9040390849113464,0.916747510433197,1.1789381504058838,0.8759221434593201,0.8772740364074707,0.9157232046127319,0.43977320194244385,0.9537912607192993,0.9634577035903931,0.6752182841300964,1.143544316291809,0.8574526309967041,0.9162577986717224,1.0800681114196777,0.9155485033988953,0.7866063714027405,0.762438952922821,0.9164403080940247,0.9717615842819214,0.9166883230209351,0.9393531084060669,0.9178929924964905,0.9009691476821899,1.059883713722229,0.8785086274147034,0.9479981660842896,0.9160804748535156,0.1349450945854187,0.9954419136047363,0.9166883230209351,0.9537297487258911,1.4613220691680908,0.9227831363677979,1.1811171770095825,0.9170730113983154,0.9072582721710205,0.6940075159072876,0.916064441204071,0.8313498497009277,1.0234627723693848,0.9385685920715332,0.8049063682556152,0.9904998540878296,0.881631076335907,0.8652036786079407,0.9675724506378174,1.1756360530853271,0.8532618284225464,0.9169991612434387,0.997931718826294,0.917078971862793,0.7967270612716675,0.7348122596740723,0.9145603179931641,0.9172138571739197,1.2094457149505615,0.9175482988357544,0.7273042798042297,0.9158578515052795,0.8967214226722717,1.013414978981018,0.9060205817222595,0.9166054129600525,0.9168254137039185,0.9181673526763916,1.0375006198883057,1.0290347337722778,1.0439531803131104,1.1763945817947388,1.019293189048767,1.0894137620925903,0.9150991439819336,0.9200990796089172,0.9109216928482056,0.8713088631629944,1.1175850629806519,1.6581119298934937,1.117761492729187,0.9158578515052795,0.9445153474807739,0.9265646934509277,0.8974860906600952,0.9369325637817383,0.8801027536392212,0.9168996214866638,0.9150722622871399,0.7748391628265381,0.7369122505187988,0.9174990653991699,0.9145216941833496,0.9317164421081543,0.9164063334465027,0.915499210357666,0.9174468517303467,0.9085875749588013,0.7364912629127502,0.915499210357666,0.930850625038147,0.8081938028335571,0.8359522819519043,0.8129185438156128,0.915499210357666,1.4614495038986206,0.99400794506073,0.8482398986816406,0.7085622549057007,0.9602802991867065,1.1018345355987549,0.9177565574645996,1.2025651931762695,0.7734270095825195,0.9180857539176941,0.9167259335517883,0.8967214226722717,0.9158578515052795,1.0645277500152588,0.888999342918396,0.9153954982757568,0.9158578515052795,0.8990318179130554,0.8233596086502075,0.9203717708587646,0.9468505382537842,0.8349282741546631,0.9165315628051758,0.9090654850006104,0.9926148653030396,1.097642421722412,0.8084848523139954,0.9000219702720642,0.8622815608978271,0.917788028717041,0.8892373442649841,0.7443265914916992,0.9165315628051758,0.8833471536636353,0.9269424676895142,0.12463454157114029,1.0369484424591064,0.9156158566474915,0.6396218538284302,0.9043235778808594,1.5514329671859741,0.9157052040100098,0.924622654914856,0.5433642268180847,0.9158578515052795,1.0386805534362793,0.8416516184806824,0.8259716033935547,0.7189720869064331,1.7201555967330933,0.522454023361206,1.0636229515075684,1.0852046012878418,0.1649007499217987,0.8703324794769287,0.9671704769134521,0.9536586999893188,1.1449522972106934,0.9158578515052795,0.7890547513961792,0.9270952939987183,0.8319223523139954,0.8917500972747803,1.388012409210205,0.8814041018486023,0.9164010882377625,0.9158578515052795,0.9174466133117676,0.9343187808990479,0.9158578515052795,0.9102440476417542,0.9374276399612427,0.9052214622497559,0.5682843327522278,0.8097248077392578,0.18132320046424866,1.889664888381958,1.012015461921692,0.8689634203910828,0.9726771116256714,0.9083911180496216,0.7672469019889832,1.0724461078643799,0.8293760418891907,1.3783175945281982,0.9124723076820374,1.0915734767913818,0.8909925818443298,0.6487530469894409,1.1791402101516724,1.2788431644439697,0.8938684463500977,0.35337570309638977,1.0009706020355225,0.8093178868293762,0.8493554592132568,0.8810752630233765,0.8951323628425598,0.9163069128990173,0.915499210357666,0.9171740412712097,0.9160374402999878,0.9194082021713257,0.9167702198028564,0.8575531244277954,0.9168536067008972,0.746695339679718,0.9150994420051575,1.1453375816345215,1.0320419073104858,0.9053865671157837,0.8958884477615356,0.5355353355407715,0.9162395000457764,0.7738157510757446,0.7573789358139038,1.1922303438186646,0.9423885345458984,1.0595805644989014,0.8682429790496826,0.9160605072975159,0.9173277020454407,0.7783958911895752,0.9571963548660278,0.930639386177063,0.9160266518592834,0.3058798611164093,0.9159895181655884,0.7955403923988342,0.8424580097198486,0.9943850040435791,0.9169594049453735,0.915499210357666,0.9069305062294006,0.916201114654541,0.9160287976264954,0.8327023983001709,0.9659241437911987,0.7151123881340027,0.7939872145652771,0.946523904800415,0.903666079044342,0.919087290763855,0.9169372916221619,0.7281348705291748,0.9260458946228027,0.8802481293678284,1.0306273698806763,0.8909581303596497,0.4002615213394165,0.9181563854217529,0.9243031740188599,0.6822831034660339,0.9173619747161865,0.912697434425354,0.8969029188156128,0.9425663948059082,0.6240389347076416,0.954833984375,0.9177390933036804,0.9458158016204834,1.7325527667999268,1.1078650951385498,0.9158578515052795,0.2523394227027893,0.8823949694633484,0.9414685964584351,1.4184091091156006,0.9169840812683105,0.9163296222686768,0.9165182113647461,0.9206520318984985,0.7396305203437805,0.7400912046432495,0.8388010263442993,0.9159845113754272,0.9376691579818726,1.0019127130508423,1.2093902826309204,1.0163596868515015,1.1957381963729858,1.042936086654663,1.1640022993087769,0.9162395000457764,0.6263149976730347,0.9365293979644775,0.8471916913986206,0.9044544100761414,0.9735268354415894,0.6278046369552612,0.9158578515052795,0.9428194761276245,1.3589757680892944,0.9136282205581665,0.7539564967155457,0.8913923501968384,0.916779637336731,1.0991952419281006,0.937131404876709,0.9167982339859009,0.9175177812576294,0.8933952450752258,0.9174960851669312,0.9503908157348633,1.1881074905395508,0.9051718711853027,0.9177588224411011,0.9165687561035156,0.9176379442214966,0.9810997247695923,0.917788028717041,0.9858565330505371,0.9179818630218506,0.6047200560569763,0.915499210357666,0.8884317874908447,1.1022874116897583,0.9159872531890869,0.9167333245277405,0.8200483918190002,0.9367613792419434,1.5482580661773682,0.9163520336151123,0.7992980480194092,0.6876223683357239,1.0904101133346558,0.713008463382721,0.992607831954956,0.9170414209365845,0.8086745142936707,0.8612196445465088,1.0996325016021729,0.9579137563705444,1.0791758298873901,0.9160904884338379,0.9259270429611206,0.8963875770568848,0.88169926404953,0.9741625785827637,0.9167505502700806,0.9378942251205444,0.9011656045913696,0.917822539806366,1.3769370317459106,0.9160281419754028,0.9093906283378601,0.950384259223938,-0.16125750541687012,0.9177094101905823,0.7185283899307251,0.9180951118469238,0.9553964138031006,0.9166668653488159,0.9155982136726379,0.915175199508667,1.2493617534637451,0.9158578515052795,0.915499210357666,0.9163069128990173,0.8813555240631104,0.9392951726913452,0.941806435585022,1.0562130212783813,0.9174123406410217,0.9205236434936523,0.9036458134651184,0.9072441458702087,1.2586698532104492,0.9081282615661621,0.7992376089096069,0.9162107706069946,0.6403505802154541,1.1008143424987793,0.9163069128990173,0.9178597331047058,0.4991734027862549,0.9139345288276672,1.586472749710083,1.6948293447494507,0.9713641405105591,0.8967214226722717,0.9174916744232178,0.9085848331451416,0.8048081994056702,1.111104130744934,0.5516794919967651,1.0352836847305298,0.8489072322845459,0.918146014213562,0.917788028717041,0.8388299942016602,0.8613705635070801,0.8942922949790955,1.104372501373291,0.8937711715698242,1.011234998703003,0.8967214226722717,1.8073999881744385,0.9156329035758972,0.8676124811172485,1.2247759103775024,0.8970310091972351,1.2730677127838135,0.8553503751754761,0.34468111395835876,0.9174123406410217,0.978154182434082,0.9509286880493164,1.4803335666656494,0.8834521174430847,0.9170351028442383,0.9164038896560669,1.2266875505447388,0.9163069128990173,0.9608875513076782,0.8318876028060913,0.4888037443161011,1.319738745689392,0.8311722874641418,0.2769659757614136,0.9169540405273438,0.8887758255004883,0.9158578515052795,0.847838282585144,0.9300425052642822,0.7343389987945557,1.5382424592971802,0.9158578515052795,1.1541708707809448,1.5079247951507568,0.7248326539993286,0.23705479502677917,0.9018799662590027,0.9196081161499023,0.704735517501831,0.9159514307975769,1.3524380922317505,0.3516921401023865,0.9162395000457764,0.8169224262237549,0.8625578880310059,1.2040832042694092,0.835052490234375,1.3503360748291016,0.9642360210418701,0.9796544313430786,0.9175529479980469,0.9168825149536133,0.7781173586845398,0.9176949858665466,0.9163069128990173,0.9165538549423218,0.8386604189872742,1.0079180002212524,0.917788028717041,0.9377199411392212,0.8052381277084351,1.304837703704834,0.7819296717643738,0.9166883230209351,0.9150087833404541,1.1418566703796387,0.6774929165840149,1.1789703369140625,0.9163296222686768,0.9162760972976685,0.8300153613090515,0.9386097192764282,1.0520342588424683,0.8717361092567444,0.9440503120422363,0.9680473804473877,1.6539427042007446,1.1431242227554321,0.988802433013916,0.7571130990982056,0.9212896227836609,1.0109140872955322,0.7018746137619019,0.8025404810905457,0.9166038036346436,0.9139074683189392,0.9237560033798218,0.9157232046127319,0.6739189028739929,0.9657859802246094,0.9162395000457764,0.9165315628051758,0.8854106068611145,1.0202856063842773,0.7398077249526978,0.9386287927627563,0.915499210357666,1.0642169713974,1.1463377475738525,0.9709130525588989,0.8888689279556274,0.9486626386642456,0.8150535821914673,0.8840920329093933,1.3284748792648315,0.9985365867614746,0.9162455797195435,0.8398036360740662,0.9157861471176147,0.5466288924217224,0.6894580125808716,1.050646424293518,1.707714557647705,0.8239347338676453,0.942493200302124,0.9158578515052795,0.11734173446893692,0.7616490721702576,0.9162548184394836,0.9165538549423218,0.9775431156158447,0.6712148785591125,0.9164010882377625,0.6814603209495544,0.43031442165374756,0.8967813849449158,0.9847463369369507,0.8950108885765076,0.7991061210632324,1.1816089153289795,0.8429476618766785,0.7821714282035828,0.6920103430747986,0.9258272647857666,1.722412109375,0.9184337258338928,0.8020582795143127,0.8855170607566833,0.909443199634552,0.7048795819282532,0.6817213892936707,0.957776665687561,0.24117165803909302,1.1747946739196777,1.3359737396240234,0.932948112487793,0.8259857892990112,0.9470012187957764,0.916806697845459,0.886017918586731,1.1894927024841309,1.7388838529586792,0.9247663021087646,1.0050803422927856,0.9158578515052795,0.6433179974555969,0.9112812876701355,0.7007859349250793,0.8879711031913757,0.5973652601242065,0.8891273736953735,0.9666721820831299,0.9581238031387329,0.9081012010574341,1.020936369895935,0.915581464767456,0.8990814089775085,0.8357022404670715,0.847107470035553,0.8812793493270874,1.1083883047103882,0.7805202603340149,0.8008568286895752,1.7826635837554932,0.7691737413406372,0.9162395000457764,0.9163492918014526,0.9179141521453857,0.5999154448509216,0.9394440650939941,0.9163069128990173,0.8599374294281006,0.33498263359069824,0.885429322719574,0.8618758916854858,0.9175979495048523,0.8005980253219604,0.8795739412307739,0.943173885345459,0.9447494745254517,0.8678053617477417,0.9621518850326538,0.9169641137123108,0.8884130120277405,0.9162395000457764,0.7313340902328491,1.1480789184570312,0.5908263325691223,0.9278440475463867,0.7279267907142639,0.9135033488273621,0.6366969347000122,0.9172424077987671,0.8491818308830261,0.917695939540863,0.9183931350708008,0.909164547920227,0.9164674878120422,0.46124231815338135,0.8554302453994751,0.8959414958953857,0.8729560375213623,0.9224326610565186,0.9573090076446533,1.057552695274353,0.9173444509506226,0.9163069128990173,0.9264353513717651,0.8815612196922302,0.9158735871315002,0.9578664302825928,0.9163362979888916,0.9165528416633606,0.9167333245277405,0.8954476118087769,0.915499210357666,1.0255446434020996,0.9282548427581787,0.8534732460975647,0.6989567279815674,0.7653175592422485,0.9241089820861816,0.8403448462486267,0.9176080226898193,0.9903583526611328,0.9037647247314453,1.1492489576339722,0.9108937382698059,0.9165538549423218,1.2501263618469238,1.1114308834075928,1.162645697593689,0.8454083800315857,0.7381479740142822,0.6517465114593506,0.9160374402999878,0.9979743957519531,1.0076638460159302,0.8469715714454651,0.8859970569610596,0.9169102907180786,0.9157018065452576,1.0030486583709717,0.9167336225509644,0.42325013875961304,0.9387795925140381,1.03037691116333,0.915499210357666,0.9173390865325928,0.9165315628051758,0.9162395000457764,0.5974478125572205,0.6798921823501587,0.9542738199234009,1.0828865766525269,0.8874021768569946,1.2279268503189087,0.7467334866523743,0.7139440774917603,1.0760135650634766,0.8798108100891113,0.7663664817810059,0.9115704894065857,0.935064435005188,0.8635163903236389,0.9163069128990173,0.5515834093093872,0.9163917899131775,0.9059002995491028,1.716801643371582,0.8003801107406616,0.43931424617767334,0.9172865152359009,0.9164138436317444,1.1475273370742798,0.9124393463134766,0.5044811964035034,0.70357745885849,0.9829612970352173,0.10209693759679794,0.9449679851531982,1.082255244255066,1.570025086402893,0.7679723501205444,1.424088478088379,0.8967214226722717,0.7283082604408264,0.8389000296592712,1.3016141653060913,1.0787371397018433,1.252529501914978,1.1547601222991943,0.8977794647216797,0.9191804528236389,1.247417688369751,0.8464652299880981,0.7143279314041138,1.1467785835266113,1.0606565475463867,0.9168579578399658,0.8779699802398682,0.9804661273956299,0.5169244408607483,1.1551433801651,0.9504622220993042,0.9153071045875549,0.2589743733406067,1.016519546508789,0.21500107645988464,1.0008037090301514,0.9171825051307678,1.0568958520889282,0.9162395000457764,0.9167556762695312,0.9162395000457764,1.1000257730484009,1.0282061100006104,0.9179158210754395,0.8085927367210388,0.7395086288452148,0.910237729549408,1.2971161603927612,1.0237116813659668,1.1333069801330566,0.9163150787353516,0.8215832710266113,0.9080053567886353,0.8350462317466736,0.9162382483482361,1.0448319911956787,0.9167107343673706,0.9445005655288696,0.9261908531188965,0.9158578515052795,1.5036183595657349,0.915499210357666,0.917788028717041,1.0281195640563965,0.9344837665557861,0.6966968178749084,0.8967214226722717,0.9158578515052795,0.9165019392967224,1.135817527770996,0.916237473487854,0.9852415323257446,0.9268215894699097,0.9287651777267456,0.9679319858551025,0.9301720857620239,0.9041153192520142,0.9166883230209351,0.905718207359314,0.8001603484153748,1.1411749124526978,0.9221118688583374,0.8581955432891846,1.0509076118469238,0.9470432996749878,0.7543321251869202,1.0745658874511719,0.8903438448905945,0.6510314345359802,0.8967985510826111,0.8885124325752258,0.8888360261917114,0.4873262643814087,0.7457565665245056,0.13957369327545166,0.9162395000457764,0.5465460419654846,0.922230064868927,0.8882105350494385,0.91495281457901,0.9351119995117188,0.4993578791618347,0.8566758036613464,0.9053924083709717,1.0583833456039429,0.9167556762695312,0.9819142818450928,0.9163069128990173,0.8441325426101685,0.9988745450973511,1.234815001487732,0.8646137118339539,0.9158578515052795,0.8426663279533386,0.9412827491760254,0.21718773245811462,0.9167512059211731,0.7748870253562927,1.005859375,0.97376549243927,0.9172149300575256,0.9275510311126709,1.0024415254592896,0.9161255955696106,0.16866734623908997,1.4576966762542725,0.8799424171447754,1.0264753103256226,0.915499210357666,0.9950394630432129,0.917788028717041,0.8144010305404663,1.3139355182647705,0.9615546464920044,0.7550362348556519,0.911980152130127,0.9966574907302856,0.9164010882377625,0.9164259433746338,0.9162276387214661,0.9178528785705566,0.9229084253311157,0.8418387174606323,0.9863325357437134,0.9163069128990173,1.1154340505599976,0.9640345573425293,0.9169025421142578,0.8085654973983765,0.9445046186447144,1.0124613046646118,0.7898712754249573,0.8922009468078613,0.9178466200828552,0.990380048751831,0.9163023233413696,0.9180763959884644,1.1335543394088745,1.6201766729354858,0.9433184862136841,0.9158047437667847,0.9644283056259155,1.020376205444336,0.9160374402999878,1.6947277784347534,1.0731748342514038,0.9824780225753784,0.5497890114784241,0.9159979224205017,0.9103400707244873,1.1441081762313843,0.9483969211578369,0.743547260761261,0.9162395000457764,1.6995928287506104,0.7323113679885864,0.9760545492172241,0.9693348407745361,0.9162395000457764,0.9628499746322632,0.7955662608146667,0.9628050327301025,0.8569788336753845,0.9166883230209351,0.8657146692276001,0.9164010882377625,0.9874204397201538,0.8860437870025635,0.8771120309829712,0.9177879691123962,0.8420300483703613,0.9428969621658325,0.9957212209701538,0.8967214226722717,0.5409051179885864,0.9157232046127319,1.0814473628997803,0.9099103808403015,0.9721106290817261,0.8506924510002136,-0.11856836825609207,1.0107805728912354,0.24031099677085876,0.9152875542640686,0.8796566128730774,1.1917959451675415,0.87140291929245,1.8093903064727783,0.8672301173210144,0.9137633442878723,0.945216178894043,1.2364908456802368,0.9169600605964661,1.0679445266723633,0.9258025884628296,0.78456711769104,0.9642993211746216,0.8856890201568604,0.9154729247093201,0.915499210357666,0.9146226644515991,0.9157858490943909,0.9153355360031128,0.9158622026443481,1.4558378458023071,0.9360499382019043,0.9534015655517578,0.9054805636405945,1.6225916147232056,0.9120481610298157,0.23009240627288818,1.1318106651306152,0.9175006151199341,1.2142889499664307,1.0936825275421143,0.9254443645477295,0.916260302066803,1.1529011726379395,0.9220921993255615,1.472549319267273,1.022412657737732,0.9158578515052795,0.9350869655609131,0.28284531831741333,0.9161778092384338,0.7703155875205994,0.16186293959617615,1.1438543796539307,0.9140194058418274,1.119708776473999,0.8752292394638062,0.859862744808197,0.7668366432189941,0.9166883230209351,1.111844539642334,0.9160374402999878,0.9614893198013306,0.7308666706085205,0.9066977500915527,1.2036558389663696,0.8062087297439575,0.8516728281974792,0.9188946485519409,0.7395129203796387,0.218103289604187,1.888278841972351,0.8747177124023438,0.9184666275978088,0.8153835535049438,0.894984245300293,0.9833159446716309,0.641418993473053,1.7188233137130737,0.976534366607666,1.4897619485855103,0.9172861576080322,0.9162395000457764,0.8083016276359558,0.4881572127342224,0.7953835129737854,0.8321701288223267,0.15928280353546143,0.9321988821029663,0.5345359444618225,0.4254913926124573,0.6186753511428833,0.8651635050773621,0.8727595210075378,1.008966326713562,1.0101925134658813,0.9163069128990173,0.9899539947509766,0.8737654685974121,0.9154314994812012,0.9042908549308777,0.9149934649467468,0.9667747020721436,0.6575895547866821,1.3014721870422363,0.9680032730102539,0.9877519607543945,1.8001071214675903,0.8830201625823975,0.7950456142425537,0.9161033630371094,1.3242440223693848,0.9988633394241333,0.9129756093025208,0.8449894785881042,0.9154804944992065,1.0667845010757446,1.639472484588623,0.9152727723121643,0.9035043716430664,0.9625942707061768,0.670089602470398,0.8512241244316101,0.7880337834358215,0.8277336955070496,0.9391769170761108,1.7899008989334106,0.9848635196685791,0.9165538549423218,0.9526685476303101,1.1215649843215942,0.9252451658248901,0.9972941875457764,1.0735737085342407,0.9164010882377625,0.918285608291626,0.9162395000457764,0.8776890635490417,0.8777946829795837,0.9164010882377625,0.9585089683532715,0.8439306020736694,0.7466466426849365,0.7540618181228638,0.9187555909156799,0.9197075963020325,0.7319741249084473,0.9281982183456421,0.6691574454307556,0.9167333245277405,0.915499210357666,1.093261957168579,0.9199257493019104,0.9816529750823975,0.9333400726318359,1.0889688730239868,1.1959552764892578,0.9163069128990173,0.9574403762817383,0.9171246886253357,0.9200066924095154,1.0649174451828003,0.8730053901672363,0.8676562905311584,0.9788432121276855,1.061911940574646,0.8729459047317505,0.9167556762695312,0.9901256561279297,0.6921480894088745,0.9571120738983154,0.9168961644172668,0.29959502816200256,0.8850007057189941,0.9317291975021362,0.1262243390083313,0.11403503268957138,0.8887895345687866,0.9162395000457764,0.9136370420455933,0.8172404766082764,0.7500242590904236,0.9067425727844238,1.2156121730804443,0.9151552319526672,0.8930141925811768,0.8497118949890137,0.8670303821563721,1.0120247602462769,1.1744593381881714,0.8840053677558899,1.021636962890625,0.7937551140785217,0.9160374402999878,0.9648154973983765,1.0798274278640747,0.8205920457839966,0.952771782875061,0.9173390865325928,0.8975098133087158,0.8369899988174438,0.8275834918022156,1.033292531967163,0.9340360164642334,0.9954317808151245,0.765471339225769,1.0857646465301514,0.913743793964386,0.9178991317749023,0.6567220091819763,0.9180829524993896,1.1381170749664307,0.9084622859954834,1.1946687698364258,0.9180628061294556,0.27587440609931946,0.9168164730072021,0.8494559526443481,0.9164010882377625,0.915499210357666,0.9061847925186157,1.2133649587631226,1.1238255500793457,0.9127026796340942,0.9166558384895325,0.9160374402999878,0.9160141944885254,-0.06035197526216507,0.682725727558136,1.0265579223632812,0.9174495339393616,0.6645768880844116,0.883630633354187,0.9575893878936768,0.9169214963912964,0.4152373969554901,0.9173518419265747,0.7769997715950012,1.570576786994934,0.9096624851226807,1.0231624841690063,0.7778587341308594,0.7933739423751831,0.9158578515052795,0.9150625467300415,0.9164010882377625,0.6793690919876099,0.9098442792892456,0.9246958494186401,0.9746240377426147,0.915061891078949,0.8882081508636475,1.2840152978897095,0.9167556762695312,1.2264043092727661,0.9324926137924194,0.578883171081543,0.1285591721534729,0.9545083045959473,0.8945203423500061,0.9173518419265747,0.9166883230209351,0.8551269173622131,0.5626218318939209,0.9155367612838745,0.9804160594940186,1.0296800136566162,0.9321894645690918,0.957262396812439,0.7450022101402283,0.8736803531646729,0.7588084936141968,0.9599608182907104,0.877304196357727,0.9097946286201477,0.9160050749778748,0.5290365219116211,0.9160374402999878,1.209121823310852,1.0723974704742432,0.8465051651000977,0.587904691696167,0.9163069128990173,0.9180698394775391,0.8785068392753601,0.8833867907524109,0.9288511276245117,0.968315839767456,0.8593766689300537,0.7992740869522095,0.8865116238594055,1.5804598331451416,0.8785644769668579,0.8557456135749817,0.9743719100952148,1.1993992328643799,1.2909660339355469,0.9165538549423218,0.9158578515052795,0.26170268654823303,1.1450872421264648,0.9270799160003662,0.917788028717041,0.9766862392425537,0.9172115921974182,0.9287658929824829,0.8986020088195801,0.9151548147201538,0.915253221988678,0.8201714158058167,0.6791804432868958,0.9699304103851318,0.916610598564148,0.9033368229866028,1.044886589050293,0.40962955355644226,0.9169866442680359,0.916652500629425,0.9673035144805908,0.928779125213623,1.0260975360870361,0.9150034189224243,0.9169815182685852,0.947830080986023,1.3867924213409424,0.9164454936981201,0.9253393411636353,0.8033199310302734,0.916772723197937,1.0268090963363647,0.9163069128990173,0.915499210357666,0.8164948225021362,0.8890878558158875,0.9000123143196106,1.3485909700393677,0.9167535305023193,1.0576738119125366,0.9173246622085571,0.9164010882377625,1.3880109786987305,0.8916889429092407,0.9059568643569946,0.9155859351158142,0.9620511531829834,0.9158578515052795,0.9289120435714722,0.9163296222686768,0.8658715486526489,0.9658693075180054,1.7485098838806152,0.925310492515564,0.9502580165863037,0.9146844148635864,0.9577089548110962,0.9159486889839172,0.9170827269554138,1.0069916248321533,1.0213731527328491,0.9166883230209351,1.3287802934646606,0.9158578515052795,0.9153586626052856,0.9722964763641357,0.9163069128990173,0.9001609086990356,0.9897091388702393,0.8589722514152527,1.5515973567962646,1.0533722639083862,0.9224058389663696,0.9463554620742798,0.637760579586029,0.9172558188438416,0.37609797716140747,0.7872191071510315,0.639146625995636,0.38035595417022705,0.9120358228683472,0.805407702922821,0.9874179363250732,0.959996223449707,0.9124711751937866,0.9624998569488525,1.4442139863967896,0.9161775708198547,0.8191162347793579,0.9955497980117798,0.917788028717041,0.9170121550559998,0.8980593681335449,1.3110146522521973,0.9808081388473511,0.9171121716499329,0.9162395000457764,0.3860141634941101,0.8683611154556274,0.9158578515052795,0.9241155385971069,0.9167556762695312,1.1235544681549072,0.9161406755447388,0.8816547393798828,0.9166236519813538,0.9963903427124023,0.9485547542572021,1.249203085899353,1.006235122680664,0.9250426292419434,0.6523186564445496,1.0365369319915771,0.5966877341270447,0.9819376468658447,0.8955246806144714,0.9157232046127319,0.9146806597709656,0.8597429990768433,0.9549533128738403,0.9324297904968262,0.9089551568031311,0.9158578515052795,0.9205280542373657,1.0490955114364624,0.917788028717041,0.922836184501648,0.9142788648605347,0.9186714291572571,0.8511043787002563,1.0400311946868896,0.9184512495994568,0.9316266775131226,1.4946722984313965,1.15314781665802,0.9160355925559998,0.9659796953201294,1.2323583364486694,0.9157232046127319,0.9169793725013733,0.8554688692092896,0.8999637961387634,0.9133690595626831,0.8869143724441528,0.9105902910232544,0.6597414016723633,0.9499853849411011,0.7587805390357971,0.9574761390686035,0.6265938878059387,0.9165449142456055,0.9340728521347046,0.9160374402999878,1.5604456663131714,1.737229347229004,0.915499210357666,0.9378998279571533,0.8335939645767212,0.9182501435279846,0.9174651503562927,0.8576217293739319,0.9262473583221436,0.915499210357666,1.2373170852661133,0.915499210357666,0.9271581172943115,0.9197496175765991,0.7967280745506287,0.9709970951080322,0.9665700197219849,0.9139026403427124,0.6820081472396851,1.103238582611084,0.9163069128990173,0.9167290329933167,0.8796912431716919,0.9775986671447754,1.053550124168396,1.0688362121582031,0.8970851898193359,1.1143772602081299,0.915499210357666,0.6238417625427246,0.9453693628311157,0.9176372289657593,0.9172250628471375,1.112676739692688,1.1084104776382446,0.9160374402999878,0.9151337742805481,0.9078010320663452,0.9164026975631714,0.9173818230628967,0.8573538661003113,1.736860752105713,0.9144906997680664,0.916347086429596,0.9164010882377625,0.916126549243927,0.9169291853904724,0.7682552933692932,0.9164010882377625,0.9319009780883789,0.7910856604576111,0.8253230452537537,0.9725793600082397,0.9167816638946533,1.1217564344406128,0.9444855451583862,0.29375767707824707,0.9519878625869751,0.9809863567352295,1.0094223022460938,0.915499210357666,1.2132916450500488,0.9176536202430725,0.9153047800064087,0.9168012738227844,0.9162875413894653,0.8576196432113647,0.28213393688201904,1.0981476306915283,0.615469217300415,1.034332036972046,0.3907526135444641,0.9225409030914307,0.9166883230209351,1.097887396812439,1.1276510953903198,0.9347655773162842,0.8134475946426392,0.8613337278366089,1.1598131656646729,0.9164010882377625,0.9165338277816772,0.9161556363105774,0.9173341393470764,0.9171730875968933,0.9278130531311035,0.916926383972168,0.9163069128990173,0.9994922876358032,0.9139769673347473,0.8821321129798889,0.9562535285949707,0.9162395000457764,0.15666231513023376,0.9168103337287903,0.9610118865966797,0.6110314130783081,0.9168407320976257,0.8719905614852905,0.8461732268333435,1.1336865425109863,1.1515114307403564,0.998666524887085,0.8542799353599548,0.9850237369537354,0.9308788776397705,0.8873646855354309,0.8822014927864075,0.9138940572738647,1.7062578201293945,0.9430544376373291,0.9103500247001648,0.9080618619918823,0.9142282009124756,0.9158578515052795,1.1403682231903076,0.9135033488273621,1.235865592956543,0.9864919185638428,0.0873560830950737,1.083314061164856,0.9167333245277405,0.9169226288795471,0.9166514277458191,0.9154253005981445,1.0689380168914795,1.644892692565918,0.9465435743331909,1.0048274993896484,0.9062994718551636,0.9157232046127319,0.915499210357666,0.8913382887840271,0.9506462812423706,0.9171137809753418,0.8269006013870239,0.9490476846694946,0.842411994934082,0.8757101893424988,0.8390849232673645,0.6368497610092163,0.9061123132705688,0.8486732244491577,0.9168949723243713,0.9162836074829102,0.49634671211242676,0.9168363809585571,0.9969427585601807,0.9065082669258118,0.892963171005249,0.8127447366714478,0.9182646870613098,1.532904863357544,0.9162395000457764,0.9186170101165771,0.9168959856033325,0.9163069128990173,0.9514261484146118,0.9162395000457764,0.953016996383667,0.7204316258430481,1.054173469543457,0.9167556762695312,0.10805927962064743,0.8711994886398315,0.9353140592575073,0.9163243770599365,1.0441960096359253,0.9171009659767151,1.744495153427124,0.910190999507904,0.6731176972389221,1.0075064897537231,0.9097329378128052,0.9163021445274353,0.939831018447876,0.9233726263046265,1.18854558467865,0.8416077494621277,0.915499210357666,0.915499210357666,1.0289539098739624,0.915344774723053,0.9609693288803101,0.78281569480896,0.9007716774940491,0.9826053380966187,0.9163261651992798,0.9067286849021912,0.9152842164039612,1.0778915882110596,0.9158578515052795,0.9409605264663696,0.9633877277374268,0.47466760873794556,0.8887758255004883,0.9158578515052795,1.0481303930282593,0.8156477808952332,0.81197589635849,0.9508202075958252,1.734944224357605,0.5586972236633301,0.9222123026847839,1.1639503240585327,0.2998577356338501,0.8608417510986328,1.5328714847564697,0.9996553659439087,1.5731602907180786,0.9701358079910278,0.9163392186164856,0.9152956008911133,0.9558831453323364,0.9163069128990173,0.7609302997589111,0.9165315628051758,0.6865065693855286,1.532047986984253,0.9474360942840576,0.916176974773407,0.9156688451766968,0.8396698236465454,0.9403483867645264,0.9158578515052795,0.9158578515052795,0.9163069128990173,1.0983933210372925,0.8979874849319458,0.21264612674713135,0.9572293758392334,0.33358097076416016,0.923650860786438,0.9129769206047058,0.9190576672554016,0.9162395000457764,0.8115974068641663,0.8557544946670532,0.6782958507537842,0.9412680864334106,0.8773135542869568,0.9161996841430664,0.3925710916519165,1.0290418863296509,0.9162380695343018,0.9374698400497437,0.8524392247200012,0.9161761999130249,0.8546626567840576,0.33530378341674805,0.9152136445045471,0.9167333245277405,0.7665120363235474,0.9176902174949646,1.593032956123352,0.8240282535552979,0.9401634931564331,0.9796184301376343,0.9159666895866394,0.9161863327026367,0.9163069128990173,0.9149888753890991,0.9175258874893188,0.9945447444915771,0.923287034034729,1.0626519918441772,0.11542399972677231,1.0393415689468384,1.0695182085037231,1.1109607219696045,0.9175626635551453,0.9162126779556274,0.8482031226158142,1.1072916984558105,0.9162395000457764,0.8710289001464844,0.915499210357666,0.7972756624221802,0.8489659428596497,0.9166883230209351,0.9163280129432678,0.8732556700706482,0.16634023189544678,1.2825466394424438,0.9158578515052795,0.916986346244812,0.9164010882377625,1.1494799852371216,0.5528020858764648,0.9179986119270325,1.2020351886749268,0.9161569476127625,1.0745594501495361,0.9158578515052795,0.915499210357666,0.8116089701652527,0.8047270178794861,0.5368146300315857,0.8988218307495117,1.5787297487258911,0.9162766337394714,0.9385908842086792,1.058471918106079,0.944999098777771,0.9163069128990173,0.10507308691740036,1.0671130418777466,1.0037046670913696,1.2229233980178833,0.9584764242172241,0.9158593416213989,0.8596984148025513,0.8754726052284241,0.9096741676330566,0.915499210357666,0.4994789958000183,0.9156296253204346,0.894650399684906,0.9548002481460571,0.8275971412658691,0.9241716861724854,0.9157232046127319,0.8580184578895569,1.1682504415512085,1.69935941696167,1.0431885719299316,0.9110125303268433,0.11393528431653976,0.9163296222686768,0.9166328310966492,0.7380995750427246,0.25301098823547363,0.9065157175064087,0.8803369998931885,1.1107890605926514,1.122124433517456,0.9172362685203552,0.9213056564331055,0.9175512194633484,0.8315626978874207,0.5343575477600098,0.9012348651885986,0.917794942855835,0.814617931842804,0.6608537435531616,0.9219140410423279,0.7772996425628662,0.5315144658088684,0.8324344754219055,0.9467431306838989,0.7650601863861084,0.9166883230209351,0.7406304478645325,0.9301003217697144,1.0993778705596924,1.478583574295044,0.9406380653381348,0.6693817973136902,1.7557740211486816,1.5542956590652466,0.9329692125320435,1.7180957794189453,0.9322162866592407,0.9158578515052795,0.9142666459083557,0.9508668184280396,0.7861132025718689,0.877286970615387,0.9076988697052002,0.9373797178268433,0.9355179071426392,0.9157232046127319,0.8756256699562073,0.8454049229621887,0.9245554208755493,0.9167503118515015,0.9173871278762817,0.7718895077705383,0.9329193830490112,0.29507091641426086,0.9167174100875854,0.22745448350906372,0.85235995054245,0.9162395000457764,0.8839707374572754,0.7104418277740479,0.9079568982124329,0.9163296222686768,1.037135124206543,0.7337862253189087,0.2133074402809143,0.9403918981552124,0.9232187271118164,0.9160374402999878,1.1055744886398315,0.7436133623123169,0.8587659597396851,0.9139394760131836,0.9594944715499878,1.0310178995132446,0.9890627861022949,0.858565092086792,0.8763181567192078,0.8790996074676514,0.9185171127319336,0.4024916887283325,1.0387376546859741,0.9174111485481262,0.9140998125076294,0.9160374402999878,0.9169282913208008,0.9163152575492859,1.0316448211669922,0.9125602841377258,0.1997784674167633,1.8368678092956543,0.9571117162704468,0.5193360447883606,0.7323452234268188,0.8924522995948792,0.8578307628631592,1.127181887626648,0.8125982284545898,0.9247722625732422,1.1952542066574097,0.9751691818237305,0.9158578515052795,0.9184409379959106,0.8606775403022766,0.9152808785438538,0.7269791960716248,0.9042962193489075,1.2778453826904297,0.8129706978797913,0.847576916217804,0.9163069128990173,0.9163135290145874,0.8048941493034363,0.8779808282852173,0.8790421485900879,1.0691449642181396,0.9167556762695312,0.8379156589508057,0.923230767250061,1.10349440574646,1.6161373853683472,0.8387896418571472,0.910056471824646,0.9684350490570068,1.0296530723571777,1.1490432024002075,0.9655117988586426,0.8823455572128296,0.9244135618209839,1.1018588542938232,1.071581482887268,0.9170510172843933,0.674613356590271,0.9041141867637634,0.8036410212516785,1.5568140745162964,0.25848206877708435,0.9165922999382019,0.9826341867446899,0.8891462087631226,0.982172966003418,0.8167327642440796,1.0251621007919312,1.2782821655273438,1.17815101146698,1.3042129278182983,0.8794775605201721,0.9176717400550842,0.8997237086296082,0.9161250591278076,1.7338303327560425,0.9164817929267883,1.5293041467666626,0.9172599911689758,0.9409710168838501,0.8617952466011047,1.2979826927185059,0.9159114360809326,0.761608898639679,0.8759641051292419,1.0980342626571655,1.224517583847046,0.9158578515052795,0.9166883230209351,1.0098904371261597,0.9290355443954468,0.9632025957107544,0.8967214226722717,0.9035691022872925,0.9157232046127319,0.9158578515052795,0.9163966178894043,1.5434108972549438,0.9038670063018799,0.819987416267395,0.9160813093185425,1.1227772235870361,0.91765958070755,0.9155223369598389,1.0340861082077026,0.9757323265075684,0.9168428182601929,0.9175474643707275,0.9234083890914917,1.105068325996399,0.9164010882377625,0.8532707691192627,0.9879262447357178,0.8772498369216919,0.948944091796875,0.9172032475471497,0.9196208715438843,0.8154304027557373,0.9165291786193848,0.9153060913085938,0.9173932671546936,0.9151629209518433,1.2056816816329956,0.749921441078186,0.8438236713409424,0.8809880614280701,1.1055165529251099,0.9103867411613464,0.9930675029754639,0.8485426902770996,0.8620924353599548,0.9157232046127319,0.8249189853668213,0.9362485408782959,0.9524496793746948,0.20465704798698425,0.8967214226722717,0.8507784008979797,0.8859846591949463,1.2106232643127441,1.5399398803710938,1.652916431427002,0.910779595375061,0.9056540727615356,0.9188696146011353,0.6634515523910522,0.7786067724227905,1.178283929824829,1.727325439453125,0.8962775468826294,0.921287477016449,0.9173191785812378,0.5875920057296753,1.012390375137329,0.8967214226722717,0.8299410343170166,0.9026361703872681,1.0715646743774414,0.9164573550224304,1.1274409294128418,1.1414803266525269,0.6455329060554504,0.9944707155227661,0.1369384527206421,1.007094144821167,0.916935384273529,0.915499210357666,0.7923453450202942,0.8675597906112671,0.8865485191345215,0.915499210357666,1.1451829671859741,0.9146824479103088,0.7260733246803284,0.9162395000457764,0.8140460848808289,0.9163069128990173,0.9470986127853394,0.8453689813613892,0.9509578943252563,0.9487966299057007,0.9150677919387817,0.9817968606948853,0.9171896576881409,0.6904725432395935,0.8667562007904053,0.8940272927284241,0.9522128105163574,0.6946715116500854,1.4105799198150635,0.9191645979881287,0.9162395000457764,1.2354580163955688,0.917788028717041,0.7633757591247559,0.9183216691017151,0.9165844917297363,0.9149549603462219,1.2261966466903687,0.2400696873664856,0.38156643509864807,0.9158578515052795,1.1784032583236694,1.0233304500579834,0.9166765809059143,0.9165576100349426,1.2499505281448364,0.9058078527450562,0.7743069529533386,0.916947603225708,0.8122967481613159,1.1087414026260376,0.8758766651153564,0.8781839609146118,0.8341572880744934,0.8563677668571472,1.1016969680786133,1.654702067375183,1.0983452796936035,0.7544121146202087,0.9166883230209351,0.9145137071609497,0.8885136842727661,0.9157481789588928,1.163689136505127,0.9190313220024109,0.9163069128990173,0.9618078470230103,1.619802713394165,0.9522912502288818,0.9256103038787842,0.8552788496017456,1.2541418075561523,0.4458308815956116,0.9163341522216797,1.0558557510375977,0.9933395385742188,1.1408498287200928,0.9158578515052795,1.1740094423294067,1.0909605026245117,0.9139245748519897,0.9174922108650208,0.9158578515052795,1.5200073719024658,0.9845415353775024,1.0546512603759766,0.9926007986068726,0.5214812755584717,0.9033020734786987,0.9813977479934692,1.4580905437469482,0.9096872806549072,1.16146981716156,1.1359267234802246,0.916047215461731,0.9019502997398376,1.0239453315734863,0.9180911183357239,1.0249674320220947,1.1513208150863647,0.8721544146537781,0.9055362939834595,0.9178986549377441,0.9251865148544312,0.9274483919143677,0.9643884897232056,0.9160374402999878,0.9167344570159912,1.1199816465377808,0.9163960814476013,0.8430063128471375,1.0705910921096802,0.8970620036125183,0.5569339990615845,1.0099684000015259,0.4154905080795288,0.8943352699279785,0.8606009483337402,1.0326567888259888,0.8792710900306702,1.1224350929260254,0.9177497029304504,0.8696892857551575,0.9366031885147095,0.21279513835906982,0.9562263488769531,0.7352774143218994,1.3428515195846558,0.7419148683547974,0.9172866344451904,1.1666150093078613,1.6950178146362305,0.8850719928741455,0.9157232046127319,1.4704800844192505,0.9176220893859863,0.25639450550079346,1.1280758380889893,1.5530400276184082,1.009098768234253,1.0322303771972656,0.9250186681747437,0.7293616533279419,0.9163069128990173,0.915499210357666,0.5615084171295166,0.9173524975776672,0.9176726937294006,0.9167587161064148,0.9203960299491882,0.9246716499328613,0.9161960482597351,0.8829281330108643,1.2654508352279663,1.002078652381897,1.1689807176589966,0.9240045547485352,0.8474158048629761,0.5419756174087524,0.9170148372650146,0.8679055571556091,0.917788028717041,0.9730387926101685,0.9158578515052795,0.9174289703369141,0.9531543254852295,0.9159506559371948,0.9084482192993164,0.9163296222686768,1.0271823406219482,0.9317538738250732,0.9167556762695312,0.9162395000457764,1.3590691089630127,0.8756727576255798,0.9381219148635864,0.9173194766044617,0.9286408424377441,0.7528907060623169,0.9552185535430908,0.8809714913368225,0.8647952675819397,0.9168145060539246,1.040595293045044,0.9058481454849243,0.917102038860321,0.8161170482635498,1.7525837421417236,0.926227331161499,0.9064189791679382,0.9267991781234741,1.060319423675537,0.9173574447631836,0.9330071210861206,1.1005066633224487,1.7927545309066772,0.924543023109436,0.9089928865432739,0.5204037427902222,0.9067057967185974,0.9157369136810303,0.9601284265518188,0.7523695826530457,0.811477780342102,0.9150983095169067,0.9180423021316528,0.9161425828933716,1.0354899168014526,0.9754549264907837,0.9157232046127319,0.9485408067703247,0.9039583802223206,1.33172607421875,0.9363707304000854,0.8757047057151794,0.5516930818557739,0.878000020980835,0.7677194476127625,1.3597391843795776,0.9165315628051758,0.9162500500679016,0.9165698289871216,1.0428985357284546,0.7773301005363464,0.8774845600128174,1.5007753372192383,0.8906877040863037,0.9096335768699646,0.916534423828125,1.0953888893127441,0.9084393382072449,0.9158578515052795,0.8922025561332703,0.12426593154668808,0.9163069128990173,0.5352001190185547,0.9261643886566162,0.8526856303215027,0.9181889295578003,0.9154631495475769,1.0887030363082886,0.7680792212486267,1.1986585855484009,0.9079710841178894,1.0682690143585205,0.8605486154556274,0.7052079439163208,0.7455021739006042,0.9277687072753906,1.7541917562484741,0.9903448820114136,0.9154489636421204,1.1872978210449219,1.254343032836914,0.9167513847351074,1.222548246383667,0.924224853515625,0.8527330160140991,0.6395467519760132,1.056947946548462,0.8751102089881897,0.9167538285255432,1.4337122440338135,0.9876030683517456,0.9174005389213562,0.9107845425605774,1.3414957523345947,0.9165538549423218,0.8989963531494141,0.9100180268287659,0.8649203181266785,0.8903114795684814,0.9226758480072021,1.0328154563903809,1.1031453609466553,1.0542420148849487,0.8892658352851868,0.9158578515052795,0.9414478540420532,1.4751083850860596,0.8653589487075806,0.9072079658508301,0.9438338279724121,1.467057228088379,0.9170055985450745,1.4840024709701538,0.9158578515052795,0.9163532853126526,0.9222410321235657,1.0262306928634644,0.9519416093826294,0.7882170677185059,0.7952498197555542,0.44940614700317383,0.915287435054779,0.8834034204483032,0.9159125089645386,1.264495611190796,0.9176046848297119,0.8175972104072571,0.9354795217514038,0.7408689856529236,0.7395692467689514,0.9157232046127319,0.9167991280555725,0.9490066766738892,0.9167556762695312,0.9398746490478516,0.9135280251502991,0.9166883230209351,0.7601703405380249,0.9182466268539429,1.640572428703308,1.1188851594924927,0.702062726020813,0.9167556762695312,1.724518895149231,0.10858755558729172,0.9799872636795044,0.8970901966094971,0.4210358262062073,0.9162395000457764,0.9169226884841919,0.9148659110069275,0.7436233758926392,0.9161554574966431,0.9482805728912354,0.9156384468078613,0.9153620004653931,1.77131986618042,0.9165629744529724,0.9165830016136169,0.7991984486579895,1.1224887371063232,0.9162395000457764,0.9160374402999878,0.9175906181335449,0.8699814677238464,0.9157899022102356,1.0532277822494507,0.8751850128173828,0.8939302563667297,0.9298652410507202,1.0856459140777588,0.9343457221984863,0.9148154258728027,0.9169644117355347,0.7187086343765259,0.9838505983352661,0.914867639541626,0.6630568504333496,0.9173523187637329,0.44951581954956055,0.9530943632125854,0.3882822096347809,1.0513241291046143,0.9161439538002014,0.9991207122802734,1.1964149475097656,0.7823930382728577,0.932574987411499,0.39391401410102844,0.9666073322296143,0.9165315628051758,0.8144028186798096,0.9154714941978455,0.9161606431007385,0.9888519048690796,0.8812385201454163,0.9176447987556458,0.9436105489730835,0.9171184301376343,0.9158578515052795,0.8591887354850769,0.950376033782959,0.8997088670730591,0.9360055923461914,0.863979697227478,0.9159643650054932,1.8773258924484253,1.0052133798599243,0.7068597674369812,0.916304349899292,0.9158578515052795,0.9158578515052795,0.7628581523895264,0.8710625767707825,0.916161060333252,0.8078591227531433,0.8867008090019226,0.8676558136940002,1.1016072034835815,1.0163872241973877,0.9288530349731445,0.9167333245277405,1.1750409603118896,0.9068719744682312,0.8412833213806152,1.5088948011398315,1.3290480375289917,0.8562973141670227,0.9155291318893433,0.8851684927940369,1.1189568042755127,0.8352411389350891,0.9227202534675598,0.8489787578582764,0.8926659226417542,0.9643045663833618,0.9481288194656372,0.45183074474334717,0.9345507621765137,1.0106369256973267,1.4299900531768799,0.7304043769836426,0.9027543067932129,0.9163069128990173,0.6909326910972595,0.915499210357666,1.7377551794052124,0.9080591201782227,0.9048659801483154,0.9239034652709961,0.8924404978752136,0.899074912071228,0.9166086912155151,0.9133058190345764,0.9143319129943848,0.9156817197799683,0.915573239326477,0.9962928295135498,0.9154430031776428,0.9009000658988953,0.9183448553085327,0.9669400453567505,0.5106000304222107,0.9141878485679626,0.9168917536735535,0.9389322996139526,0.949862003326416,0.9154199361801147,0.916178286075592,0.8207666873931885,0.511162281036377,1.057664394378662,1.2394788265228271,0.9158578515052795,1.0141280889511108,0.7906754612922668,0.8394019603729248,1.4550496339797974,0.9162210822105408,0.9902843236923218,0.865922212600708,0.917788028717041,0.915977954864502,0.9162395000457764,1.0881086587905884,0.9162395000457764,1.1833146810531616,1.0649847984313965,0.8439632058143616,1.4863240718841553,0.9165315628051758,1.0628831386566162,0.8647245168685913,0.8884238004684448,0.8458483815193176,0.8157414197921753,0.894158661365509,0.9156174659729004,0.9286772012710571,1.8293519020080566,0.9162395000457764,0.915824294090271,0.681559145450592,0.9001864194869995,0.9357004165649414,0.6571941375732422,1.028875708580017,0.8422508239746094,0.8804024457931519,1.359445571899414,0.9180846810340881,0.35756707191467285,1.0780624151229858,1.1274199485778809,0.9258296489715576,0.9327059984207153,0.9167556762695312,0.8261152505874634,0.8457340002059937,0.7524346113204956,0.7286897897720337,0.7594384551048279,1.1064772605895996,0.7078397870063782,0.8318760991096497,1.129042387008667,0.8630704879760742,0.991391658782959,0.8533195853233337,0.8369227051734924,0.9158578515052795,1.383931040763855,1.1657122373580933,0.9158578515052795,0.8879905343055725,0.9156887531280518,0.9166883230209351,0.5363606214523315,0.9144545197486877,1.0721280574798584,1.1267626285552979,0.935057520866394,0.922078013420105,0.689216673374176,0.9198395609855652,1.0340310335159302,0.8959816694259644,0.9144858717918396,0.9169723987579346,0.7876982688903809,1.1215569972991943,0.9177052974700928,0.7955515384674072,0.915499210357666,1.1397048234939575,1.2367304563522339,0.8967214226722717,0.9182946085929871,0.926032543182373,0.9155236482620239,0.9166555404663086,1.7392096519470215,0.917788028717041,0.917788028717041,0.9167166352272034,0.9156912565231323,0.8131349682807922,0.10105615109205246,1.0329930782318115,0.9085875749588013,0.9158578515052795,0.9169711470603943,0.9154783487319946,0.900606632232666,0.9175840020179749,1.1706135272979736,1.0717310905456543,1.3960208892822266,0.9177645444869995,1.3275206089019775,0.9826333522796631,0.8384170532226562,0.917448103427887,0.21702197194099426,0.958701491355896,0.9182653427124023,0.891743004322052,0.9355376958847046,0.9440399408340454,0.920259952545166,0.9162331819534302,0.9818885326385498,0.9158850312232971,0.8311898708343506,0.9159461259841919,0.915499210357666,0.9165287613868713,0.9456726312637329,0.9545170068740845,0.8206644058227539,0.915499210357666,1.1553810834884644,0.7942638993263245,0.9517402648925781,0.7630937695503235,0.9162395000457764,1.0053753852844238,0.42842888832092285,0.9117238521575928,0.9163500070571899,0.7941271066665649,0.8833999037742615,0.7354282140731812,0.9257602691650391,0.8900211453437805,1.2422970533370972,0.9294747114181519,0.9157048463821411,1.0179619789123535,0.8168054819107056,0.917788028717041,0.9164010882377625,0.7544964551925659,0.6672304272651672,1.1075316667556763,0.6743050813674927,0.9158983826637268,0.915499210357666,0.9293643236160278,1.1871979236602783,0.9158578515052795,0.9154330492019653,0.9292219877243042,0.9158578515052795,0.9158763885498047,0.9027100205421448,0.2886645197868347,0.9045370221138,1.401370882987976,0.8471499681472778,0.923509955406189,1.5383963584899902,0.7587084770202637,0.7693578600883484,1.747244119644165,0.8719767332077026,1.1200060844421387,0.9893569946289062,0.9482181072235107,0.9174482226371765,0.8127107620239258,0.9163069128990173,0.9142675995826721,0.9541631937026978,0.915499210357666,0.9162395000457764,0.9181066751480103,0.9163069128990173,0.8406229615211487,0.48191702365875244,0.928781270980835,0.9122216701507568,1.0242412090301514,0.9557340145111084,0.9351160526275635,0.8825425505638123,1.0347275733947754,1.1972423791885376,0.6710062623023987,0.8936778903007507,0.38549286127090454,0.771592378616333,0.9058004021644592,0.9480082988739014,1.125512957572937,0.9154177308082581,1.401496410369873,0.8967214226722717,1.0240219831466675,0.8869514465332031,0.9161535501480103,0.7768679857254028,0.9171521067619324,1.2449527978897095,0.9159825444221497,0.557278037071228,1.0312421321868896,0.9509676694869995,0.9165664911270142,0.9072697162628174,0.9180973768234253,0.9166758060455322,0.9154463410377502,0.9177643656730652,1.5395492315292358,0.9407954216003418,0.869756281375885,0.915499210357666,0.9171397686004639,0.9158578515052795,0.9425767660140991,0.9201382994651794,1.4849926233291626,0.7805277705192566,1.072095274925232,0.9386786222457886,0.9246444702148438,0.9371706247329712,0.962448000907898,0.48589128255844116,0.9263629913330078,0.9119300842285156,0.9168668985366821,0.9159075617790222,0.23204082250595093,0.9427913427352905,0.917788028717041,1.7305296659469604,0.9615334272384644,0.9561066627502441,0.9163069128990173,0.8505992889404297,0.9333099126815796,0.8207740187644958,0.9163051843643188,0.9160374402999878,0.9203101992607117,1.0008715391159058,0.9602688550949097,1.1025424003601074,0.9543875455856323,0.9183189868927002,0.9167507886886597,1.20332670211792,0.9167333245277405,0.9863048791885376,0.873020350933075,0.5650525093078613,1.0954807996749878,0.9153990745544434,0.9166883230209351,0.9394468069076538,1.0942555665969849,0.8002139329910278,1.2530828714370728,0.5607982277870178,0.15618515014648438,0.916480302810669,1.235368013381958,0.9174382090568542,0.6533199548721313,0.915757954120636,0.9732213020324707,0.9076074957847595,0.916758120059967,0.9247535467147827,0.38780146837234497,0.3105374276638031,0.7473832964897156,0.7200747132301331,0.9498671293258667,0.7497172951698303,0.9986138343811035,0.9182703495025635,0.9151686429977417,0.9244598150253296,0.8686165809631348,1.0075238943099976,0.916776180267334,1.0159538984298706,0.9269247055053711,1.2029558420181274,0.9165143370628357,0.9044276475906372,0.9141805171966553,0.9163069128990173,1.485623836517334,0.9397448301315308,1.0580036640167236,0.917788028717041,0.8852500915527344,0.9159488677978516,0.8692153096199036,0.9177829623222351,0.6925994157791138,1.1808559894561768,0.8963801264762878,0.8467835187911987,1.178357720375061,0.23815929889678955,0.47261589765548706,0.9158578515052795,0.915499210357666,0.9158578515052795,0.47860145568847656,0.9370598793029785,1.2252726554870605,0.7513286471366882,0.9163069128990173,0.9162395000457764,0.9583072662353516,1.2199254035949707,0.9803264141082764,0.9472528696060181,0.958185076713562,0.9162395000457764,0.8892049193382263,1.3727010488510132,0.8141356706619263,0.8512464761734009,1.233632206916809,0.8865534067153931,0.8824547529220581,0.9816967248916626,1.2458348274230957,0.8562924861907959,1.6348398923873901,0.4156312346458435,0.917424738407135,0.9230198860168457,0.55999356508255,0.9316649436950684,0.8945163488388062,0.9145697951316833,0.9168969392776489,0.7360220551490784,1.5408893823623657,0.9160374402999878,0.9756122827529907,1.0474984645843506,1.6015633344650269,0.9189503192901611,0.7355791926383972,0.9166402816772461,0.915599524974823,0.8342025279998779,0.917788028717041,0.9619419574737549,0.8920677900314331,0.916170060634613,0.9341720342636108,0.9780316352844238,0.9162395000457764,0.9158578515052795,0.6620846390724182,0.9156681299209595,0.915077805519104,0.9164010882377625,0.9161792993545532,1.2587412595748901,0.9131380319595337,1.2243678569793701,0.9397255182266235,0.917788028717041,1.1307202577590942,0.7915245890617371,0.7509092688560486,0.9831026792526245,0.9111008048057556,0.9442229270935059,0.9167732000350952,1.079580307006836,1.004440426826477,1.1453510522842407,0.9576197862625122,0.9760880470275879,0.9158578515052795,0.915499210357666,0.9167469143867493,0.9049402475357056,0.9109169244766235,1.3585433959960938,0.9155722260475159,0.9155044555664062,1.0404317378997803,0.9160374402999878,0.2322334349155426,0.7651876211166382,0.9057807922363281,0.9671648740768433,0.9179826974868774,0.9237312078475952,1.1230331659317017,0.8817000985145569,1.7318763732910156,0.9173203110694885,0.858722984790802,0.7304906249046326,1.4459034204483032,0.9165538549423218,1.0542829036712646,0.47782695293426514,0.9166883230209351,0.9574183225631714,0.9164010882377625,0.7588728666305542,0.9424493312835693,1.0479028224945068,0.9106635451316833,0.9002779126167297,0.9211053252220154,0.8424460887908936,1.0005555152893066,0.9314604997634888,0.7267138957977295,1.1756789684295654,0.9884225130081177,0.913341760635376,0.5150001049041748,0.9390416145324707,0.5465428233146667,0.9327143430709839,0.9491053819656372,0.9107198119163513,0.917788028717041,0.5942305326461792,0.8881312608718872,0.9160361886024475,0.9162395000457764,0.7811597585678101,0.9166883230209351,1.109279751777649,1.2221310138702393,0.8931387066841125,0.40260764956474304,0.8025182485580444,0.915499210357666,0.9142409563064575,1.487229824066162,0.9162395000457764,0.8152380585670471,0.9174628853797913,0.869592547416687,1.1118308305740356,0.8294664025306702,0.8681008219718933,0.7196162939071655,0.9154535531997681,0.9470223188400269,0.916011393070221,0.9803663492202759,0.9325557947158813,0.9158578515052795,0.8912894129753113,1.1322015523910522,1.033714771270752,0.44041138887405396,0.7944914698600769,1.7428185939788818,0.8712222576141357,0.9162395000457764,1.283131718635559,0.08581282943487167,0.15357458591461182,0.7992405295372009,0.8398818969726562,1.0600117444992065,0.8945415019989014,0.9215379953384399,1.25949227809906,0.8143093585968018,0.9162594676017761,0.8054391741752625,0.9158578515052795,0.9163765907287598,0.9553085565567017,0.9049604535102844,1.193437099456787,0.27856773138046265,0.3199443221092224,0.814447820186615,0.8589938879013062,0.9455180168151855,0.9158902764320374,0.9902753829956055,0.915499210357666,0.9165315628051758,1.0037758350372314,0.9158958196640015,0.9166051149368286,0.9168141484260559,0.4646737575531006,0.9301315546035767,0.8232260346412659,0.9437685012817383,0.9349958896636963,0.9155059456825256,0.8835368156433105,0.9162395000457764,0.8043638467788696,0.917788028717041,0.8838668465614319,0.9165138602256775,0.927701473236084,0.6375173330307007,0.9700295925140381,0.966040849685669,0.9168280363082886,0.3809177279472351,0.23358339071273804,0.8997122645378113,0.9167791604995728,0.9161193370819092,0.9122713208198547,0.4151315987110138,0.11694227904081345,1.2242968082427979,0.9168311357498169,0.9171103835105896,0.900058925151825,0.6985228061676025,0.31185075640678406,1.1601638793945312,0.9318311214447021,1.147542953491211,0.9341367483139038,0.776162326335907,0.7767689228057861,0.10652484744787216,0.9414814710617065,0.7405779361724854,1.0089243650436401,1.1230568885803223,1.2120200395584106,0.8485046029090881,0.9770599603652954,0.9150364398956299,1.0258630514144897,1.0225045680999756,1.064234972000122,1.3533682823181152,0.7686885595321655,0.9096506237983704,0.9803421497344971,0.9220320582389832,1.1917550563812256,0.7623259425163269,0.8707305788993835,0.9166883230209351,1.1573280096054077,0.9159471988677979,0.8830711841583252,0.8900727033615112,1.3507356643676758,0.9158578515052795,1.0414695739746094,1.2913137674331665,0.917788028717041,0.9462571144104004,0.9147780537605286,0.8689397573471069,0.9053848385810852,0.9022485613822937,0.9163069128990173,0.878528892993927,0.37789562344551086,0.7883102297782898,0.7210956811904907,0.9092099666595459,0.7721594572067261,0.35143807530403137,0.8865889310836792,0.7231645584106445,0.8148296475410461,1.1842567920684814,1.2340176105499268,0.917788028717041,0.9169027209281921,1.5068211555480957,0.9162395000457764,0.9183985590934753,0.9315052032470703,1.044632077217102,1.5396448373794556,0.9158578515052795,1.2591959238052368,0.9165315628051758,0.9164573550224304,0.9521623849868774,0.8181894421577454,0.8265238404273987,0.9811500310897827,1.0110204219818115,0.7124771475791931,0.8896230459213257,1.0895212888717651,0.8923108577728271,0.8725611567497253,0.31749436259269714,0.9081758260726929,1.715619683265686,0.7055819034576416,0.8961919546127319,0.9166514873504639,0.9730541706085205,0.915499210357666,1.038461446762085,0.9174597859382629,0.9663949012756348,0.829351007938385,0.7762897610664368,1.280484914779663,0.9171620607376099,0.9166083335876465,0.946711540222168,0.9267458915710449,0.9448521137237549,0.9148476719856262,0.8419402241706848,0.8904539942741394,0.917788028717041,0.7911300659179688,1.001908540725708,0.7278581857681274,0.28666120767593384,1.188855528831482,0.9167071580886841,0.7487621307373047,0.9435670375823975,0.9167333245277405,0.9097069501876831,0.815823495388031,0.9232618808746338,0.9177874326705933,0.8561890721321106,0.9176238179206848,0.5255944728851318,1.031534194946289,0.9284608364105225,0.9180765748023987,1.0409371852874756,0.9157232046127319,1.00491464138031,0.9158578515052795,0.9436002969741821,0.9166883230209351,0.8661408424377441,0.9730677604675293,1.139053225517273,0.9158578515052795,0.917302668094635,0.9177674055099487,0.916638970375061,0.9158578515052795,0.9517413377761841,0.9227834343910217,1.7259719371795654,0.9118369221687317,0.5271379947662354,0.9057971239089966,0.9156401753425598,0.9163069128990173,0.9504919052124023,0.9711881875991821,0.7615817785263062,0.9480044841766357,0.8453531861305237,0.9192671775817871,0.7406506538391113,0.9155956506729126,0.7714723348617554,0.987190842628479,0.9172648191452026,1.0199439525604248,1.3033720254898071,1.0267791748046875,0.8284554481506348,0.9162663817405701,0.16762438416481018,0.916426956653595,0.9587632417678833,0.9165538549423218,0.9165538549423218,0.9169560074806213,0.7488593459129333,0.9181581139564514,0.955303430557251,0.8964489102363586,0.9141972661018372,1.2541940212249756,1.2655316591262817,0.8995904326438904,0.9169503450393677,0.7289965748786926,0.9673053026199341,0.9723571538925171,0.9514421224594116,1.1345208883285522,0.40750646591186523,0.9168153405189514,0.9169412851333618,0.8148685097694397,0.9441949129104614,0.9155104160308838,0.864986777305603,0.7482404112815857,0.9381582736968994,0.8213995695114136,0.8527907729148865,0.9156469702720642,0.947035551071167,0.9092830419540405,0.9154173135757446,0.6384384632110596,0.9162308573722839,0.8989473581314087,0.8548441529273987,1.0374419689178467,0.9155800938606262,0.9187214374542236,1.190833568572998,0.9150429964065552,0.970050573348999,0.8699933290481567,0.8814250230789185,0.9162395000457764,1.1059422492980957,0.9250384569168091,0.9147947430610657,0.9157232046127319,0.9838132858276367,0.9173390865325928,1.0956581830978394,0.8869951963424683,0.9863957166671753,1.035810947418213,0.9125459790229797,0.9014453887939453,0.8059146404266357,0.6983060240745544,0.9158578515052795,0.9410679340362549,0.9591420888900757,0.7934589982032776,1.1960746049880981,0.8925201296806335,0.981809139251709,1.1965937614440918,0.9146554470062256,0.9163069128990173,0.9130232334136963,0.9453412294387817,1.1544991731643677,0.9158578515052795,0.9470924139022827,1.0508078336715698,0.9808036088943481,0.9147788882255554,0.9174091815948486,1.1815277338027954,0.8967001438140869,0.9163687825202942,0.9014421105384827,0.7626044154167175,0.9724011421203613,1.0340429544448853,0.9228805303573608,0.8218238949775696,0.9637131690979004,0.7734988927841187,0.8891600370407104,0.9160179495811462,0.9158357381820679,0.9163296222686768,1.0529911518096924,0.9008934497833252,0.9163296222686768,1.0570610761642456,0.9427220821380615,0.9165533781051636,0.9365177154541016,0.8951320052146912,1.7977863550186157,0.9163069128990173,0.9158578515052795,0.8632212281227112,0.8738162517547607,0.7758312225341797,0.7447667717933655,0.9158578515052795,1.0459532737731934,0.9227054119110107,0.9357906579971313,0.9565542936325073,0.8972561955451965,0.9162387847900391,1.0918792486190796,0.9176938533782959,0.8878347873687744,0.9172748923301697,1.2630635499954224,1.178407907485962,0.9270168542861938,0.9347110986709595,1.0220755338668823,0.7808901071548462,0.7793686985969543,0.9157232046127319,1.3626693487167358,0.9158578515052795,0.9775006771087646,0.9781323671340942,0.9974758625030518,0.9161846041679382,0.7788344621658325,0.9528313875198364,0.9165538549423218,0.9196263551712036,0.9158578515052795,1.0756925344467163,0.7916884422302246,0.7792583107948303,0.9163069128990173,0.92264723777771,1.1192525625228882,0.8250167965888977,0.714693009853363,0.881449282169342,0.8826208710670471,0.9612469673156738,0.8756234049797058,1.0930655002593994,0.8497329950332642,0.7808794379234314,0.9899535179138184,0.9171576499938965,0.9164340496063232,0.7615959048271179,0.9123328924179077,1.0583255290985107,0.917788028717041,0.9800665378570557,1.1203293800354004,0.530423104763031,0.8937873840332031,0.9222881197929382,0.9471707344055176,0.8482466340065002,0.9430379867553711,0.9162395000457764,0.7738434076309204,0.8511189818382263,0.9163069128990173,0.875478208065033,0.9397883415222168,0.7626250982284546,0.916593074798584,0.5236315727233887,0.7710154056549072,0.9155128002166748,0.9161856770515442,0.9233810901641846,0.8714931011199951,0.7763418555259705,0.837446928024292,0.9165310859680176,0.9175379872322083,0.883249819278717,0.9163580536842346,0.8426718711853027,1.1181460618972778,0.9946802854537964,0.8904708623886108,0.6947695016860962,0.30223071575164795,1.5833646059036255,0.9119118452072144,0.7081512808799744,0.9519623517990112,0.8021386861801147,0.9126664400100708,0.8374152183532715,0.9372472763061523,0.9149850010871887,0.9583852291107178,0.9162395000457764,0.9178429841995239,0.9660184383392334,0.868938684463501,0.6914929747581482,0.8961285948753357,0.8961691856384277,1.0865989923477173,0.9291921854019165,0.17282912135124207,0.8620104193687439,0.9177722930908203,0.887024462223053,1.0799751281738281,0.9156268835067749,1.0184366703033447,0.8225749135017395,0.9130306839942932,0.9605599641799927,0.9159969091415405,1.0102747678756714,0.8150106072425842,0.9082575440406799,0.9160801768302917,0.9136019945144653,0.73302161693573,0.9024871587753296,0.9163069128990173,0.9787260293960571,1.23255455493927,0.3061191439628601,0.9162395000457764,1.0685060024261475,0.6673265099525452,0.9151310324668884,0.10217615216970444,0.7408055067062378,0.902306854724884,0.9438506364822388,0.937088131904602,0.9162395000457764,1.4429295063018799,0.20479848980903625,0.8396267294883728,1.1085219383239746,0.9172794818878174,0.6471688747406006,0.678156852722168,0.916914165019989,1.7391315698623657,1.0473606586456299,0.6797409057617188,1.0413789749145508,0.7424519658088684,0.9158499836921692,0.9169378876686096,1.2282485961914062,0.9156182408332825,0.8686369061470032,0.9177408814430237,0.8849228024482727,0.974217414855957,0.9164054989814758,0.8727951049804688,0.7730672359466553,0.9173390865325928,0.2357097566127777,0.9063815474510193,0.5557609796524048,1.1917933225631714,0.7926998138427734,1.0121392011642456,1.1110217571258545,0.9170619249343872,1.5257474184036255,0.7107114791870117,0.8574636578559875,0.9163389801979065,0.917788028717041,0.6672618389129639,0.9167556762695312,1.081583023071289,0.9160557985305786,0.658397912979126,0.9471868276596069,0.7340996265411377,1.0318388938903809,0.9166191220283508,1.577815294265747,0.9728266000747681,1.0539618730545044,0.9238197803497314,0.8885024785995483,0.8922157883644104,0.9158578515052795,0.8899850249290466,1.2157185077667236,0.9590590000152588,1.2130924463272095,0.8535110950469971,0.4006667733192444,1.046701192855835,0.917306661605835,0.9101001620292664,1.100165605545044,0.9174434542655945,1.148132562637329,0.9673800468444824,0.9824810028076172,0.8857851624488831,1.405956506729126,0.8567497134208679,0.9050637483596802,0.9545228481292725,0.8260419368743896,0.9153997898101807,0.926742672920227,0.8829793334007263,0.11463352292776108,1.3714250326156616,0.9675322771072388,1.0265586376190186,0.8843593597412109,0.8763842582702637,0.7576043009757996,0.8662130236625671,0.9899652004241943,0.9163069128990173,1.0408167839050293,0.9154883027076721,0.7608950734138489,0.9238157272338867,0.9175446629524231,0.9635933637619019,0.9162395000457764,0.916387677192688,0.9158968329429626,0.8945207595825195,0.8079887628555298,1.2469996213912964,1.2715548276901245,0.9187731146812439,0.8621213436126709,0.958793044090271,0.9420903921127319,0.916297197341919,0.7883778214454651,1.0951062440872192,0.9176401495933533,0.9155176281929016,1.2204161882400513,0.9158578515052795,0.7589167952537537,1.0398110151290894,1.0349971055984497,0.9765622615814209,0.9455859661102295,0.9872338771820068,1.1137232780456543,0.8752623796463013,0.9159954190254211,0.9663195610046387,0.8220946788787842,0.9164259433746338,0.8860183954238892,0.915384829044342,0.2682928442955017,0.9166322946548462,1.0934462547302246,0.9163296222686768,0.943841814994812,0.9165773987770081,0.8880303502082825,0.9252405166625977,0.824154794216156,0.916877806186676,0.916267454624176,0.8746438026428223,0.9173390865325928,0.9037129282951355,0.9210246205329895,0.9148505926132202,0.9164404273033142,1.160233974456787,0.7630505561828613,0.6503865718841553,0.9158578515052795,0.8465766310691833,1.0061413049697876,1.3611700534820557,0.9158578515052795,1.3188472986221313,0.9158578515052795,0.8889031410217285,0.9612886905670166,0.915238618850708,0.9159145951271057,0.9417446851730347,0.9173390865325928,0.9523319005966187,0.49014121294021606,0.9175841808319092,0.9306254386901855,1.0982604026794434,0.9162395000457764,0.9165729880332947,1.1131701469421387,0.8215851783752441,0.9162395000457764,0.920455276966095,0.5207540988922119,0.9165222644805908,0.9170940518379211,0.7661703824996948,0.9263511896133423,0.4709530472755432,0.9340896606445312,0.9067433476448059,0.9161475300788879,0.7636730670928955,0.9169101119041443,1.4006550312042236,0.916113018989563,0.9170206189155579,0.9086223244667053,0.6948057413101196,0.8997851610183716,0.8782854676246643,0.9175215363502502,0.9177966117858887,1.1952176094055176,0.9174609780311584,0.9798033237457275,0.9157232046127319,0.7665886282920837,1.0479154586791992,0.9475659132003784,0.9087432622909546,0.9150657057762146,0.9103548526763916,0.6309581995010376,0.984430193901062,0.8443394899368286,0.7279108762741089,1.028102993965149,0.7725718021392822,0.8143284916877747,0.8512744903564453,0.7422618865966797,0.915499210357666,0.9434775114059448,0.9169518947601318,0.8654430508613586,0.8737657070159912,0.7428234815597534,0.7732902765274048,0.9176357388496399,0.8474597930908203,1.1720770597457886,0.9224139451980591,0.7193731069564819,1.2603670358657837,1.0241338014602661,0.9386874437332153,0.915777862071991,0.9303083419799805,0.8474753499031067,0.6912640333175659,0.7816697359085083,1.203139066696167,1.0516598224639893,0.9202269911766052,0.7926080226898193,0.9158578515052795,0.9164628386497498,0.9304447174072266,0.9155383110046387,0.8096753358840942,0.9160374402999878,0.9164851903915405,0.6889828443527222,0.872267484664917,1.014162302017212,0.9176082015037537,0.7571486234664917,0.915499210357666,0.8331735730171204,0.9096964001655579,0.9156699776649475,0.7524145245552063,0.8919094800949097,0.8907777667045593,0.9922518730163574,1.0653965473175049,0.9167423248291016,1.7777132987976074,0.8381593227386475,0.9166857004165649,0.9468100070953369,0.9579306840896606,1.4353221654891968,0.9525413513183594,0.9067568778991699,0.9930357933044434,0.8675568103790283,0.8860011100769043,0.6792962551116943,0.21465903520584106,0.9652116298675537,0.9158430695533752,1.1184253692626953,1.0034047365188599,0.9158578515052795,1.1842303276062012,0.8841827511787415,0.8717550039291382,0.9361040592193604,0.9166883230209351,0.9455361366271973,0.9173374772071838,0.9001049399375916,0.9555103778839111,1.3867623805999756,0.9189004302024841,0.9386175870895386,0.9506765604019165,1.0107249021530151,0.9157527089118958,0.9163069128990173,0.73153156042099,0.9132509827613831,1.093786597251892,0.9393365383148193,1.0974452495574951,0.9163069128990173,0.9158578515052795,0.8199871778488159,0.6000577211380005,0.8755425810813904,0.9480977058410645,0.9018118381500244,1.0766253471374512,0.9157232046127319,0.9169192314147949,0.9509669542312622,0.8864107131958008,0.9158578515052795,0.931401252746582,0.8082901239395142,0.7283646464347839,1.0955125093460083,1.0040299892425537,1.6447594165802002,1.0525795221328735,0.8967214226722717,0.8967214226722717,0.27386927604675293,0.9156568050384521,0.9098706841468811,0.8501338362693787,1.335370659828186,0.9114137291908264,0.9175347685813904,1.093841552734375,0.9447934627532959,0.3406490981578827,0.4729490876197815,0.9475998878479004,1.1366150379180908,1.1209616661071777,0.8521766662597656,0.9151461720466614,0.915499210357666,0.8178172707557678,0.9163081049919128,0.8905422687530518,0.956331729888916,0.9473385810852051,1.1286133527755737,1.5682250261306763,0.6279661655426025,0.8687335252761841,0.8739237785339355,0.9487831592559814,0.9151669144630432,0.9176231622695923,1.093307375907898,0.9169293642044067,0.9159736633300781,1.712432622909546,0.9392544031143188,0.9667432308197021,1.866680383682251,0.24989169836044312,0.8913565278053284,0.9152079224586487,0.37992843985557556,0.9202991724014282,1.146083116531372,0.916900634765625,0.8677542805671692,0.9155855774879456,0.9162395000457764,0.9165874719619751,0.6884994506835938,0.8606564998626709,1.3757407665252686,0.9196627140045166,1.4222464561462402,0.9164347648620605,0.9666296243667603,0.9040775895118713,0.9196333289146423,1.0291329622268677,0.9180936813354492,0.7734454870223999,1.3129346370697021,0.9160771369934082,0.9749995470046997,0.9267209768295288,0.9165623784065247,0.9641742706298828,0.8794808387756348,0.8170264363288879,0.915499210357666,1.045019507408142,0.917788028717041,0.9125561118125916,0.8967214226722717,0.9176276326179504,1.014331579208374,0.8166108727455139,1.251125693321228,0.9178811311721802,0.9170985221862793,0.5575116276741028,0.8302733302116394,0.8479211926460266,0.919157862663269,0.8695709109306335,0.9421267509460449,0.8030511736869812,0.2784745991230011,0.9052189588546753,0.8825975060462952,0.9176966547966003,0.7387841939926147,0.9158578515052795,0.5958793759346008,0.8776986598968506,0.9687191247940063,0.9156016707420349,1.001503825187683,0.9164010882377625,0.9586664438247681,0.917694091796875,0.9043189287185669,1.2353483438491821,0.8533310890197754,0.9158578515052795,1.0022064447402954,1.2101597785949707,0.154901385307312,0.9166883230209351,0.9166145920753479,0.9065452218055725,0.9158578515052795,0.9232099056243896,0.9422503709793091,0.9228440523147583,0.3408355116844177,0.9169324040412903,0.9172409176826477,1.4563963413238525,0.9181017279624939,0.9158948659896851,0.9126251935958862,0.9374210834503174,0.8872516751289368,0.9163385033607483,0.9191278219223022,0.9399234056472778,0.9173390865325928,0.9274337291717529,0.9163069128990173,0.470650851726532,0.9695924520492554,0.8804938793182373,0.9168860912322998,0.8371484279632568,0.2781484127044678,1.065016269683838,1.5891432762145996,0.9202613830566406,0.8556491136550903,0.8452672958374023,0.7399567365646362,1.4412676095962524,0.9166883230209351,1.2642351388931274,0.878082811832428,1.8136324882507324,1.0446803569793701,0.2814418077468872,0.9057151079177856,0.9165315628051758,0.9164679646492004,0.9671015739440918,0.5000376105308533,0.9477629661560059,0.9362360239028931,1.0534074306488037,0.9340372085571289,1.4252620935440063,0.9162395000457764,0.9984655380249023,1.674978256225586,0.9500710964202881,0.914962112903595,0.8786258101463318,0.9634981155395508,0.9167431592941284,0.9168828725814819,0.8747104406356812,0.9113503694534302,0.726100504398346,0.9172967076301575,1.0215954780578613,0.9180096983909607,0.9165294766426086,0.22693458199501038,0.9069769978523254,0.8508623838424683,1.2160537242889404,0.9159407615661621,0.9158578515052795,1.1024967432022095,0.7171094417572021,1.228179693222046,0.9167333245277405,0.9178046584129333,0.9179968237876892,1.1192799806594849,0.9765539169311523,1.1414214372634888,0.7245673537254333,0.9164605736732483,0.6098982691764832,0.8529708385467529,0.8638514280319214,0.9161321520805359,0.9154807329177856,0.916015088558197,1.0457537174224854,0.7405868172645569,0.8665347099304199,0.5926637053489685,0.9161855578422546,0.9991343021392822,-0.6836178302764893,0.8977951407432556,0.9150385856628418,0.9760633707046509,0.9162395000457764,0.43095529079437256,0.8899993896484375,0.9403461217880249,0.9172688722610474,1.0844306945800781,1.5810635089874268,0.9288889169692993,0.9171529412269592,0.9634276628494263,0.9402321577072144,0.9248920679092407,0.9321291446685791,0.9163917303085327,0.9629900455474854,0.8876675367355347,0.91670823097229,0.9159979224205017,0.9163069128990173,0.9162395000457764,1.275744915008545,0.9170872569084167,0.5185187458992004,0.8872301578521729,1.000882863998413,0.856033980846405,0.9292960166931152,0.917788028717041,0.8538165092468262,0.95194411277771,1.0049773454666138,0.9654620885848999,0.9169822335243225,0.9085991978645325,0.9689418077468872,0.8957192897796631,0.9287307262420654,0.8146864771842957,0.8297179937362671,0.8882668018341064,0.8625975251197815,0.8280704021453857,0.9171199202537537,0.9876078367233276,0.9202156066894531,1.0678967237472534,1.747002363204956,0.9167336821556091,1.1871225833892822,0.9568281173706055,0.9137072563171387,0.8934969305992126,0.7899753451347351,0.9313862323760986,0.9313361644744873,0.9703191518783569,0.9473279714584351,0.9181530475616455,0.909272313117981,0.9161911010742188,0.9163296222686768,1.0241957902908325,0.33034366369247437,1.0150794982910156,0.8509225845336914,0.8540909886360168,0.8998602628707886,0.9763181209564209,0.9165538549423218,0.8133522272109985,1.0344996452331543,0.990379810333252,1.0353021621704102,0.8580275177955627,0.916206955909729,0.9180274605751038,0.8010562658309937,1.165352702140808,0.687489926815033,0.9161416292190552,0.9176920056343079,0.9161384105682373,0.9580608606338501,0.8636438846588135,0.8441786170005798,0.5098572969436646,0.7743030190467834,0.9907596111297607,0.9167919754981995,1.0495916604995728,1.0670047998428345,1.0216675996780396,0.9145262241363525,1.1182185411453247,0.9883962869644165,0.9332555532455444,0.9288114309310913,0.9942066669464111,0.9375388622283936,1.0092803239822388,1.4592071771621704,0.9164267182350159,0.926398515701294,0.9160420298576355,0.9161763787269592,0.9171234369277954,0.9160374402999878,0.8749443292617798,0.7331843376159668,0.9158578515052795,0.9163069128990173,0.9155591130256653,0.9158420562744141,1.6490353345870972,1.4710441827774048,0.8939418792724609,0.7199342250823975,0.893203616142273,0.917987048625946,0.917003333568573,1.3940532207489014,0.9162395000457764,0.9158009886741638,0.88135826587677,0.7413288950920105,0.7011655569076538,0.8893698453903198,0.9166041016578674,0.9173914790153503,0.9177606701850891,0.8843691349029541,0.9111799597740173,1.0751092433929443,1.2994340658187866,0.9011045098304749,0.633747398853302,0.9158578515052795,0.7434866428375244,0.9059225916862488,0.753383994102478,0.9066081047058105,0.931883692741394,1.418031096458435,0.910120964050293,0.9167557954788208,0.8626894950866699,0.9991978406906128,0.15401208400726318,0.8833520412445068,0.8752829432487488,0.9728158712387085,1.0553909540176392,0.9135848879814148,0.9453867673873901,0.9158578515052795,0.5648188591003418,0.8118706941604614,0.22819894552230835,0.8835094571113586,0.33950161933898926,0.6838743686676025,0.8630761504173279,0.9820864200592041,0.9156699776649475,0.9526561498641968,0.864202082157135,0.9158578515052795,0.9151604771614075,0.936560869216919,0.9306222200393677,0.9167333245277405,0.6655115485191345,1.0103175640106201,0.48369699716567993,0.8669379949569702,0.9171668887138367,0.9166882634162903,0.8811824321746826,0.9173337817192078,0.9402408599853516,0.8049502968788147,0.787165641784668,1.0480505228042603,1.3531136512756348,0.55025315284729,1.0409053564071655,0.9427239894866943,0.7609118223190308,1.1665955781936646,1.0644350051879883,1.3762259483337402,0.8534948825836182,0.6094031929969788,0.6086867451667786,0.9107203483581543,0.9361648559570312,0.9163122177124023,0.8916710019111633,1.2813186645507812,1.2765045166015625,0.9166883230209351,0.8169836401939392,0.1046316847205162,1.0953806638717651,0.9162395000457764,0.9166883230209351,0.9474630355834961,0.9744077920913696,0.917788028717041,0.6878484487533569,0.8759497404098511,0.916336178779602,0.37415245175361633,0.9191884994506836,0.9145083427429199,1.0968623161315918,0.8967214226722717,0.9169402122497559,1.2383283376693726,0.47755634784698486,0.9173390865325928,0.9142409563064575,0.9388519525527954,0.9160374402999878,0.8501428961753845,0.914817214012146,1.4604822397232056,0.915499210357666,0.8662326335906982,0.9152804613113403,0.7067375779151917,0.7743070125579834,0.8998796939849854,0.8162950873374939,0.23324212431907654,0.9173692464828491,0.9225365519523621,0.7145951390266418,0.8968544006347656,0.9172623753547668,0.5698304176330566,0.7242794632911682,1.0675435066223145,0.41559356451034546,0.8441464900970459,0.9165315628051758,0.9154129028320312,0.9135255813598633,0.9337379932403564,0.9163069128990173,0.8688429594039917,0.19113951921463013,0.46342039108276367,0.9993128776550293,1.003510594367981,0.8719158172607422,0.9164020419120789,0.9925847053527832,1.108964204788208,0.7123833894729614,0.925058126449585,1.207918643951416,0.8393974900245667,0.9163069128990173,0.9163069128990173,0.8194907307624817,0.915183961391449,1.014227271080017,0.872270405292511,0.9158578515052795,-0.32384321093559265,0.9190764427185059,1.1355527639389038,0.8415651917457581,0.8974323868751526,0.939160943031311,0.915592610836029,0.5837033987045288,1.7195950746536255,1.1470692157745361,0.9173390865325928,0.915789783000946,0.9164010882377625,0.9647678136825562,0.8711562156677246,1.7302626371383667,0.9162395000457764,0.9390777349472046,1.1287221908569336,0.8776018619537354,0.8977729082107544,0.9143382906913757,0.8820052146911621,0.9173778295516968,0.7559556365013123,0.9161402583122253,0.9805088043212891,0.9816118478775024,0.9094933271408081,1.1027549505233765,1.0192910432815552,0.9164433479309082,0.9167556762695312,0.8996537923812866,0.7427266240119934,1.734505295753479,0.9547297954559326,0.8820840716362,0.9164491295814514,0.6348393559455872,0.4076206088066101,1.0771381855010986,0.9150777459144592,0.4321960210800171,0.2897891700267792,1.0703779458999634,1.1911840438842773,0.915499210357666,1.0010114908218384,0.9162204265594482,1.3258028030395508,0.9079834818840027,0.9372514486312866,0.915957510471344,0.8661971688270569,0.9171213507652283,1.1191836595535278,0.528921902179718,0.9104472994804382,0.9257808923721313,0.9325342178344727,1.7208747863769531,0.9182519316673279,0.9668591022491455,0.7994373440742493,0.9158578515052795,1.1239243745803833,0.9175406098365784,0.9805923700332642,1.2130327224731445,0.8818773627281189,0.9043012261390686,0.8563445210456848,1.315389633178711,1.4167444705963135,1.0882681608200073,0.8934053182601929,0.9145715832710266,0.9197191596031189,0.7217962741851807,0.9081494808197021,0.9158578515052795,0.9169150590896606,1.2032101154327393,0.7510190010070801,0.879850447177887,1.429763913154602,0.8221921324729919,0.2813960611820221,0.8957213163375854,1.0642329454421997,0.9157232046127319,0.9160023927688599,0.9166896343231201,0.917788028717041,1.1993910074234009,0.6951237916946411,0.9163212776184082,0.9172133207321167,0.9452782869338989,1.1181554794311523,1.6410088539123535,0.4975295662879944,0.954753041267395,0.9161249399185181,0.9172762632369995,0.9163296222686768,0.9099465012550354,1.0451734066009521,0.9176160097122192,0.6908569931983948,1.2046924829483032,0.9171810746192932,0.9171157479286194,0.916201114654541,0.8533656001091003,1.638659119606018,0.88221675157547,0.912987470626831,1.2371166944503784,0.9916081428527832,1.0238051414489746,1.5366673469543457,0.6971645355224609,0.9227400422096252,0.8699389696121216,0.9165538549423218,0.8783812522888184,0.17931094765663147,0.917788028717041,1.1447571516036987,0.9078930020332336,0.9163069128990173,0.35967615246772766,0.916076123714447,0.21846002340316772,0.8904814720153809,0.6843226552009583,0.9386260509490967,0.915499210357666,0.9163721203804016,0.9165315628051758,1.1535570621490479,1.0506445169448853,0.8359912633895874,0.9168648719787598,0.9127556085586548,0.5843565464019775,1.175528645515442,0.9169220328330994,1.6545957326889038,0.8473033905029297,0.963538646697998,0.9984757900238037,1.4043844938278198,0.9155517220497131,0.7238133549690247,1.3572020530700684,1.2347439527511597,1.0072853565216064,0.915499210357666,1.5828144550323486,1.1072320938110352,0.9172071814537048,0.9170124530792236,0.915499210357666,0.9139553904533386,1.1291526556015015,1.101456880569458,0.8882745504379272,1.0239371061325073,0.9094305038452148,0.8158621191978455,1.3275599479675293,0.7969790697097778,1.0484827756881714,0.6740480065345764,1.1328439712524414,0.9173428416252136,1.0183392763137817,0.9175025224685669,0.7647609710693359,0.9709354639053345,0.9159142374992371,0.9481086730957031,0.9294391870498657,0.9163296222686768,0.9416364431381226,0.9369421005249023,1.1236411333084106,0.7821711897850037,0.8771002292633057,0.9234020709991455,0.14340749382972717,0.815551221370697,0.8572105169296265,1.093980073928833,0.9156719446182251,0.7791894674301147,1.0020253658294678,0.8789358735084534,0.9163296222686768,0.9170681238174438,1.1666038036346436,1.2057125568389893,0.9162395000457764,0.705443799495697,1.7660285234451294,0.9510065317153931,0.9160133004188538,0.9165538549423218,0.9162395000457764,0.6865875720977783,1.1671720743179321,0.9384565353393555,0.8326382040977478,0.8935326933860779,0.9168601036071777,0.9230914115905762,1.2282700538635254,1.012619137763977,0.9157232046127319,0.9178845286369324,1.091802716255188,0.9768087863922119,0.9109829068183899,0.9140315651893616,0.8169368505477905,0.8137072324752808,0.9634480476379395,0.459628164768219,0.7903940081596375,0.9108060002326965,0.9657328128814697,0.9153575301170349,1.0782549381256104,0.6817584037780762,0.917788028717041,0.9159974455833435,0.9160800576210022,0.9167333245277405,0.8336626887321472,0.8769212961196899,0.7604306936264038,1.0136079788208008,1.1003766059875488,0.91644686460495,1.0958044528961182,0.9304091930389404,0.6058659553527832,0.8173304796218872,0.9164999127388,1.0537667274475098,0.8507987260818481,1.0854604244232178,0.9659579992294312,0.9163069128990173,0.7675301432609558,0.9177533984184265,0.9753422737121582,0.9157232046127319,0.8294872641563416,0.825412392616272,0.9160053133964539,0.9221667647361755,0.9192408323287964,0.678132951259613,0.9153920412063599,0.9153534770011902,0.9200688600540161,1.0671683549880981,0.9158578515052795,0.7937366366386414,0.9163296222686768,0.5618323087692261,0.9164010882377625,1.3146955966949463,0.903684675693512,0.9165315628051758,0.9846335649490356,0.9431926012039185,0.9021518230438232,0.6235839128494263,0.9158578515052795,0.866769552230835,0.9176005125045776,0.9165315628051758,0.8308272957801819,0.6970218420028687,1.030104637145996,0.9165538549423218,0.8026624321937561,1.277806043624878,0.9162395000457764,1.731088638305664,1.1796858310699463,0.5576414465904236,0.97132408618927,0.9154491424560547,0.9181017279624939,0.9086290597915649,0.9158452749252319,0.8345212936401367,1.0032198429107666,0.8437711596488953,1.3820616006851196,0.9168320298194885,0.9158578515052795,0.9214088916778564,0.9161370992660522,0.9613182544708252,0.9158578515052795,0.5263103246688843,0.9164656400680542,0.9418128728866577,0.8505334854125977,1.7284966707229614,0.8967214226722717,0.9976197481155396,1.1077667474746704,0.6282833814620972,0.9142356514930725,0.915967583656311,0.989193320274353,0.9337995052337646,0.791012167930603,0.7237693071365356,0.8715625405311584,0.9075554013252258,0.916403591632843,0.9158578515052795,0.9155402183532715,0.7604814171791077,0.9163069128990173,0.9164992570877075,1.0008795261383057,0.9162395000457764,0.6665319204330444,1.2871921062469482,0.9157232046127319,1.1748778820037842,0.9158578515052795,0.7843018174171448,0.8566734790802002,0.9163068532943726,0.6884663105010986],\"y\":[-2.3722944259643555,-1.268092393875122,-1.3145575523376465,-1.7199878692626953,-1.2327179908752441,-1.891726016998291,-1.7729332447052002,-1.7498974800109863,-1.690490484237671,-1.629915475845337,-1.6905808448791504,-1.690490484237671,-1.8004298210144043,-1.767209768295288,-1.7050704956054688,-1.7521412372589111,-1.690493106842041,-1.2469773292541504,-1.6893067359924316,-1.459928274154663,-1.629915475845337,-1.9265165328979492,-1.351759433746338,-1.9055719375610352,-1.3751699924468994,-1.690490484237671,-1.6407113075256348,-1.6905982494354248,-1.749192476272583,-1.8323163986206055,-2.0209333896636963,-1.6715397834777832,-1.690490484237671,-2.030674457550049,-1.9376635551452637,-1.5314379930496216,-1.3332602977752686,-1.6630949974060059,-1.9179720878601074,-1.690812110900879,-1.1962080001831055,-1.9596939086914062,-1.8811650276184082,-1.2576920986175537,-1.84791898727417,-1.4964847564697266,-1.6146605014801025,-1.695629596710205,-1.6407461166381836,-1.6877450942993164,-1.550114393234253,-1.682107925415039,-1.5263876914978027,-1.3993324041366577,-1.644225835800171,-1.4011402130126953,-1.6922147274017334,-1.51729416847229,-1.6913790702819824,-1.6955573558807373,-1.6934013366699219,-1.6910696029663086,-1.7071185111999512,-1.6983833312988281,-1.687410831451416,-1.938720703125,-1.579634428024292,-1.6891369819641113,-1.5204076766967773,-1.6629085540771484,-1.6632142066955566,-0.8769406080245972,-2.359349250793457,-1.6855568885803223,-1.6952965259552002,-1.6881928443908691,-1.7097039222717285,-1.6416070461273193,-1.1737602949142456,-1.761336326599121,-1.6914658546447754,-1.667281150817871,-2.0164971351623535,-1.6893067359924316,-1.6081480979919434,-1.6960573196411133,-1.6952128410339355,-1.3317960500717163,-1.4255743026733398,-2.1202964782714844,-1.6718153953552246,-1.2832872867584229,-2.0720698833465576,-1.8028991222381592,-2.0497922897338867,-1.716855764389038,-1.6952965259552002,-1.3372583389282227,-1.4246938228607178,-1.9578156471252441,-1.5400922298431396,-1.337990403175354,-1.8004274368286133,-1.763408899307251,-1.687422752380371,-1.7674334049224854,-1.253896713256836,-1.6903314590454102,-1.7136571407318115,-1.9778563976287842,-1.700171947479248,-1.6928772926330566,-1.68986177444458,-1.7178945541381836,-1.6891350746154785,-1.5525636672973633,-1.6897006034851074,-1.6639645099639893,-1.685485601425171,-1.6952965259552002,-1.8295650482177734,-1.7540545463562012,-1.690629482269287,-1.6893067359924316,-1.6909162998199463,-1.546868085861206,-1.63966703414917,-1.6962244510650635,-1.5769507884979248,-1.690490484237671,-1.7956621646881104,-1.6886825561523438,-1.8943228721618652,-1.7516114711761475,-1.7172350883483887,-1.7063851356506348,-1.745478630065918,-1.6071248054504395,-1.688084602355957,-1.690734624862671,-1.6012077331542969,-1.6905691623687744,-1.8933467864990234,-1.6896049976348877,-1.5916123390197754,-1.7892296314239502,-1.689864158630371,-1.6893067359924316,-1.3424642086029053,-1.5941886901855469,-1.7155661582946777,-1.6881928443908691,-1.6759297847747803,-1.695876121520996,-1.6881928443908691,-1.3449393510818481,-1.6910858154296875,-1.5799556970596313,-2.0782644748687744,-1.689669132232666,-1.7132322788238525,-1.6922316551208496,-2.247389078140259,-1.7152140140533447,-1.4883248805999756,-1.6888885498046875,-1.6511421203613281,-1.7106821537017822,-1.6981840133666992,-2.9483606815338135,-1.905468463897705,-1.8151373863220215,-1.692737102508545,-1.9705393314361572,-1.6905221939086914,-2.0525667667388916,-1.332220196723938,-1.6906859874725342,-1.5697972774505615,-1.6918838024139404,-1.6092944145202637,-1.6949472427368164,-1.8561925888061523,-1.3974132537841797,-1.6868324279785156,-1.7031073570251465,-1.6936378479003906,-1.5891317129135132,-1.8188717365264893,-1.6918838024139404,-1.708233118057251,-2.1137800216674805,-1.6950569152832031,-1.9142694473266602,-1.6920201778411865,-1.6895816326141357,-1.6834561824798584,-1.687967300415039,-1.553523063659668,-1.858567237854004,-2.0175328254699707,-1.5235168933868408,-1.6890158653259277,-1.6751794815063477,-1.6419615745544434,-1.7725410461425781,-2.0151586532592773,-2.0721309185028076,-1.691514015197754,-1.824434757232666,-1.6937980651855469,-2.8007311820983887,-1.7532610893249512,-1.6874051094055176,-1.6941287517547607,-1.9768280982971191,-1.69256591796875,-2.281933069229126,-1.6893067359924316,-1.629915475845337,-1.731147289276123,-1.7641522884368896,-1.6904301643371582,-1.6900386810302734,-1.7130920886993408,-1.878206729888916,-1.3062314987182617,-1.7330379486083984,-2.134920835494995,-1.705009937286377,-1.6990106105804443,-1.6876442432403564,-1.6923158168792725,-1.7254328727722168,-1.3360655307769775,-1.8636770248413086,-1.404008150100708,-1.7890167236328125,-1.6893067359924316,-1.700829029083252,-1.999521255493164,-1.6864807605743408,-1.9388928413391113,-1.6946749687194824,-1.6936814785003662,-1.6928129196166992,-1.5498321056365967,-1.1701197624206543,-1.6903555393218994,-1.6967086791992188,-1.701322078704834,-1.6906869411468506,-1.6881928443908691,-1.691650629043579,-1.688072681427002,-1.423775315284729,-1.6881928443908691,-1.5309650897979736,-1.6399970054626465,-1.8435540199279785,-1.530299186706543,-1.6881928443908691,-1.4590420722961426,-1.9738054275512695,-2.0350394248962402,-1.8320987224578857,-1.7069923877716064,-2.02885103225708,-1.6627531051635742,-1.8784816265106201,-1.9123306274414062,-1.709031581878662,-1.6923556327819824,-1.629915475845337,-1.6893067359924316,-1.9262089729309082,-1.7559199333190918,-1.6914234161376953,-1.6893067359924316,-1.683919906616211,-1.3709096908569336,-1.7107625007629395,-1.755692481994629,-1.3717483282089233,-1.6913959980010986,-1.6974787712097168,-1.7938172817230225,-1.7706034183502197,-1.3297955989837646,-1.671140193939209,-1.6590056419372559,-1.6952965259552002,-1.67966628074646,-1.974362850189209,-1.6913959980010986,-1.6771328449249268,-1.8020210266113281,-1.5533514022827148,-1.7423462867736816,-1.6903493404388428,-1.7391080856323242,-1.7051119804382324,-1.8619093894958496,-1.6899805068969727,-2.0801799297332764,-1.517868995666504,-1.6893067359924316,-2.113880157470703,-1.7732510566711426,-1.6464111804962158,-1.3925632238388062,-1.7945306301116943,-1.701777458190918,-1.523926854133606,-1.6129493713378906,-1.5665333271026611,-1.857008457183838,-1.7814052104949951,-1.788578987121582,-1.1166805028915405,-1.6893067359924316,-1.3830783367156982,-1.5174651145935059,-1.5695581436157227,-1.8178822994232178,-2.218395709991455,-1.6763379573822021,-1.6909921169281006,-1.6893067359924316,-1.6934716701507568,-1.669978380203247,-1.6893067359924316,-1.6883559226989746,-1.6857013702392578,-1.800436019897461,-2.015913486480713,-1.471949577331543,-1.616539478302002,-2.0745701789855957,-1.8530182838439941,-1.3147653341293335,-1.9366912841796875,-1.6681957244873047,-2.002143383026123,-1.7609825134277344,-1.650881290435791,-1.8876798152923584,-2.2278621196746826,-1.7642319202423096,-1.6568307876586914,-1.9451985359191895,-2.056102752685547,-1.0628814697265625,-1.7403109073638916,-0.9995057582855225,-1.9211621284484863,-1.6850709915161133,-2.0813345909118652,-1.5825703144073486,-1.645117998123169,-1.690699577331543,-1.6881928443908691,-1.6910099983215332,-1.689864158630371,-1.69193696975708,-1.6928234100341797,-1.9099278450012207,-1.6899027824401855,-1.4855711460113525,-1.6895654201507568,-1.5527739524841309,-1.1802763938903809,-1.7065176963806152,-1.6567261219024658,-1.4730851650238037,-1.690490484237671,-2.1189098358154297,-1.517177700996399,-1.7850127220153809,-1.833627462387085,-1.7475895881652832,-1.3715639114379883,-1.6907014846801758,-1.690664291381836,-1.6306240558624268,-2.3108205795288086,-1.6358675956726074,-1.692561149597168,-1.5810637474060059,-1.6920781135559082,-1.5346969366073608,-1.883059024810791,-1.7310161590576172,-1.692183494567871,-1.6881928443908691,-1.6883034706115723,-1.6904032230377197,-1.6867437362670898,-1.7244148254394531,-1.692244291305542,-1.6084203720092773,-1.5411561727523804,-1.6934034824371338,-1.686549186706543,-1.6985650062561035,-1.6918153762817383,-1.4798128604888916,-1.7327251434326172,-1.8974971771240234,-1.399726152420044,-1.5465102195739746,-1.6689045429229736,-1.6947050094604492,-1.6623351573944092,-1.8193671703338623,-1.6947407722473145,-1.6745541095733643,-1.6842145919799805,-1.8239729404449463,-1.6432116031646729,-1.621323585510254,-1.690911054611206,-1.7599201202392578,-1.8225359916687012,-1.7742249965667725,-1.6893067359924316,-1.6639049053192139,-2.1558475494384766,-1.754126787185669,-1.9732184410095215,-1.6920385360717773,-1.6907694339752197,-1.554596185684204,-2.0962908267974854,-2.368635416030884,-1.6132349967956543,-1.6921470165252686,-1.6912181377410889,-1.7029814720153809,-1.785649299621582,-0.7812074422836304,-1.8458619117736816,-1.771984577178955,-1.7036004066467285,-2.203761339187622,-1.690490484237671,-0.706954836845398,-1.683311939239502,-1.6594107151031494,-1.6688034534454346,-1.7654895782470703,-1.6212315559387207,-1.6893067359924316,-1.705491304397583,-1.905808448791504,-1.6899933815002441,-1.74755859375,-1.649458885192871,-1.6905076503753662,-1.9440412521362305,-1.3611847162246704,-1.6920075416564941,-1.6914677619934082,-1.6802568435668945,-1.69354248046875,-2.0028188228607178,-1.817326307296753,-1.7642254829406738,-1.6945645809173584,-1.6912877559661865,-1.6943445205688477,-1.716909408569336,-1.6952965259552002,-1.7202372550964355,-1.6925485134124756,-1.3299376964569092,-1.6881928443908691,-1.6744015216827393,-1.9935765266418457,-1.6908483505249023,-1.6920228004455566,-1.5743179321289062,-1.720588207244873,-1.8770906925201416,-1.6891672611236572,-1.6410284042358398,-1.5130114555358887,-1.824733018875122,-1.7422080039978027,-1.6819162368774414,-1.691849708557129,-1.596827745437622,-1.6726608276367188,-1.786482810974121,-1.6496186256408691,-1.9606266021728516,-1.6905016899108887,-1.6633081436157227,-1.6302845478057861,-1.757455587387085,-1.7150545120239258,-1.6934051513671875,-1.7266535758972168,-1.8481097221374512,-1.6905338764190674,-2.1106081008911133,-1.692002773284912,-1.687502384185791,-1.68986177444458,-1.2529265880584717,-1.6946282386779785,-1.9875953197479248,-1.6955740451812744,-1.7126612663269043,-1.6921722888946533,-1.7160167694091797,-1.6922638416290283,-1.3453752994537354,-1.6893067359924316,-1.6881928443908691,-1.690699577331543,-2.003687620162964,-1.7009267807006836,-1.766676425933838,-1.7521753311157227,-1.6928176879882812,-1.5729122161865234,-1.612745761871338,-1.6886615753173828,-1.6949377059936523,-1.7134575843811035,-1.7000765800476074,-1.690948486328125,-1.599968671798706,-1.0690727233886719,-1.690699577331543,-1.6930389404296875,-1.9605896472930908,-1.7159805297851562,-2.150733470916748,-1.3141326904296875,-1.7816824913024902,-1.629915475845337,-1.6926357746124268,-2.0941243171691895,-1.809011459350586,-1.9288904666900635,-0.9616014957427979,-1.4039690494537354,-1.6641950607299805,-1.6042866706848145,-1.6952965259552002,-1.656689167022705,-1.6449637413024902,-1.590890645980835,-1.4908660650253296,-1.6791739463806152,-2.0499701499938965,-1.629915475845337,-1.0142202377319336,-1.6907124519348145,-1.7728819847106934,-1.7258081436157227,-1.6804113388061523,-1.8505654335021973,-1.6674902439117432,-1.4580148458480835,-1.6930673122406006,-1.758544683456421,-1.8500356674194336,-2.0086121559143066,-1.657066822052002,-1.6919848918914795,-1.6912431716918945,-1.947678565979004,-1.690699577331543,-1.7114808559417725,-1.8715546131134033,-1.2661957740783691,-2.177624225616455,-1.556160807609558,-1.6539192199707031,-1.6919574737548828,-1.679908037185669,-1.6893067359924316,-1.566328525543213,-1.7501749992370605,-1.7370054721832275,-1.7208342552185059,-1.6893067359924316,-1.9789049625396729,-1.8106145858764648,-1.4101531505584717,-1.5978100299835205,-2.126002788543701,-1.6983542442321777,-1.4999027252197266,-1.693781852722168,-2.042532444000244,-1.2441682815551758,-1.690490484237671,-1.5379784107208252,-1.6048567295074463,-1.8206636905670166,-1.8097033500671387,-2.380692958831787,-1.6805663108825684,-1.4541044235229492,-1.6920928955078125,-1.6909844875335693,-2.0268120765686035,-1.694549798965454,-1.690699577331543,-1.6914658546447754,-1.7785747051239014,-1.2434535026550293,-1.6952965259552002,-1.6169991493225098,-1.8049421310424805,-1.8549623489379883,-1.6793174743652344,-1.6918838024139404,-1.6889081001281738,-1.973356008529663,-2.3165149688720703,-1.6015856266021729,-1.6907694339752197,-1.6898479461669922,-1.9161577224731445,-1.9292519092559814,-1.7461612224578857,-1.6943550109863281,-1.7642691135406494,-1.7375903129577637,-1.8342838287353516,-0.7660349607467651,-1.721484661102295,-1.682875633239746,-1.6994564533233643,-1.686033010482788,-1.6665053367614746,-1.5080740451812744,-1.6907529830932617,-1.6644017696380615,-1.6961970329284668,-1.6888885498046875,-1.4667344093322754,-1.6678965091705322,-1.690490484237671,-1.6913959980010986,-1.6754939556121826,-1.6972672939300537,-1.4790546894073486,-1.408360242843628,-1.6881928443908691,-1.8510329723358154,-2.009186267852783,-1.710374116897583,-1.6805815696716309,-1.701657772064209,-2.127366065979004,-1.4900331497192383,-2.340834856033325,-1.4044089317321777,-1.690129280090332,-2.0265793800354004,-1.6918816566467285,-1.4100927114486694,-1.2827119827270508,-1.7848694324493408,-1.9004061222076416,-1.3528233766555786,-1.7545254230499268,-1.6893067359924316,-1.3537845611572266,-1.3152313232421875,-1.6908535957336426,-1.6914658546447754,-1.6926624774932861,-1.633659839630127,-1.6909921169281006,-1.5663936138153076,-1.719597578048706,-1.633223056793213,-2.0299317836761475,-1.68198561668396,-1.8788607120513916,-1.8974206447601318,-1.5810754299163818,-1.6291999816894531,-1.7063140869140625,-1.695211410522461,-1.9868195056915283,-1.6918225288391113,-1.6601972579956055,-1.6621992588043213,-1.6857120990753174,-1.3141735792160034,-1.6314620971679688,-1.6167407035827637,-1.7685368061065674,-1.6685123443603516,-2.169102668762207,-1.6960082054138184,-1.6983046531677246,-1.704193115234375,-1.6907968521118164,-1.8640644550323486,-1.8835067749023438,-2.008378505706787,-1.746274709701538,-2.0531187057495117,-1.6893067359924316,-1.844111680984497,-1.5292822122573853,-1.5589301586151123,-1.7726573944091797,-1.3644976615905762,-1.6713902950286865,-1.7232694625854492,-1.3075611591339111,-1.6889829635620117,-1.7461743354797363,-1.690521478652954,-1.6723175048828125,-1.9143781661987305,-1.8818597793579102,-1.8034296035766602,-1.4783947467803955,-1.636254072189331,-1.6647660732269287,-2.0548171997070312,-1.536177396774292,-1.690490484237671,-1.6924340724945068,-1.6945505142211914,-1.2713017463684082,-1.6351039409637451,-1.690699577331543,-1.6722955703735352,-1.4496800899505615,-1.6694796085357666,-1.608093500137329,-1.6930384635925293,-1.5944125652313232,-1.692396640777588,-1.944654941558838,-1.7127366065979004,-1.3228896856307983,-1.7672767639160156,-1.69466233253479,-1.656247854232788,-1.690490484237671,-1.8694958686828613,-2.072152853012085,-1.4710173606872559,-1.7402987480163574,-0.7076517343521118,-1.6901555061340332,-1.2833497524261475,-1.6919026374816895,-1.5504757165908813,-1.7048828601837158,-1.696408748626709,-1.6613168716430664,-1.6922962665557861,-1.751065731048584,-1.8246502876281738,-1.6808080673217773,-1.7107441425323486,-1.6924469470977783,-1.829146385192871,-1.4233335256576538,-1.6927475929260254,-1.690699577331543,-1.7360811233520508,-1.6809663772583008,-1.6904351711273193,-1.712090015411377,-1.6903491020202637,-1.6903562545776367,-1.6920228004455566,-1.689709186553955,-1.6881928443908691,-1.8595118522644043,-1.767782211303711,-1.6453557014465332,-2.0503973960876465,-1.503413200378418,-1.7316703796386719,-1.5760681629180908,-1.6894021034240723,-1.989135980606079,-1.2180626392364502,-1.4321136474609375,-1.655379295349121,-1.6914658546447754,-2.013237476348877,-1.76100492477417,-1.9084141254425049,-2.1305999755859375,-1.6378629207611084,-1.66209077835083,-1.689864158630371,-1.2439626455307007,-1.615642786026001,-1.6913986206054688,-1.6318140029907227,-1.69297456741333,-1.6890273094177246,-1.6605861186981201,-1.690216064453125,-1.4772831201553345,-1.7013871669769287,-1.7058892250061035,-1.6881928443908691,-1.6939034461975098,-1.6913959980010986,-1.690490484237671,-1.5270551443099976,-1.8442401885986328,-1.8548097610473633,-2.0775914192199707,-1.8767900466918945,-1.7767693996429443,-1.4863852262496948,-1.6994318962097168,-1.8713645935058594,-1.6672883033752441,-1.4319148063659668,-1.6943473815917969,-1.734884262084961,-1.6061320304870605,-1.690699577331543,-1.8910582065582275,-1.6924257278442383,-1.6861095428466797,-2.0087761878967285,-1.9424610137939453,-0.9967334270477295,-1.693821907043457,-1.692133903503418,-1.819608211517334,-1.818976879119873,-1.583465337753296,-1.4119504690170288,-1.915294885635376,-1.3870759010314941,-1.7021703720092773,-0.848721981048584,-2.2194740772247314,-2.02482008934021,-1.9510607719421387,-1.629915475845337,-1.8599576950073242,-1.6298713684082031,-2.306070327758789,-1.7771425247192383,-2.2354631423950195,-1.8763935565948486,-1.7640724182128906,-1.6894941329956055,-1.6232705116271973,-1.6779375076293945,-2.29386043548584,-1.60563325881958,-1.5679879188537598,-1.6900439262390137,-1.6222341060638428,-1.72499418258667,-1.524183988571167,-1.6979029178619385,-1.8798246383666992,-1.6903984546661377,-1.8915457725524902,-1.7307908535003662,-1.77311372756958,-1.944859504699707,-1.6927378177642822,-1.6733713150024414,-1.690490484237671,-1.6920924186706543,-1.690490484237671,-1.845524787902832,-1.6897778511047363,-1.6922173500061035,-1.5398139953613281,-1.601119041442871,-1.6859896183013916,-1.7965948581695557,-1.321798324584961,-1.8987736701965332,-1.6907262802124023,-1.6557085514068604,-2.0169644355773926,-1.6514418125152588,-1.6904149055480957,-1.5723586082458496,-1.6918532848358154,-1.708883285522461,-1.7119433879852295,-1.6893067359924316,-2.4244003295898438,-1.6881928443908691,-1.6952965259552002,-1.760232925415039,-1.7477738857269287,-1.4076488018035889,-1.629915475845337,-1.6893067359924316,-1.6896705627441406,-2.139431953430176,-1.6848230361938477,-1.7226018905639648,-1.6932122707366943,-1.719458818435669,-0.6267955303192139,-2.1279048919677734,-2.153761386871338,-1.6918838024139404,-2.0816500186920166,-1.0424286127090454,-2.0178587436676025,-1.686370849609375,-1.6741299629211426,-1.34839928150177,-1.7366094589233398,-1.3536169528961182,-1.9464097023010254,-1.6690945625305176,-1.397519826889038,-1.7965178489685059,-2.131239414215088,-1.6246342658996582,-1.295854091644287,-1.3936957120895386,-1.3277311325073242,-1.690490484237671,-1.5468897819519043,-1.6766526699066162,-1.6794219017028809,-1.6881399154663086,-1.7859785556793213,-1.6694107055664062,-1.368363380432129,-1.640516996383667,-2.1428444385528564,-1.6920924186706543,-1.9453306198120117,-1.690699577331543,-1.3764522075653076,-1.2443037033081055,-1.825463056564331,-1.610365867614746,-1.6893067359924316,-1.4015191793441772,-1.7782151699066162,-1.895613193511963,-1.6923556327819824,-1.6263446807861328,-1.7052292823791504,-1.7699942588806152,-1.6924819946289062,-1.518017292022705,-2.312354803085327,-1.6915488243103027,-1.5564314126968384,-1.611166000366211,-1.7449193000793457,-1.8399543762207031,-1.6881928443908691,-1.7344415187835693,-1.6952965259552002,-1.6464917659759521,-1.752645492553711,-1.6810898780822754,-1.6422984600067139,-1.6428778171539307,-1.8377394676208496,-1.6909921169281006,-1.6898729801177979,-1.692467212677002,-1.793447732925415,-1.6940689086914062,-1.7994179725646973,-1.5837273597717285,-1.690699577331543,-1.8790030479431152,-1.7908978462219238,-1.6910061836242676,-1.8378772735595703,-1.7649214267730713,-1.7386794090270996,-1.6326813697814941,-1.684340476989746,-1.6957874298095703,-1.851715087890625,-1.6906602382659912,-1.6928093433380127,-1.787473440170288,-2.3397507667541504,-1.6128573417663574,-1.6905698776245117,-1.763597011566162,-1.8696165084838867,-1.689864158630371,-2.4758095741271973,-1.8456850051879883,-1.3343613147735596,-2.8066999912261963,-1.6909644603729248,-1.6385316848754883,-2.0143604278564453,-1.7013721466064453,-1.8838698863983154,-1.690490484237671,-2.0448296070098877,-2.3344948291778564,-1.7308297157287598,-1.5255002975463867,-1.690490484237671,-1.720092535018921,-1.7835631370544434,-1.802565574645996,-1.8144230842590332,-1.6918838024139404,-1.6112136840820312,-1.6909921169281006,-1.7227504253387451,-1.7756423950195312,-1.719860315322876,-1.6946089267730713,-1.3364989757537842,-1.7034075260162354,-1.7398595809936523,-1.629915475845337,-1.4480750560760498,-1.6888885498046875,-1.8137080669403076,-1.69907808303833,-1.747279167175293,-2.3202786445617676,-1.2873930931091309,-1.9386787414550781,-1.7700858116149902,-1.713801622390747,-1.5827679634094238,-1.9300050735473633,-1.611112117767334,-2.071261167526245,-1.3135859966278076,-2.005138397216797,-1.5851712226867676,-1.806058406829834,-1.690528154373169,-1.7523841857910156,-1.728487491607666,-1.4552311897277832,-1.717799186706543,-1.7749202251434326,-2.100891590118408,-1.6881928443908691,-1.6725528240203857,-1.6901593208312988,-1.6909592151641846,-1.6905601024627686,-1.964472770690918,-1.824707269668579,-1.653144359588623,-1.686692237854004,-1.7929749488830566,-1.7483386993408203,-1.1269639730453491,-1.7061476707458496,-1.694756031036377,-1.8006131649017334,-2.3785014152526855,-1.6909217834472656,-1.6904692649841309,-1.625443458557129,-1.691497564315796,-1.7277700901031494,-1.7977516651153564,-1.6893067359924316,-1.5353500843048096,-1.4194259643554688,-1.6906280517578125,-1.4952847957611084,-0.6570907831192017,-1.8888790607452393,-1.6867027282714844,-1.8763830661773682,-1.6642699241638184,-1.5927135944366455,-1.6176743507385254,-1.6918838024139404,-0.9812381267547607,-1.689864158630371,-1.7351112365722656,-1.6893889904022217,-1.6877658367156982,-1.2364572286605835,-2.1892619132995605,-1.6013178825378418,-1.6919457912445068,-1.3844585418701172,-1.6020052433013916,-2.181034564971924,-1.6198763847351074,-1.698441505432129,-1.635725498199463,-1.6511666774749756,-1.8681361675262451,-1.3323067426681519,-1.8322863578796387,-1.8002538681030273,-1.9669122695922852,-1.6918630599975586,-1.690490484237671,-1.3181325197219849,-1.5986347198486328,-1.5540153980255127,-2.0528626441955566,-1.5994863510131836,-1.6391639709472656,-1.6450095176696777,-1.1733527183532715,-0.7319546937942505,-1.7240691184997559,-1.620370864868164,-0.8881199359893799,-1.4415827989578247,-1.690699577331543,-1.585896372795105,-1.3771698474884033,-1.690352439880371,-1.8721346855163574,-1.6882867813110352,-1.716907024383545,-2.241896867752075,-1.8084313869476318,-1.5261173248291016,-1.8063230514526367,-2.0216245651245117,-1.7331039905548096,-1.5317142009735107,-1.692176342010498,-2.112196922302246,-1.1163389682769775,-1.6668806076049805,-1.6141033172607422,-1.691103219985962,-1.9430694580078125,-1.829338550567627,-1.6890604496002197,-1.7928662300109863,-1.736234188079834,-1.5490771532058716,-2.283320426940918,-1.6403868198394775,-1.8551335334777832,-2.170154571533203,-2.042485237121582,-1.734236717224121,-1.6914658546447754,-1.7081046104431152,-1.5745435953140259,-0.951120138168335,-1.774247169494629,-1.9402050971984863,-1.6909921169281006,-1.69635009765625,-1.690490484237671,-1.4235498905181885,-2.459315299987793,-1.6909921169281006,-1.4135066270828247,-1.7112908363342285,-1.7244703769683838,-2.114903450012207,-1.6877093315124512,-1.5030089616775513,-1.6147181987762451,-1.7906899452209473,-1.8230137825012207,-1.6920228004455566,-1.6881928443908691,-1.8968899250030518,-1.6894536018371582,-1.726611614227295,-1.740373134613037,-1.8275723457336426,-1.8633232116699219,-1.690699577331543,-1.7204384803771973,-1.6900014877319336,-1.9135518074035645,-1.3224444389343262,-1.8151345252990723,-1.9612278938293457,-1.8782727718353271,-1.8722913265228271,-1.633239984512329,-1.6920924186706543,-2.0287208557128906,-1.6969878673553467,-1.7105636596679688,-1.6954529285430908,-1.6709785461425781,-1.6644184589385986,-1.5429645776748657,-1.5785048007965088,-1.4128549098968506,-1.6778650283813477,-1.690490484237671,-1.6937861442565918,-1.5221441984176636,-1.4650282859802246,-1.7645699977874756,-2.110487937927246,-2.278690814971924,-1.7832720279693604,-1.6770648956298828,-1.6034595966339111,-1.8679494857788086,-1.640376091003418,-1.6762113571166992,-1.701244831085205,-1.8165950775146484,-1.689864158630371,-1.761491060256958,-1.97627592086792,-1.6860146522521973,-1.685142993927002,-1.6939034461975098,-1.7674639225006104,-1.6620583534240723,-1.9635183811187744,-1.7228810787200928,-1.9187092781066895,-1.875758171081543,-1.6190390586853027,-0.9083384275436401,-1.6954221725463867,-1.692575454711914,-1.3839471340179443,-1.6925530433654785,-1.3538939952850342,-1.6374478340148926,-2.0997262001037598,-1.694976568222046,-1.4777641296386719,-1.6919748783111572,-1.727442979812622,-1.6909921169281006,-1.6881928443908691,-1.6876158714294434,-2.1545004844665527,-1.782416582107544,-1.690687656402588,-1.6900100708007812,-1.689864158630371,-1.6912543773651123,-1.2442188262939453,-1.4309592247009277,-2.0242350101470947,-1.6939995288848877,-1.4219563007354736,-2.067121982574463,-1.718686580657959,-1.692152500152588,-1.5325071811676025,-1.690654993057251,-1.3094521760940552,-1.7907443046569824,-1.8757662773132324,-1.897993564605713,-1.2897379398345947,-1.7829320430755615,-1.6893067359924316,-1.6919476985931396,-1.6909921169281006,-1.6376235485076904,-1.6868433952331543,-1.7007684707641602,-1.802776575088501,-1.6901812553405762,-1.994495153427124,-1.830486536026001,-1.6920924186706543,-1.1674706935882568,-1.7240705490112305,-1.996492624282837,-1.584977149963379,-1.5202535390853882,-1.6951496601104736,-1.693871021270752,-1.6918838024139404,-1.9958817958831787,-2.0001730918884277,-1.6920175552368164,-1.942612648010254,-1.6901397705078125,-1.6901178359985352,-1.8861472606658936,-1.4584145545959473,-1.8598980903625488,-1.6228125095367432,-1.7159099578857422,-1.3204034566879272,-1.8605947494506836,-1.6917121410369873,-1.9906539916992188,-1.689864158630371,-1.9644622802734375,-2.060293674468994,-1.6641845703125,-1.4477720260620117,-1.690699577331543,-1.6969761848449707,-1.6457796096801758,-1.614908218383789,-1.6888432502746582,-1.7179489135742188,-1.6543893814086914,-1.5487852096557617,-1.825517177581787,-1.7752301692962646,-1.674746036529541,-1.6655793190002441,-1.7587199211120605,-1.7830102443695068,-1.604943037033081,-1.6914658546447754,-1.6893067359924316,-1.7697007656097412,-1.887784481048584,-1.728165626525879,-1.6952965259552002,-1.7490315437316895,-1.6909112930297852,-1.7071268558502197,-1.879199504852295,-1.6919662952423096,-1.6865668296813965,-1.2734158039093018,-1.5604544878005981,-1.7656068801879883,-1.6919803619384766,-1.7270686626434326,-1.7086424827575684,-1.860163688659668,-1.6923303604125977,-1.6918225288391113,-1.7139389514923096,-1.6929569244384766,-1.906968593597412,-1.6870498657226562,-1.6919903755187988,-1.7048728466033936,-1.7609751224517822,-1.692716360092163,-1.6933603286743164,-1.4055414199829102,-1.690624713897705,-1.8687167167663574,-1.690699577331543,-1.6881928443908691,-1.3243916034698486,-1.970686912536621,-1.7471134662628174,-2.092846393585205,-1.6920950412750244,-1.8827927112579346,-1.6954727172851562,-1.6909921169281006,-1.9316058158874512,-1.6821184158325195,-1.693990707397461,-1.687298059463501,-1.621103048324585,-1.6893067359924316,-1.6880629062652588,-1.6907694339752197,-2.1638436317443848,-1.7158098220825195,-1.604386329650879,-1.6092631816864014,-1.8605189323425293,-1.6991050243377686,-1.7115623950958252,-1.690704107284546,-1.6922264099121094,-1.7322993278503418,-1.301853895187378,-1.6918838024139404,-1.4946436882019043,-1.6893067359924316,-1.6889736652374268,-1.7274925708770752,-1.690699577331543,-1.660093069076538,-1.5598258972167969,-1.8265180587768555,-1.876452922821045,-1.9330604076385498,-1.635730266571045,-1.7455086708068848,-1.5297057628631592,-1.6876811981201172,-1.0848898887634277,-1.8075103759765625,-1.5520970821380615,-1.584721565246582,-1.6884520053863525,-1.558500051498413,-1.8272194862365723,-1.708007574081421,-1.6869523525238037,-1.7840452194213867,-1.974766731262207,-1.6915082931518555,-1.3284965753555298,-1.7211456298828125,-1.6952965259552002,-1.6945388317108154,-1.866933822631836,-2.1150333881378174,-1.7746860980987549,-1.690516471862793,-1.690490484237671,-1.0716251134872437,-1.6474158763885498,-1.6893067359924316,-1.6039700508117676,-1.6920924186706543,-0.9231747388839722,-1.690521240234375,-1.8160159587860107,-1.6904044151306152,-1.67698073387146,-1.6367220878601074,-1.9887027740478516,-1.4002656936645508,-1.6086711883544922,-1.6275057792663574,-1.917022943496704,-1.3201643228530884,-1.6025710105895996,-1.708559513092041,-1.6888885498046875,-1.797203540802002,-1.8703546524047852,-1.6075928211212158,-1.756255865097046,-1.658874750137329,-1.6893067359924316,-1.7129607200622559,-1.695488691329956,-1.6952965259552002,-1.8699712753295898,-1.6903307437896729,-1.701106071472168,-1.5879862308502197,-1.7440788745880127,-1.6964149475097656,-1.774341106414795,-2.273833751678467,-1.9895310401916504,-1.6906495094299316,-1.7351007461547852,-1.8128623962402344,-1.6888885498046875,-1.6905012130737305,-1.5996019840240479,-1.7061376571655273,-1.5455060005187988,-1.8903801441192627,-1.7691936492919922,-1.3616578578948975,-1.7085156440734863,-1.6657357215881348,-1.8026807308197021,-1.7946348190307617,-1.6922192573547363,-1.715207576751709,-1.689864158630371,-1.8787150382995605,-1.6708564758300781,-1.6881928443908691,-1.7198197841644287,-1.2770763635635376,-1.6947660446166992,-1.6933188438415527,-2.3782339096069336,-1.7055318355560303,-1.6881928443908691,-1.8813538551330566,-1.6881928443908691,-1.6958448886871338,-1.691096305847168,-1.821119785308838,-1.5860755443572998,-1.7693202495574951,-1.693114995956421,-1.412656307220459,-1.9282634258270264,-1.690699577331543,-1.6923742294311523,-1.48402738571167,-1.8976967334747314,-1.9296536445617676,-1.7576990127563477,-1.6347768306732178,-1.818589210510254,-1.6881928443908691,-1.9731950759887695,-1.7027506828308105,-1.6947927474975586,-1.689807415008545,-1.8214192390441895,-1.7260923385620117,-1.689864158630371,-1.6907024383544922,-1.687563419342041,-1.6915388107299805,-1.6955344676971436,-2.098191022872925,-1.4482595920562744,-1.6879265308380127,-1.6920890808105469,-1.6909921169281006,-1.6915082931518555,-1.6910974979400635,-1.6813831329345703,-1.6909921169281006,-1.687103271484375,-1.5945377349853516,-1.6571860313415527,-1.6282052993774414,-1.6915252208709717,-1.8544509410858154,-1.7077515125274658,-1.1270277500152588,-1.7056686878204346,-1.7873294353485107,-1.7277445793151855,-1.6881928443908691,-1.8223016262054443,-1.6935036182403564,-1.6893510818481445,-1.6937675476074219,-1.691596269607544,-1.9191951751708984,-1.5574746131896973,-1.8405795097351074,-0.8503851890563965,-1.8114347457885742,-1.5951740741729736,-1.687157154083252,-1.6918838024139404,-1.768303394317627,-1.7704014778137207,-1.565220832824707,-2.2408833503723145,-1.6608502864837646,-1.4367247819900513,-1.6909921169281006,-1.6904256343841553,-1.6926870346069336,-1.690791130065918,-1.691667079925537,-1.6884737014770508,-1.692931890487671,-1.690699577331543,-1.7261714935302734,-1.7748799324035645,-1.7008180618286133,-1.4060524702072144,-1.690490484237671,-1.5687072277069092,-1.6935248374938965,-1.7960610389709473,-1.2998565435409546,-1.6887834072113037,-2.0102195739746094,-1.7723798751831055,-1.7716054916381836,-2.0748918056488037,-1.7548422813415527,-1.5887484550476074,-1.829052448272705,-1.7532312870025635,-1.5722408294677734,-1.9879250526428223,-1.686570167541504,-1.9396624565124512,-1.7065727710723877,-1.6894080638885498,-1.7556700706481934,-1.6873726844787598,-1.6893067359924316,-1.4480018615722656,-1.6901555061340332,-1.8249046802520752,-1.5849215984344482,-1.371181845664978,-1.8158621788024902,-1.6920228004455566,-1.6918635368347168,-1.6925792694091797,-1.6902515888214111,-1.768327236175537,-1.6154632568359375,-1.651522159576416,-1.6073026657104492,-1.8607606887817383,-1.6888885498046875,-1.6881928443908691,-1.6679232120513916,-1.6634392738342285,-1.6919617652893066,-1.8911237716674805,-1.693735122680664,-1.6586790084838867,-1.6469554901123047,-2.2813005447387695,-1.3684768676757812,-1.6865134239196777,-1.596348762512207,-1.6902966499328613,-1.6920008659362793,-1.277942180633545,-1.6907329559326172,-1.827843427658081,-1.6915817260742188,-1.6218481063842773,-1.4381165504455566,-1.6956031322479248,-1.9932894706726074,-1.690490484237671,-1.6913039684295654,-1.6932485103607178,-1.690699577331543,-1.804628849029541,-1.690490484237671,-1.503279209136963,-1.428283929824829,-1.7527165412902832,-1.6920924186706543,-1.401205062866211,-1.8075244426727295,-1.7768127918243408,-1.6919384002685547,-2.391177177429199,-1.6909277439117432,-1.8869657516479492,-1.7290847301483154,-1.5796613693237305,-1.67277193069458,-1.6670722961425781,-1.6899216175079346,-1.742138385772705,-1.6947810649871826,-1.9682793617248535,-1.6164684295654297,-1.6881928443908691,-1.6881928443908691,-2.1464245319366455,-1.6873323917388916,-1.717094898223877,-2.2768893241882324,-1.7389461994171143,-1.648461103439331,-1.6920981407165527,-1.7260208129882812,-1.6902728080749512,-1.7321224212646484,-1.6893067359924316,-1.7606103420257568,-1.918614387512207,-1.3073980808258057,-1.679908037185669,-1.6893067359924316,-1.5017762184143066,-1.8547370433807373,-1.9446830749511719,-1.7083511352539062,-1.8201022148132324,-1.53267502784729,-1.6784765720367432,-1.793233871459961,-1.0203957557678223,-2.1306443214416504,-1.7092809677124023,-1.7256946563720703,-1.780106544494629,-1.3391929864883423,-1.6882047653198242,-1.6870160102844238,-1.888443946838379,-1.690699577331543,-2.0346388816833496,-1.6913959980010986,-1.6034348011016846,-2.059166193008423,-1.727950096130371,-1.6907501220703125,-1.6901605129241943,-1.374467134475708,-1.7018940448760986,-1.6893067359924316,-1.6893067359924316,-1.690699577331543,-2.62508487701416,-1.7213315963745117,-1.3071696758270264,-1.8055319786071777,-1.7820038795471191,-1.6958954334259033,-1.675685167312622,-1.6944918632507324,-1.690490484237671,-1.5478198528289795,-1.7013323307037354,-1.6218349933624268,-1.751945972442627,-1.9714891910552979,-1.68986177444458,-1.6110107898712158,-1.7403130531311035,-1.6904399394989014,-1.7181785106658936,-1.7006382942199707,-1.6910364627838135,-1.9580249786376953,-1.8006372451782227,-1.689708948135376,-1.6920228004455566,-1.7531518936157227,-1.6928133964538574,-1.9490797519683838,-1.992516040802002,-1.7073640823364258,-2.131840467453003,-1.6900711059570312,-1.6906335353851318,-1.690699577331543,-1.6892850399017334,-1.6924538612365723,-1.4072132110595703,-1.6942589282989502,-1.8305683135986328,-1.573469877243042,-1.4132013320922852,-1.753993272781372,-1.5577235221862793,-1.6924338340759277,-1.6883933544158936,-1.5956566333770752,-1.779271125793457,-1.690490484237671,-1.808946132659912,-1.6881928443908691,-1.8731656074523926,-1.5167580842971802,-1.6918838024139404,-1.694108486175537,-2.0571744441986084,-1.484783411026001,-2.0044798851013184,-1.6893067359924316,-1.6929504871368408,-1.6909921169281006,-1.3394958972930908,-1.4781200885772705,-1.695866584777832,-1.9561173915863037,-1.6904220581054688,-1.8179130554199219,-1.6893067359924316,-1.6881928443908691,-1.6439051628112793,-1.7020721435546875,-1.580337643623352,-1.936941385269165,-1.6085119247436523,-1.6920416355133057,-1.701136589050293,-1.686659336090088,-1.5861104726791382,-1.690699577331543,-1.5864918231964111,-1.7403547763824463,-1.6799523830413818,-1.6712896823883057,-1.7394342422485352,-1.6920623779296875,-1.5417711734771729,-1.6457018852233887,-1.7664175033569336,-1.6881928443908691,-1.44954252243042,-1.6903839111328125,-1.6816329956054688,-1.6283464431762695,-1.6504020690917969,-1.4291393756866455,-1.6888885498046875,-1.7038159370422363,-1.607424259185791,-2.5858592987060547,-1.747753620147705,-1.7425165176391602,-0.9548163414001465,-1.6907694339752197,-1.6903393268585205,-1.3528664112091064,-1.8182792663574219,-1.6852779388427734,-1.6713345050811768,-1.1947450637817383,-1.8067879676818848,-1.6921799182891846,-1.6926054954528809,-1.6934089660644531,-1.801762342453003,-1.5149298906326294,-1.6978342533111572,-1.6958224773406982,-2.2835922241210938,-1.7193126678466797,-1.6917874813079834,-1.6337018013000488,-1.4977977275848389,-1.658647060394287,-1.7102289199829102,-1.640044927597046,-1.6918838024139404,-1.557072639465332,-1.6939210891723633,-1.7716968059539795,-1.9023716449737549,-1.7113525867462158,-1.3661472797393799,-1.8963823318481445,-1.7982356548309326,-1.6962754726409912,-2.0082249641418457,-2.289954662322998,-1.6893067359924316,-1.3351290225982666,-1.711808681488037,-1.6328275203704834,-1.687136173248291,-1.687500238418579,-1.735428810119629,-1.7177753448486328,-1.6888885498046875,-1.6544194221496582,-1.4388338327407837,-1.665029764175415,-1.6923143863677979,-1.6917989253997803,-1.6243574619293213,-1.7293901443481445,-1.7718701362609863,-1.690924882888794,-1.6827282905578613,-1.9574313163757324,-1.690490484237671,-1.6676361560821533,-1.329798936843872,-1.6872665882110596,-1.6907694339752197,-1.9095864295959473,-1.6004765033721924,-1.6058933734893799,-1.7132885456085205,-1.697523593902588,-1.689864158630371,-1.74387788772583,-1.8172838687896729,-1.8917524814605713,-1.6826775074005127,-1.718818187713623,-1.745615005493164,-1.6983776092529297,-1.6050987243652344,-1.6752161979675293,-1.6949419975280762,-1.6958041191101074,-1.6913762092590332,-1.2890852689743042,-1.6946978569030762,-1.6914432048797607,-1.689864158630371,-1.6923985481262207,-1.6922023296356201,-0.8675436973571777,-1.6942205429077148,-1.6437788009643555,-2.688310146331787,-1.7080111503601074,-1.241518497467041,-1.6463251113891602,-1.706559181213379,-1.682584524154663,-1.8246164321899414,-1.6447243690490723,-1.6904122829437256,-1.6587462425231934,-1.7192165851593018,-1.6893067359924316,-1.6893796920776367,-1.606818675994873,-1.6876726150512695,-1.6232609748840332,-1.7075223922729492,-1.330601453781128,-1.648045539855957,-2.0479354858398438,-1.690699577331543,-1.6883189678192139,-2.3639092445373535,-1.7823517322540283,-1.6811442375183105,-1.8342406749725342,-1.6920924186706543,-1.7411463260650635,-1.7871713638305664,-1.7712602615356445,-1.8469958305358887,-1.8342971801757812,-1.6975042819976807,-1.736119031906128,-1.6843817234039307,-2.003760814666748,-1.8215970993041992,-1.8002495765686035,-1.7109291553497314,-1.8056998252868652,-1.9161524772644043,-1.6901235580444336,-1.570778489112854,-1.6872332096099854,-1.6890981197357178,-1.9112629890441895,-1.3524389266967773,-1.6913299560546875,-1.6545002460479736,-1.6683802604675293,-1.9358487129211426,-1.7073533535003662,-1.697826862335205,-1.7294554710388184,-1.9326353073120117,-1.9707062244415283,-1.5110597610473633,-1.6942155361175537,-1.6942403316497803,-1.6906254291534424,-1.0663894414901733,-1.6907358169555664,-1.7251107692718506,-1.706080436706543,-1.414236068725586,-1.6121737957000732,-1.8561115264892578,-1.6920270919799805,-1.6273584365844727,-1.912226676940918,-1.9960169792175293,-1.997410774230957,-1.6893067359924316,-1.6918838024139404,-1.8441133499145508,-1.5266814231872559,-1.5712658166885376,-1.629915475845337,-1.6851179599761963,-1.6888885498046875,-1.6893067359924316,-1.689877986907959,-1.762855052947998,-1.93585205078125,-1.643308162689209,-1.6906654834747314,-1.804175853729248,-1.6949305534362793,-1.6942639350891113,-1.7412307262420654,-1.7239196300506592,-1.6916658878326416,-1.6922657489776611,-1.5750433206558228,-1.7617387771606445,-1.6909921169281006,-2.095339059829712,-1.7198905944824219,-1.6754937171936035,-1.8085904121398926,-1.6917190551757812,-1.330136775970459,-2.001275062561035,-1.6938598155975342,-1.689629077911377,-1.695681095123291,-1.690669059753418,-2.1614909172058105,-1.504629135131836,-1.8774676322937012,-1.688645362854004,-1.8531227111816406,-1.689483404159546,-1.73695707321167,-1.5187833309173584,-1.6749186515808105,-1.6888885498046875,-1.6520962715148926,-1.720975637435913,-1.7072367668151855,-1.6182973384857178,-1.629915475845337,-1.5928630828857422,-1.5184125900268555,-2.1927080154418945,-1.7791390419006348,-1.8331718444824219,-1.680699348449707,-1.770153522491455,-1.6885967254638672,-1.4652165174484253,-1.3073126077651978,-2.362607955932617,-1.843449354171753,-1.6829400062561035,-1.7229728698730469,-1.6910462379455566,-1.4819090366363525,-1.6800925731658936,-1.629915475845337,-1.794266700744629,-1.8610098361968994,-2.014739513397217,-1.6917939186096191,-2.122835397720337,-1.8954355716705322,-1.4252233505249023,-1.7241692543029785,-1.4381588697433472,-1.8518223762512207,-1.6899938583374023,-1.6881928443908691,-1.7666702270507812,-1.6698238849639893,-1.8039889335632324,-1.6881928443908691,-1.766678810119629,-1.689756155014038,-1.4665050506591797,-1.690490484237671,-1.9910624027252197,-1.690699577331543,-1.697798252105713,-1.6626930236816406,-1.7104332447052002,-1.7105016708374023,-1.689497470855713,-1.753868579864502,-1.690777063369751,-1.3350119590759277,-1.672450304031372,-1.8408794403076172,-1.7146832942962646,-1.6163053512573242,-1.9923286437988281,-2.2455363273620605,-1.690490484237671,-1.7666301727294922,-1.6952965259552002,-2.615852117538452,-1.6964421272277832,-1.6900341510772705,-1.6911582946777344,-1.583686351776123,-1.656050205230713,-1.852782964706421,-1.6893067359924316,-1.843590259552002,-1.7929975986480713,-1.690366268157959,-1.6919934749603271,-1.5811704397201538,-1.8052120208740234,-1.6347906589508057,-1.6928486824035645,-1.6493093967437744,-1.7982478141784668,-1.7461049556732178,-1.6632750034332275,-2.016296148300171,-2.031956672668457,-1.807929515838623,-1.8923048973083496,-1.7062749862670898,-1.4895573854446411,-1.6918838024139404,-2.2907767295837402,-1.6757848262786865,-1.6924819946289062,-1.958000659942627,-1.6357076168060303,-1.690699577331543,-1.749802589416504,-1.852001428604126,-1.7074663639068604,-1.3504655361175537,-2.616966724395752,-1.7231338024139404,-1.256877064704895,-1.6917755603790283,-1.9837827682495117,-1.7275474071502686,-1.8554246425628662,-1.6893067359924316,-1.9107201099395752,-1.7285094261169434,-1.68434476852417,-1.6947343349456787,-1.6893067359924316,-1.9757463932037354,-1.6846060752868652,-1.740633249282837,-1.8194732666015625,-1.4213260412216187,-1.6422066688537598,-1.2995041608810425,-1.6893701553344727,-1.6555914878845215,-1.6215999126434326,-1.0722639560699463,-1.6906392574310303,-1.6823430061340332,-1.7377161979675293,-1.696326494216919,-1.7382702827453613,-2.094160318374634,-1.641350507736206,-1.857750654220581,-1.695568561553955,-1.7835912704467773,-1.7484734058380127,-1.918147087097168,-1.689864158630371,-1.6923832893371582,-0.7484945058822632,-1.6909279823303223,-1.6968326568603516,-1.7905490398406982,-1.923868179321289,-1.681403636932373,-1.840790033340454,-1.5695040225982666,-1.6153895854949951,-1.6489453315734863,-1.8677663803100586,-2.1265149116516113,-1.772705078125,-1.6936447620391846,-1.9803500175476074,-1.6957392692565918,-0.9108676910400391,-1.713083267211914,-1.4154306650161743,-2.296170711517334,-1.9966444969177246,-1.694451093673706,-1.806203842163086,-1.4560356140136719,-1.6775891780853271,-1.6888885498046875,-1.593307375907898,-1.693838119506836,-1.6395535469055176,-1.791734218597412,-1.8095388412475586,-1.8373489379882812,-1.2609732151031494,-1.6903276443481445,-1.3934998512268066,-1.690699577331543,-1.6881928443908691,-1.7427701950073242,-1.6923346519470215,-1.6908881664276123,-1.694286584854126,-1.6939821243286133,-1.8266401290893555,-1.6929066181182861,-1.8141109943389893,-1.8981029987335205,-1.0555148124694824,-1.547562599182129,-1.7542414665222168,-1.6603736877441406,-2.1353869438171387,-1.6907076835632324,-1.6561079025268555,-1.6952965259552002,-1.7945094108581543,-1.6893067359924316,-1.693187952041626,-1.0352038145065308,-1.6915793418884277,-1.814422607421875,-1.6907694339752197,-1.9196803569793701,-1.7500019073486328,-1.6920924186706543,-1.690490484237671,-1.0487325191497803,-1.76993727684021,-1.5920565128326416,-1.6947057247161865,-1.7070198059082031,-1.8106670379638672,-1.7146446704864502,-1.6314032077789307,-1.6924548149108887,-1.690467357635498,-1.839763879776001,-1.8747212886810303,-1.690896987915039,-1.5360870361328125,-2.008779764175415,-1.880502462387085,-1.6599256992340088,-1.719641923904419,-1.7234249114990234,-1.6905598640441895,-1.901353359222412,-1.7407195568084717,-1.0313758850097656,-1.7355058193206787,-1.6867175102233887,-1.5143623352050781,-1.767134189605713,-1.6906068325042725,-1.7043333053588867,-1.6230721473693848,-1.633960247039795,-1.6870090961456299,-1.6967976093292236,-1.6906471252441406,-1.7418837547302246,-2.030787467956543,-1.6888885498046875,-1.710284948348999,-1.8176250457763672,-1.7282235622406006,-1.7428686618804932,-1.6773979663848877,-1.6310491561889648,-1.67244291305542,-1.6613688468933105,-1.6344997882843018,-1.6913959980010986,-1.687063455581665,-1.691497802734375,-1.8231844902038574,-1.6311113834381104,-1.6738831996917725,-2.118504524230957,-1.6803531646728516,-1.6556117534637451,-1.690432071685791,-1.853924036026001,-1.7054824829101562,-1.6893067359924316,-1.545638084411621,-1.584200382232666,-1.690699577331543,-1.5326285362243652,-1.75431227684021,-1.3101863861083984,-1.6954429149627686,-1.9316844940185547,-1.6410045623779297,-1.5006636381149292,-1.7782893180847168,-1.6993927955627441,-1.6637356281280518,-1.6252260208129883,-1.781449317932129,-1.3786709308624268,-1.513481616973877,-2.0124785900115967,-1.7745280265808105,-1.689013957977295,-2.206458806991577,-1.7239668369293213,-1.692389726638794,-1.0679787397384644,-1.6979734897613525,-1.8480448722839355,-1.5316500663757324,-1.9449856281280518,-1.6728053092956543,-1.691504716873169,-1.9147157669067383,-1.970870018005371,-1.6936604976654053,-1.6896727085113525,-0.7778328657150269,-1.6914658546447754,-1.7436473369598389,-1.6883773803710938,-1.7019286155700684,-2.073197841644287,-1.6933085918426514,-1.7850580215454102,-2.043755054473877,-1.7570054531097412,-2.1666693687438965,-1.6893067359924316,-1.702275276184082,-1.6410090923309326,-1.6584830284118652,-1.6849486827850342,-1.700779914855957,-1.5931107997894287,-1.6956546306610107,-2.033585548400879,-1.6893067359924316,-1.6903324127197266,-1.6919214725494385,-1.801692247390747,-2.1511049270629883,-1.2978286743164062,-1.7724707126617432,-1.62119460105896,-1.6866157054901123,-1.6924200057983398,-1.6903564929962158,-1.8794384002685547,-1.694767951965332,-1.503462553024292,-1.6997568607330322,-1.6147608757019043,-1.4664621353149414,-1.6888885498046875,-1.6921119689941406,-1.690791368484497,-1.6920924186706543,-1.6993169784545898,-1.6720483303070068,-1.6918838024139404,-1.4641797542572021,-1.6963601112365723,-1.8258819580078125,-1.9929134845733643,-1.6038551330566406,-1.6920924186706543,-1.8906056880950928,-1.564718246459961,-1.7729740142822266,-1.5441572666168213,-1.605712652206421,-1.690490484237671,-1.6910769939422607,-1.6945464611053467,-1.4712169170379639,-1.6997997760772705,-1.7064523696899414,-1.6913642883300781,-1.6912097930908203,-1.9138946533203125,-1.6915669441223145,-1.691535234451294,-1.6370418071746826,-1.0310311317443848,-1.690490484237671,-1.689864158630371,-1.6909887790679932,-1.9537584781646729,-1.690152645111084,-1.9020028114318848,-1.6299858093261719,-1.7493493556976318,-1.6941890716552734,-2.1242828369140625,-1.7522249221801758,-1.6901240348815918,-1.6917364597320557,-1.728771448135376,-1.6770687103271484,-1.6901772022247314,-1.7791411876678467,-1.6913163661956787,-1.6909737586975098,-2.18320369720459,-1.389716386795044,-1.7476730346679688,-1.6907453536987305,-1.4682072401046753,-1.810770034790039,-1.519299030303955,-1.696300745010376,-1.6039223670959473,-1.7127437591552734,-1.6913959980010986,-1.764819622039795,-1.6899445056915283,-1.6914963722229004,-1.7867953777313232,-1.6891088485717773,-1.6951608657836914,-1.685805082321167,-1.6840121746063232,-1.6893067359924316,-1.6921648979187012,-1.7090706825256348,-1.77213454246521,-1.7511584758758545,-2.2622733116149902,-1.690622329711914,-2.0977590084075928,-1.4164378643035889,-1.4113272428512573,-1.6900815963745117,-1.6893067359924316,-1.6893067359924316,-1.4801578521728516,-1.6021718978881836,-1.6906628608703613,-1.8576889038085938,-1.6881585121154785,-1.6239964962005615,-1.7688214778900146,-1.3317335844039917,-1.6800477504730225,-1.6920228004455566,-1.4830782413482666,-1.7718207836151123,-1.6600241661071777,-1.6786267757415771,-1.9188733100891113,-2.006774425506592,-1.6912031173706055,-1.9566986560821533,-1.7731590270996094,-1.7137277126312256,-1.6899175643920898,-1.5764992237091064,-1.5425214767456055,-1.7095119953155518,-1.7766671180725098,-2.1066391468048096,-1.6998777389526367,-1.7537367343902588,-1.894118309020996,-1.7375483512878418,-1.8329596519470215,-1.690699577331543,-1.5875142812728882,-1.6881928443908691,-1.8363583087921143,-1.6864190101623535,-1.3490601778030396,-1.69537353515625,-1.729207992553711,-1.6433906555175781,-1.6906063556671143,-1.689929723739624,-1.6880097389221191,-1.6929044723510742,-1.6923389434814453,-1.770787239074707,-1.6870684623718262,-1.5975909233093262,-1.4098587036132812,-1.7668471336364746,-0.7041491270065308,-1.6867468357086182,-1.690762996673584,-1.6490821838378906,-1.7231025695800781,-1.6893310546875,-1.6911325454711914,-1.3447426557540894,-1.7709097862243652,-1.911515712738037,-1.8311245441436768,-1.6893067359924316,-1.3970470428466797,-1.2938017845153809,-1.6680645942687988,-0.7372357845306396,-1.6904406547546387,-1.4863947629928589,-1.6383466720581055,-1.6952965259552002,-1.691654920578003,-1.690490484237671,-2.1823108196258545,-1.690490484237671,-0.5388134717941284,-1.7558612823486328,-2.046049118041992,-1.5801444053649902,-1.6913959980010986,-1.682065486907959,-1.6996078491210938,-1.8364009857177734,-1.5858429670333862,-1.6528544425964355,-1.6845107078552246,-1.6921138763427734,-1.773078203201294,-2.2143774032592773,-1.690490484237671,-1.6909441947937012,-1.9970793724060059,-1.6854281425476074,-1.858137607574463,-1.373724341392517,-1.6951961517333984,-1.6606695652008057,-1.6560440063476562,-1.7037079334259033,-1.6947875022888184,-1.431800365447998,-1.8054289817810059,-2.3279905319213867,-1.7097485065460205,-1.7042100429534912,-1.6920924186706543,-1.7135283946990967,-1.6601364612579346,-1.753824234008789,-1.7042086124420166,-1.6584746837615967,-2.01662540435791,-1.5966739654541016,-2.1566038131713867,-1.5644720792770386,-1.6850833892822266,-1.852724552154541,-1.6711387634277344,-2.0627989768981934,-1.6893067359924316,-2.0731418132781982,-1.254166841506958,-1.6893067359924316,-1.6777081489562988,-1.6723530292510986,-1.6918838024139404,-1.5985748767852783,-1.6871693134307861,-1.4811367988586426,-1.8670878410339355,-1.7044174671173096,-1.693479299545288,-1.6353437900543213,-1.6885547637939453,-1.874528408050537,-1.9021356105804443,-1.6876959800720215,-1.6918840408325195,-1.514737844467163,-1.6458258628845215,-1.6934025287628174,-1.835536003112793,-1.6881928443908691,-1.5134774446487427,-1.3316237926483154,-1.629915475845337,-1.6956593990325928,-1.7006440162658691,-1.6912975311279297,-1.6894125938415527,-1.7805533409118652,-1.6952965259552002,-1.6952965259552002,-1.693697452545166,-1.6904091835021973,-1.6480846405029297,-1.5679919719696045,-1.7668404579162598,-1.688072681427002,-1.6893067359924316,-1.6926276683807373,-1.6894326210021973,-1.8795700073242188,-1.6892876625061035,-1.9344172477722168,-1.7578074932098389,-1.7579631805419922,-1.695101261138916,-1.4688427448272705,-1.7141766548156738,-2.067633628845215,-1.6987504959106445,-1.593956708908081,-1.7642736434936523,-1.696338415145874,-2.818892478942871,-1.7517974376678467,-1.7549362182617188,-1.9795196056365967,-1.6903753280639648,-1.6808700561523438,-1.8379690647125244,-2.1343026161193848,-1.690617561340332,-1.6881928443908691,-1.6902894973754883,-1.6982393264770508,-1.8824701309204102,-1.6509730815887451,-1.6881928443908691,-2.012967109680176,-1.8463358879089355,-0.6918011903762817,-1.9587912559509277,-1.690490484237671,-1.8004560470581055,-2.0531845092773438,-1.6865077018737793,-1.692244529724121,-1.423879861831665,-1.6116933822631836,-1.4910304546356201,-1.7852020263671875,-1.7666730880737305,-2.2065398693084717,-1.700587272644043,-1.6886930465698242,-1.733119249343872,-1.448333501815796,-1.6952965259552002,-1.6909921169281006,-1.6608867645263672,-1.563330888748169,-1.927210807800293,-1.6606385707855225,-1.6906161308288574,-1.6881928443908691,-1.9799730777740479,-1.677743911743164,-1.6893067359924316,-1.6888952255249023,-1.6976473331451416,-1.6893067359924316,-1.6906898021697998,-1.9619102478027344,-1.105210304260254,-1.6856236457824707,-2.0034618377685547,-1.5924296379089355,-1.581599473953247,-1.727494239807129,-1.958052635192871,-2.4646899700164795,-1.903782844543457,-1.6319892406463623,-1.8703100681304932,-1.5636310577392578,-1.6246607303619385,-1.6920554637908936,-1.4114432334899902,-1.690699577331543,-1.6872687339782715,-1.6530828475952148,-1.6881928443908691,-1.690490484237671,-1.6963355541229248,-1.690699577331543,-1.6978142261505127,-1.9934053421020508,-1.6922590732574463,-1.7543911933898926,-1.7330660820007324,-1.7480578422546387,-1.6784896850585938,-1.7594990730285645,-1.3307949304580688,-1.8106558322906494,-1.3821749687194824,-1.7424921989440918,-1.6263799667358398,-1.6950149536132812,-1.6623520851135254,-1.7052619457244873,-1.7994680404663086,-1.69081711769104,-0.6306418180465698,-1.629915475845337,-1.6983063220977783,-1.6780917644500732,-1.690969467163086,-2.641486644744873,-1.6923820972442627,-2.3638253211975098,-1.6920123100280762,-1.7497544288635254,-1.8510215282440186,-1.7129383087158203,-1.6906242370605469,-1.687699556350708,-1.6962761878967285,-1.6907951831817627,-1.6782963275909424,-1.6943480968475342,-1.793189525604248,-1.6719951629638672,-1.6596019268035889,-1.6881928443908691,-1.6897788047790527,-1.6893067359924316,-1.5358033180236816,-1.7275347709655762,-1.691969633102417,-1.9051761627197266,-1.8604764938354492,-1.7220759391784668,-1.719489574432373,-1.433569073677063,-1.7527194023132324,-1.506597638130188,-1.6199672222137451,-1.6907222270965576,-1.6931824684143066,-1.6902785301208496,-1.7669906616210938,-1.7019712924957275,-1.6952965259552002,-1.8569533824920654,-1.8049237728118896,-1.684462547302246,-1.690699577331543,-2.069056510925293,-1.6500391960144043,-1.916294813156128,-1.6903531551361084,-1.689864158630371,-1.709596872329712,-1.7284467220306396,-1.2594945430755615,-1.769256591796875,-1.7770013809204102,-1.6807734966278076,-1.6930160522460938,-1.940077304840088,-1.6920228004455566,-1.7339131832122803,-1.615967035293579,-0.7326662540435791,-1.2146470546722412,-1.6785709857940674,-1.6918838024139404,-1.5485459566116333,-1.5022858381271362,-1.6426053047180176,-0.9047614336013794,-2.004535675048828,-0.9325814247131348,-1.690903902053833,-1.3489811420440674,-1.692296028137207,-1.7804410457611084,-1.6904547214508057,-1.7167258262634277,-1.3721466064453125,-1.692124605178833,-1.6792280673980713,-1.5986554622650146,-1.3250716924667358,-1.478003740310669,-1.8546521663665771,-1.6994118690490723,-1.5007895231246948,-1.3785271644592285,-1.6925334930419922,-1.6874651908874512,-1.8103175163269043,-2.0872316360473633,-1.276180386543274,-1.6951863765716553,-1.7477612495422363,-1.694915771484375,-1.7918987274169922,-1.691030502319336,-1.6940960884094238,-1.6911609172821045,-1.690699577331543,-1.7760298252105713,-1.7008540630340576,-2.6106338500976562,-1.6952965259552002,-1.8232924938201904,-1.6913669109344482,-1.6123650074005127,-1.693835735321045,-1.519253134727478,-1.9000120162963867,-1.568605899810791,-1.6662869453430176,-1.8999550342559814,-1.6068224906921387,-1.8090333938598633,-1.6893067359924316,-1.6881928443908691,-1.6893067359924316,-1.4236884117126465,-1.7499465942382812,-1.8500633239746094,-1.9967515468597412,-1.690699577331543,-1.690490484237671,-1.7507143020629883,-1.802994728088379,-1.825761318206787,-1.6917240619659424,-1.6314377784729004,-1.690490484237671,-1.6763148307800293,-2.0598011016845703,-1.7332353591918945,-2.149216651916504,-1.9497666358947754,-1.988093376159668,-1.963632583618164,-1.7440791130065918,-1.8445487022399902,-1.6994471549987793,-1.8416168689727783,-1.5924540758132935,-1.6915104389190674,-1.7504427433013916,-1.6310479640960693,-1.882195234298706,-1.683223009109497,-1.6879987716674805,-1.6903834342956543,-1.801938533782959,-2.2022275924682617,-1.689864158630371,-1.7366611957550049,-0.9724791049957275,-2.6690430641174316,-1.6955475807189941,-1.3106307983398438,-1.6908211708068848,-1.689664602279663,-1.5729444026947021,-1.6952965259552002,-1.731290578842163,-1.2938441038131714,-1.692739725112915,-1.7100210189819336,-1.922818660736084,-1.690490484237671,-1.6893067359924316,-1.6694612503051758,-1.691915512084961,-1.6867527961730957,-1.6909921169281006,-1.6949822902679443,-1.6748931407928467,-1.6894621849060059,-1.5674450397491455,-1.752842664718628,-1.6952965259552002,-1.782806634902954,-1.6349365711212158,-1.8154902458190918,-1.6831164360046387,-1.866835355758667,-1.7057344913482666,-1.6924104690551758,-1.9152932167053223,-1.2722662687301636,-1.768500804901123,-1.7116279602050781,-1.7675237655639648,-1.6893067359924316,-1.6881928443908691,-1.6921489238739014,-1.8642795085906982,-1.68986177444458,-1.6799113750457764,-1.6891067028045654,-1.6911096572875977,-1.9065513610839844,-1.689864158630371,-1.6770129203796387,-1.8584656715393066,-1.6973423957824707,-1.551426887512207,-1.6920289993286133,-1.6935162544250488,-1.8078112602233887,-1.6830525398254395,-1.3594093322753906,-1.692194938659668,-1.6497735977172852,-2.2462873458862305,-1.7748594284057617,-1.6914658546447754,-1.9416298866271973,-0.9541972875595093,-1.6918838024139404,-2.300291061401367,-1.6909921169281006,-1.6000535488128662,-1.6701655387878418,-1.1244711875915527,-1.699023723602295,-1.3904516696929932,-1.6375226974487305,-1.5925675630569458,-1.6904144287109375,-1.7182092666625977,-1.2911677360534668,-1.7002835273742676,-1.81675124168396,-1.6920881271362305,-1.396960973739624,-1.702430248260498,-1.4501298666000366,-2.1148834228515625,-1.7174227237701416,-1.687974452972412,-1.6952965259552002,-1.5261085033416748,-1.680098295211792,-1.689666509628296,-1.690490484237671,-1.666579008102417,-1.6918838024139404,-1.8495068550109863,-1.5063942670822144,-1.7095184326171875,-0.7424684762954712,-1.9070241451263428,-1.6881928443908691,-1.6895833015441895,-2.031395435333252,-1.690490484237671,-1.5644478797912598,-1.6947336196899414,-1.402687668800354,-1.0828659534454346,-2.0444846153259277,-1.7910709381103516,-1.4558377265930176,-1.687612533569336,-1.435321569442749,-1.6897528171539307,-1.7230172157287598,-1.6944737434387207,-1.6893067359924316,-1.6682183742523193,-2.053722620010376,-1.6274402141571045,-1.446207046508789,-1.6522133350372314,-1.6097240447998047,-1.68546724319458,-1.690490484237671,-2.269010543823242,-1.5577255487442017,-1.581007957458496,-1.770815134048462,-1.5929858684539795,-1.8106002807617188,-1.6851887702941895,-1.6927659511566162,-1.5951790809631348,-1.6860194206237793,-1.6916146278381348,-1.4534693956375122,-1.6893067359924316,-1.6899690628051758,-1.8031046390533447,-1.7571303844451904,-1.929643154144287,-1.7316267490386963,-1.816154956817627,-1.767359972000122,-1.665579080581665,-1.7069945335388184,-1.6912338733673096,-1.7278611660003662,-1.6881928443908691,-1.6913959980010986,-1.7634224891662598,-1.6915674209594727,-1.6921491622924805,-1.6903376579284668,-2.014098644256592,-1.6414179801940918,-2.4388198852539062,-1.6025164127349854,-1.798910140991211,-1.6908485889434814,-1.6783115863800049,-1.690490484237671,-1.3161401748657227,-1.6952965259552002,-1.8162627220153809,-1.6905782222747803,-1.7349684238433838,-1.515580177307129,-2.0964856147766113,-2.101003646850586,-1.6901535987854004,-1.552124261856079,-1.3733599185943604,-1.6825551986694336,-1.6903448104858398,-1.9045541286468506,-1.6507019996643066,-2.545991897583008,-1.567670226097107,-2.110687255859375,-1.6902437210083008,-1.6908752918243408,-1.8483877182006836,-1.7063417434692383,-1.4421985149383545,-1.4879745244979858,-1.6475818157196045,-1.8570480346679688,-1.7007384300231934,-1.9850859642028809,-1.9934613704681396,-1.5708801746368408,-1.7079050540924072,-1.667358160018921,-1.3245290517807007,-1.8806746006011963,-1.4977056980133057,-2.085592746734619,-1.7473864555358887,-1.6921448707580566,-2.083672046661377,-1.7091026306152344,-1.7078542709350586,-1.3597373962402344,-1.7318756580352783,-1.6872398853302002,-1.7442278861999512,-1.6936571598052979,-1.6425917148590088,-1.4690139293670654,-1.6867806911468506,-1.6918838024139404,-1.9119527339935303,-1.6907320022583008,-1.6704926490783691,-1.3352848291397095,-1.7070841789245605,-1.6893067359924316,-1.985569953918457,-2.0474672317504883,-1.6952965259552002,-1.7557106018066406,-1.6874454021453857,-1.619610071182251,-1.686333417892456,-1.6334891319274902,-1.690699577331543,-1.8051538467407227,-1.9696402549743652,-2.031643867492676,-1.7027268409729004,-1.6888306140899658,-2.1483497619628906,-1.5674405097961426,-1.6724369525909424,-1.611581802368164,-1.5324674844741821,-2.002033233642578,-1.9016659259796143,-1.6952965259552002,-1.6925499439239502,-1.8501665592193604,-1.690490484237671,-1.6936674118041992,-1.7598581314086914,-2.100588798522949,-1.8260040283203125,-1.6893067359924316,-1.9979968070983887,-1.6913959980010986,-1.6914665699005127,-1.7072086334228516,-1.7959661483764648,-1.6341326236724854,-1.7716970443725586,-1.6889433860778809,-1.3650660514831543,-1.6785061359405518,-1.38508939743042,-1.9552836418151855,-1.6197175979614258,-1.1561577320098877,-1.6622469425201416,-1.8317856788635254,-1.4319779872894287,-1.8292107582092285,-1.691357135772705,-1.8506455421447754,-1.6881928443908691,-1.8112261295318604,-1.6905782222747803,-1.7734041213989258,-1.5681021213531494,-1.8944048881530762,-1.921933650970459,-1.6929912567138672,-1.6915709972381592,-1.78248929977417,-1.9127652645111084,-1.584788203239441,-1.6874935626983643,-1.5763435363769531,-1.780412197113037,-1.6952965259552002,-1.056579828262329,-1.8352537155151367,-1.3217825889587402,-0.9783413410186768,-1.8617072105407715,-1.6968092918395996,-1.299241542816162,-1.7107973098754883,-1.6920228004455566,-1.769728422164917,-1.7888622283935547,-1.6925303936004639,-1.692260503768921,-1.9774396419525146,-1.6917393207550049,-2.0313472747802734,-1.889749526977539,-1.7066798210144043,-1.6965548992156982,-1.7492773532867432,-1.6888885498046875,-1.4132832288742065,-1.6893067359924316,-1.7170484066009521,-1.6918838024139404,-1.6668469905853271,-1.7813365459442139,-1.6549265384674072,-1.6893067359924316,-1.690399408340454,-1.694542407989502,-1.6903769969940186,-1.6893067359924316,-1.996798038482666,-1.6945619583129883,-1.6019151210784912,-1.689427137374878,-1.3531460762023926,-1.6341781616210938,-1.690199851989746,-1.690699577331543,-1.7588133811950684,-1.836437463760376,-1.6768224239349365,-1.5210652351379395,-1.59775972366333,-1.6963608264923096,-1.9430551528930664,-1.6907110214233398,-1.8572988510131836,-1.79530930519104,-1.694242238998413,-1.8391449451446533,-1.825007438659668,-1.8266968727111816,-1.6378555297851562,-1.691941738128662,-0.8307738304138184,-1.689293384552002,-2.182314157485962,-1.6914658546447754,-1.6914658546447754,-1.691845178604126,-1.754323959350586,-1.6981616020202637,-1.7032828330993652,-1.8278040885925293,-1.687669038772583,-1.7121295928955078,-2.094975471496582,-1.6283481121063232,-1.6916007995605469,-1.4758410453796387,-1.7108995914459229,-1.7903144359588623,-1.5240991115570068,-1.3429367542266846,-1.220658540725708,-1.691272258758545,-1.6927015781402588,-1.6986465454101562,-1.7534513473510742,-1.6867504119873047,-1.692901611328125,-2.190464496612549,-1.755037546157837,-1.3827286958694458,-1.787956953048706,-1.6905016899108887,-1.706019639968872,-1.6955454349517822,-1.690223217010498,-1.5256922245025635,-1.690547227859497,-1.9138917922973633,-1.657466173171997,-1.3339004516601562,-1.687516212463379,-1.6900317668914795,-1.9111037254333496,-1.6903586387634277,-1.5338904857635498,-1.9405364990234375,-1.6730070114135742,-1.690490484237671,-1.7566859722137451,-1.9683115482330322,-1.689272165298462,-1.6888885498046875,-1.733431100845337,-1.6939034461975098,-2.0011048316955566,-1.693589687347412,-1.8600637912750244,-1.8290565013885498,-1.6779890060424805,-1.6258680820465088,-1.5679858922958374,-1.7798542976379395,-1.6893067359924316,-1.6984341144561768,-1.7683801651000977,-1.3133304119110107,-1.4617875814437866,-1.693089485168457,-1.6797759532928467,-1.9312686920166016,-1.68770432472229,-1.690699577331543,-1.7161035537719727,-1.7929887771606445,-2.0889453887939453,-1.6893067359924316,-1.9979305267333984,-1.760676383972168,-1.7309119701385498,-1.6912553310394287,-1.708188772201538,-1.7407660484313965,-1.9295625686645508,-1.6908574104309082,-1.6252126693725586,-2.345764636993408,-1.8295111656188965,-1.9294755458831787,-1.731351375579834,-1.869842290878296,-2.061830520629883,-1.4517215490341187,-1.673410177230835,-1.6901745796203613,-1.6900291442871094,-1.6907694339752197,-1.990936040878296,-1.6851158142089844,-1.6907694339752197,-1.850313663482666,-1.9154839515686035,-1.6950294971466064,-1.5760817527770996,-1.3264272212982178,-2.021010398864746,-1.690699577331543,-1.6893067359924316,-1.621140956878662,-1.9881887435913086,-1.6323168277740479,-1.4932929277420044,-1.6893067359924316,-1.7612926959991455,-1.6903142929077148,-1.74918532371521,-1.223258376121521,-1.8350870609283447,-1.691009521484375,-1.7678039073944092,-1.6945164203643799,-1.7331476211547852,-1.6904773712158203,-1.1907641887664795,-1.9826974868774414,-1.7024040222167969,-1.6852130889892578,-1.8201119899749756,-1.6655266284942627,-1.7738656997680664,-1.6888885498046875,-1.5876176357269287,-1.6893067359924316,-1.7320663928985596,-1.7245514392852783,-1.3317663669586182,-1.690493106842041,-1.9890477657318115,-1.6924712657928467,-1.6914658546447754,-1.7963342666625977,-1.6893067359924316,-2.2530174255371094,-1.818295955657959,-1.7758262157440186,-1.690699577331543,-1.6938714981079102,-1.1402678489685059,-1.5082074403762817,-2.0932137966156006,-1.346956491470337,-1.6699676513671875,-1.9002513885498047,-1.6731913089752197,-1.8758883476257324,-1.6448569297790527,-1.5383119583129883,-1.8207354545593262,-1.6921021938323975,-1.6921274662017822,-1.3578875064849854,-1.681136131286621,-1.723339557647705,-1.6952965259552002,-1.7633719444274902,-2.0357487201690674,-1.0651460886001587,-1.3384300470352173,-1.6929535865783691,-1.3340504169464111,-1.485917329788208,-1.7015998363494873,-1.690490484237671,-1.6796550750732422,-1.6664016246795654,-1.690699577331543,-1.6815016269683838,-1.9683070182800293,-1.8480048179626465,-1.6884474754333496,-2.0488178730010986,-1.7505841255187988,-1.6871397495269775,-1.6903040409088135,-1.696378469467163,-1.7026495933532715,-1.5733131170272827,-2.3268256187438965,-1.690718650817871,-1.6922540664672852,-1.6802434921264648,-1.6878488063812256,-1.5820196866989136,-1.3978843688964844,-1.72700834274292,-1.6309795379638672,-2.043930768966675,-0.8912124633789062,-1.8339428901672363,-1.6786541938781738,-1.9057223796844482,-1.7567024230957031,-1.6428251266479492,-1.6930210590362549,-1.966862440109253,-1.699822187423706,-1.6886518001556396,-1.2500829696655273,-1.690490484237671,-1.6945104598999023,-1.0837199687957764,-1.3079543113708496,-1.6712298393249512,-1.6842854022979736,-1.8208985328674316,-1.985705852508545,-1.7528526782989502,-1.5299755334854126,-1.6957335472106934,-1.6947760581970215,-1.930859088897705,-1.7622129917144775,-1.6905713081359863,-1.7186992168426514,-1.652348518371582,-1.6886324882507324,-1.9218664169311523,-1.6918840408325195,-1.7245209217071533,-1.64992094039917,-1.6933717727661133,-1.6903371810913086,-1.6898202896118164,-1.7997159957885742,-1.6853971481323242,-1.690699577331543,-1.8108453750610352,-2.078538417816162,-1.5371453762054443,-1.690490484237671,-1.734199047088623,-1.5181455612182617,-1.6904897689819336,-1.5781035423278809,-1.6134264469146729,-1.9347143173217773,-1.666597843170166,-1.7045986652374268,-1.690490484237671,-1.6352200508117676,-1.34339439868927,-1.589837670326233,-1.8094921112060547,-1.9267354011535645,-1.0347011089324951,-1.687415599822998,-1.6905148029327393,-1.5996155738830566,-0.9553622007369995,-1.4494578838348389,-1.8976190090179443,-1.3227378129959106,-1.6929991245269775,-1.69246244430542,-1.1545295715332031,-1.6869902610778809,-1.6760528087615967,-1.6923980712890625,-1.650339126586914,-1.8190319538116455,-1.6909873485565186,-1.4409422874450684,-1.8361973762512207,-1.6939034461975098,-1.7328848838806152,-1.6875536441802979,-1.4613277912139893,-1.7913613319396973,-1.3492968082427979,-1.7319777011871338,-2.0114879608154297,-1.6937546730041504,-2.7132039070129395,-1.6009178161621094,-1.6656749248504639,-1.6920435428619385,-1.6952965259552002,-1.5669569969177246,-1.6920924186706543,-1.2244272232055664,-1.6907777786254883,-1.5997095108032227,-1.9008581638336182,-1.8460195064544678,-1.7205226421356201,-1.688878059387207,-1.766226053237915,-1.6942780017852783,-1.8452789783477783,-1.694641351699829,-1.671703577041626,-1.719064712524414,-1.6893067359924316,-1.6731114387512207,-0.6357207298278809,-1.8782269954681396,-1.7675137519836426,-1.7880239486694336,-1.2572907209396362,-2.2199621200561523,-1.6906895637512207,-1.6879758834838867,-2.25671124458313,-1.6927168369293213,-0.6445554494857788,-1.730607509613037,-1.7685184478759766,-1.9518251419067383,-1.7644462585449219,-1.686589241027832,-1.4695250988006592,-1.632143497467041,-1.6533894538879395,-1.2794973850250244,-1.6423835754394531,-1.1852083206176758,-1.5655393600463867,-1.8905441761016846,-1.7303547859191895,-1.8766682147979736,-1.7056005001068115,-1.6490821838378906,-1.3810503482818604,-1.6181895732879639,-1.7893829345703125,-1.690699577331543,-1.7509195804595947,-1.6870179176330566,-2.4245171546936035,-1.697765827178955,-1.6970622539520264,-1.9897093772888184,-1.690490484237671,-1.6908555030822754,-1.6928467750549316,-1.6806740760803223,-1.7723362445831299,-1.94761323928833,-1.7913880348205566,-1.6956520080566406,-1.6878600120544434,-1.7147974967956543,-1.7451229095458984,-1.6877501010894775,-1.6302785873413086,-1.8916866779327393,-1.8068737983703613,-1.691300392150879,-1.240756630897522,-1.6893067359924316,-1.6240110397338867,-1.9826946258544922,-1.7477140426635742,-1.8161962032318115,-1.9122459888458252,-1.8075392246246338,-1.8432979583740234,-1.7385025024414062,-1.6906285285949707,-1.7187190055847168,-1.8857574462890625,-1.6889591217041016,-2.0375967025756836,-1.6872243881225586,-1.337446928024292,-1.691816806793213,-1.9307398796081543,-1.6907694339752197,-1.6757662296295166,-1.6905169486999512,-1.8767857551574707,-1.68009352684021,-1.6687309741973877,-1.6899611949920654,-1.6908433437347412,-1.9943220615386963,-1.6939034461975098,-1.6517715454101562,-1.319536805152893,-1.6884257793426514,-1.692079782485962,-1.1872384548187256,-1.6167876720428467,-1.033233880996704,-1.6893067359924316,-1.913090705871582,-1.6732957363128662,-1.9075937271118164,-1.6893067359924316,-2.245889186859131,-1.6893067359924316,-1.8913769721984863,-1.6974339485168457,-1.6901047229766846,-1.691854476928711,-1.3430204391479492,-1.6939034461975098,-1.7629985809326172,-1.1016497611999512,-1.6952056884765625,-1.860689640045166,-1.7037088871002197,-1.690490484237671,-1.6900951862335205,-1.8281536102294922,-1.9456286430358887,-1.690490484237671,-1.8621764183044434,-1.094130277633667,-1.6925110816955566,-1.6920444965362549,-1.43258535861969,-1.7026476860046387,-1.3581562042236328,-1.7474942207336426,-1.687845230102539,-1.6906142234802246,-1.4518988132476807,-1.6943743228912354,-2.350130081176758,-1.6891558170318604,-1.6925177574157715,-1.6876769065856934,-1.7991440296173096,-1.3890957832336426,-1.6161458492279053,-1.6930344104766846,-1.695286750793457,-1.2581099271774292,-1.6951625347137451,-1.791719913482666,-1.6888885498046875,-1.488450527191162,-2.081326484680176,-1.7243754863739014,-1.8443853855133057,-1.6885786056518555,-1.6928682327270508,-1.6290688514709473,-1.681755781173706,-1.6056442260742188,-1.742189884185791,-1.6910412311553955,-1.6307034492492676,-1.9560699462890625,-1.6577746868133545,-1.7712688446044922,-1.6881928443908691,-1.701094150543213,-1.6933462619781494,-1.9681673049926758,-1.8577899932861328,-1.336774230003357,-1.6268093585968018,-1.6956803798675537,-1.5979502201080322,-1.5131285190582275,-1.7159373760223389,-1.8728222846984863,-1.996574878692627,-1.7849221229553223,-1.6973621845245361,-1.6920630931854248,-1.7504396438598633,-1.6340000629425049,-1.718921422958374,-2.0883405208587646,-1.4790760278701782,-1.7476696968078613,-1.7070379257202148,-2.0723252296447754,-1.6893067359924316,-1.6872215270996094,-1.5388908386230469,-1.6927111148834229,-1.6438586711883545,-1.689864158630371,-1.690739393234253,-1.2328779697418213,-1.6706886291503906,-1.510941743850708,-2.093351364135742,-1.6293306350708008,-1.6881928443908691,-1.831294059753418,-1.6650056838989258,-1.690450668334961,-1.867002010345459,-1.6186792850494385,-1.8203458786010742,-1.5856616497039795,-1.8882036209106445,-1.691329002380371,-2.011089324951172,-1.9338195323944092,-1.692798376083374,-1.7077503204345703,-1.6892123222351074,-2.1809136867523193,-1.709812879562378,-1.676121473312378,-1.8214073181152344,-1.8971872329711914,-1.691704511642456,-1.59602689743042,-1.758286714553833,-1.9544997215270996,-1.6925630569458008,-1.487259030342102,-1.285239338874817,-1.6893067359924316,-2.053933620452881,-1.6556015014648438,-1.6920146942138672,-1.6381423473358154,-1.6918838024139404,-1.8763082027435303,-1.691192388534546,-2.2194151878356934,-1.723731279373169,-0.7150536775588989,-1.6984007358551025,-1.8051061630249023,-1.735888957977295,-1.6990039348602295,-1.6925461292266846,-1.690699577331543,-1.3250253200531006,-1.8212151527404785,-1.647599697113037,-1.6697325706481934,-1.9910016059875488,-1.690699577331543,-1.6893067359924316,-1.6282939910888672,-1.7377243041992188,-1.6546692848205566,-1.7054648399353027,-1.6712861061096191,-1.576598048210144,-1.6888885498046875,-1.6935091018676758,-1.7038590908050537,-1.6738507747650146,-1.6893067359924316,-1.7779483795166016,-1.7455968856811523,-1.440262794494629,-1.7799327373504639,-1.8076457977294922,-1.8289964199066162,-1.7587378025054932,-1.629915475845337,-1.629915475845337,-1.6407222747802734,-1.6900858879089355,-1.6878814697265625,-1.6631038188934326,-1.9863994121551514,-1.8168158531188965,-1.6930391788482666,-1.726773738861084,-1.6975164413452148,-1.2188888788223267,-1.4850623607635498,-1.645460844039917,-1.7686567306518555,-1.754619836807251,-1.7405433654785156,-1.689087152481079,-1.6881928443908691,-1.5564782619476318,-1.691577434539795,-1.8200325965881348,-1.7593302726745605,-1.7545223236083984,-2.0674142837524414,-1.9690923690795898,-1.800086498260498,-1.6166234016418457,-1.6817560195922852,-1.412139654159546,-1.752241611480713,-1.6927547454833984,-1.8567447662353516,-1.6954293251037598,-1.6900489330291748,-1.8275835514068604,-1.7092905044555664,-1.7181456089019775,-2.1149044036865234,-1.0452086925506592,-1.6272892951965332,-1.6880135536193848,-1.6817264556884766,-1.8070335388183594,-1.6262524127960205,-1.6920595169067383,-1.6867077350616455,-1.6958415508270264,-1.690490484237671,-1.691664695739746,-1.8639798164367676,-1.7175798416137695,-1.9675753116607666,-1.6986072063446045,-1.9108343124389648,-1.690420389175415,-1.7478947639465332,-1.8272104263305664,-1.6909916400909424,-1.8553242683410645,-1.6968655586242676,-1.5199625492095947,-1.8150062561035156,-1.6906206607818604,-1.9383034706115723,-1.6962990760803223,-1.6916568279266357,-1.7083721160888672,-1.6761794090270996,-2.9380102157592773,-1.6881928443908691,-1.6482570171356201,-1.6952965259552002,-1.6443238258361816,-1.629915475845337,-1.6918776035308838,-1.8094401359558105,-1.6429576873779297,-1.719547986984253,-1.6929256916046143,-1.6943950653076172,-1.524352788925171,-1.5432076454162598,-1.661233901977539,-1.6451201438903809,-1.7870445251464844,-1.7100658416748047,-2.443302631378174,-1.5117436647415161,-1.94856595993042,-1.678004503250122,-1.6949422359466553,-1.4078378677368164,-1.6893067359924316,-1.555849552154541,-1.6242709159851074,-1.7245006561279297,-1.6902718544006348,-1.7331163883209229,-1.6909921169281006,-1.7114953994750977,-1.6939661502838135,-1.6696515083312988,-1.8528552055358887,-1.7301175594329834,-1.6893067359924316,-1.565598487854004,-2.1804676055908203,-1.4459079504013062,-1.6918838024139404,-1.6909494400024414,-1.720637321472168,-1.6893067359924316,-1.693352222442627,-1.582329273223877,-1.7275068759918213,-1.081824779510498,-1.6930031776428223,-1.6919300556182861,-1.6202425956726074,-1.6962714195251465,-1.6905856132507324,-1.687253713607788,-1.655196189880371,-1.6823415756225586,-1.6907596588134766,-1.7155065536499023,-1.8834524154663086,-1.6939034461975098,-1.695221185684204,-1.690699577331543,-1.930544376373291,-1.5226565599441528,-1.7411189079284668,-1.6917262077331543,-1.7970759868621826,-0.8829386234283447,-1.9345035552978516,-1.9600000381469727,-1.6549973487854004,-1.8830442428588867,-1.780574083328247,-1.6087713241577148,-1.593043565750122,-1.6918838024139404,-1.6505210399627686,-1.7982206344604492,-1.0574976205825806,-1.5038609504699707,-1.5119707584381104,-1.7012951374053955,-1.6913959980010986,-1.6901061534881592,-1.7116186618804932,-1.7741968631744385,-1.636474609375,-1.7358863353729248,-1.436966896057129,-1.4977850914001465,-1.9242563247680664,-1.690490484237671,-1.757112979888916,-1.6080033779144287,-1.3226900100708008,-1.6958644390106201,-1.6690349578857422,-1.6839497089385986,-1.6922345161437988,-1.695279598236084,-2.075047016143799,-1.7572665214538574,-1.2961761951446533,-1.6640572547912598,-1.3323391675949097,-1.6919898986816406,-1.6914527416229248,-1.5919151306152344,-1.685797929763794,-1.6715433597564697,-1.8396368026733398,-1.6899371147155762,-1.6893067359924316,-2.2211456298828125,-1.7639508247375488,-1.962740182876587,-1.6920228004455566,-1.6924386024475098,-1.6911852359771729,-1.8922128677368164,-1.8050603866577148,-1.9680933952331543,-1.603424072265625,-1.7072079181671143,-1.4016478061676025,-1.6570429801940918,-1.658461332321167,-1.6901187896728516,-1.6868114471435547,-1.692631721496582,-1.8418853282928467,-1.67555832862854,-1.850261926651001,-1.406318187713623,-1.6904728412628174,-1.6011686325073242,-1.0242486000061035,-1.68949556350708,-1.6915788650512695,-1.7886381149291992,-1.690490484237671,-1.9122633934020996,-2.0422298908233643,-1.9509305953979492,-1.6925878524780273,-1.8117971420288086,-2.0336074829101562,-1.707289695739746,-1.6917364597320557,-1.7236883640289307,-1.7253594398498535,-1.708662748336792,-1.6677634716033936,-1.6925148963928223,-1.7134897708892822,-1.6155281066894531,-1.6923410892486572,-1.6906542778015137,-1.690699577331543,-1.690490484237671,-1.8804028034210205,-1.6930012702941895,-1.475103497505188,-1.6787028312683105,-1.4452764987945557,-1.6895372867584229,-1.6614630222320557,-1.6952965259552002,-1.872709035873413,-1.6793076992034912,-1.725975751876831,-2.345463752746582,-1.7063875198364258,-1.6890013217926025,-1.4370040893554688,-1.5679106712341309,-1.6956002712249756,-1.45469069480896,-1.7020957469940186,-1.674208641052246,-2.16347599029541,-1.96044921875,-1.691850185394287,-1.7789855003356934,-1.6926145553588867,-2.0838756561279297,-1.8456439971923828,-1.6939189434051514,-0.7920119762420654,-1.7173216342926025,-1.6853513717651367,-1.681551218032837,-1.8710477352142334,-1.7003471851348877,-1.7341339588165283,-1.5251425504684448,-1.8524539470672607,-1.6924748420715332,-1.6892156600952148,-1.6899781227111816,-1.6907694339752197,-1.6976385116577148,-1.825169563293457,-1.8499369621276855,-1.2750076055526733,-1.6640899181365967,-1.6836528778076172,-1.6898655891418457,-1.6914658546447754,-1.7524347305297852,-1.7912354469299316,-1.735476016998291,-1.8785381317138672,-1.6030282974243164,-1.6907498836517334,-1.6943914890289307,-2.277853012084961,-1.717869758605957,-1.7010140419006348,-1.6932096481323242,-1.6930005550384521,-1.6868834495544434,-1.8437159061431885,-1.6986770629882812,-1.6370315551757812,-2.0441770553588867,-2.0893826484680176,-1.5623934268951416,-1.6927220821380615,-1.0912115573883057,-1.777921199798584,-1.345949411392212,-1.687748908996582,-1.4369473457336426,-1.7282893657684326,-1.7924208641052246,-1.7216858863830566,-1.6439838409423828,-1.6995327472686768,-1.7829570770263672,-2.329843521118164,-1.6909325122833252,-1.8478593826293945,-1.692786693572998,-1.6906347274780273,-1.6929185390472412,-1.689864158630371,-1.96826171875,-1.7733368873596191,-1.6893067359924316,-1.690699577331543,-1.6904494762420654,-1.6901822090148926,-1.781895637512207,-1.017925500869751,-1.6976745128631592,-1.6796681880950928,-1.8219611644744873,-1.6948165893554688,-1.6937530040740967,-0.8494924306869507,-1.690490484237671,-1.6897478103637695,-1.6744070053100586,-1.6941261291503906,-1.4432748556137085,-1.7422723770141602,-1.6904492378234863,-1.6922798156738281,-1.6951875686645508,-1.6257598400115967,-1.775033712387085,-1.7510004043579102,-0.8318541049957275,-1.6728899478912354,-1.3551779985427856,-1.6893067359924316,-1.366346836090088,-1.6534905433654785,-1.700817584991455,-1.624082088470459,-1.6971240043640137,-1.9808011054992676,-1.6896274089813232,-1.6920192241668701,-1.6652626991271973,-1.838078498840332,-1.3764019012451172,-1.7623252868652344,-1.6780438423156738,-1.8558988571166992,-1.7516098022460938,-1.692000389099121,-1.6247749328613281,-1.6893067359924316,-2.079519748687744,-1.6367642879486084,-1.5981800556182861,-1.65850830078125,-1.0026143789291382,-2.08671236038208,-1.685366153717041,-1.740955114364624,-1.690101146697998,-1.7181944847106934,-1.666982650756836,-1.6893067359924316,-1.7000367641448975,-1.657386302947998,-1.7888875007629395,-1.6920228004455566,-1.830873727798462,-1.7128901481628418,-1.7837443351745605,-1.597032070159912,-1.6909267902374268,-1.6924242973327637,-1.6537055969238281,-1.6914527416229248,-1.7029104232788086,-2.495781898498535,-1.5792105197906494,-1.8620305061340332,-1.8252248764038086,-1.4958430528640747,-1.8538141250610352,-1.6910443305969238,-1.8107705116271973,-1.8556554317474365,-1.5867440700531006,-1.9146966934204102,-2.119007110595703,-1.5312023162841797,-2.0025062561035156,-1.6595125198364258,-1.0925308465957642,-1.6920690536499023,-1.6715638637542725,-1.4697668552398682,-1.7030177116394043,-1.6918838024139404,-1.6447765827178955,-1.5722382068634033,-1.986219882965088,-1.690490484237671,-1.6918838024139404,-1.76784348487854,-1.7741284370422363,-1.6952965259552002,-1.6975607872009277,-1.932084560394287,-1.6885292530059814,-1.200890302658081,-1.6904933452606201,-1.687140941619873,-1.9185104370117188,-1.629915475845337,-1.6908478736877441,-0.9646323919296265,-2.6393067836761475,-1.6939034461975098,-1.6895833015441895,-1.7453739643096924,-1.689864158630371,-1.686023235321045,-1.6898536682128906,-1.8957574367523193,-1.6881928443908691,-1.7134997844696045,-1.692779302597046,-1.5310895442962646,-1.6895687580108643,-1.682295560836792,-1.8611364364624023,-1.0373533964157104,-1.692495346069336,-1.6955244541168213,-1.931365728378296,-1.5661948919296265,-1.692533016204834,-2.5154709815979004,-2.329193592071533,-1.8694913387298584,-1.136979579925537,-1.7247095108032227,-1.6913959980010986,-1.6898958683013916,-1.7012362480163574,-1.4959101676940918,-1.690699577331543,-1.6711812019348145,-1.7971994876861572,-1.3739159107208252,-1.8232741355895996,-1.7325327396392822,-1.2973231077194214,-1.7738938331604004,-1.7852668762207031,-1.9539239406585693,-1.0151324272155762,-1.8954682350158691,-1.2754486799240112,-1.3932411670684814,-1.690699577331543,-1.690699577331543,-1.6485648155212402,-1.6905555725097656,-1.7318639755249023,-1.9050836563110352,-1.6893067359924316,-1.1479268074035645,-1.7122814655303955,-1.871368408203125,-1.4944002628326416,-1.8658292293548584,-1.7096366882324219,-1.6915156841278076,-1.7429792881011963,-1.8358898162841797,-1.941192388534546,-1.6939034461975098,-1.6903564929962158,-1.6909921169281006,-1.71500563621521,-2.0488929748535156,-1.853792667388916,-1.690490484237671,-1.6998817920684814,-2.0318994522094727,-1.6809399127960205,-1.6254873275756836,-1.690441608428955,-2.014641284942627,-1.6921381950378418,-1.9175782203674316,-1.8314142227172852,-2.0294556617736816,-1.7196192741394043,-1.6603453159332275,-1.5910444259643555,-1.7381172180175781,-1.6905815601348877,-1.6920924186706543,-1.8606672286987305,-1.4993505477905273,-1.841456413269043,-1.71012282371521,-1.6183409690856934,-1.691605567932129,-1.5813970565795898,-1.257908821105957,-2.045058250427246,-1.6898713111877441,-1.9729180335998535,-1.5685648918151855,-1.9570956230163574,-1.99666166305542,-1.6881928443908691,-1.7232186794281006,-1.6925184726715088,-1.8385891914367676,-1.8042948246002197,-1.5760717391967773,-1.69077730178833,-1.614788293838501,-1.6911470890045166,-1.9112248420715332,-1.5279408693313599,-1.6872262954711914,-1.6913483142852783,-1.8008923530578613,-1.8274714946746826,-1.6963648796081543,-1.796454906463623,-1.934004306793213,-1.6893067359924316,-1.7852885723114014,-1.692991018295288,-1.2259438037872314,-1.4738516807556152,-1.676311731338501,-1.6899874210357666,-1.9269375801086426,-1.8749651908874512,-1.9313583374023438,-1.9262769222259521,-1.6851248741149902,-1.6873903274536133,-1.6984171867370605,-1.4410258531570435,-1.6865639686584473,-1.6893067359924316,-1.6910953521728516,-1.266967535018921,-1.6013760566711426,-1.6093599796295166,-2.0194814205169678,-1.6351394653320312,-1.641355276107788,-1.6601784229278564,-1.9251532554626465,-1.6888885498046875,-1.69069504737854,-1.6905512809753418,-1.6952965259552002,-1.5044596195220947,-1.3473503589630127,-1.6903738975524902,-1.6922862529754639,-1.7039144039154053,-1.675184726715088,-1.8799004554748535,-1.50897216796875,-1.6941719055175781,-1.6915099620819092,-1.6936862468719482,-1.6907694339752197,-1.688154697418213,-2.162097930908203,-1.7058124542236328,-1.5897880792617798,-1.6785430908203125,-1.6929893493652344,-1.6879160404205322,-1.689729928970337,-2.0749497413635254,-1.4512861967086792,-1.6726481914520264,-1.6913456916809082,-2.017549991607666,-1.9401977062225342,-2.0089683532714844,-1.7850303649902344,-1.855546474456787,-1.6932661533355713,-1.7315430641174316,-1.6914658546447754,-1.7156567573547363,-1.770512342453003,-1.6952965259552002,-1.586061716079712,-1.6946239471435547,-1.690699577331543,-1.862475872039795,-1.690643072128296,-1.7149055004119873,-1.6705443859100342,-1.5019242763519287,-1.816622018814087,-1.6881928443908691,-1.6918516159057617,-1.6913959980010986,-1.469793677330017,-1.6302950382232666,-1.6565310955047607,-1.6908094882965088,-1.7189521789550781,-1.321951150894165,-2.1087632179260254,-1.6910400390625,-1.7999582290649414,-1.6929805278778076,-1.314814567565918,-2.2006521224975586,-2.274693489074707,-1.6890101432800293,-1.9533839225769043,-2.1881885528564453,-1.8580245971679688,-1.8002896308898926,-1.6881928443908691,-1.9313359260559082,-1.3993773460388184,-1.690777063369751,-1.6955018043518066,-1.6881928443908691,-1.6908886432647705,-1.886970043182373,-1.7665371894836426,-1.6824486255645752,-1.7424259185791016,-1.6611034870147705,-1.6974105834960938,-1.6704943180084229,-1.712897539138794,-1.6953668594360352,-1.5890703201293945,-1.8680815696716309,-1.6923394203186035,-1.7844676971435547,-1.6923308372497559,-2.0543084144592285,-1.713707447052002,-1.6923820972442627,-1.6833393573760986,-1.7612016201019287,-1.6907694339752197,-1.7542009353637695,-1.700835943222046,-1.6914355754852295,-1.3306455612182617,-1.8098902702331543,-1.698133945465088,-1.2399280071258545,-1.7247047424316406,-2.149944305419922,-1.4950459003448486,-1.690317153930664,-1.9554648399353027,-1.2808685302734375,-1.7021934986114502,-1.6907694339752197,-1.692216396331787,-1.6281545162200928,-1.6505844593048096,-1.690490484237671,-1.7132415771484375,-1.9839091300964355,-1.7269823551177979,-1.6871216297149658,-1.6914658546447754,-1.690490484237671,-1.387050747871399,-1.7546215057373047,-1.700331687927246,-1.7771859169006348,-1.6175868511199951,-1.695518970489502,-1.6616737842559814,-1.7505674362182617,-1.7303423881530762,-1.6888885498046875,-1.6952853202819824,-1.9761419296264648,-1.9729254245758057,-1.6897006034851074,-1.6866998672485352,-1.4662163257598877,-1.6355326175689697,-1.7100920677185059,-1.778231143951416,-1.5130870342254639,-1.6680684089660645,-1.8851523399353027,-1.6898436546325684,-1.9381022453308105,-1.5644500255584717,-1.6952965259552002,-1.690706491470337,-1.6897892951965332,-1.6920228004455566,-1.5863701105117798,-1.9532756805419922,-1.3334252834320068,-2.0770254135131836,-1.7805514335632324,-1.6921346187591553,-1.7690458297729492,-1.7032110691070557,-1.513254165649414,-1.5617284774780273,-1.6889939308166504,-1.4326527118682861,-1.6458806991577148,-1.8566184043884277,-1.8397741317749023,-1.690699577331543,-1.6177363395690918,-1.6916429996490479,-1.7264370918273926,-1.6888885498046875,-1.9732961654663086,-1.8302297592163086,-1.690427541732788,-1.6909241676330566,-1.6986312866210938,-1.7661998271942139,-1.6902704238891602,-1.6892971992492676,-1.7982215881347656,-1.8035285472869873,-1.6893067359924316,-1.6780028343200684,-1.6907694339752197,-1.56282377243042,-1.6909921169281006,-2.3580942153930664,-1.6815600395202637,-1.6913959980010986,-1.7202906608581543,-1.7071921825408936,-1.7429790496826172,-1.870445728302002,-1.6893067359924316,-1.8117754459381104,-1.6942780017852783,-1.6913959980010986,-1.9419288635253906,-1.8047866821289062,-1.7567191123962402,-1.6914658546447754,-1.6412379741668701,-1.8169348239898682,-1.690490484237671,-1.8225154876708984,-2.1277599334716797,-1.8109025955200195,-1.7774412631988525,-1.6892204284667969,-1.6962714195251465,-1.6841156482696533,-1.6903276443481445,-1.6542952060699463,-1.7047011852264404,-1.7201194763183594,-1.5522782802581787,-1.6909816265106201,-1.6893067359924316,-1.2723356485366821,-1.6886427402496338,-1.561213731765747,-1.6893067359924316,-1.5796031951904297,-1.6904799938201904,-1.9871060848236084,-1.6605212688446045,-1.8231747150421143,-1.629915475845337,-1.8832006454467773,-1.9074597358703613,-1.5693542957305908,-1.6864910125732422,-1.6902873516082764,-1.7227139472961426,-1.6972692012786865,-1.9932775497436523,-1.8070335388183594,-1.6525602340698242,-1.6848692893981934,-1.6906383037567139,-1.6893067359924316,-1.6904845237731934,-1.679286241531372,-1.690699577331543,-1.692115306854248,-1.761756420135498,-1.690490484237671,-1.6184067726135254,-1.5189424753189087,-1.6888885498046875,-1.6803765296936035,-1.6893067359924316,-1.6992871761322021,-1.6068365573883057,-1.690699577331543,-2.2734546661376953],\"z\":[-0.2110031545162201,-0.3385704457759857,-0.3773805797100067,-0.25989794731140137,-0.29728856682777405,-0.3153965175151825,-0.29262611269950867,-0.15305697917938232,-0.25147560238838196,-0.238185852766037,-0.2512342631816864,-0.25147560238838196,-0.5365580320358276,0.13750532269477844,-0.24816370010375977,-0.2639066278934479,-0.3026711642742157,-0.3243803083896637,-0.25121599435806274,-0.2949378192424774,-0.238185852766037,-0.2525363564491272,-0.3447001874446869,-0.20091107487678528,-0.16362634301185608,-0.25147560238838196,-0.25013479590415955,-0.2513059079647064,-0.05175304412841797,-0.3698593080043793,-0.5915254354476929,-0.21919658780097961,-0.25147560238838196,-0.22141841053962708,-0.25725558400154114,-0.03835374116897583,-0.1814996302127838,-0.23244065046310425,-0.32583871483802795,-0.2516394257545471,-0.06537002325057983,-0.4088456332683563,-0.22284650802612305,-0.12341350317001343,-0.28115537762641907,-0.08860573172569275,-0.22489777207374573,-0.25207415223121643,-0.23627513647079468,-0.25128352642059326,-0.22053596377372742,-0.35425105690956116,-0.28631678223609924,-0.15220561623573303,-0.26082971692085266,-0.17878353595733643,-0.2519092559814453,-0.25401899218559265,-0.251960813999176,-0.25278663635253906,-0.25152984261512756,-0.2513493597507477,-0.2522708475589752,-0.2533740699291229,-0.2532222270965576,-0.06672978401184082,-0.2739475667476654,-0.203629732131958,-0.19261935353279114,-0.23108065128326416,-0.24363651871681213,-0.2213994860649109,0.06247352063655853,0.0033383965492248535,-0.2525300085544586,-0.2509715259075165,-0.2543558180332184,-0.10426497459411621,-0.48606380820274353,-0.33765098452568054,-0.25168952345848083,-0.2280195653438568,-0.22884324193000793,-0.25121599435806274,-0.39818015694618225,0.04329083859920502,-0.251290887594223,-0.5465348958969116,-0.006106197834014893,-0.3839206397533417,-0.2500813901424408,-0.09889891743659973,-0.25377157330513,-0.26641860604286194,-0.36793336272239685,-0.2817050516605377,-0.2525300085544586,-0.18681296706199646,-0.15316703915596008,-0.3616611063480377,-0.07861843705177307,-0.24665114283561707,-0.05706736445426941,-0.24895954132080078,-0.25279709696769714,-0.4123665988445282,-0.3148416578769684,-0.4490971863269806,-0.25169363617897034,-0.35259220004081726,-0.23263362050056458,-0.2515912652015686,-0.2520662844181061,-0.2763974368572235,-0.25090357661247253,-0.2523234784603119,-0.2514921724796295,-0.26132819056510925,-0.25232577323913574,-0.2525300085544586,-0.05717748403549194,-0.3406854569911957,-0.2516079843044281,-0.25121599435806274,-0.2515818774700165,-0.22924083471298218,-0.24762389063835144,-0.21412310004234314,-0.20943686366081238,-0.25147560238838196,-0.22504660487174988,-0.251284122467041,-0.22816237807273865,-0.2645898759365082,-0.24110910296440125,-0.2513505220413208,-0.4789271056652069,-0.20847401022911072,-0.25165510177612305,-0.25120246410369873,-0.2658199965953827,-0.25173160433769226,-0.39984163641929626,-0.1702040433883667,-0.49612340331077576,-0.39275291562080383,-0.25133833289146423,-0.25121599435806274,-0.1617109775543213,-0.49006387591362,-0.26466163992881775,-0.2509715259075165,-0.2520384192466736,-0.2521294355392456,-0.2509715259075165,-0.15952804684638977,-0.25151482224464417,-0.2482224404811859,-0.17780864238739014,-0.25136858224868774,-0.25170692801475525,-0.2518369257450104,-0.12948623299598694,-0.32091835141181946,-0.21088680624961853,-0.25112420320510864,-0.43505558371543884,-0.2535494863986969,-0.22449013590812683,-0.010971873998641968,-0.3677370846271515,-0.15133291482925415,-0.252043753862381,-0.33864709734916687,-0.2517322897911072,-0.5031546354293823,-0.38580355048179626,-0.25135400891304016,-0.2684043347835541,-0.2517814338207245,-0.09043148159980774,-0.2525849938392639,-0.2697075307369232,-0.2698831260204315,-0.2743876874446869,-0.24164143204689026,-0.25260329246520996,-0.5036869049072266,-0.32526180148124695,-0.2517814338207245,-0.24849531054496765,0.039289504289627075,-0.2520013451576233,-0.3157327473163605,-0.2519488036632538,-0.2587330639362335,-0.4457893669605255,-0.2506824731826782,-0.21502527594566345,-0.30421456694602966,-0.166233628988266,-0.19336876273155212,-0.2266678810119629,-0.246730238199234,-0.2135058045387268,-0.2814655601978302,-0.263760507106781,-0.4030139744281769,-0.25148677825927734,-0.3232131898403168,-0.2519713044166565,-0.09536674618721008,-0.32911285758018494,-0.2506423890590668,-0.25213655829429626,-0.368190735578537,-0.25138217210769653,-0.4449577033519745,-0.25121599435806274,-0.238185852766037,-0.251569002866745,-0.23572194576263428,-0.25142350792884827,-0.2516193389892578,-0.28859254717826843,-0.31200966238975525,-0.3138595521450043,-0.22208324074745178,-0.39672866463661194,-0.18519577383995056,-0.2081986665725708,-0.25026917457580566,-0.25151577591896057,-0.24347874522209167,-0.17295822501182556,-0.2924710810184479,-0.12579038739204407,-0.2572149634361267,-0.25121599435806274,-0.25087860226631165,-0.2260173261165619,-0.2606689929962158,-0.2986237704753876,-0.29335853457450867,-0.25189709663391113,-0.2515909969806671,-0.1316814124584198,0.21678830683231354,-0.25103476643562317,-0.2521975040435791,-0.2533572018146515,-0.2516704499721527,-0.2509715259075165,-0.2518579661846161,-0.2516709566116333,-0.1565748155117035,-0.2509715259075165,-0.2848961055278778,-0.23560842871665955,-0.3041127622127533,-0.19793564081192017,-0.2509715259075165,0.1741219162940979,-0.16535186767578125,-0.26385921239852905,-0.018946021795272827,-0.25122806429862976,-0.4117065966129303,-0.17747604846954346,-0.4051552712917328,-0.2289058268070221,-0.24688169360160828,-0.2517722249031067,-0.238185852766037,-0.25121599435806274,-0.30069974064826965,-0.28626111149787903,-0.2515026032924652,-0.25121599435806274,-0.2505113184452057,-0.15878155827522278,-0.24478456377983093,-0.2642153203487396,-0.35200169682502747,-0.25167450308799744,-0.25306612253189087,-0.4093308746814728,-0.2556550204753876,-0.23093929886817932,-0.2613264322280884,-0.25656434893608093,-0.2525300085544586,-0.24921056628227234,-0.2609594166278839,-0.25167450308799744,-0.2544207274913788,-0.22173890471458435,-0.5072746276855469,-0.24141034483909607,-0.25170934200286865,-0.24602282047271729,-0.2741682827472687,-0.11494731903076172,-0.251120388507843,-0.406537264585495,-0.19938227534294128,-0.25121599435806274,-0.22102266550064087,-0.25242024660110474,-0.22824883460998535,-0.2867870628833771,-0.05391961336135864,-0.20080754160881042,-0.3382458984851837,-0.23536717891693115,-0.41955921053886414,-0.22398722171783447,-0.28627821803092957,-0.21227863430976868,-0.3371024429798126,-0.25121599435806274,-0.16016995906829834,-0.2900867760181427,-0.20918068289756775,-0.229363352060318,-0.4577729403972626,-0.2519519329071045,-0.25158578157424927,-0.25121599435806274,-0.2523852288722992,-0.23717358708381653,-0.25121599435806274,-0.25184366106987,-0.25871264934539795,-0.2278839647769928,-0.28401800990104675,-0.1407359540462494,-0.4740166962146759,-0.0672045648097992,-0.3022666871547699,-0.18125692009925842,-0.27265849709510803,-0.25954708456993103,-0.24705597758293152,-0.2568296492099762,-0.2408313751220703,-0.061754435300827026,-0.1262476146221161,-0.24947711825370789,-0.32862648367881775,-0.34005388617515564,-0.3477095067501068,-0.1482924222946167,-0.25372835993766785,0.03736361861228943,-0.2689690887928009,-0.3312291204929352,-0.19716030359268188,-0.08826032280921936,-0.39157888293266296,-0.2515215277671814,-0.2509715259075165,-0.25118595361709595,-0.25133833289146423,-0.2525477707386017,-0.25237807631492615,-0.2998903691768646,-0.25174713134765625,-0.18697044253349304,-0.2513563334941864,-0.12053418159484863,0.007078617811203003,-0.2507047951221466,-0.19065961241722107,-0.1657526195049286,-0.25147560238838196,-0.5748205184936523,-0.19668233394622803,-0.18872478604316711,-0.20985260605812073,-0.24586626887321472,-0.19071713089942932,-0.25151529908180237,-0.25108787417411804,-0.24276626110076904,-0.48130103945732117,-0.26075419783592224,-0.2521146237850189,-0.4670570194721222,-0.2519066333770752,-0.1976316273212433,-0.2940751016139984,-0.26628127694129944,-0.2518412470817566,-0.2509715259075165,-0.2517870366573334,-0.2516558766365051,-0.2502593398094177,-0.3946886956691742,-0.21271273493766785,-0.21147069334983826,-0.1920822262763977,-0.23534074425697327,-0.25205400586128235,-0.25341519713401794,-0.2517246901988983,-0.17847570776939392,-0.24052298069000244,-0.1585027277469635,-0.17507889866828918,-0.2919212281703949,-0.4694425165653229,-0.2521268129348755,-0.2566381096839905,-0.29048672318458557,-0.2524014711380005,-0.25642380118370056,-0.2486306130886078,-0.24719640612602234,-0.4513692557811737,-0.2579077184200287,-0.250916451215744,-0.26037663221359253,-0.0039657652378082275,-0.249984472990036,-0.25121599435806274,-0.4684946835041046,-0.12549862265586853,-0.2643669545650482,-0.1665753722190857,-0.2517447769641876,-0.25153687596321106,-0.16329115629196167,-0.34911003708839417,-0.21657690405845642,-0.2414623200893402,-0.2900218069553375,-0.2517522871494293,-0.2550073564052582,-0.29002299904823303,-0.3973762094974518,-0.29778704047203064,-0.18955358862876892,-0.1594882309436798,-0.49444642663002014,-0.25147560238838196,-0.6094894409179688,-0.17368853092193604,-0.25041553378105164,-0.20795810222625732,-0.36568477749824524,-0.37499335408210754,-0.25121599435806274,-0.2593211531639099,-0.10046443343162537,-0.2537079155445099,-0.18959003686904907,-0.2680598199367523,-0.251327782869339,-0.38625749945640564,-0.16658833622932434,-0.25162044167518616,-0.2516598403453827,-0.24924951791763306,-0.2521218955516815,-0.18105527758598328,-0.2826431691646576,-0.2694362699985504,-0.2524510324001312,-0.2516866624355316,-0.25236544013023376,-0.24799874424934387,-0.2525300085544586,-0.25643667578697205,-0.2518903315067291,-0.1195279061794281,-0.2509715259075165,-0.23517796397209167,-0.35250845551490784,-0.2516555190086365,-0.25181177258491516,-0.2091343104839325,-0.30028244853019714,-0.11118859052658081,-0.2511191964149475,-0.2500764727592468,-0.31517669558525085,-0.3331478536128998,-0.3338230550289154,-0.35495540499687195,-0.2518801689147949,-0.2232041358947754,-0.2611766755580902,-0.23724833130836487,-0.22566643357276917,-0.3456434905529022,-0.2517258822917938,-0.16764125227928162,-0.23819652199745178,-0.24559247493743896,-0.25189366936683655,-0.25186070799827576,-0.2698260247707367,-0.21701088547706604,-0.2509459853172302,-0.39137765765190125,-0.25176405906677246,-0.2512805759906769,-0.21433082222938538,-0.3228578269481659,-0.2523718774318695,-0.5645625591278076,-0.25272250175476074,-0.2707858979701996,-0.2517341673374176,-0.3085344731807709,-0.251724511384964,-0.2534432113170624,-0.25121599435806274,-0.2509715259075165,-0.2515215277671814,-0.18480318784713745,-0.25156983733177185,-0.28614094853401184,-0.2594100534915924,-0.25173112750053406,-0.27782902121543884,-0.273191899061203,-0.25230565667152405,-0.015736788511276245,-0.3153909742832184,-0.39088812470436096,-0.2514664828777313,-0.1797599494457245,-0.35248854756355286,-0.2515215277671814,-0.2513933777809143,-0.21079519391059875,-0.29610851407051086,-0.23372429609298706,-0.13263750076293945,-0.28289738297462463,-0.238185852766037,-0.2516343891620636,-0.3134787976741791,-0.19008618593215942,-0.23867878317832947,0.21384304761886597,-0.39243099093437195,-0.2523849904537201,-0.2568538784980774,-0.2525300085544586,-0.2458820939064026,-0.22762241959571838,-0.28178539872169495,-0.14162087440490723,-0.2511264979839325,-0.5449091196060181,-0.238185852766037,-0.13199210166931152,-0.25179997086524963,-0.24649035930633545,-0.06328189373016357,-0.2509361207485199,-0.2766833007335663,-0.2556382119655609,-0.5631308555603027,-0.25190451741218567,-0.2889498174190521,-0.20467764139175415,-0.09059962630271912,-0.20321598649024963,-0.2521117627620697,-0.25206464529037476,-0.229998379945755,-0.2515215277671814,-0.2591211497783661,-0.3109792172908783,0.07068635523319244,-0.44391128420829773,-0.20581379532814026,-0.5065393447875977,-0.2517828047275543,-0.2519555985927582,-0.25121599435806274,-0.2128637433052063,-0.2685210406780243,-0.31645503640174866,-0.05125108361244202,-0.25121599435806274,-0.3896048367023468,-0.05482789874076843,-0.031602293252944946,-0.4713902175426483,-0.2809865176677704,-0.2529695928096771,0.002895861864089966,-0.252715140581131,-0.35418573021888733,-0.5322295427322388,-0.25147560238838196,-0.02995702624320984,-0.2158612310886383,-0.2752983272075653,-0.3697427809238434,-0.472991019487381,-0.23418918251991272,-0.2624608874320984,-0.2513512670993805,-0.25101238489151,-0.49583765864372253,-0.25248822569847107,-0.2515215277671814,-0.25168952345848083,-0.24641960859298706,-0.33527418971061707,-0.2525300085544586,-0.2637772262096405,-0.3746322691440582,-0.2565899193286896,-0.34175196290016174,-0.2517814338207245,-0.2510569393634796,-0.3953773081302643,0.054002806544303894,-0.27520886063575745,-0.25153687596321106,-0.25160709023475647,-0.3387758433818817,-0.4322719871997833,-0.25121888518333435,-0.26953843235969543,-0.3729029595851898,-0.3509819805622101,-0.039590150117874146,-0.42178937792778015,-0.25538718700408936,-0.3165428340435028,-0.25014573335647583,-0.1861627697944641,-0.38962671160697937,-0.2457835078239441,-0.2514376640319824,-0.18714141845703125,-0.25220775604248047,-0.25112420320510864,-0.2111799716949463,-0.24406954646110535,-0.25147560238838196,-0.25167450308799744,-0.2437436282634735,-0.20754212141036987,-0.11980587244033813,-0.3134095370769501,-0.2509715259075165,-0.23692399263381958,-0.32848331332206726,-0.24089637398719788,-0.241685152053833,-0.23968079686164856,-0.1827302873134613,-0.31093236804008484,-0.46078750491142273,-0.2999025285243988,-0.25116708874702454,-0.21353408694267273,-0.25189587473869324,-0.02477961778640747,-0.08645060658454895,-0.3201340138912201,-0.089082270860672,-0.18394044041633606,-0.2646726667881012,-0.25121599435806274,-0.24408015608787537,-0.17616483569145203,-0.25151899456977844,-0.25168952345848083,-0.19803765416145325,-0.3563719689846039,-0.25158578157424927,-0.1876446008682251,-0.4579009711742401,-0.2460048794746399,-0.4195290505886078,-0.25182148814201355,-0.24216273427009583,-0.25290966033935547,-0.2093220353126526,-0.3191087543964386,-0.34628698229789734,-0.25154951214790344,-0.061242103576660156,-0.2526204288005829,-0.30602213740348816,-0.23123779892921448,-0.2510983943939209,-0.3297725021839142,-0.33610108494758606,-0.2582651674747467,-0.5112652778625488,-0.16178932785987854,-0.45206937193870544,-0.25159570574760437,-0.3581613600254059,-0.2571977972984314,-0.2511838972568512,-0.218840092420578,-0.2607758939266205,-0.06733962893486023,-0.2375735640525818,-0.49919936060905457,-0.25121599435806274,-0.4891573488712311,-0.19028857350349426,-0.1911480724811554,-0.2527274191379547,-0.14055147767066956,-0.23244023323059082,-0.2746843993663788,-0.33200356364250183,-0.2518680691719055,-0.26352742314338684,-0.2515586316585541,-0.2272961437702179,-0.2523953914642334,-0.35608431696891785,-0.23541447520256042,-0.3587982952594757,-0.3017165958881378,-0.29792705178260803,-0.11991250514984131,-0.15709581971168518,-0.25147560238838196,-0.2520473301410675,-0.25207433104515076,-0.26902297139167786,-0.25920888781547546,-0.2515215277671814,-0.2624565660953522,-0.2570285201072693,-0.23136085271835327,-0.22544866800308228,-0.2514718472957611,-0.149632066488266,-0.2857612669467926,-0.319073349237442,-0.27194127440452576,-0.20512855052947998,-0.27753058075904846,-0.2523389756679535,-0.2702305018901825,-0.25147560238838196,-0.3460257947444916,-0.3696160614490509,-0.2173667848110199,-0.23603153228759766,-0.13814818859100342,-0.2517378628253937,-0.09758594632148743,-0.2517288029193878,-0.19082313776016235,-0.24847057461738586,-0.2527806758880615,-0.25950488448143005,-0.2520393431186676,-0.45620134472846985,-0.4033553898334503,-0.25263047218322754,-0.27530184388160706,-0.2511068284511566,-0.31925246119499207,-0.28013375401496887,-0.25204557180404663,-0.2515215277671814,-0.23876342177391052,-0.24752148985862732,-0.2519669234752655,-0.26129162311553955,-0.250751256942749,-0.2516148090362549,-0.25181177258491516,-0.25751855969429016,-0.2509715259075165,-0.30420663952827454,-0.2783432900905609,-0.19440308213233948,-0.2594458758831024,-0.18412980437278748,-0.33526811003685,-0.21740096807479858,-0.25160953402519226,-0.3006148040294647,-0.12875589728355408,-0.35968759655952454,-0.2619803249835968,-0.25168952345848083,-0.4111400544643402,-0.2963567078113556,-0.34262439608573914,-0.16429662704467773,-0.30333516001701355,-0.42323794960975647,-0.25133833289146423,-0.340638667345047,-0.14212149381637573,-0.3074668347835541,-0.27151814103126526,-0.2520395815372467,-0.2508384883403778,-0.3497842252254486,-0.2510114014148712,-0.23770925402641296,-0.25185176730155945,-0.18687883019447327,-0.2509715259075165,-0.25222429633140564,-0.25167450308799744,-0.25147560238838196,-0.14181050658226013,-0.331350713968277,-0.3415689170360565,-0.33190128207206726,-0.21348854899406433,-0.13988646864891052,-0.1781826615333557,-0.23946824669837952,-0.3177283704280853,-0.23246708512306213,-0.3568526804447174,-0.2516660690307617,-0.24014908075332642,-0.2320420742034912,-0.2515215277671814,-0.40631744265556335,-0.25161072611808777,-0.251335084438324,-0.05659559369087219,-0.2096664309501648,0.06106403470039368,-0.251985102891922,-0.25247520208358765,-0.4877040684223175,-0.25569796562194824,-0.3378712832927704,-0.280597060918808,-0.29661741852760315,-0.37914809584617615,-0.2405855357646942,-0.3873460590839386,-0.4385557472705841,0.004892662167549133,-0.12483653426170349,-0.238185852766037,-0.3301084339618683,-0.22816261649131775,-0.44945213198661804,-0.22214338183403015,-0.43066516518592834,-0.18163415789604187,-0.23821625113487244,-0.2517297565937042,-0.33651110529899597,-0.3785952627658844,-0.3622485101222992,-0.4068010747432709,-0.1816699206829071,-0.2516915202140808,-0.2385784089565277,-0.2553318440914154,-0.13657820224761963,-0.028314054012298584,-0.1966230869293213,-0.251543790102005,-0.3410719931125641,-0.24973928928375244,-0.5335919857025146,-0.304641991853714,-0.2516023516654968,-0.13934928178787231,-0.25147560238838196,-0.251827210187912,-0.25147560238838196,-0.3412928283214569,-0.14399364590644836,-0.25159358978271484,-0.20545661449432373,-0.21184518933296204,-0.2511845529079437,-0.1310526430606842,-0.23405200242996216,-0.18615767359733582,-0.2513209283351898,-0.26035386323928833,-0.46123823523521423,-0.2402665615081787,-0.25164663791656494,-0.1338053047657013,-0.25169888138771057,-0.2663101255893707,-0.25164076685905457,-0.25121599435806274,-0.5725424289703369,-0.2509715259075165,-0.2525300085544586,-0.30064281821250916,-0.37120869755744934,-0.14526566863059998,-0.238185852766037,-0.25121599435806274,-0.2508682310581207,-0.18820276856422424,-0.23712113499641418,-0.2612673044204712,-0.2510118782520294,-0.2669413983821869,-0.4930422604084015,-0.14373168349266052,-0.11679455637931824,-0.2517814338207245,-0.16110685467720032,0.011690735816955566,-0.2336411476135254,-0.250946968793869,-0.2655623257160187,-0.1616905927658081,-0.33529481291770935,-0.17470887303352356,-0.33898451924324036,-0.23073679208755493,-0.13387098908424377,-0.3043540418148041,-0.2360961139202118,-0.2759012281894684,-0.16504144668579102,-0.21150103211402893,-0.38772639632225037,-0.25147560238838196,-0.17953860759735107,-0.20851799845695496,-0.2506651282310486,-0.25181183218955994,-0.225974440574646,-0.36596164107322693,-0.16130533814430237,-0.2545377314090729,-0.2171105444431305,-0.251827210187912,-0.30153360962867737,-0.2515215277671814,-0.34710124135017395,-0.3261044919490814,-0.2024795114994049,-0.22201478481292725,-0.25121599435806274,-0.1575881838798523,-0.29597458243370056,-0.35047659277915955,-0.25176647305488586,-0.23819640278816223,-0.20342448353767395,-0.2178249955177307,-0.2514481842517853,-0.2897217571735382,-0.5619858503341675,-0.25155189633369446,-0.41442862153053284,0.026071131229400635,-0.2485315501689911,-0.396421879529953,-0.2509715259075165,-0.27413544058799744,-0.2525300085544586,-0.245817631483078,-0.03658053278923035,-0.1796344816684723,-0.2962395250797272,-0.26369309425354004,-0.3000020682811737,-0.25158578157424927,-0.2512049973011017,-0.2524237632751465,-0.2275380790233612,-0.25155648589134216,-0.24585336446762085,-0.25917568802833557,-0.2515215277671814,-0.34190019965171814,-0.2839227020740509,-0.2528392970561981,-0.23292145133018494,-0.283759742975235,-0.2733146846294403,-0.23781219124794006,-0.2512158453464508,-0.25272563099861145,-0.3115404546260834,-0.2515709102153778,-0.2514903247356415,-0.2574000954627991,-0.4945624768733978,-0.26323339343070984,-0.25137925148010254,-0.267474502325058,-0.3109440505504608,-0.25133833289146423,-0.5537421703338623,-0.31708529591560364,-0.3243216574192047,-0.12701085209846497,-0.2513373792171478,-0.26600131392478943,-0.2645474970340729,-0.24455180764198303,-0.23519396781921387,-0.25147560238838196,-0.1463647484779358,-0.16669496893882751,-0.2808345854282379,-0.27710971236228943,-0.25147560238838196,-0.2588638365268707,-0.33620378375053406,-0.29874858260154724,-0.33171364665031433,-0.2517814338207245,-0.2282801866531372,-0.25158578157424927,-0.2510959208011627,-0.24038541316986084,-0.26407429575920105,-0.2523703873157501,-0.17098942399024963,-0.24952483177185059,-0.29026439785957336,-0.238185852766037,-0.11357671022415161,-0.25112420320510864,-0.34636691212654114,-0.2757779061794281,-0.1721419394016266,-0.114522784948349,-0.35670122504234314,-0.3011496365070343,-0.5144981145858765,-0.3084946572780609,-0.28868570923805237,-0.20384946465492249,-0.2175513207912445,-0.12511485815048218,-0.17991900444030762,-0.3060050904750824,-0.26972392201423645,-0.35432401299476624,-0.2514203190803528,-0.24373501539230347,-0.23968133330345154,-0.14656096696853638,-0.26893433928489685,-0.3251897394657135,-0.15317001938819885,-0.2509715259075165,-0.25615450739860535,-0.2514222264289856,-0.251698762178421,-0.2513851821422577,-0.065924733877182,-0.13099607825279236,-0.2374705970287323,-0.25128987431526184,-0.010809451341629028,-0.23951271176338196,-0.08177882432937622,-0.19466426968574524,-0.2522963881492615,-0.18907979130744934,-0.2295272946357727,-0.25083714723587036,-0.25131091475486755,-0.38049158453941345,-0.25099095702171326,-0.019620388746261597,-0.25202256441116333,-0.25121599435806274,-0.177021324634552,-0.23351678252220154,-0.2511451840400696,-0.1652047336101532,-0.010725289583206177,-0.31670472025871277,-0.2508474886417389,-0.31683680415153503,-0.2315210998058319,-0.22467923164367676,-0.2277909219264984,-0.2517814338207245,-0.2739057242870331,-0.25133833289146423,-0.2853001654148102,-0.41993412375450134,-0.25166404247283936,-0.09501910209655762,-0.16094377636909485,-0.21992525458335876,-0.2526472508907318,-0.17667797207832336,-0.4768354594707489,-0.13811713457107544,-0.23625844717025757,-0.2534908950328827,-0.22172823548316956,-0.23353534936904907,-0.3901340067386627,-0.11842897534370422,-0.018546104431152344,-0.37433454394340515,-0.11931031942367554,-0.25160470604896545,-0.25147560238838196,-0.15817633271217346,-0.41749271750450134,-0.2658107578754425,-0.40016111731529236,-0.467669278383255,-0.2602975070476532,-0.23677018284797668,-0.011140868067741394,-0.3298099935054779,-0.18409770727157593,-0.23928746581077576,0.19539280235767365,-0.11799317598342896,-0.2515215277671814,-0.2601834833621979,-0.16449090838432312,-0.2514308989048004,-0.28925326466560364,-0.2517591714859009,-0.2673451602458954,-0.3940005600452423,-0.12639662623405457,-0.27881965041160583,-0.2843015491962433,-0.09159919619560242,-0.3691078722476959,-0.3680770695209503,-0.2521727979183197,-0.425576776266098,-0.384525865316391,-0.26034027338027954,-0.3722390830516815,-0.2514600455760956,-0.33170285820961,-0.04774913191795349,-0.25113362073898315,-0.2312922179698944,-0.28126439452171326,-0.16395840048789978,-0.5546038150787354,-0.25096312165260315,-0.20647981762886047,-0.23897910118103027,-0.10730487108230591,-0.2633901536464691,-0.25168952345848083,-0.2608516812324524,-0.1813371479511261,-0.4765831530094147,-0.286181777715683,-0.29786762595176697,-0.25158578157424927,-0.2527492344379425,-0.25147560238838196,-0.19333234429359436,-0.07983753085136414,-0.25158578157424927,-0.2605561912059784,-0.35991570353507996,-0.18158528208732605,-0.32768771052360535,-0.2513953149318695,-0.29716774821281433,-0.29493698477745056,-0.2452622950077057,-0.4713048040866852,-0.25181177258491516,-0.2509715259075165,-0.37454983592033386,-0.2495580017566681,-0.27216044068336487,-0.19113650918006897,-0.29210880398750305,-0.26155272126197815,-0.2515215277671814,-0.2772035300731659,-0.25177857279777527,-0.19630217552185059,-0.303750604391098,-0.4404432475566864,-0.3805854022502899,-0.3676917850971222,-0.38662102818489075,-0.23231473565101624,-0.251827210187912,-0.3343662917613983,-0.3373464047908783,-0.25402647256851196,-0.25296711921691895,-0.47297176718711853,-0.23019108176231384,-0.28223660588264465,-0.4991019070148468,-0.4133455455303192,-0.24490702152252197,-0.25147560238838196,-0.2516763508319855,-0.3172192871570587,-0.14567050337791443,-0.23497918248176575,-0.46446433663368225,-0.11799255013465881,-0.49066415429115295,-0.2908397614955902,-0.35776326060295105,-0.3032821714878082,-0.3304552733898163,-0.24887245893478394,-0.1834147572517395,-0.2953076660633087,-0.25133833289146423,-0.2640931308269501,-0.44083699584007263,-0.19919881224632263,-0.2024039924144745,-0.25222429633140564,-0.24000200629234314,-0.2629944384098053,-0.28356286883354187,-0.21604856848716736,-0.1764119565486908,-0.42815205454826355,-0.22857394814491272,-0.3478398621082306,-0.2519233524799347,-0.2511340379714966,-0.13440394401550293,-0.25174564123153687,-0.27501246333122253,-0.13205930590629578,-0.418453186750412,-0.2525344789028168,-0.48899176716804504,-0.2521466910839081,-0.39709898829460144,-0.25158578157424927,-0.2509715259075165,-0.2519614100456238,-0.38715633749961853,-0.22981280088424683,-0.2516957223415375,-0.2516697645187378,-0.25133833289146423,-0.2518174946308136,-0.2789294421672821,-0.10874864459037781,-0.23415923118591309,-0.25211194157600403,-0.15216964483261108,-0.1688174605369568,-0.27298638224601746,-0.2520278990268707,-0.4584555923938751,-0.2510800361633301,-0.17780104279518127,-0.1147245466709137,-0.22645995020866394,-0.30229803919792175,-0.1703406572341919,-0.2842535674571991,-0.25121599435806274,-0.2522166073322296,-0.25158578157424927,-0.37674036622047424,-0.25134190917015076,-0.2482815980911255,-0.2863146960735321,-0.251882404088974,-0.18725493550300598,-0.4305969774723053,-0.251827210187912,-0.256941556930542,-0.24638810753822327,-0.042345672845840454,-0.5057569742202759,-0.28327861428260803,-0.2794831693172455,-0.25187286734580994,-0.2517814338207245,-0.19547805190086365,-0.34780076146125793,-0.25220900774002075,-0.41998448967933655,-0.1442807912826538,-0.24617719650268555,-0.4757784903049469,-0.16581767797470093,-0.1585814654827118,-0.2428550124168396,-0.2682196795940399,-0.30802884697914124,-0.26636233925819397,-0.2521044611930847,-0.504745364189148,-0.25133833289146423,-0.4994031488895416,-0.35257068276405334,-0.24457812309265137,-0.05347791314125061,-0.2515215277671814,-0.252210408449173,-0.18386155366897583,-0.2110784351825714,-0.2278461456298828,-0.36667606234550476,-0.2298511564731598,-0.20246300101280212,-0.36450061202049255,-0.08213528990745544,-0.2432141900062561,-0.24847394227981567,-0.28352734446525574,-0.18604353070259094,0.03420872986316681,-0.25168952345848083,-0.25121599435806274,-0.5183779001235962,-0.3493225872516632,-0.24020785093307495,-0.2525300085544586,-0.2771358788013458,-0.25092434883117676,-0.27810290455818176,-0.38318148255348206,-0.25218987464904785,-0.25122496485710144,-0.18661659955978394,-0.17546486854553223,-0.19940689206123352,-0.25189009308815,-0.40720900893211365,-0.2249472737312317,-0.5493971109390259,-0.25184524059295654,-0.2521493136882782,-0.2516510784626007,-0.24697351455688477,-0.3070596754550934,-0.2513217628002167,-0.25175198912620544,-0.25391754508018494,-0.10663393139839172,-0.2520918846130371,-0.24666890501976013,-0.19236183166503906,-0.25134822726249695,-0.30881592631340027,-0.2515215277671814,-0.2509715259075165,-0.17262700200080872,-0.19017592072486877,-0.2438216507434845,-0.4200122058391571,-0.251827210187912,-0.25729086995124817,-0.2524239122867584,-0.25158578157424927,-0.10682916641235352,-0.2531132698059082,-0.25277382135391235,-0.25039055943489075,-0.2495156228542328,-0.25121599435806274,-0.2502526342868805,-0.25153687596321106,-0.16053596138954163,-0.25728514790534973,-0.05111387372016907,-0.2683824598789215,-0.31387338042259216,-0.252503365278244,-0.2639327943325043,-0.25159382820129395,-0.25182029604911804,-0.25695687532424927,-0.2687890827655792,-0.2517814338207245,0.11821544170379639,-0.25121599435806274,-0.2510082721710205,-0.2807214558124542,-0.2515215277671814,-0.2615232765674591,-0.2643371522426605,-0.23466944694519043,-0.0477147102355957,-0.2819935977458954,-0.2630673050880432,-0.32081201672554016,-0.1632898449897766,-0.2505451440811157,-0.022286564111709595,-0.26024413108825684,-0.20523253083229065,-0.4419890344142914,-0.2508038580417633,-0.20574873685836792,-0.292524129152298,-0.2516980469226837,-0.2510349452495575,-0.22224682569503784,-0.2731185853481293,-0.25165608525276184,-0.19060727953910828,-0.23303082585334778,-0.2525300085544586,-0.2523309588432312,-0.2145792841911316,-0.16273677349090576,-0.26704004406929016,-0.251234769821167,-0.25147560238838196,-0.5660033226013184,-0.22360488772392273,-0.25121599435806274,-0.3645174205303192,-0.251827210187912,-0.3993913233280182,-0.25158390402793884,-0.23091042041778564,-0.25155022740364075,-0.23852497339248657,-0.2464476227760315,-0.3548787534236908,-0.24722951650619507,-0.2686561048030853,-0.4308290183544159,-0.34663280844688416,-0.11351573467254639,-0.1918957233428955,-0.2514459788799286,-0.25112420320510864,-0.3281002342700958,-0.2256910800933838,-0.2617350220680237,-0.26581665873527527,-0.2611391842365265,-0.25121599435806274,-0.2450842261314392,-0.21954283118247986,-0.2525300085544586,-0.20851442217826843,-0.2519393861293793,-0.24823495745658875,-0.21778246760368347,-0.2591627538204193,-0.2527390122413635,-0.2278815507888794,-0.40452250838279724,-0.38480839133262634,-0.2514136731624603,-0.28724226355552673,-0.19634339213371277,-0.25112420320510864,-0.2512703537940979,-0.22872447967529297,-0.25125589966773987,-0.2892163097858429,-0.3205607831478119,-0.26406678557395935,-0.1281423270702362,-0.26039719581604004,-0.34507909417152405,-0.2151888608932495,-0.05471041798591614,-0.2520267069339752,-0.29693594574928284,-0.25133833289146423,-0.05878239870071411,-0.04308509826660156,-0.2509715259075165,-0.27137061953544617,-0.377182275056839,-0.2520478069782257,-0.2522858679294586,-0.10829707980155945,-0.27322229743003845,-0.2509715259075165,-0.2592834532260895,-0.2509715259075165,-0.2531123757362366,-0.25104227662086487,-0.37715044617652893,-0.16847050189971924,-0.2786943018436432,-0.24706289172172546,-0.14353975653648376,-0.26285499334335327,-0.2515215277671814,-0.25169503688812256,-0.19549840688705444,-0.5517691373825073,-0.3302139937877655,-0.25067761540412903,-0.23692286014556885,-0.2509470283985138,-0.2509715259075165,-0.34467813372612,-0.25127020478248596,-0.2522472143173218,-0.25254952907562256,-0.23763957619667053,-0.14867982268333435,-0.25133833289146423,-0.25179240107536316,-0.25253942608833313,-0.25167858600616455,-0.2523564100265503,-0.17167767882347107,0.08516818284988403,-0.25128284096717834,-0.25199517607688904,-0.25158578157424927,-0.25148025155067444,-0.25170499086380005,-0.2931821048259735,-0.25158578157424927,-0.22407406568527222,-0.14258268475532532,-0.2520202100276947,-0.2519652545452118,-0.25212010741233826,-0.257866770029068,-0.26307031512260437,-0.13099977374076843,-0.25937458872795105,-0.2313525378704071,-0.24096617102622986,-0.2509715259075165,-0.26134374737739563,-0.25211918354034424,-0.2511375844478607,-0.2521813213825226,-0.2517339587211609,-0.2585753798484802,-0.4216674864292145,-0.15916821360588074,0.10265038907527924,-0.302995890378952,-0.45485153794288635,-0.2418259084224701,-0.2517814338207245,-0.2554768919944763,-0.23388323187828064,-0.27520546317100525,-0.14700078964233398,-0.23672029376029968,-0.3272415101528168,-0.25158578157424927,-0.25218167901039124,-0.2520483136177063,-0.2513009309768677,-0.25188660621643066,-0.24051830172538757,-0.2520992159843445,-0.2515215277671814,-0.25195419788360596,-0.26746484637260437,-0.3141244351863861,-0.3703267276287079,-0.25147560238838196,-0.4348231852054596,-0.2518117129802704,-0.27981969714164734,-0.1098722517490387,-0.25102272629737854,-0.18879905343055725,-0.36412498354911804,-0.22396066784858704,-0.37058815360069275,-0.26557984948158264,-0.21491995453834534,-0.2947542369365692,-0.23343253135681152,-0.2872758209705353,-0.16002318263053894,-0.25085777044296265,-0.07698863744735718,-0.2604968845844269,-0.25194546580314636,-0.23737218976020813,-0.25044965744018555,-0.25121599435806274,-0.11626693606376648,-0.2517378628253937,-0.25608405470848083,-0.2598576843738556,-0.3696896731853485,-0.27963075041770935,-0.25181177258491516,-0.2518759071826935,-0.25215771794319153,-0.25171273946762085,-0.2913527190685272,-0.0819600522518158,-0.2495322823524475,-0.20846229791641235,-0.21367555856704712,-0.25112420320510864,-0.2509715259075165,-0.23056864738464355,-0.24938306212425232,-0.2516014277935028,-0.307038277387619,-0.22998836636543274,-0.24612006545066833,-0.23324179649353027,-0.39349988102912903,-0.2763691246509552,-0.25155940651893616,-0.2175169587135315,-0.2511119544506073,-0.2524430453777313,0.03251248598098755,-0.25145888328552246,-0.2854876220226288,-0.2540808916091919,-0.22733399271965027,-0.29956552386283875,-0.2526719272136688,-0.251718133687973,-0.25147560238838196,-0.2516274154186249,-0.25185492634773254,-0.2515215277671814,-0.3390769064426422,-0.25147560238838196,-0.2889081537723541,-0.14955425262451172,-0.259670227766037,-0.251827210187912,-0.3953973948955536,-0.27068760991096497,-0.2264235019683838,-0.25175219774246216,-0.17946621775627136,-0.25135537981987,-0.07918485999107361,-0.345382958650589,-0.23326703906059265,-0.21905303001403809,-0.21019676327705383,-0.2508648931980133,-0.35436180233955383,-0.2516532838344574,-0.15956592559814453,-0.22027933597564697,-0.2509715259075165,-0.2509715259075165,-0.24366873502731323,-0.25053638219833374,-0.27232369780540466,-0.5003437995910645,-0.2437271773815155,-0.23297354578971863,-0.2518601715564728,-0.2455534040927887,-0.25183039903640747,-0.23566770553588867,-0.25121599435806274,-0.27923783659935,-0.25997161865234375,-0.028730154037475586,-0.2519555985927582,-0.25121599435806274,-0.09239545464515686,-0.3189009130001068,-0.3299656808376312,-0.25971922278404236,-0.1010814905166626,-0.35496464371681213,-0.2520504891872406,-0.24818828701972961,-0.0014155209064483643,-0.15998372435569763,-0.05262604355812073,-0.255405992269516,-0.10311630368232727,-0.36935684084892273,-0.25071629881858826,-0.2554551661014557,-0.3923843801021576,-0.2515215277671814,-0.32917144894599915,-0.25167450308799744,-0.27731403708457947,-0.31721577048301697,-0.2696073353290558,-0.2515464425086975,-0.25190842151641846,-0.2862075865268707,-0.25131887197494507,-0.25121599435806274,-0.25121599435806274,-0.2515215277671814,-0.7119510173797607,-0.2488858699798584,-0.5454646348953247,-0.30461499094963074,-0.5301523208618164,-0.2521534264087677,-0.25623655319213867,-0.24986135959625244,-0.25147560238838196,-0.20012709498405457,-0.34034374356269836,-0.3255451023578644,-0.2658851444721222,-0.1838689148426056,-0.2518366873264313,-0.4759853780269623,-0.260136216878891,-0.2516650855541229,-0.282080739736557,-0.3310454189777374,-0.25115957856178284,-0.2121778428554535,-0.5468252897262573,-0.251379132270813,-0.25181177258491516,-0.173872172832489,-0.2519126534461975,-0.13936689496040344,-0.2500358521938324,-0.26753148436546326,-0.2539252042770386,-0.25127607583999634,-0.25159499049186707,-0.2515215277671814,-0.2514634430408478,-0.25157830119132996,-0.3177640736103058,-0.2521549463272095,-0.254204660654068,-0.5080482959747314,-0.3074452579021454,-0.2496376931667328,-0.2879479229450226,-0.2514790892601013,-0.25112035870552063,-0.21338573098182678,-0.26813504099845886,-0.25147560238838196,-0.23556849360466003,-0.2509715259075165,-0.2390415072441101,-0.24472391605377197,-0.2517814338207245,-0.25260093808174133,-0.17499437928199768,-0.42236825823783875,-0.3976646363735199,-0.25121599435806274,-0.2523219883441925,-0.25158578157424927,-0.3275372087955475,-0.17776155471801758,-0.2525196671485901,-0.3755623996257782,-0.2516540288925171,-0.3223223388195038,-0.25121599435806274,-0.2509715259075165,-0.24341121315956116,-0.2942593991756439,-0.36967960000038147,-0.19776687026023865,0.055662959814071655,-0.2525421679019928,-0.25406086444854736,-0.25418272614479065,-0.2697640359401703,-0.2515215277671814,-0.4985499680042267,-0.24981331825256348,-0.21983185410499573,-0.39568981528282166,-0.22837939858436584,-0.25194251537323,-0.42069342732429504,-0.23215198516845703,-0.23752808570861816,-0.2509715259075165,-0.2217349410057068,-0.25163155794143677,-0.2553490698337555,-0.24962565302848816,-0.24933764338493347,-0.2845841944217682,-0.25112420320510864,-0.27265819907188416,-0.5272424221038818,-0.6230701208114624,-0.2550729811191559,-0.23955264687538147,-0.03629511594772339,-0.25153687596321106,-0.2515111267566681,-0.09350892901420593,-0.5930308103561401,-0.2525624632835388,-0.23259112238883972,-0.18197965621948242,-0.3347454369068146,-0.2519768178462982,-0.2517595887184143,-0.252296507358551,-0.2951641380786896,-0.2180541753768921,-0.25989657640457153,-0.2526533305644989,-0.25909242033958435,-0.2054865062236786,-0.2510891556739807,-0.25091874599456787,-0.1768057644367218,-0.2519327998161316,-0.2672675549983978,-0.27579888701438904,-0.2517814338207245,-0.15804865956306458,-0.2508608400821686,-0.2618720531463623,-0.08620980381965637,-0.27257809042930603,-0.11058282852172852,-0.059895604848861694,-0.11924239993095398,-0.25077852606773376,-0.06059125065803528,-0.1044774055480957,-0.25121599435806274,-0.3424932658672333,-0.26649561524391174,-0.23215901851654053,-0.2710561454296112,-0.2546227276325226,-0.27032992243766785,-0.2731635272502899,-0.25112420320510864,-0.22462713718414307,-0.3336472809314728,-0.25457262992858887,-0.2517828345298767,-0.2516579329967499,-0.22651711106300354,-0.3279809057712555,-0.522181510925293,-0.2517075836658478,-0.533613920211792,-0.3862880766391754,-0.25147560238838196,-0.2310253381729126,-0.2023971974849701,-0.25153326988220215,-0.25153687596321106,-0.3558059632778168,-0.18934261798858643,-0.4830034673213959,-0.2725668251514435,-0.252420574426651,-0.25133833289146423,-0.18643823266029358,-0.2638629674911499,-0.22359371185302734,-0.25450998544692993,-0.273908406496048,-0.26380130648612976,-0.23227545619010925,-0.2218967080116272,-0.25252318382263184,-0.28995421528816223,-0.2520029544830322,-0.45693907141685486,-0.30345985293388367,-0.2522592842578888,-0.25175732374191284,-0.25133833289146423,-0.2517366111278534,-0.25167617201805115,-0.3171933591365814,-0.2512548267841339,-0.4712285101413727,-0.6370455026626587,-0.25087717175483704,-0.08483478426933289,-0.3171473443508148,-0.23520681262016296,-0.29279598593711853,-0.24212142825126648,-0.245520681142807,-0.251030832529068,-0.15008822083473206,-0.26226264238357544,-0.25121599435806274,-0.2516292631626129,-0.2183677852153778,-0.25013819336891174,-0.2803804576396942,-0.2986049950122833,-0.19758275151252747,-0.25194859504699707,-0.26648131012916565,-0.2515215277671814,-0.25072887539863586,-0.12277999520301819,-0.3481580317020416,-0.2637040317058563,-0.30360081791877747,-0.251827210187912,-0.4849304258823395,-0.2656649053096771,-0.2572147846221924,-0.038588374853134155,-0.3098330795764923,-0.2516666054725647,-0.3085204064846039,-0.29023459553718567,-0.30641594529151917,-0.40068987011909485,-0.20589259266853333,-0.24558624625205994,-0.3164817988872528,-0.3114480674266815,-0.25126004219055176,-0.20475822687149048,-0.2521434724330902,-0.28776440024375916,-0.11199834942817688,-0.4142027795314789,-0.2517424523830414,-0.2987920939922333,-0.22918924689292908,-0.29242369532585144,-0.3230612576007843,-0.17723220586776733,-0.2703315317630768,-0.3370263874530792,-0.20901253819465637,-0.1795421540737152,-0.25181618332862854,-0.2739177644252777,-0.25180575251579285,0.14732897281646729,-0.25129038095474243,-0.049817681312561035,-0.24619978666305542,-0.3111740052700043,-0.2283594310283661,-0.2559383809566498,-0.25217294692993164,-0.2419898808002472,-0.21069136261940002,-0.30346497893333435,-0.22234109044075012,-0.25121599435806274,-0.2517814338207245,-0.2984701097011566,-0.28696760535240173,-0.26793691515922546,-0.238185852766037,-0.25160932540893555,-0.25112420320510864,-0.25121599435806274,-0.2510514259338379,0.006047636270523071,-0.1975077986717224,-0.23819735646247864,-0.25148066878318787,-0.3094746768474579,-0.25264909863471985,-0.25215622782707214,-0.25518032908439636,-0.27412232756614685,-0.25128430128097534,-0.25181272625923157,-0.2773437201976776,-0.26680096983909607,-0.25158578157424927,-0.2866951525211334,-0.24974960088729858,-0.2518899440765381,-0.27422574162483215,-0.2518480122089386,-0.44691190123558044,-0.303786963224411,-0.2522696256637573,-0.2513752579689026,-0.25268441438674927,-0.25146913528442383,-0.3992585241794586,-0.1898042857646942,-0.22810688614845276,-0.2283899486064911,-0.2480098009109497,-0.25196048617362976,-0.2782653868198395,-0.287011057138443,-0.2722209393978119,-0.25112420320510864,-0.25141993165016174,-0.24152329564094543,-0.25771382451057434,-0.4777476489543915,-0.238185852766037,-0.07827013731002808,-0.29425135254859924,-0.4902714788913727,0.0035971105098724365,-0.045243263244628906,-0.2555100619792938,-0.2339726984500885,-0.2521110475063324,-0.2507513761520386,-0.4576946794986725,-0.5005106925964355,-0.0004006624221801758,-0.2499331533908844,-0.24161168932914734,-0.2511070966720581,-0.08916276693344116,-0.2219860851764679,-0.238185852766037,-0.28582337498664856,-0.30033013224601746,-0.24881777167320251,-0.2514648139476776,-0.4012846052646637,-0.3681788146495819,-0.13610363006591797,-0.24973466992378235,-0.4239146411418915,-0.30519261956214905,-0.250842422246933,-0.2509715259075165,-0.3111855685710907,-0.250125527381897,-0.23360082507133484,-0.2509715259075165,-0.31089356541633606,-0.2519996464252472,-0.1790601909160614,-0.25147560238838196,-0.27932968735694885,-0.2515215277671814,-0.23749661445617676,-0.2530497610569,-0.2593368589878082,-0.2676525413990021,-0.2508319914340973,-0.278246134519577,-0.2511541247367859,-0.24943819642066956,-0.24101561307907104,-0.2321433126926422,-0.2723500430583954,-0.393748015165329,-0.10151124000549316,-0.21471765637397766,-0.25147560238838196,-0.12168720364570618,-0.2525300085544586,-0.07082065939903259,-0.2529207766056061,-0.2515380084514618,-0.25173747539520264,-0.39281174540519714,-0.5446748733520508,-0.5760688781738281,-0.25121599435806274,-0.19781875610351562,-0.302103191614151,-0.25163960456848145,-0.25200220942497253,-0.40789636969566345,-0.2322464883327484,-0.2527429759502411,-0.2521893382072449,-0.2547436058521271,-0.3339785635471344,-0.20795488357543945,-0.22727277874946594,-0.2387259602546692,-0.1847955882549286,-0.2608223855495453,-0.022060692310333252,-0.28452596068382263,-0.3631928861141205,-0.2517814338207245,-0.10854682326316833,-0.2533821761608124,-0.25210678577423096,-0.35579928755760193,-0.22951582074165344,-0.2515215277671814,-0.22741305828094482,-0.039502620697021484,-0.26296326518058777,-0.412924200296402,-0.5530956983566284,-0.1785672903060913,-0.01652619242668152,-0.25204193592071533,-0.2664875090122223,-0.257620245218277,-0.3372943699359894,-0.25121599435806274,-0.1996667981147766,-0.16606256365776062,-0.2528776526451111,-0.25227293372154236,-0.25121599435806274,-0.17454972863197327,-0.19244596362113953,-0.23434075713157654,-0.28721490502357483,-0.518072247505188,-0.2663426101207733,-0.39633020758628845,-0.17561373114585876,-0.15862858295440674,-0.20010381937026978,-0.10811308026313782,-0.25123459100723267,-0.2244201898574829,-0.25700369477272034,-0.2528892159461975,-0.2507740557193756,-0.3812117874622345,-0.23361971974372864,-0.21277639269828796,-0.2527773678302765,-0.2281365692615509,-0.23502710461616516,-0.31336089968681335,-0.25133833289146423,-0.2517923414707184,-0.10322600603103638,-0.25160253047943115,-0.32360216975212097,-0.21310201287269592,-0.31030312180519104,-0.4543311893939972,0.018804609775543213,-0.385349303483963,-0.27272531390190125,-0.19851085543632507,-0.3125142753124237,-0.16001185774803162,-0.24370166659355164,-0.25225868821144104,-0.194627583026886,-0.25249966979026794,0.09423032402992249,-0.24886545538902283,-0.13408136367797852,-0.5256040096282959,-0.008811920881271362,-0.25203147530555725,-0.27381011843681335,-0.1240837574005127,-0.2552293539047241,-0.25112420320510864,0.05129000544548035,-0.2518773376941681,-0.547563910484314,-0.28324589133262634,-0.028426557779312134,-0.29094424843788147,0.16877423226833344,-0.25089600682258606,-0.0566214919090271,-0.2515215277671814,-0.2509715259075165,-0.47960183024406433,-0.2518104016780853,-0.25100448727607727,-0.2524432837963104,-0.25198811292648315,-0.2159201204776764,-0.25226420164108276,-0.22991204261779785,-0.3128303587436676,-0.4798049032688141,-0.27866896986961365,-0.26747581362724304,-0.27354392409324646,-0.03514888882637024,-0.25167563557624817,-0.22731426358222961,-0.2525300085544586,-0.2883138954639435,-0.25121599435806274,-0.2516336739063263,0.17891214787960052,-0.25183746218681335,-0.25324976444244385,-0.25153687596321106,-0.28920671343803406,-0.2684777081012726,-0.251827210187912,-0.25147560238838196,-0.39994969964027405,-0.26496991515159607,-0.27141478657722473,-0.25235602259635925,-0.24432581663131714,-0.3674223721027374,-0.22593015432357788,-0.27435383200645447,-0.30410346388816833,-0.2516099810600281,-0.2831432521343231,-0.1711294949054718,-0.2511513829231262,-0.20841923356056213,-0.065121591091156,-0.37471869587898254,-0.2625458836555481,-0.24214300513267517,-0.17353194952011108,-0.25101590156555176,-0.3253120481967926,-0.18301868438720703,-0.20246049761772156,-0.23873966932296753,-0.25095075368881226,-0.09007763862609863,-0.23864132165908813,-0.2513512074947357,-0.2447693645954132,-0.2536443769931793,-0.21897533535957336,-0.2506854236125946,-0.25263407826423645,-0.25121334195137024,-0.25563696026802063,-0.39859071373939514,-0.25112420320510864,-0.27021685242652893,-0.1772351861000061,0.018827274441719055,-0.26630738377571106,-0.2533855140209198,-0.42346104979515076,-0.24742618203163147,-0.336577445268631,-0.03280177712440491,-0.25167450308799744,-0.2503516674041748,-0.2517565190792084,-0.3898729979991913,-0.2465997338294983,-0.2490040361881256,-0.38827142119407654,-0.25237032771110535,-0.262033611536026,-0.25164973735809326,-0.2580675184726715,-0.2828530967235565,-0.25121599435806274,-0.2931181490421295,-0.5033161640167236,-0.2515215277671814,-0.25628259778022766,-0.26694104075431824,-0.36255547404289246,-0.2527303695678711,-0.3548419773578644,-0.2827472984790802,-0.19026413559913635,-0.18742728233337402,-0.2520592510700226,-0.160546213388443,-0.22539713978767395,-0.328962117433548,-0.17417874932289124,-0.29088422656059265,-0.06675463914871216,-0.37050631642341614,-0.2510308623313904,-0.524971604347229,-0.03779911994934082,-0.251756489276886,-0.37280890345573425,-0.2520224153995514,-0.23402303457260132,-0.32742610573768616,-0.29156073927879333,-0.24766722321510315,-0.2516935467720032,-0.05815577507019043,-0.15825799107551575,-0.25222134590148926,-0.25203844904899597,-0.3056805431842804,-0.25168952345848083,-0.24324873089790344,-0.2517194151878357,-0.3240605294704437,-0.4057227075099945,-0.25138816237449646,-0.5288420915603638,-0.29903045296669006,-0.2744612395763397,-0.1480378806591034,-0.25121599435806274,-0.2517431080341339,-0.0032511353492736816,-0.2564592659473419,-0.2515190541744232,-0.2566840350627899,-0.41085192561149597,-0.25282829999923706,0.02180302143096924,-0.25121599435806274,-0.2515973746776581,-0.25108233094215393,-0.2974311411380768,-0.23663052916526794,-0.16939690709114075,-0.3671186864376068,-0.46028652787208557,-0.25130489468574524,-0.2617546319961548,-0.2514825463294983,-0.12885335087776184,-0.2523033916950226,-0.3161931335926056,-0.25116509199142456,-0.24307593703269958,-0.1705891191959381,-0.25112420320510864,-0.25145724415779114,-0.21050062775611877,-0.251827210187912,-0.2526039779186249,-0.2560059726238251,-0.2517814338207245,-0.3114328682422638,-0.2528136074542999,-0.05056825280189514,-0.3935134708881378,-0.264830082654953,-0.251827210187912,-0.08806484937667847,-0.504520058631897,-0.27127787470817566,-0.17563271522521973,-0.4408904016017914,-0.25147560238838196,-0.2516784965991974,-0.25160035490989685,-0.17832928895950317,-0.2743399441242218,-0.2516878843307495,-0.2539534270763397,-0.25175023078918457,-0.07200232148170471,-0.25156164169311523,-0.25220605731010437,-0.21814307570457458,-0.202106773853302,-0.25147560238838196,-0.25133833289146423,-0.25110942125320435,-0.1963142454624176,-0.25147008895874023,-0.32036492228507996,-0.2365640103816986,-0.3969072997570038,-0.25090429186820984,-0.4102080762386322,-0.2670838534832001,-0.2519321143627167,-0.2513946294784546,-0.3498465120792389,-0.1580125391483307,-0.2518382668495178,-0.19442346692085266,-0.2511904537677765,-0.4611190855503082,-0.23014214634895325,-0.5096640586853027,-0.24990323185920715,-0.2513717710971832,-0.2849300801753998,-0.2591182589530945,-0.20159685611724854,-0.2512238323688507,-0.46799537539482117,-0.2501280605792999,-0.25167450308799744,-0.26102784276008606,-0.2513611614704132,-0.25218310952186584,-0.29155126214027405,-0.27570030093193054,-0.2525881826877594,-0.24097469449043274,-0.25088152289390564,-0.25121599435806274,-0.30280086398124695,-0.26580771803855896,-0.23653554916381836,-0.23266205191612244,-0.13338348269462585,-0.25113800168037415,-0.093106210231781,-0.09281349182128906,-0.19835519790649414,-0.2509210705757141,-0.25121599435806274,-0.25121599435806274,-0.1626972258090973,-0.2283383011817932,-0.25158947706222534,-0.30407604575157166,-0.2596520185470581,-0.2323487102985382,-0.3312179148197174,-0.2628467082977295,-0.20981857180595398,-0.25181177258491516,-0.21909800171852112,-0.23483142256736755,-0.25500503182411194,-0.004239559173583984,-0.27963802218437195,-0.35806187987327576,-0.25201985239982605,-0.1999659538269043,-0.24863070249557495,-0.26740458607673645,-0.24575117230415344,-0.29646506905555725,-0.2934066951274872,-0.25104013085365295,-0.2568567097187042,-0.35736390948295593,-0.2499828338623047,-0.3056228458881378,-0.0739533007144928,-0.5221099853515625,-0.29821088910102844,-0.2515215277671814,-0.28648117184638977,-0.2509715259075165,-0.002560734748840332,-0.2514464557170868,-0.19472059607505798,-0.25500035285949707,-0.2485145628452301,-0.22861775755882263,-0.2513104975223541,-0.25128573179244995,-0.2509183883666992,-0.2521094083786011,-0.2520320415496826,-0.292558878660202,-0.2503134310245514,-0.2848447859287262,-0.08385041356086731,-0.27240046858787537,-0.604332685470581,-0.25082921981811523,-0.2513573169708252,-0.25529900193214417,-0.29082950949668884,-0.2511180341243744,-0.2522471249103546,-0.16746675968170166,-0.43612203001976013,-0.32467225193977356,-0.2150602638721466,-0.25121599435806274,-0.21651163697242737,-0.0722578763961792,-0.27058449387550354,-0.18340986967086792,-0.251679390668869,-0.3493129312992096,-0.22398805618286133,-0.2525300085544586,-0.25235411524772644,-0.25147560238838196,-0.22784355282783508,-0.25147560238838196,-0.5301158428192139,-0.25277256965637207,-0.1870110034942627,-0.4829753339290619,-0.25167450308799744,-0.08198666572570801,-0.277278870344162,-0.22387227416038513,-0.3054519593715668,-0.25283879041671753,-0.24752506613731384,-0.2520532011985779,-0.23013949394226074,-0.16649040579795837,-0.25147560238838196,-0.25133049488067627,-0.39077702164649963,-0.25170114636421204,-0.4905311167240143,-0.13447239995002747,-0.15492337942123413,-0.25120845437049866,-0.20290112495422363,0.13414961099624634,-0.2519700229167938,-0.19260138273239136,-0.25245967507362366,-0.5750324726104736,-0.28298506140708923,-0.24334397912025452,-0.251827210187912,-0.29789718985557556,-0.24914976954460144,-0.36354967951774597,-0.42871275544166565,-0.4103552997112274,-0.35281118750572205,-0.23724818229675293,-0.1732684075832367,-0.3511415421962738,-0.27593496441841125,-0.2943405210971832,-0.2701694667339325,-0.19352856278419495,-0.25121599435806274,-0.27284136414527893,-0.2946227490901947,-0.25121599435806274,-0.24594274163246155,-0.25583603978157043,-0.2517814338207245,-0.41838398575782776,-0.2504178285598755,-0.30555519461631775,-0.23535668849945068,-0.24096164107322693,-0.2513660490512848,-0.36129358410835266,-0.25082576274871826,-0.39527931809425354,-0.20627906918525696,-0.25131866335868835,-0.2518964409828186,-0.31437256932258606,-0.08836415410041809,-0.25151821970939636,-0.373855322599411,-0.2509715259075165,-0.5681012868881226,-0.2837770879268646,-0.238185852766037,-0.2524813413619995,-0.2489657700061798,-0.25218844413757324,-0.25078463554382324,-0.014874428510665894,-0.2525300085544586,-0.2525300085544586,-0.2520904541015625,-0.25150856375694275,-0.2501448690891266,-0.5032789707183838,-0.3112489879131317,-0.2516709566116333,-0.25121599435806274,-0.2520819902420044,-0.25093451142311096,-0.33752718567848206,-0.25148069858551025,-0.1746579110622406,-0.26038315892219543,-0.109254390001297,-0.2523552179336548,0.15386173129081726,-0.24462643265724182,-0.25937357544898987,-0.2534387707710266,-0.4890708029270172,-0.2739867866039276,-0.2527404725551605,0.009663209319114685,-0.23415476083755493,-0.260731965303421,-0.18089082837104797,-0.2516957223415375,-0.1605132520198822,-0.2894323170185089,-0.1679522693157196,-0.25165101885795593,-0.2509715259075165,-0.2515996992588043,-0.2426770031452179,-0.29447826743125916,-0.2484838366508484,-0.2509715259075165,-0.4012833535671234,-0.3686155378818512,-0.49814853072166443,-0.2279427945613861,-0.25147560238838196,-0.2877367436885834,-0.3839889466762543,-0.25071102380752563,-0.251790314912796,-0.17735883593559265,-0.22919869422912598,-0.1887313425540924,-0.22554174065589905,-0.24058109521865845,-0.4170297682285309,-0.2635217308998108,-0.2510738670825958,-0.2517223656177521,-0.25319939851760864,-0.2525300085544586,-0.25158578157424927,-0.3432256281375885,-0.23274880647659302,-0.1522471010684967,-0.2797646224498749,-0.2513124942779541,-0.2509715259075165,-0.246328204870224,-0.07891187071800232,-0.25121599435806274,-0.25123128294944763,-0.2514533996582031,-0.25121599435806274,-0.25109025835990906,-0.40267160534858704,-0.021120458841323853,-0.2486354112625122,-0.10145369172096252,-0.22275203466415405,-0.27533969283103943,0.002135634422302246,-0.4660138785839081,-0.12207004427909851,-0.041484713554382324,-0.23266682028770447,-0.2568155527114868,-0.26249977946281433,-0.2608780562877655,-0.25158169865608215,-0.20024877786636353,-0.2515215277671814,-0.2506721317768097,-0.23791515827178955,-0.2509715259075165,-0.25147560238838196,-0.2527271509170532,-0.2515215277671814,-0.32330796122550964,-0.14810174703598022,-0.2452368438243866,-0.23768523335456848,-0.23552975058555603,-0.2659412920475006,-0.2491200864315033,-0.24496737122535706,-0.3106531798839569,-0.2554202973842621,-0.14800557494163513,-0.24524563550949097,-0.4784993827342987,-0.4121074974536896,-0.1351853311061859,-0.25442180037498474,-0.3023926317691803,-0.25175824761390686,-0.40919676423072815,-0.238185852766037,-0.1766493320465088,-0.2524634003639221,-0.25129300355911255,-0.30091115832328796,-0.2518424391746521,-0.49175694584846497,-0.252243310213089,-0.40693792700767517,-0.17520779371261597,-0.2702716290950775,-0.2514687478542328,-0.25147783756256104,-0.25274398922920227,-0.2511134445667267,-0.2542913258075714,-0.25187280774116516,-0.01758134365081787,-0.2068648636341095,-0.22923269867897034,-0.2509715259075165,-0.2514948546886444,-0.25121599435806274,-0.28097817301750183,-0.2427833378314972,-0.2763592302799225,-0.48917749524116516,-0.2759641110897064,-0.23945632576942444,-0.30579134821891785,-0.32013508677482605,-0.2885470688343048,-0.4255073368549347,-0.26501980423927307,-0.25205329060554504,-0.2519136965274811,-0.25157371163368225,-0.5357645750045776,-0.25119152665138245,-0.2525300085544586,0.0040683746337890625,-0.24795785546302795,-0.21294373273849487,-0.2515215277671814,-0.18828418850898743,-0.25733864307403564,-0.29762157797813416,-0.25169503688812256,-0.25133833289146423,-0.24702906608581543,-0.25276458263397217,-0.38924333453178406,-0.2516791820526123,-0.28596732020378113,-0.2544170320034027,-0.25202006101608276,-0.2287902534008026,-0.25181177258491516,-0.2856379449367523,-0.23434120416641235,-0.5086194276809692,-0.13045579195022583,-0.254672646522522,-0.2517814338207245,-0.2783021032810211,-0.2919631898403168,-0.25082147121429443,0.03244525194168091,-0.09273529052734375,0.03902733325958252,-0.2512746751308441,-0.14461755752563477,-0.25145789980888367,-0.40745624899864197,-0.2516162693500519,-0.2530949115753174,-0.2344902753829956,-0.2518187165260315,-0.208732008934021,-0.4733412563800812,-0.3846891224384308,-0.15994319319725037,-0.43098166584968567,-0.24313777685165405,-0.2840249836444855,-0.16996672749519348,-0.2517961263656616,-0.25055983662605286,-0.3529488146305084,-0.16972333192825317,-0.3045273721218109,-0.2525235116481781,-0.28968873620033264,-0.2517257630825043,-0.19354864954948425,-0.2519555389881134,-0.26421990990638733,-0.2512696087360382,-0.2515215277671814,-0.10568958520889282,-0.2528488337993622,-0.7762980461120605,-0.2525300085544586,-0.3257797658443451,-0.2514432370662689,-0.2221188247203827,-0.25197961926460266,-0.007486939430236816,-0.29323092103004456,-0.2873997986316681,-0.2542067766189575,-0.24245211482048035,-0.47703489661216736,-0.4616677463054657,-0.25121599435806274,-0.2509715259075165,-0.25121599435806274,-0.06589511036872864,-0.2694890797138214,-0.2492949366569519,-0.2893781363964081,-0.2515215277671814,-0.25147560238838196,-0.2466130256652832,-0.19378992915153503,-0.3577638566493988,-0.21988406777381897,-0.33252963423728943,-0.25147560238838196,-0.24353522062301636,-0.3110925853252411,-0.26872172951698303,-0.15091323852539062,-0.17815765738487244,-0.19767948985099792,-0.35502079129219055,-0.21919357776641846,-0.22999519109725952,-0.2766105830669403,-0.042775869369506836,-0.44735702872276306,-0.2512042224407196,-0.26835325360298157,-0.440796822309494,-0.1736639440059662,-0.2520236074924469,-0.2511446177959442,-0.251451313495636,-0.27235350012779236,-0.3391261398792267,-0.25133833289146423,-0.2761012613773346,0.11619871109724045,-0.6887706518173218,-0.2522793114185333,-0.17162248492240906,-0.25121569633483887,-0.2513463497161865,-0.2096468210220337,-0.2525300085544586,-0.27641817927360535,-0.1809024214744568,-0.2518458962440491,-0.24141648411750793,-0.2812342941761017,-0.25147560238838196,-0.25121599435806274,-0.4898454248905182,-0.2516624629497528,-0.25071102380752563,-0.25158578157424927,-0.24954098463058472,-0.5179131031036377,-0.25160378217697144,-0.11760079860687256,-0.2647652328014374,-0.2525300085544586,-0.25432026386260986,-0.2352711260318756,-0.3265937864780426,-0.16139712929725647,-0.2920744717121124,-0.2594301402568817,-0.2519255578517914,-0.2935123145580292,-0.30578383803367615,-0.2105703353881836,-0.2593522071838379,-0.2643676698207855,-0.25121599435806274,-0.2509715259075165,-0.2519751489162445,-0.2116246521472931,-0.2521020770072937,-0.033991724252700806,-0.25122547149658203,-0.252286434173584,-0.3231578767299652,-0.25133833289146423,-0.5508459806442261,-0.3604676425457001,-0.2514689564704895,-0.2719765603542328,-0.25163984298706055,-0.25145652890205383,-0.31668946146965027,-0.2661319673061371,-0.08762279152870178,-0.25134503841400146,-0.27693554759025574,-0.49351927638053894,0.033535346388816833,-0.25168952345848083,-0.2153404951095581,-0.5892965793609619,-0.2517814338207245,-0.5236748456954956,-0.25158578157424927,-0.3305750787258148,-0.21781525015830994,-0.3628052771091461,-0.2517714202404022,-0.1856115460395813,-0.26255002617836,-0.21095529198646545,-0.32814452052116394,-0.24303969740867615,-0.28481581807136536,-0.19863751530647278,-0.28273990750312805,-0.25129643082618713,-0.3335132896900177,-0.2523549497127533,-0.2862306535243988,-0.2787022888660431,-0.23138689994812012,-0.25530073046684265,-0.2525300085544586,-0.2041822075843811,-0.25211912393569946,-0.25118911266326904,-0.25147560238838196,-0.31997165083885193,-0.2517814338207245,-0.1839183270931244,-0.5701974630355835,-0.25247594714164734,-0.03246277570724487,-0.37557682394981384,-0.2509715259075165,-0.2510116696357727,-0.37416109442710876,-0.25147560238838196,-0.199748694896698,-0.25228527188301086,-0.32378873229026794,-0.019101113080978394,-0.21444442868232727,-0.24314162135124207,-0.16170939803123474,-0.25041571259498596,-0.12879794836044312,-0.25116488337516785,-0.2585182785987854,-0.2453441321849823,-0.25121599435806274,-0.22903943061828613,-0.36323001980781555,-0.2391749918460846,-0.23750320076942444,-0.30787476897239685,-0.029575973749160767,-0.23198962211608887,-0.25147560238838196,-0.43660494685173035,-0.49400296807289124,-0.45374593138694763,-0.3000146448612213,-0.21444189548492432,-0.19094786047935486,-0.25894662737846375,-0.25189825892448425,-0.478181928396225,-0.3380785286426544,-0.2522263824939728,-0.3314944803714752,-0.25121599435806274,-0.2509680986404419,-0.2970152795314789,-0.23985692858695984,-0.15783128142356873,-0.37806496024131775,-0.5628679990768433,-0.3023698031902313,-0.25160080194473267,-0.2523556053638458,-0.25235918164253235,-0.25254106521606445,-0.2509715259075165,-0.25167450308799744,-0.33844658732414246,-0.2522715628147125,-0.2519342005252838,-0.2512624263763428,-0.5824743509292603,-0.25971683859825134,-0.0956154465675354,-0.26554200053215027,-0.22093182802200317,-0.25142744183540344,-0.2522483170032501,-0.25147560238838196,-0.18866154551506042,-0.2525300085544586,-0.23097261786460876,-0.25176045298576355,-0.32810327410697937,-0.0201912522315979,-0.41854801774024963,-0.3975478708744049,-0.2513822019100189,-0.39169058203697205,-0.3738851845264435,-0.2512168288230896,-0.25143349170684814,-0.20283243060112,-0.26028332114219666,-0.5890923738479614,-0.5016803741455078,-0.09696981310844421,-0.25152936577796936,-0.25089821219444275,-0.21683630347251892,-0.3460175096988678,-0.26777389645576477,-0.32660892605781555,-0.2567676603794098,-0.20683768391609192,-0.25223472714424133,-0.2682602107524872,-0.2781318128108978,-0.5053879022598267,-0.2712258994579315,-0.3686136305332184,-0.14199289679527283,-0.45000341534614563,-0.13675883412361145,-0.34892866015434265,-0.28512218594551086,-0.2521175146102905,-0.27440670132637024,-0.22055476903915405,-0.26786765456199646,-0.07901424169540405,-0.18162941932678223,-0.2512812316417694,-0.2882349193096161,-0.2482883632183075,-0.08822795748710632,-0.1566588580608368,-0.2786339223384857,-0.2517814338207245,-0.24562308192253113,-0.25138095021247864,-0.2365504801273346,-0.070619136095047,-0.10120365023612976,-0.25121599435806274,-0.41439417004585266,-0.40480926632881165,-0.2525300085544586,-0.07263168692588806,-0.25022417306900024,-0.1469455063343048,-0.25145843625068665,-0.3154807984828949,-0.2515215277671814,-0.34820982813835144,-0.43255987763404846,-0.20381507277488708,-0.1962699592113495,-0.2511982023715973,-0.1819067895412445,-0.43516191840171814,-0.23699519038200378,-0.2511157989501953,-0.20175758004188538,-0.40154531598091125,-0.33000871539115906,-0.2525300085544586,-0.2518306076526642,-0.02173703908920288,-0.25147560238838196,-0.2501990795135498,-0.16836565732955933,-0.2519836723804474,-0.11669257283210754,-0.25121599435806274,-0.3842369019985199,-0.25167450308799744,-0.2517656683921814,-0.25149333477020264,-0.2688552439212799,-0.20203536748886108,-0.266630619764328,-0.14855894446372986,-0.23155182600021362,-0.24643731117248535,-0.3796369135379791,-0.19452685117721558,-0.23829787969589233,0.060020118951797485,-0.18450424075126648,-0.01099097728729248,-0.15314516425132751,-0.22342634201049805,-0.2515491843223572,-0.2833769619464874,-0.2509715259075165,-0.21231454610824585,-0.2512270212173462,-0.28319594264030457,-0.20590519905090332,-0.18543139100074768,-0.15562918782234192,-0.2515753507614136,-0.25187137722969055,-0.22206023335456848,-0.2527790665626526,-0.26900216937065125,-0.2502097189426422,-0.21211928129196167,-0.23786669969558716,-0.2525300085544586,0.09903411567211151,-0.2921123802661896,-0.20067870616912842,0.11218768358230591,-0.37986138463020325,-0.2500882148742676,-0.18667420744895935,-0.27141955494880676,-0.25181177258491516,-0.23380166292190552,-0.37050434947013855,-0.25084438920021057,-0.25175490975379944,-0.36592671275138855,-0.2514943778514862,-0.20829090476036072,-0.31631848216056824,-0.24516457319259644,-0.25268247723579407,-0.2679550349712372,-0.25112420320510864,-0.29459986090660095,-0.25121599435806274,-0.28549709916114807,-0.2517814338207245,-0.2717561423778534,-0.2802030146121979,-0.28779539465904236,-0.25121599435806274,-0.2512965202331543,-0.2524857819080353,-0.2515871822834015,-0.25121599435806274,-0.2671235501766205,-0.2517009675502777,0.004014045000076294,-0.254290908575058,-0.0973290503025055,-0.26809272170066833,-0.251240611076355,-0.2515215277671814,-0.27006563544273376,-0.24369922280311584,-0.2922736704349518,-0.2833700478076935,-0.21770507097244263,-0.25129756331443787,-0.2376055121421814,-0.2514694035053253,-0.4336613714694977,-0.301878958940506,-0.2525855004787445,-0.20790526270866394,-0.01787179708480835,-0.3038819134235382,-0.38052496314048767,-0.2519749402999878,0.02681279182434082,-0.2507091164588928,-0.0032326728105545044,-0.25168952345848083,-0.25168952345848083,-0.252096027135849,-0.18062350153923035,-0.25357291102409363,-0.24446919560432434,-0.3392963111400604,-0.25072523951530457,0.014358773827552795,-0.44546905159950256,-0.22805160284042358,-0.2518792748451233,-0.25030845403671265,-0.26271530985832214,-0.28307732939720154,-0.31629636883735657,-0.3076646029949188,-0.1444529891014099,-0.2514142096042633,-0.25216013193130493,-0.28784266114234924,-0.2607678771018982,-0.25100019574165344,-0.2992580831050873,-0.19807890057563782,-0.27347972989082336,-0.17551261186599731,-0.3858005106449127,-0.2517479658126831,-0.25309693813323975,-0.26421502232551575,-0.2513330578804016,-0.17124846577644348,-0.25162360072135925,-0.20310992002487183,-0.23784223198890686,-0.18604037165641785,-0.2502554953098297,-0.25192901492118835,-0.3598238527774811,-0.25177499651908875,-0.1749439835548401,-0.39313140511512756,-0.2429666519165039,-0.25147560238838196,-0.22266098856925964,-0.2934153378009796,-0.25157228112220764,-0.25112420320510864,-0.28852084279060364,-0.25222429633140564,-0.3480251133441925,-0.263087660074234,-0.21241715550422668,-0.4439355432987213,-0.25642967224121094,-0.27080515027046204,-0.3656029999256134,-0.2640477120876312,-0.25121599435806274,-0.25429704785346985,-0.22200605273246765,-0.16367107629776,-0.3582063615322113,-0.27943554520606995,-0.3503328263759613,-0.2477245032787323,-0.2512843608856201,-0.2515215277671814,-0.2453117072582245,-0.29475441575050354,-0.37784096598625183,-0.25121599435806274,-0.4778399169445038,-0.28401389718055725,-0.2839178144931793,-0.25137850642204285,-0.2465774118900299,-0.19874975085258484,-0.19841277599334717,-0.2517046332359314,-0.27162566781044006,-0.13429901003837585,-0.29414626955986023,-0.2843622863292694,-0.23987850546836853,-0.21357837319374084,-0.15821591019630432,-0.16827240586280823,-0.236390620470047,-0.2512883245944977,-0.252052366733551,-0.25153687596321106,-0.05283525586128235,-0.25138184428215027,-0.25153687596321106,-0.31364384293556213,-0.27893051505088806,-0.2527576982975006,-0.27183714509010315,-0.22120881080627441,-0.0660390555858612,-0.2515215277671814,-0.25121599435806274,-0.23333027958869934,-0.19736790657043457,-0.248918354511261,-0.17903271317481995,-0.25121599435806274,-0.3190325200557709,-0.24877864122390747,-0.2649061381816864,-0.3489159643650055,-0.2232286036014557,-0.2514214515686035,-0.24413731694221497,-0.2524537146091461,-0.32497158646583557,-0.25168296694755554,-0.2729932367801666,-0.3656592071056366,-0.24577084183692932,-0.24597051739692688,-0.29408523440361023,-0.3308125436306,-0.2345973551273346,-0.25112420320510864,0.08498081564903259,-0.25121599435806274,-0.2900741994380951,-0.27039918303489685,-0.10933449864387512,-0.25127938389778137,-0.2593688368797302,-0.2212793231010437,-0.25168952345848083,-0.11452749371528625,-0.25121599435806274,-0.28982749581336975,-0.2026197612285614,-0.37763383984565735,-0.2515215277671814,-0.25114965438842773,-0.04970550537109375,-0.4317725598812103,-0.29664501547813416,-0.15941119194030762,-0.2358841598033905,-0.19031909108161926,-0.2518865764141083,-0.24178797006607056,-0.19618883728981018,-0.20265796780586243,-0.2991703450679779,-0.2516222894191742,-0.25217747688293457,-0.1551530659198761,-0.23836979269981384,-0.18334364891052246,-0.2525300085544586,-0.16264045238494873,-0.35861101746559143,0.18791139125823975,-0.15659141540527344,-0.25153565406799316,-0.15626394748687744,-0.15134325623512268,-0.2549281716346741,-0.25147560238838196,-0.33240029215812683,-0.25592803955078125,-0.2515215277671814,-0.262219101190567,-0.36376115679740906,-0.31755778193473816,-0.2512398660182953,-0.10696586966514587,-0.29111137986183167,-0.2509289085865021,-0.25147655606269836,-0.25209781527519226,-0.3102230727672577,-0.32216379046440125,-0.1582987904548645,-0.2513734698295593,-0.2519085705280304,-0.25710776448249817,-0.2507016956806183,-0.21669980883598328,-0.2721010744571686,-0.2604674994945526,-0.22010833024978638,-0.4188462197780609,0.15916946530342102,-0.034625351428985596,-0.2263842523097992,-0.27181151509284973,-0.26287347078323364,-0.2489376664161682,-0.2589759826660156,-0.16256409883499146,-0.2511250376701355,-0.2515254020690918,0.2123953402042389,-0.25147560238838196,-0.2523327171802521,0.09581322968006134,-0.07568851113319397,-0.22559624910354614,-0.25459790229797363,-0.22664740681648254,-0.34397009015083313,-0.2680148184299469,-0.41633889079093933,-0.2072468400001526,-0.25224950909614563,-0.3468773066997528,-0.2528783082962036,-0.25167980790138245,-0.21565231680870056,-0.25881028175354004,-0.25140130519866943,-0.3118714988231659,-0.25236091017723083,-0.2543250024318695,-0.25274792313575745,-0.2520742118358612,-0.251729279756546,-0.2519730031490326,-0.3246023952960968,-0.2506580650806427,-0.2515215277671814,-0.2908910810947418,-0.4218784272670746,-0.4363158047199249,-0.25147560238838196,-0.21552956104278564,-0.07311058044433594,-0.2518840730190277,-0.4998159110546112,-0.2421467900276184,-0.2483011782169342,-0.2494497001171112,-0.266425758600235,-0.25147560238838196,-0.3461301624774933,-0.5194547176361084,-0.21272623538970947,-0.24293997883796692,-0.41654637455940247,0.019374370574951172,-0.2291335165500641,-0.25137439370155334,-0.059771448373794556,-0.45893940329551697,-0.012236952781677246,-0.3188774883747101,-0.1586814522743225,-0.2518400251865387,-0.2514288127422333,-0.19851741194725037,-0.25104913115501404,-0.2538819909095764,-0.2516964375972748,-0.2713698446750641,-0.3475593626499176,-0.25158563256263733,-0.15896078944206238,-0.17122569680213928,-0.25222429633140564,-0.5184801816940308,-0.2518746554851532,-0.0970524251461029,-0.18981501460075378,-0.2112424671649933,-0.24197342991828918,-0.35600778460502625,-0.2519243061542511,-0.7178758382797241,-0.35079118609428406,-0.2530614733695984,-0.25209176540374756,-0.2525300085544586,-0.2132229506969452,-0.251827210187912,-0.041900306940078735,-0.2510169446468353,-0.17274585366249084,-0.341271311044693,-0.1620788872241974,-0.41684690117836,-0.25101953744888306,-0.06248578429222107,-0.21235701441764832,-0.2564561665058136,-0.2515539526939392,-0.23435530066490173,-0.2506685256958008,-0.25121599435806274,-0.23591336607933044,-0.5332417488098145,-0.25845035910606384,-0.15700161457061768,-0.24426504969596863,0.023211896419525146,-0.13683831691741943,-0.25118330121040344,-0.2514156699180603,-0.1751529574394226,-0.25168347358703613,-0.33794984221458435,-0.2915157973766327,-0.27979913353919983,-0.18685659766197205,0.06745877861976624,-0.3017468750476837,-0.3110525906085968,-0.2555690407752991,-0.2512448728084564,-0.17103376984596252,-0.2613208293914795,-0.3936634361743927,-0.5023900270462036,-0.26547643542289734,-0.2782697379589081,-0.262688010931015,-0.3166137635707855,-0.19088363647460938,-0.1072397232055664,-0.22027042508125305,-0.28683510422706604,-0.2515215277671814,-0.19326546788215637,-0.25042924284935,-0.15486934781074524,-0.25240379571914673,-0.24894824624061584,0.02242918312549591,-0.25147560238838196,-0.25182244181632996,-0.2522221505641937,-0.24906966090202332,-0.2939741909503937,-0.3380317986011505,-0.40117427706718445,-0.2524094581604004,-0.2881246507167816,-0.2714119851589203,-0.26920029520988464,-0.2507968842983246,-0.22726640105247498,-0.32567837834358215,-0.2573716640472412,-0.2515542209148407,-0.2986079156398773,-0.25121599435806274,-0.24738150835037231,-0.4091051518917084,-0.25531601905822754,-0.2908637821674347,-0.17253515124320984,-0.3908480703830719,-0.25440606474876404,-0.2520644962787628,-0.25148555636405945,-0.2720213234424591,-0.24551889300346375,-0.2515290081501007,-0.17675641179084778,-0.2507154047489166,-0.4721876084804535,-0.2514593303203583,-0.3357979953289032,-0.25153687596321106,-0.3891611397266388,-0.25133439898490906,-0.34311923384666443,-0.37319615483283997,-0.2052246332168579,-0.2507781982421875,-0.2518048882484436,-0.2774032652378082,-0.25222429633140564,-0.2623637020587921,-0.3350308835506439,-0.25160226225852966,-0.2521178722381592,-0.20209971070289612,-0.22654324769973755,0.10560035705566406,-0.25121599435806274,-0.21782252192497253,-0.21804821491241455,-0.12742844223976135,-0.25121599435806274,-0.04893088340759277,-0.25121599435806274,-0.3104605972766876,-0.2212488055229187,-0.2513989508152008,-0.25212401151657104,-0.39774414896965027,-0.25222429633140564,-0.228623628616333,0.09607943892478943,-0.2525160014629364,-0.3882724344730377,-0.3570532500743866,-0.25147560238838196,-0.2517622709274292,-0.2575226128101349,-0.14897695183753967,-0.25147560238838196,-0.34304389357566833,0.005229279398918152,-0.25146108865737915,-0.2519840896129608,-0.4215875566005707,-0.26601681113243103,-0.03776523470878601,-0.23328897356987,-0.2516796886920929,-0.2516656816005707,-0.23799917101860046,-0.2524290978908539,-0.5271815061569214,-0.24805143475532532,-0.25166401267051697,-0.25178566575050354,-0.33982953429222107,-0.07952633500099182,-0.15179353952407837,-0.2515298128128052,-0.25252971053123474,-0.2858920395374298,-0.25253692269325256,-0.2837219536304474,-0.25112420320510864,-0.18126672506332397,-0.19711914658546448,-0.29237261414527893,-0.26353245973587036,-0.25150689482688904,-0.25341007113456726,-0.40165695548057556,-0.16097936034202576,-0.13245487213134766,-0.3570808470249176,-0.14434251189231873,-0.25073668360710144,-0.2072712481021881,-0.23910748958587646,-0.32931289076805115,-0.2509715259075165,-0.24985173344612122,-0.25181517004966736,-0.3391347825527191,-0.3455990254878998,-0.055387645959854126,-0.24088677763938904,-0.25270843505859375,-0.2167288362979889,-0.4183945953845978,-0.2454899251461029,-0.1701551377773285,-0.3080066740512848,-0.33709725737571716,-0.24970093369483948,-0.2518617808818817,-0.2683328688144684,-0.3086887300014496,-0.3837334215641022,-0.23913678526878357,-0.15231570601463318,-0.2588784694671631,-0.24686643481254578,-0.20776599645614624,-0.25121599435806274,-0.25088194012641907,-0.28385016322135925,-0.2521163523197174,-0.2454347312450409,-0.25133833289146423,-0.25118008255958557,-0.27055832743644714,-0.2477467954158783,-0.07292568683624268,-0.3974728286266327,-0.25346723198890686,-0.2509715259075165,-0.3634461462497711,-0.28190580010414124,-0.25165605545043945,-0.21451690793037415,-0.27158865332603455,-0.2266245186328888,-0.5415228605270386,-0.2707253396511078,-0.2517877519130707,-0.07653507590293884,-0.3692341148853302,-0.25222688913345337,-0.25211653113365173,-0.2055736780166626,-0.36931392550468445,-0.25436875224113464,-0.2331579029560089,-0.2736561596393585,-0.2024233043193817,-0.28770413994789124,-0.37212637066841125,-0.5379992723464966,-0.23641440272331238,-0.25206342339515686,-0.26750990748405457,-0.3567829430103302,-0.25121599435806274,-0.459071546792984,-0.23026880621910095,-0.271074503660202,-0.2596670091152191,-0.2517814338207245,-0.5612157583236694,-0.2511107623577118,-0.12973752617835999,-0.28896233439445496,-0.3676081597805023,-0.25351056456565857,-0.253302663564682,-0.3242965042591095,-0.17295056581497192,-0.25209012627601624,-0.2515215277671814,-0.28473642468452454,-0.22244128584861755,-0.1479424238204956,-0.19123908877372742,-0.41086384654045105,-0.2515215277671814,-0.25121599435806274,-0.19515231251716614,-0.39659813046455383,-0.2709696590900421,-0.25380077958106995,-0.2296874225139618,-0.22061443328857422,-0.25112420320510864,-0.25178858637809753,-0.25087496638298035,-0.23984134197235107,-0.25121599435806274,-0.22826078534126282,-0.3555559813976288,-0.15861055254936218,-0.2827102243900299,-0.29559579491615295,-0.050024986267089844,-0.2748197615146637,-0.238185852766037,-0.238185852766037,-0.5338402986526489,-0.2515965402126312,-0.2516089975833893,-0.25207746028900146,-0.4284498989582062,-0.22911259531974792,-0.2515588104724884,-0.15860515832901,-0.23831495642662048,-0.3859976828098297,-0.21021908521652222,-0.22417262196540833,-0.24134883284568787,-0.20849162340164185,-0.26582321524620056,-0.25106069445610046,-0.2509715259075165,-0.20386704802513123,-0.2516896426677704,-0.11641797423362732,-0.26479437947273254,-0.26202142238616943,-0.2567892074584961,-0.2001405954360962,-0.09949198365211487,-0.22910085320472717,-0.27122893929481506,-0.21777084469795227,-0.18246302008628845,-0.2515333294868469,-0.291255921125412,-0.25238698720932007,-0.251471608877182,-0.02034035325050354,-0.2737050950527191,-0.2660904824733734,-0.12741246819496155,0.012805938720703125,-0.2372467815876007,-0.2507523000240326,-0.47437945008277893,-0.22260314226150513,-0.4443819224834442,-0.25187361240386963,-0.2896255552768707,-0.2522802948951721,-0.25147560238838196,-0.25187045335769653,-0.25110337138175964,-0.34803590178489685,-0.09251904487609863,-0.2532162666320801,-0.08417388796806335,-0.2515917718410492,-0.3281325399875641,-0.22078362107276917,-0.251476526260376,-0.18196508288383484,-0.25260257720947266,-0.19056913256645203,-0.14572879672050476,-0.2518341839313507,-0.17675721645355225,-0.2512322962284088,-0.25141847133636475,-0.24975746870040894,-0.24180740118026733,0.010381728410720825,-0.2509715259075165,-0.13072800636291504,-0.2525300085544586,-0.26457592844963074,-0.238185852766037,-0.25136634707450867,-0.25729289650917053,-0.22747406363487244,-0.03715643286705017,-0.2511109411716461,-0.2520260512828827,-0.2199549376964569,-0.2085886299610138,-0.25202181935310364,-0.18340706825256348,-0.24017128348350525,-0.2708880603313446,-0.10065296292304993,-0.43668416142463684,-0.19213423132896423,-0.25228676199913025,-0.25263991951942444,-0.18682178854942322,-0.25121599435806274,-0.21257442235946655,-0.2798401415348053,-0.2804506719112396,-0.2518732249736786,-0.2668575942516327,-0.25158578157424927,-0.2652617394924164,-0.25184839963912964,-0.25802597403526306,-0.17802700400352478,-0.3237603008747101,-0.25121599435806274,-0.2623102366924286,-0.41358283162117004,-0.5276117324829102,-0.2517814338207245,-0.25168463587760925,-0.21431642770767212,-0.25121599435806274,-0.25134173035621643,-0.2680834233760834,-0.24242514371871948,-0.554578423500061,-0.2519032955169678,-0.2516767978668213,0.007879555225372314,-0.2527438700199127,-0.2514238953590393,-0.2525421679019928,-0.253216952085495,-0.2597913444042206,-0.2515365779399872,-0.24473652243614197,-0.2221793830394745,-0.25222429633140564,-0.24951812624931335,-0.2515215277671814,-0.32085099816322327,-0.2786937654018402,-0.24914389848709106,-0.25177088379859924,-0.24710902571678162,0.16033217310905457,-0.33564940094947815,-0.16146868467330933,-0.23221638798713684,-0.3187998831272125,-0.2584281861782074,-0.23295137286186218,0.0403401255607605,-0.2517814338207245,-0.1258378028869629,-0.23700115084648132,-0.005835801362991333,-0.08781978487968445,-0.43921294808387756,-0.285396009683609,-0.25167450308799744,-0.251248300075531,-0.2511269450187683,-0.4648294746875763,-0.29972389340400696,-0.2679295241832733,-0.09135743975639343,-0.1825670599937439,-0.14899399876594543,-0.25147560238838196,-0.2849315106868744,0.039038628339767456,-0.08225339651107788,-0.2518182694911957,-0.23637303709983826,-0.234343022108078,-0.2517964243888855,-0.2530290186405182,-0.19983938336372375,-0.23584222793579102,-0.18675777316093445,-0.25867581367492676,-0.1424485743045807,-0.2516651153564453,-0.252157062292099,-0.44832441210746765,-0.25164058804512024,-0.2717016041278839,-0.3001944124698639,-0.2514338195323944,-0.25121599435806274,-0.07269802689552307,-0.45575985312461853,-0.37365230917930603,-0.25181177258491516,-0.25149062275886536,-0.24919214844703674,-0.3366066515445709,-0.2823666036128998,-0.4334392249584198,-0.2858158051967621,-0.2620895206928253,-0.2030598521232605,-0.23946332931518555,-0.2573469579219818,-0.2517808973789215,-0.25025680661201477,-0.2520492374897003,-0.3047645390033722,-0.37065228819847107,-0.22635549306869507,-0.32666102051734924,-0.25131067633628845,-0.18135201930999756,-0.14820566773414612,-0.2550283372402191,-0.2532099485397339,-0.3278731405735016,-0.25147560238838196,-0.5795037746429443,-0.23340675234794617,-0.2135566771030426,-0.2521039545536041,-0.2819407284259796,-0.2829783260822296,-0.2791048586368561,-0.25160372257232666,-0.27813562750816345,-0.30777254700660706,-0.2606734335422516,-0.2377277910709381,-0.2520153522491455,-0.25996026396751404,-0.2794870436191559,-0.25152066349983215,-0.25146105885505676,-0.2515215277671814,-0.25147560238838196,-0.1526373028755188,-0.2518928349018097,-0.14136454463005066,-0.2517881691455841,-0.33836349844932556,-0.27171286940574646,-0.16543862223625183,-0.2525300085544586,-0.06588694453239441,-0.18732601404190063,-0.25056418776512146,0.07673364877700806,-0.24620863795280457,-0.25196415185928345,-0.2292860746383667,-0.28643283247947693,-0.2511957883834839,-0.1359342634677887,-0.3500010073184967,-0.2459089457988739,-0.19051596522331238,-0.3532998859882355,-0.2519631087779999,-0.28535589575767517,-0.25174903869628906,-0.47123637795448303,-0.004096418619155884,-0.2525707483291626,-0.41723403334617615,-0.27383479475975037,-0.25280120968818665,-0.25201016664505005,-0.2975248396396637,-0.2604310214519501,-0.2392464280128479,-0.27820560336112976,-0.2868623435497284,-0.2517618238925934,-0.25153210759162903,-0.2517717182636261,-0.25153687596321106,-0.1770712435245514,-0.573933482170105,-0.2572653591632843,-0.08399331569671631,-0.2479463815689087,-0.2513236999511719,-0.19469952583312988,-0.25168952345848083,-0.2609582841396332,-0.27936461567878723,-0.2675614058971405,-0.31141331791877747,-0.21744585037231445,-0.2514822781085968,-0.2520420551300049,-0.14164099097251892,-0.19194594025611877,-0.34594449400901794,-0.25203797221183777,-0.2521516680717468,-0.25098857283592224,-0.3778352439403534,-0.3132152259349823,-0.16816267371177673,-0.6062556505203247,-0.20898911356925964,-0.26380959153175354,-0.25175410509109497,0.040972739458084106,-0.31092241406440735,-0.41418513655662537,-0.25077271461486816,-0.30004724860191345,-0.27171775698661804,-0.22254306077957153,-0.30077269673347473,-0.24304059147834778,-0.25143924355506897,-0.29252490401268005,-0.49674007296562195,-0.25183725357055664,-0.36429861187934875,-0.2520577013492584,-0.251884788274765,-0.25224730372428894,-0.25133833289146423,-0.19832643866539001,-0.32024791836738586,-0.25121599435806274,-0.2515215277671814,-0.25164932012557983,-0.2512925863265991,-0.046797484159469604,0.2357153594493866,-0.2923370897769928,-0.3591044843196869,-0.21839436888694763,-0.25259286165237427,-0.25194886326789856,-0.37124136090278625,-0.25147560238838196,-0.2511180341243744,-0.24767467379570007,-0.29857054352760315,-0.1758173406124115,-0.30867770314216614,-0.25154128670692444,-0.251463919878006,-0.2525639832019806,-0.2408885657787323,-0.23151573538780212,-0.2254539430141449,-0.3237919509410858,-0.22465676069259644,-0.12334483861923218,-0.25121599435806274,-0.21305963397026062,-0.26174643635749817,-0.40842798352241516,-0.2701285183429718,-0.25395721197128296,-0.11661896109580994,-0.25202250480651855,-0.2512306272983551,-0.24030140042304993,-0.2970154583454132,-0.3618299067020416,-0.24305689334869385,-0.26395806670188904,-0.2814771831035614,-0.25437918305397034,-0.2518802881240845,-0.2602481245994568,-0.25121599435806274,-0.33940181136131287,-0.22404256463050842,-0.48614785075187683,-0.20368537306785583,0.08709892630577087,-0.2128412425518036,-0.2954654395580292,-0.22380021214485168,-0.25634926557540894,-0.2848859131336212,-0.2472051978111267,-0.25121599435806274,-0.2527112364768982,-0.23784208297729492,-0.31975874304771423,-0.25181177258491516,-0.24447762966156006,-0.25201940536499023,-0.4400619566440582,-0.12428209185600281,-0.2509382367134094,-0.25176599621772766,-0.232237309217453,-0.2517201006412506,-0.25959512591362,-0.09180048108100891,-0.1285063624382019,-0.2104666531085968,-0.28369757533073425,-0.06186994910240173,-0.18764612078666687,-0.2254764437675476,-0.14849957823753357,-0.24625086784362793,-0.24426263570785522,-0.3199683129787445,-0.340269535779953,-0.43057218194007874,-0.4750731289386749,-0.2423037588596344,0.015496671199798584,-0.2521904408931732,-0.2311934232711792,-0.5563037395477295,0.05258336663246155,-0.2517814338207245,-0.24615436792373657,-0.5023947954177856,-0.27658137679100037,-0.25147560238838196,-0.2517814338207245,-0.28631505370140076,-0.2771913707256317,-0.2525300085544586,-0.3427514135837555,-0.22315052151679993,-0.2518414855003357,-0.09624674916267395,-0.25110602378845215,-0.250757098197937,-0.29052600264549255,-0.238185852766037,-0.25114062428474426,-0.4505321681499481,-0.3253384530544281,-0.25222429633140564,-0.2510116696357727,-0.23264440894126892,-0.25133833289146423,-0.3010808527469635,-0.2519662082195282,-0.19483283162117004,-0.2509715259075165,-0.2697128355503082,-0.25171440839767456,-0.08852219581604004,-0.379517525434494,-0.25081172585487366,-0.38354912400245667,-0.07780042290687561,-0.25192609429359436,-0.2520686984062195,-0.40343186259269714,-0.2874715030193329,-0.25181642174720764,-0.7641452550888062,-0.2804007828235626,-0.15312275290489197,-0.04966321587562561,-0.28010162711143494,-0.25167450308799744,-0.25182172656059265,-0.25268444418907166,-0.1742907464504242,-0.2515215277671814,-0.24747928977012634,-0.40392372012138367,-0.06940889358520508,-0.20402514934539795,-0.26057255268096924,-0.16827547550201416,-0.25785747170448303,-0.11967751383781433,-0.3504989445209503,0.19567641615867615,-0.2019224464893341,-0.36162951588630676,-0.1517506241798401,-0.2515215277671814,-0.2515215277671814,-0.24816733598709106,-0.2517605125904083,-0.25061461329460144,-0.26391634345054626,-0.25121599435806274,-0.19139045476913452,-0.2997482717037201,-0.1886354386806488,-0.2532951533794403,-0.18396592140197754,-0.2664470374584198,-0.25204578042030334,-0.39007678627967834,-0.002320230007171631,-0.3421260416507721,-0.25222429633140564,-0.2515212297439575,-0.25158578157424927,-0.2575623095035553,-0.20375224947929382,0.004257827997207642,-0.25147560238838196,-0.250686913728714,-0.36035487055778503,-0.2681926190853119,-0.22782278060913086,-0.25325849652290344,-0.18496960401535034,-0.2518307566642761,-0.17287281155586243,-0.21832966804504395,-0.15107271075248718,-0.2555893659591675,-0.261532723903656,-0.29767414927482605,-0.2585456967353821,-0.2514897286891937,-0.251827210187912,-0.31518444418907166,-0.19870644807815552,-0.011884301900863647,-0.25226274132728577,-0.11460146307945251,-0.25229892134666443,-0.2465246021747589,-0.026198118925094604,-0.30760499835014343,-0.25133243203163147,-0.5190091133117676,-0.5614960193634033,-0.33672478795051575,-0.4231410324573517,-0.2509715259075165,-0.24759861826896667,-0.25163713097572327,-0.06506305932998657,-0.22675755620002747,-0.1518162190914154,-0.2512185275554657,-0.23113945126533508,-0.25144532322883606,-0.3333047926425934,-0.24942907691001892,-0.2521430552005768,-0.24934402108192444,-0.18805626034736633,-0.007507622241973877,-0.25282081961631775,-0.29778942465782166,-0.43255677819252014,-0.25121599435806274,-0.34215572476387024,-0.2518630623817444,-0.3435836136341095,-0.3723371922969818,-0.25192126631736755,-0.25726744532585144,-0.3990394175052643,-0.32917073369026184,-0.07743105292320251,-0.22717177867889404,-0.25875920057296753,-0.25061720609664917,-0.2530544400215149,-0.16273730993270874,-0.2513348460197449,-0.25121599435806274,-0.2517222762107849,0.0927376002073288,-0.21607449650764465,-0.1265271008014679,-0.005305945873260498,-0.21286478638648987,-0.4844610393047333,-0.22799110412597656,-0.33001431822776794,-0.25112420320510864,-0.25155696272850037,-0.25148648023605347,-0.2525300085544586,-0.381696492433548,-0.17422187328338623,-0.25137531757354736,-0.2518356740474701,-0.2515679895877838,-0.2262440323829651,-0.02544015645980835,-0.23726752400398254,-0.22184354066848755,-0.25205370783805847,-0.2522987127304077,-0.25153687596321106,-0.2516705393791199,-0.18923214077949524,-0.28311219811439514,-0.24778404831886292,0.07244282960891724,-0.25173643231391907,-0.25117582082748413,-0.25195619463920593,-0.17968934774398804,0.15363740921020508,-0.2444288730621338,-0.25204020738601685,-0.3703337609767914,-0.3102630078792572,-0.2206595242023468,-0.005741804838180542,-0.36348965764045715,-0.2511725425720215,-0.28776755928993225,-0.25168952345848083,-0.343840628862381,-0.5573538541793823,-0.2525300085544586,-0.26838311553001404,-0.0906355082988739,-0.2515215277671814,-0.5798690319061279,-0.25120243430137634,-0.546027660369873,-0.23138025403022766,-0.36278870701789856,-0.2987358272075653,-0.2509715259075165,-0.25158533453941345,-0.25167450308799744,-0.24013906717300415,-0.10978075861930847,-0.24896961450576782,-0.25125277042388916,-0.24595966935157776,-0.298111230134964,-0.3840886056423187,-0.25159403681755066,-0.11545398831367493,-0.32423779368400574,-0.3307913839817047,-0.22542372345924377,-0.4798004925251007,-0.2510138154029846,-0.4501008093357086,-0.07514876127243042,-0.06631651520729065,-0.24934232234954834,-0.2509715259075165,-0.12280476093292236,-0.37572506070137024,-0.2515031397342682,-0.25292521715164185,-0.2509715259075165,-0.25467103719711304,-0.33298608660697937,-0.2569631040096283,-0.2649478018283844,-0.2684656083583832,-0.26167193055152893,-0.3626575171947479,0.03944230079650879,-0.3697125017642975,-0.21406206488609314,-0.25464892387390137,-0.1976618468761444,-0.25182682275772095,-0.2567446529865265,-0.2517049312591553,-0.34442034363746643,-0.2519611716270447,-0.25180426239967346,-0.2169964611530304,-0.23248291015625,-0.25153687596321106,-0.26734474301338196,-0.25393757224082947,-0.24175715446472168,-0.1940281093120575,-0.23365405201911926,-0.2525423467159271,-0.2605103552341461,-0.27232685685157776,-0.15789350867271423,-0.3492744266986847,-0.2516833245754242,-0.20109781622886658,-0.4036116898059845,-0.31216612458229065,-0.25153687596321106,-0.25181207060813904,-0.06224343180656433,-0.5548855066299438,-0.25147560238838196,-0.3457997739315033,-0.06459954380989075,-0.3011729419231415,-0.2506205439567566,-0.25168952345848083,-0.25147560238838196,-0.13383659720420837,-0.12268233299255371,-0.2514815032482147,-0.25136691331863403,-0.2726533114910126,-0.2527506947517395,-0.18099439144134521,-0.454871267080307,-0.2504722476005554,-0.25112420320510864,-0.2524283230304718,-0.34724101424217224,-0.4080064594745636,-0.25168731808662415,-0.24274209141731262,-0.37279507517814636,-0.22375214099884033,-0.2511293590068817,-0.11088129878044128,-0.18776872754096985,-0.25894638895988464,-0.30866292119026184,-0.2513639032840729,-0.37796780467033386,-0.34854355454444885,-0.2525300085544586,-0.25173166394233704,-0.2512328326702118,-0.25181177258491516,-0.21138474345207214,-0.20002543926239014,-0.2601431608200073,-0.47066017985343933,-0.27733930945396423,-0.251800537109375,-0.41207507252693176,-0.2531757056713104,-0.14616143703460693,-0.21183720231056213,-0.251400351524353,-0.25160375237464905,-0.20169749855995178,-0.1905231475830078,-0.34015926718711853,-0.2515215277671814,-0.22627589106559753,-0.2516193389892578,-0.28009113669395447,-0.25112420320510864,-0.3139209449291229,-0.2418448030948639,-0.2518001198768616,-0.2508320212364197,-0.2533464729785919,-0.24146118760108948,-0.2517779767513275,-0.251007616519928,-0.22525116801261902,-0.2904984652996063,-0.25121599435806274,-0.3248758614063263,-0.25153687596321106,-0.19630837440490723,-0.25158578157424927,-0.5428550243377686,-0.2393743097782135,-0.25167450308799744,-0.24975746870040894,-0.2527172267436981,-0.24323654174804688,-0.43015894293785095,-0.25121599435806274,-0.22069066762924194,-0.25230562686920166,-0.25167450308799744,-0.21351253986358643,-0.3317146599292755,-0.3008962571620941,-0.25168952345848083,-0.24538221955299377,-0.10748240351676941,-0.25147560238838196,-0.00519600510597229,-0.3947913944721222,-0.47798117995262146,-0.1628408133983612,-0.2511324882507324,-0.2527438700199127,-0.2465355098247528,-0.2516475021839142,-0.24603736400604248,-0.19485312700271606,-0.2914727032184601,-0.4810695946216583,-0.2515425980091095,-0.25121599435806274,-0.19784080982208252,-0.25122904777526855,-0.24284175038337708,-0.25121599435806274,-0.3490529954433441,-0.25160667300224304,-0.30201587080955505,-0.24356380105018616,-0.00488850474357605,-0.238185852766037,-0.34703555703163147,-0.2138366401195526,-0.251920223236084,-0.25077950954437256,-0.2510312497615814,-0.24395811557769775,-0.2512562572956085,-0.2706432044506073,-0.5833027362823486,-0.22784537076950073,-0.25096815824508667,-0.2516789734363556,-0.25121599435806274,-0.2517010271549225,-0.36950549483299255,-0.2515215277671814,-0.25160130858421326,-0.2142164409160614,-0.25147560238838196,-0.32976672053337097,-0.11141768097877502,-0.25112420320510864,-0.17670410871505737,-0.25121599435806274,-0.3077952563762665,-0.22196948528289795,-0.25152161717414856,-0.2619442641735077],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"pred\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"height\":1000,\"width\":1200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('35f0d94c-6794-4a74-bfd1-6222ee4536eb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70M96KLSdCHW"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e72-bHKzdZve"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list[0:]"
      ],
      "metadata": {
        "id": "M1t90K4auzlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb1d5b88-8749-4463-a0d3-9b4413b00eae"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.0869117 , -2.3722944 , -0.21100315],\n",
              "       [ 0.9855263 , -1.2680924 , -0.33857045],\n",
              "       [ 0.002409  , -1.3145576 , -0.37738058],\n",
              "       ...,\n",
              "       [ 0.8566735 , -1.6068366 , -0.22196949],\n",
              "       [ 0.91630685, -1.6906996 , -0.25152162],\n",
              "       [ 0.6884663 , -2.2734547 , -0.26194426]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_embedding(embedding_list[:],y_train[:],index2=1)"
      ],
      "metadata": {
        "id": "BO3xdrHVGXhL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "7f446b51-0e38-4573-cc73-49ee90ead96b"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMRd5eGU9ASA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "006d9d5c-6119-44f6-b306-def657b6dc2f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-4a83869fc985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_embedding_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-115-3c0a1a3d084d>\u001b[0m in \u001b[0;36mplot_embedding_3d\u001b[0;34m(list_in, predicted)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mykoordinata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mzkoordinata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mzipped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxkoordinata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mykoordinata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzkoordinata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf_tmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"z\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot iterate over a scalar.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m       raise TypeError(\n\u001b[0m\u001b[1;32m    369\u001b[0m           'Cannot iterate over a Tensor with unknown first dimension.')\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_KerasTensorIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot iterate over a Tensor with unknown first dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_df=pd.DataFrame({\"epoch\":history.epoch, \"loss\":history.history[\"loss\"],\"val_loss\":history.history[\"val_loss\"]})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "eff6a00d-cee4-4e5d-8422-b8bedd96d64e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-e7d36d1446ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "id": "4ENvDCA-0U1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnCwvyz9i7ebA04n5eWCSh",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}