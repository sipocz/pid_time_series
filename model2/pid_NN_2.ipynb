{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/model2/pid_NN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tNYnFR-6Xh",
        "outputId": "f44ee7f8-6086-4482-88eb-d5c07d3ffcf5"
      },
      "execution_count": 507,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "12ccd4e3-fac9-4c71-fd4c-e1e88ef7122c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint1\"\n",
        "data_source=\"5\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_error_part1.csv\"\n",
        "fname_bad = \"415_SC_error_part2.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "ad91d541-b155-47a2-8048-55bce8e50e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part1.csv\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part2.csv\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {
        "id": "vUcMjZAGKvtt"
      },
      "outputs": [],
      "source": [
        "df_good = pd.read_csv(path_good,usecols=None)\n",
        "df_bad = pd.read_csv(path_bad,usecols=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYuDXKraLOt4",
        "outputId": "8e28b623-2a87-49c8-eb2c-50ce6542035f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(df_good.isnull().values.any())\n",
        "print(df_bad.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "2df5210e-b4d9-4734-f594-08e7e525d310"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0          1          2          3          4        5         6  \\\n",
              "0 -54.810024 -80.342186 -60.770203 -41.081482 -21.779583 -3.82353 -0.806820   \n",
              "1 -80.342186 -60.770203 -41.081482 -21.779583  -3.823530 -0.80682  0.220875   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11bc307e-0094-4957-8511-4dcc50a726da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-54.810024</td>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.82353</td>\n",
              "      <td>-0.806820</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.823530</td>\n",
              "      <td>-0.80682</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11bc307e-0094-4957-8511-4dcc50a726da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11bc307e-0094-4957-8511-4dcc50a726da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11bc307e-0094-4957-8511-4dcc50a726da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 516
        }
      ],
      "source": [
        "df_good.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "df_ = df_good\n",
        "\n",
        "# You must normalize the data before applying the fit method\n",
        "df_good_normalized=(df_ - df_.mean()) / df_.std()\n",
        "\n",
        "# Normalize bad data with the good data parameters\n",
        "df_bad_normalized=(df_bad - df_.mean()) / df_.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": [
        "df_good_normalized[\"state\"]=0\n",
        "df_bad_normalized[\"state\"]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3W5mi70VM6hL",
        "outputId": "b2ac8cdc-a889-4214-f1e7-82b20f22ffdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0          1          2          3         4         5  \\\n",
              "0    -10.681306 -16.586266 -14.612051 -11.087981 -6.293341 -1.192618   \n",
              "1    -15.654548 -12.549683  -9.889987  -5.905180 -1.164099 -0.314249   \n",
              "2    -11.842250  -8.489023  -5.260696  -1.083756 -0.302359 -0.015017   \n",
              "3     -8.007214  -4.508142  -0.954188  -0.273732 -0.008793 -0.015017   \n",
              "4     -4.247524  -0.804833  -0.230672   0.002217 -0.008793 -0.015017   \n",
              "...         ...        ...        ...        ...       ...       ...   \n",
              "1053   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1054   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1055   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1056   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1057   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "\n",
              "             6         7         8         9  ...        11        12  \\\n",
              "0    -0.315574 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "2    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "3    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "4    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...  ...       ...       ...   \n",
              "1053 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1054 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1055 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1056 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1057 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "\n",
              "            13        14        15        16        17        18        19  \\\n",
              "0    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "2    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "3    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "4    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1053 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1054 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1055 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1056 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1057 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "\n",
              "      state  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  \n",
              "...     ...  \n",
              "1053      0  \n",
              "1054      0  \n",
              "1055      0  \n",
              "1056      0  \n",
              "1057      0  \n",
              "\n",
              "[1058 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3c1a313-fa92-41b5-9521-064a370d5d84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10.681306</td>\n",
              "      <td>-16.586266</td>\n",
              "      <td>-14.612051</td>\n",
              "      <td>-11.087981</td>\n",
              "      <td>-6.293341</td>\n",
              "      <td>-1.192618</td>\n",
              "      <td>-0.315574</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-15.654548</td>\n",
              "      <td>-12.549683</td>\n",
              "      <td>-9.889987</td>\n",
              "      <td>-5.905180</td>\n",
              "      <td>-1.164099</td>\n",
              "      <td>-0.314249</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-11.842250</td>\n",
              "      <td>-8.489023</td>\n",
              "      <td>-5.260696</td>\n",
              "      <td>-1.083756</td>\n",
              "      <td>-0.302359</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-8.007214</td>\n",
              "      <td>-4.508142</td>\n",
              "      <td>-0.954188</td>\n",
              "      <td>-0.273732</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.247524</td>\n",
              "      <td>-0.804833</td>\n",
              "      <td>-0.230672</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1058 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3c1a313-fa92-41b5-9521-064a370d5d84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3c1a313-fa92-41b5-9521-064a370d5d84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3c1a313-fa92-41b5-9521-064a370d5d84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 519
        }
      ],
      "source": [
        "df_good_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_normalized=pd.concat([df_good_normalized,df_bad_normalized],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "bb6fbd9a-4fee-43c9-8aa4-0c36dbf75b41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "1263  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1264  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1265  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1266  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1267  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "1263 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1264 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1265 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1266 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1267 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "1263 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1264 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1265 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1266 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1267 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d133835-55cd-4dd6-8b9b-e7c923570f8e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1263</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1265</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d133835-55cd-4dd6-8b9b-e7c923570f8e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d133835-55cd-4dd6-8b9b-e7c923570f8e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d133835-55cd-4dd6-8b9b-e7c923570f8e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 521
        }
      ],
      "source": [
        "df_all_normalized.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "nVvhP84S_F1y"
      },
      "execution_count": 617,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_N1_=5\n",
        "_N2_=3"
      ],
      "metadata": {
        "id": "XC5_bGE0iyi4"
      },
      "execution_count": 674,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"pid_1\", entity=\"sipoczlaszlo\",config={})\n",
        "wandb.config = {\"lr\": 0.001, \"batch_size\": 3,\"architecture\": \"NN\", \"depth\": 2, \"l1\":_N1_,  \"l2\":_N2_ }\n",
        "wandb.config.update({\"architecture\": \"NN\", \"depth\": 2, \"l1\":_N1_,  \"l2\":_N2_ })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "642f61ee5dc94d54a92bb346546e9bec",
            "70034f1b2077439b9c2af5de9fa05f0e",
            "0cfaaa59b5c847b8aa3bbef853ec4cf4",
            "43042fe567794fee82df4410fc6220a7",
            "62f49f504e2144e199ada685b4c14b1b",
            "a18efbedd141492f8d138ad062814eaf",
            "2afd1668bcf440e385b76bfa3d368be3",
            "417e20d634db42b6b477d5fd159a74b6"
          ]
        },
        "id": "nOtKllcviuoj",
        "outputId": "50133f24-c7f1-42ec-f0e8-8272b35d7d65"
      },
      "execution_count": 675,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:mdf6mh5e) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "642f61ee5dc94d54a92bb346546e9bec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▆▅▇▇▇▇▇▇▆▇▇▇▆▇▇▆▇▇▇▇▇▇▇█▇▇▇▇█▇▇█▇▇█▇███</td></tr><tr><td>epoch/val_loss</td><td>█▃▂▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.85173</td></tr><tr><td>epoch/epoch</td><td>107</td></tr><tr><td>epoch/learning_rate</td><td>0.005</td></tr><tr><td>epoch/loss</td><td>0.35762</td></tr><tr><td>epoch/val_accuracy</td><td>0.77844</td></tr><tr><td>epoch/val_loss</td><td>0.5957</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">electric-meadow-10</strong>: <a href=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/mdf6mh5e\" target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_1/runs/mdf6mh5e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221217_201821-mdf6mh5e/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:mdf6mh5e). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221217_202527-1tb49wvt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/1tb49wvt\" target=\"_blank\">dauntless-energy-11</a></strong> to <a href=\"https://wandb.ai/sipoczlaszlo/pid_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 676,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=20\n",
        "drop_frac0=0.0  \n",
        "drop_frac1=0.0  \n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "l1_out=Dense(_N1_,activation=\"swish\",kernel_initializer='glorot_uniform',)(input1) # kernel_initializer='lecun_normal'\n",
        "l2_out=Dropout(drop_frac0)(l1_out)\n",
        "\n",
        "\n",
        "l3_out=Dense(_N2_,activation=\"swish\",kernel_initializer='glorot_uniform',)(l2_out) #kernel_initializer='lecun_normal',\n",
        "l4_out=Dropout(drop_frac1)(l3_out)\n",
        "\n",
        "pred=Dense(1, activation=\"sigmoid\",)(l4_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=0.005,) #\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 677,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# 35 5 1 relu relu sigmoid SGD 0.01 loss: 0.1402 - accuracy: 0.9435 - val_loss: 0.7302 - val_accuracy: 0.8548\n",
        "# 35 12 1 relu relu sigmoid SGD 0.01 loss 0.1162 94.6% test : 85%\n",
        "# 17 5 1 relu relu sigmoid SGD 0.01  loss: 0.1714 - accuracy: 0.9300 - val_loss: 0.9535 - val_accuracy: 0.8503\n",
        "# 35 5 1 relu relu sigmoid Adam 0.01 loss: 0.1238 - accuracy: 0.9467 - val_loss: 5.7545 - val_accuracy: 0.8653\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.01 loss: 0.1184 - accuracy: 0.9525 - val_loss: 3.5327 - val_accuracy: 0.8428\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1185 - accuracy: 0.9525 - val_loss: 2.3218 - val_accuracy: 0.8593\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1041 - accuracy: 0.9576 - val_loss: 5.1465 - val_accuracy: 0.8353  +1300 epoch \n",
        "# 135 15 1 swish swish sigmoid Adamax 0.001 batch size:1 epoch 100 loss: 0.1707 - accuracy: 0.9352 - val_loss: 0.8066 - val_accuracy: 0.8892   **** egész jó\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 678,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\"]\n",
        "X_NN=df_all_normalized[prediktorok][:-100]  # \n",
        "y_NN=df_all_normalized[\"state\"][:-100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\"\n",
        "#model_file=\"model_PID__94_loss_0.116_vloss_0.115_acc_0.950_vacc_0.966.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 679,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 680,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False"
      ],
      "metadata": {
        "id": "UIxI3AS6Yw3S"
      },
      "execution_count": 681,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 682,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 683,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=0.7,shuffle=True,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "jJfOOTfGfDXi"
      },
      "execution_count": 684,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.keras import WandbMetricsLogger\n",
        "fname=\"./model_PID_\"\n",
        "callbacks = [\n",
        "        WandbMetricsLogger(),       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch:03.0f}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{accuracy:.3f}_vacc_{val_accuracy:.3f}.hdf5\", monitor='loss',\n",
        "                        verbose=2, save_best_only=True, mode='min')]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 685,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__learning__=True"
      ],
      "metadata": {
        "id": "O6ofy0moderd"
      },
      "execution_count": 686,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 687,
      "metadata": {
        "id": "9Ol0mW6WRlkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fd26cc-100b-433f-b18c-3d14040c87c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.4994\n",
            "Epoch 1: loss improved from inf to 0.68405, saving model to ./model_PID__001_loss_0.684_vloss_0.659_acc_0.501_vacc_0.512.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.6841 - accuracy: 0.5013 - val_loss: 0.6594 - val_accuracy: 0.5120\n",
            "Epoch 2/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.6472 - accuracy: 0.6327\n",
            "Epoch 2: loss improved from 0.68405 to 0.64721, saving model to ./model_PID__002_loss_0.647_vloss_0.603_acc_0.633_vacc_0.710.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.6472 - accuracy: 0.6329 - val_loss: 0.6031 - val_accuracy: 0.7096\n",
            "Epoch 3/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.6060 - accuracy: 0.7172\n",
            "Epoch 3: loss improved from 0.64721 to 0.60441, saving model to ./model_PID__003_loss_0.604_vloss_0.556_acc_0.718_vacc_0.750.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.6044 - accuracy: 0.7182 - val_loss: 0.5560 - val_accuracy: 0.7500\n",
            "Epoch 4/500\n",
            "1526/1558 [============================>.] - ETA: 0s - loss: 0.5676 - accuracy: 0.7464\n",
            "Epoch 4: loss improved from 0.60441 to 0.56738, saving model to ./model_PID__004_loss_0.567_vloss_0.521_acc_0.747_vacc_0.777.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.5674 - accuracy: 0.7471 - val_loss: 0.5207 - val_accuracy: 0.7769\n",
            "Epoch 5/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.5423 - accuracy: 0.7560\n",
            "Epoch 5: loss improved from 0.56738 to 0.54260, saving model to ./model_PID__005_loss_0.543_vloss_0.506_acc_0.756_vacc_0.772.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.5426 - accuracy: 0.7561 - val_loss: 0.5060 - val_accuracy: 0.7725\n",
            "Epoch 6/500\n",
            "1526/1558 [============================>.] - ETA: 0s - loss: 0.5249 - accuracy: 0.7615\n",
            "Epoch 6: loss improved from 0.54260 to 0.52537, saving model to ./model_PID__006_loss_0.525_vloss_0.499_acc_0.761_vacc_0.778.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.5254 - accuracy: 0.7606 - val_loss: 0.4991 - val_accuracy: 0.7784\n",
            "Epoch 7/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7719\n",
            "Epoch 7: loss improved from 0.52537 to 0.51233, saving model to ./model_PID__007_loss_0.512_vloss_0.487_acc_0.772_vacc_0.795.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.5123 - accuracy: 0.7715 - val_loss: 0.4872 - val_accuracy: 0.7949\n",
            "Epoch 8/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.7730\n",
            "Epoch 8: loss improved from 0.51233 to 0.50308, saving model to ./model_PID__008_loss_0.503_vloss_0.485_acc_0.773_vacc_0.793.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5031 - accuracy: 0.7734 - val_loss: 0.4846 - val_accuracy: 0.7934\n",
            "Epoch 9/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4957 - accuracy: 0.7766\n",
            "Epoch 9: loss improved from 0.50308 to 0.49502, saving model to ./model_PID__009_loss_0.495_vloss_0.482_acc_0.777_vacc_0.793.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4950 - accuracy: 0.7766 - val_loss: 0.4818 - val_accuracy: 0.7934\n",
            "Epoch 10/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.7763\n",
            "Epoch 10: loss improved from 0.49502 to 0.48978, saving model to ./model_PID__010_loss_0.490_vloss_0.478_acc_0.777_vacc_0.792.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4898 - accuracy: 0.7766 - val_loss: 0.4779 - val_accuracy: 0.7919\n",
            "Epoch 11/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.4826 - accuracy: 0.7819\n",
            "Epoch 11: loss improved from 0.48978 to 0.48327, saving model to ./model_PID__011_loss_0.483_vloss_0.477_acc_0.781_vacc_0.784.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4833 - accuracy: 0.7811 - val_loss: 0.4773 - val_accuracy: 0.7844\n",
            "Epoch 12/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4806 - accuracy: 0.7804\n",
            "Epoch 12: loss improved from 0.48327 to 0.48014, saving model to ./model_PID__012_loss_0.480_vloss_0.474_acc_0.780_vacc_0.787.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4801 - accuracy: 0.7805 - val_loss: 0.4740 - val_accuracy: 0.7874\n",
            "Epoch 13/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.4752 - accuracy: 0.7843\n",
            "Epoch 13: loss improved from 0.48014 to 0.47533, saving model to ./model_PID__013_loss_0.475_vloss_0.476_acc_0.784_vacc_0.780.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4753 - accuracy: 0.7837 - val_loss: 0.4761 - val_accuracy: 0.7799\n",
            "Epoch 14/500\n",
            "1524/1558 [============================>.] - ETA: 0s - loss: 0.4714 - accuracy: 0.7874\n",
            "Epoch 14: loss improved from 0.47533 to 0.47261, saving model to ./model_PID__014_loss_0.473_vloss_0.473_acc_0.787_vacc_0.781.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4726 - accuracy: 0.7869 - val_loss: 0.4727 - val_accuracy: 0.7814\n",
            "Epoch 15/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.4698 - accuracy: 0.7885\n",
            "Epoch 15: loss improved from 0.47261 to 0.46860, saving model to ./model_PID__015_loss_0.469_vloss_0.471_acc_0.789_vacc_0.783.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4686 - accuracy: 0.7895 - val_loss: 0.4708 - val_accuracy: 0.7829\n",
            "Epoch 16/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.4643 - accuracy: 0.7947\n",
            "Epoch 16: loss improved from 0.46860 to 0.46627, saving model to ./model_PID__016_loss_0.466_vloss_0.471_acc_0.794_vacc_0.786.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4663 - accuracy: 0.7940 - val_loss: 0.4707 - val_accuracy: 0.7859\n",
            "Epoch 17/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.4619 - accuracy: 0.7914\n",
            "Epoch 17: loss improved from 0.46627 to 0.46303, saving model to ./model_PID__017_loss_0.463_vloss_0.468_acc_0.791_vacc_0.784.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4630 - accuracy: 0.7908 - val_loss: 0.4685 - val_accuracy: 0.7844\n",
            "Epoch 18/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.4609 - accuracy: 0.7911\n",
            "Epoch 18: loss improved from 0.46303 to 0.46044, saving model to ./model_PID__018_loss_0.460_vloss_0.468_acc_0.791_vacc_0.783.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4604 - accuracy: 0.7914 - val_loss: 0.4682 - val_accuracy: 0.7829\n",
            "Epoch 19/500\n",
            "1526/1558 [============================>.] - ETA: 0s - loss: 0.4595 - accuracy: 0.7929\n",
            "Epoch 19: loss improved from 0.46044 to 0.45911, saving model to ./model_PID__019_loss_0.459_vloss_0.466_acc_0.793_vacc_0.786.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4591 - accuracy: 0.7933 - val_loss: 0.4663 - val_accuracy: 0.7859\n",
            "Epoch 20/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.4581 - accuracy: 0.7958\n",
            "Epoch 20: loss improved from 0.45911 to 0.45764, saving model to ./model_PID__020_loss_0.458_vloss_0.466_acc_0.797_vacc_0.784.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4576 - accuracy: 0.7965 - val_loss: 0.4660 - val_accuracy: 0.7844\n",
            "Epoch 21/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.4541 - accuracy: 0.7980\n",
            "Epoch 21: loss improved from 0.45764 to 0.45439, saving model to ./model_PID__021_loss_0.454_vloss_0.462_acc_0.798_vacc_0.793.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4544 - accuracy: 0.7978 - val_loss: 0.4624 - val_accuracy: 0.7934\n",
            "Epoch 22/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4531 - accuracy: 0.7974\n",
            "Epoch 22: loss improved from 0.45439 to 0.45359, saving model to ./model_PID__022_loss_0.454_vloss_0.461_acc_0.797_vacc_0.793.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4536 - accuracy: 0.7965 - val_loss: 0.4605 - val_accuracy: 0.7934\n",
            "Epoch 23/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.4487 - accuracy: 0.7952\n",
            "Epoch 23: loss improved from 0.45359 to 0.44985, saving model to ./model_PID__023_loss_0.450_vloss_0.458_acc_0.795_vacc_0.798.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4499 - accuracy: 0.7953 - val_loss: 0.4576 - val_accuracy: 0.7979\n",
            "Epoch 24/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4478 - accuracy: 0.7974\n",
            "Epoch 24: loss improved from 0.44985 to 0.44740, saving model to ./model_PID__024_loss_0.447_vloss_0.454_acc_0.797_vacc_0.804.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.4474 - accuracy: 0.7972 - val_loss: 0.4539 - val_accuracy: 0.8039\n",
            "Epoch 25/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.4434 - accuracy: 0.8030\n",
            "Epoch 25: loss improved from 0.44740 to 0.44343, saving model to ./model_PID__025_loss_0.443_vloss_0.459_acc_0.803_vacc_0.786.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4434 - accuracy: 0.8030 - val_loss: 0.4591 - val_accuracy: 0.7859\n",
            "Epoch 26/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.7988\n",
            "Epoch 26: loss improved from 0.44343 to 0.44173, saving model to ./model_PID__026_loss_0.442_vloss_0.451_acc_0.799_vacc_0.796.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4417 - accuracy: 0.7991 - val_loss: 0.4508 - val_accuracy: 0.7964\n",
            "Epoch 27/500\n",
            "1524/1558 [============================>.] - ETA: 0s - loss: 0.4404 - accuracy: 0.7972\n",
            "Epoch 27: loss improved from 0.44173 to 0.43982, saving model to ./model_PID__027_loss_0.440_vloss_0.456_acc_0.798_vacc_0.796.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4398 - accuracy: 0.7978 - val_loss: 0.4556 - val_accuracy: 0.7964\n",
            "Epoch 28/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.4369 - accuracy: 0.8052\n",
            "Epoch 28: loss improved from 0.43982 to 0.43812, saving model to ./model_PID__028_loss_0.438_vloss_0.451_acc_0.804_vacc_0.789.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4381 - accuracy: 0.8042 - val_loss: 0.4511 - val_accuracy: 0.7889\n",
            "Epoch 29/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.4369 - accuracy: 0.8059\n",
            "Epoch 29: loss improved from 0.43812 to 0.43582, saving model to ./model_PID__029_loss_0.436_vloss_0.453_acc_0.807_vacc_0.790.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4358 - accuracy: 0.8068 - val_loss: 0.4526 - val_accuracy: 0.7904\n",
            "Epoch 30/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4338 - accuracy: 0.8065\n",
            "Epoch 30: loss improved from 0.43582 to 0.43343, saving model to ./model_PID__030_loss_0.433_vloss_0.451_acc_0.807_vacc_0.787.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4334 - accuracy: 0.8068 - val_loss: 0.4508 - val_accuracy: 0.7874\n",
            "Epoch 31/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.4301 - accuracy: 0.8087\n",
            "Epoch 31: loss improved from 0.43343 to 0.43138, saving model to ./model_PID__031_loss_0.431_vloss_0.447_acc_0.807_vacc_0.793.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4314 - accuracy: 0.8074 - val_loss: 0.4473 - val_accuracy: 0.7934\n",
            "Epoch 32/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.4317 - accuracy: 0.8085\n",
            "Epoch 32: loss improved from 0.43138 to 0.42823, saving model to ./model_PID__032_loss_0.428_vloss_0.445_acc_0.810_vacc_0.799.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4282 - accuracy: 0.8100 - val_loss: 0.4454 - val_accuracy: 0.7994\n",
            "Epoch 33/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.4304 - accuracy: 0.8090\n",
            "Epoch 33: loss did not improve from 0.42823\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4292 - accuracy: 0.8100 - val_loss: 0.4465 - val_accuracy: 0.7964\n",
            "Epoch 34/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8093\n",
            "Epoch 34: loss improved from 0.42823 to 0.42528, saving model to ./model_PID__034_loss_0.425_vloss_0.442_acc_0.808_vacc_0.802.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4253 - accuracy: 0.8081 - val_loss: 0.4422 - val_accuracy: 0.8024\n",
            "Epoch 35/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.4238 - accuracy: 0.8084\n",
            "Epoch 35: loss improved from 0.42528 to 0.42285, saving model to ./model_PID__035_loss_0.423_vloss_0.448_acc_0.809_vacc_0.802.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4228 - accuracy: 0.8087 - val_loss: 0.4479 - val_accuracy: 0.8024\n",
            "Epoch 36/500\n",
            "1524/1558 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8130\n",
            "Epoch 36: loss improved from 0.42285 to 0.42172, saving model to ./model_PID__036_loss_0.422_vloss_0.441_acc_0.813_vacc_0.801.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4217 - accuracy: 0.8126 - val_loss: 0.4405 - val_accuracy: 0.8009\n",
            "Epoch 37/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.4213 - accuracy: 0.8105\n",
            "Epoch 37: loss improved from 0.42172 to 0.42015, saving model to ./model_PID__037_loss_0.420_vloss_0.441_acc_0.811_vacc_0.802.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4202 - accuracy: 0.8113 - val_loss: 0.4412 - val_accuracy: 0.8024\n",
            "Epoch 38/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.4145 - accuracy: 0.8153\n",
            "Epoch 38: loss improved from 0.42015 to 0.41751, saving model to ./model_PID__038_loss_0.418_vloss_0.449_acc_0.813_vacc_0.792.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4175 - accuracy: 0.8126 - val_loss: 0.4486 - val_accuracy: 0.7919\n",
            "Epoch 39/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8074\n",
            "Epoch 39: loss improved from 0.41751 to 0.41745, saving model to ./model_PID__039_loss_0.417_vloss_0.434_acc_0.807_vacc_0.808.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4175 - accuracy: 0.8074 - val_loss: 0.4344 - val_accuracy: 0.8084\n",
            "Epoch 40/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.4145 - accuracy: 0.8087\n",
            "Epoch 40: loss improved from 0.41745 to 0.41525, saving model to ./model_PID__040_loss_0.415_vloss_0.440_acc_0.809_vacc_0.805.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4152 - accuracy: 0.8087 - val_loss: 0.4400 - val_accuracy: 0.8054\n",
            "Epoch 41/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.4157 - accuracy: 0.8101\n",
            "Epoch 41: loss improved from 0.41525 to 0.41400, saving model to ./model_PID__041_loss_0.414_vloss_0.436_acc_0.812_vacc_0.810.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4140 - accuracy: 0.8119 - val_loss: 0.4357 - val_accuracy: 0.8099\n",
            "Epoch 42/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8066\n",
            "Epoch 42: loss improved from 0.41400 to 0.41364, saving model to ./model_PID__042_loss_0.414_vloss_0.429_acc_0.807_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4136 - accuracy: 0.8074 - val_loss: 0.4288 - val_accuracy: 0.8129\n",
            "Epoch 43/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.4130 - accuracy: 0.8108\n",
            "Epoch 43: loss improved from 0.41364 to 0.41150, saving model to ./model_PID__043_loss_0.411_vloss_0.432_acc_0.811_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4115 - accuracy: 0.8113 - val_loss: 0.4321 - val_accuracy: 0.8114\n",
            "Epoch 44/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.4069 - accuracy: 0.8190\n",
            "Epoch 44: loss improved from 0.41150 to 0.41001, saving model to ./model_PID__044_loss_0.410_vloss_0.425_acc_0.816_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4100 - accuracy: 0.8158 - val_loss: 0.4246 - val_accuracy: 0.8159\n",
            "Epoch 45/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8141\n",
            "Epoch 45: loss improved from 0.41001 to 0.40896, saving model to ./model_PID__045_loss_0.409_vloss_0.430_acc_0.814_vacc_0.810.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4090 - accuracy: 0.8139 - val_loss: 0.4302 - val_accuracy: 0.8099\n",
            "Epoch 46/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.4083 - accuracy: 0.8103\n",
            "Epoch 46: loss improved from 0.40896 to 0.40768, saving model to ./model_PID__046_loss_0.408_vloss_0.433_acc_0.811_vacc_0.810.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4077 - accuracy: 0.8107 - val_loss: 0.4334 - val_accuracy: 0.8099\n",
            "Epoch 47/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.4045 - accuracy: 0.8090\n",
            "Epoch 47: loss improved from 0.40768 to 0.40604, saving model to ./model_PID__047_loss_0.406_vloss_0.421_acc_0.808_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4060 - accuracy: 0.8081 - val_loss: 0.4209 - val_accuracy: 0.8159\n",
            "Epoch 48/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.4016 - accuracy: 0.8164\n",
            "Epoch 48: loss improved from 0.40604 to 0.40562, saving model to ./model_PID__048_loss_0.406_vloss_0.423_acc_0.813_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4056 - accuracy: 0.8132 - val_loss: 0.4231 - val_accuracy: 0.8174\n",
            "Epoch 49/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8096\n",
            "Epoch 49: loss did not improve from 0.40562\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4058 - accuracy: 0.8107 - val_loss: 0.4293 - val_accuracy: 0.8159\n",
            "Epoch 50/500\n",
            "1528/1558 [============================>.] - ETA: 0s - loss: 0.4025 - accuracy: 0.8115\n",
            "Epoch 50: loss improved from 0.40562 to 0.40371, saving model to ./model_PID__050_loss_0.404_vloss_0.423_acc_0.811_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.4037 - accuracy: 0.8113 - val_loss: 0.4232 - val_accuracy: 0.8189\n",
            "Epoch 51/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.8103\n",
            "Epoch 51: loss improved from 0.40371 to 0.40157, saving model to ./model_PID__051_loss_0.402_vloss_0.431_acc_0.811_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4016 - accuracy: 0.8113 - val_loss: 0.4310 - val_accuracy: 0.8174\n",
            "Epoch 52/500\n",
            "1526/1558 [============================>.] - ETA: 0s - loss: 0.4034 - accuracy: 0.8126\n",
            "Epoch 52: loss improved from 0.40157 to 0.40156, saving model to ./model_PID__052_loss_0.402_vloss_0.428_acc_0.814_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.4016 - accuracy: 0.8139 - val_loss: 0.4282 - val_accuracy: 0.8174\n",
            "Epoch 53/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.8145\n",
            "Epoch 53: loss improved from 0.40156 to 0.40102, saving model to ./model_PID__053_loss_0.401_vloss_0.424_acc_0.815_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.4010 - accuracy: 0.8145 - val_loss: 0.4239 - val_accuracy: 0.8174\n",
            "Epoch 54/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.4012 - accuracy: 0.8142\n",
            "Epoch 54: loss improved from 0.40102 to 0.39970, saving model to ./model_PID__054_loss_0.400_vloss_0.426_acc_0.816_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3997 - accuracy: 0.8158 - val_loss: 0.4256 - val_accuracy: 0.8174\n",
            "Epoch 55/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3984 - accuracy: 0.8151\n",
            "Epoch 55: loss improved from 0.39970 to 0.39881, saving model to ./model_PID__055_loss_0.399_vloss_0.428_acc_0.815_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3988 - accuracy: 0.8145 - val_loss: 0.4284 - val_accuracy: 0.8144\n",
            "Epoch 56/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8125\n",
            "Epoch 56: loss improved from 0.39881 to 0.39758, saving model to ./model_PID__056_loss_0.398_vloss_0.428_acc_0.813_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3976 - accuracy: 0.8132 - val_loss: 0.4284 - val_accuracy: 0.8144\n",
            "Epoch 57/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8154\n",
            "Epoch 57: loss improved from 0.39758 to 0.39673, saving model to ./model_PID__057_loss_0.397_vloss_0.433_acc_0.816_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3967 - accuracy: 0.8164 - val_loss: 0.4330 - val_accuracy: 0.8129\n",
            "Epoch 58/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8123\n",
            "Epoch 58: loss did not improve from 0.39673\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3968 - accuracy: 0.8126 - val_loss: 0.4246 - val_accuracy: 0.8159\n",
            "Epoch 59/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3968 - accuracy: 0.8141\n",
            "Epoch 59: loss improved from 0.39673 to 0.39580, saving model to ./model_PID__059_loss_0.396_vloss_0.421_acc_0.815_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3958 - accuracy: 0.8151 - val_loss: 0.4208 - val_accuracy: 0.8174\n",
            "Epoch 60/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3963 - accuracy: 0.8144\n",
            "Epoch 60: loss did not improve from 0.39580\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3961 - accuracy: 0.8145 - val_loss: 0.4261 - val_accuracy: 0.8174\n",
            "Epoch 61/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3935 - accuracy: 0.8168\n",
            "Epoch 61: loss improved from 0.39580 to 0.39497, saving model to ./model_PID__061_loss_0.395_vloss_0.421_acc_0.816_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3950 - accuracy: 0.8158 - val_loss: 0.4213 - val_accuracy: 0.8189\n",
            "Epoch 62/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3927 - accuracy: 0.8136\n",
            "Epoch 62: loss improved from 0.39497 to 0.39347, saving model to ./model_PID__062_loss_0.393_vloss_0.423_acc_0.813_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3935 - accuracy: 0.8132 - val_loss: 0.4228 - val_accuracy: 0.8174\n",
            "Epoch 63/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.8158\n",
            "Epoch 63: loss improved from 0.39347 to 0.39336, saving model to ./model_PID__063_loss_0.393_vloss_0.422_acc_0.816_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3934 - accuracy: 0.8158 - val_loss: 0.4218 - val_accuracy: 0.8159\n",
            "Epoch 64/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3897 - accuracy: 0.8154\n",
            "Epoch 64: loss improved from 0.39336 to 0.39144, saving model to ./model_PID__064_loss_0.391_vloss_0.421_acc_0.814_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3914 - accuracy: 0.8139 - val_loss: 0.4213 - val_accuracy: 0.8204\n",
            "Epoch 65/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3934 - accuracy: 0.8146\n",
            "Epoch 65: loss did not improve from 0.39144\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3927 - accuracy: 0.8151 - val_loss: 0.4227 - val_accuracy: 0.8114\n",
            "Epoch 66/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3910 - accuracy: 0.8156\n",
            "Epoch 66: loss improved from 0.39144 to 0.39093, saving model to ./model_PID__066_loss_0.391_vloss_0.420_acc_0.816_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3909 - accuracy: 0.8158 - val_loss: 0.4201 - val_accuracy: 0.8204\n",
            "Epoch 67/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3888 - accuracy: 0.8184\n",
            "Epoch 67: loss did not improve from 0.39093\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3914 - accuracy: 0.8164 - val_loss: 0.4210 - val_accuracy: 0.8159\n",
            "Epoch 68/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3908 - accuracy: 0.8162\n",
            "Epoch 68: loss did not improve from 0.39093\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3912 - accuracy: 0.8164 - val_loss: 0.4240 - val_accuracy: 0.8114\n",
            "Epoch 69/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8178\n",
            "Epoch 69: loss improved from 0.39093 to 0.38984, saving model to ./model_PID__069_loss_0.390_vloss_0.427_acc_0.818_vacc_0.810.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3898 - accuracy: 0.8177 - val_loss: 0.4274 - val_accuracy: 0.8099\n",
            "Epoch 70/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3926 - accuracy: 0.8135\n",
            "Epoch 70: loss did not improve from 0.38984\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3913 - accuracy: 0.8151 - val_loss: 0.4232 - val_accuracy: 0.8144\n",
            "Epoch 71/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8175\n",
            "Epoch 71: loss improved from 0.38984 to 0.38914, saving model to ./model_PID__071_loss_0.389_vloss_0.422_acc_0.818_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3891 - accuracy: 0.8184 - val_loss: 0.4219 - val_accuracy: 0.8174\n",
            "Epoch 72/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8178\n",
            "Epoch 72: loss did not improve from 0.38914\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3891 - accuracy: 0.8184 - val_loss: 0.4221 - val_accuracy: 0.8129\n",
            "Epoch 73/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8195\n",
            "Epoch 73: loss improved from 0.38914 to 0.38771, saving model to ./model_PID__073_loss_0.388_vloss_0.425_acc_0.820_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3877 - accuracy: 0.8196 - val_loss: 0.4250 - val_accuracy: 0.8129\n",
            "Epoch 74/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3890 - accuracy: 0.8139\n",
            "Epoch 74: loss did not improve from 0.38771\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3883 - accuracy: 0.8145 - val_loss: 0.4162 - val_accuracy: 0.8204\n",
            "Epoch 75/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8216\n",
            "Epoch 75: loss improved from 0.38771 to 0.38756, saving model to ./model_PID__075_loss_0.388_vloss_0.424_acc_0.822_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3876 - accuracy: 0.8222 - val_loss: 0.4239 - val_accuracy: 0.8129\n",
            "Epoch 76/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.8181\n",
            "Epoch 76: loss improved from 0.38756 to 0.38634, saving model to ./model_PID__076_loss_0.386_vloss_0.423_acc_0.818_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3863 - accuracy: 0.8184 - val_loss: 0.4233 - val_accuracy: 0.8174\n",
            "Epoch 77/500\n",
            "1532/1558 [============================>.] - ETA: 0s - loss: 0.3901 - accuracy: 0.8185\n",
            "Epoch 77: loss did not improve from 0.38634\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3875 - accuracy: 0.8203 - val_loss: 0.4173 - val_accuracy: 0.8114\n",
            "Epoch 78/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.8187\n",
            "Epoch 78: loss improved from 0.38634 to 0.38499, saving model to ./model_PID__078_loss_0.385_vloss_0.421_acc_0.820_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3850 - accuracy: 0.8196 - val_loss: 0.4207 - val_accuracy: 0.8129\n",
            "Epoch 79/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3843 - accuracy: 0.8205\n",
            "Epoch 79: loss did not improve from 0.38499\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3852 - accuracy: 0.8203 - val_loss: 0.4228 - val_accuracy: 0.8159\n",
            "Epoch 80/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.8196\n",
            "Epoch 80: loss improved from 0.38499 to 0.38435, saving model to ./model_PID__080_loss_0.384_vloss_0.416_acc_0.820_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3843 - accuracy: 0.8196 - val_loss: 0.4164 - val_accuracy: 0.8189\n",
            "Epoch 81/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3832 - accuracy: 0.8201\n",
            "Epoch 81: loss improved from 0.38435 to 0.38388, saving model to ./model_PID__081_loss_0.384_vloss_0.417_acc_0.820_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3839 - accuracy: 0.8196 - val_loss: 0.4175 - val_accuracy: 0.8174\n",
            "Epoch 82/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3838 - accuracy: 0.8209\n",
            "Epoch 82: loss improved from 0.38388 to 0.38292, saving model to ./model_PID__082_loss_0.383_vloss_0.417_acc_0.822_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3829 - accuracy: 0.8216 - val_loss: 0.4171 - val_accuracy: 0.8174\n",
            "Epoch 83/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3838 - accuracy: 0.8195\n",
            "Epoch 83: loss did not improve from 0.38292\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3832 - accuracy: 0.8190 - val_loss: 0.4160 - val_accuracy: 0.8189\n",
            "Epoch 84/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.8185\n",
            "Epoch 84: loss improved from 0.38292 to 0.38273, saving model to ./model_PID__084_loss_0.383_vloss_0.414_acc_0.818_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3827 - accuracy: 0.8184 - val_loss: 0.4136 - val_accuracy: 0.8159\n",
            "Epoch 85/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8233\n",
            "Epoch 85: loss improved from 0.38273 to 0.38175, saving model to ./model_PID__085_loss_0.382_vloss_0.418_acc_0.823_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3817 - accuracy: 0.8228 - val_loss: 0.4176 - val_accuracy: 0.8174\n",
            "Epoch 86/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.8184\n",
            "Epoch 86: loss improved from 0.38175 to 0.38103, saving model to ./model_PID__086_loss_0.381_vloss_0.411_acc_0.818_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3810 - accuracy: 0.8184 - val_loss: 0.4113 - val_accuracy: 0.8219\n",
            "Epoch 87/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8206\n",
            "Epoch 87: loss did not improve from 0.38103\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3819 - accuracy: 0.8222 - val_loss: 0.4139 - val_accuracy: 0.8159\n",
            "Epoch 88/500\n",
            "1528/1558 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8168\n",
            "Epoch 88: loss improved from 0.38103 to 0.38092, saving model to ./model_PID__088_loss_0.381_vloss_0.411_acc_0.818_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3809 - accuracy: 0.8184 - val_loss: 0.4107 - val_accuracy: 0.8174\n",
            "Epoch 89/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.8253\n",
            "Epoch 89: loss improved from 0.38092 to 0.37920, saving model to ./model_PID__089_loss_0.379_vloss_0.415_acc_0.825_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3792 - accuracy: 0.8254 - val_loss: 0.4153 - val_accuracy: 0.8159\n",
            "Epoch 90/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3807 - accuracy: 0.8228\n",
            "Epoch 90: loss did not improve from 0.37920\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3802 - accuracy: 0.8228 - val_loss: 0.4129 - val_accuracy: 0.8159\n",
            "Epoch 91/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8212\n",
            "Epoch 91: loss improved from 0.37920 to 0.37879, saving model to ./model_PID__091_loss_0.379_vloss_0.412_acc_0.820_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3788 - accuracy: 0.8203 - val_loss: 0.4117 - val_accuracy: 0.8189\n",
            "Epoch 92/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3780 - accuracy: 0.8186\n",
            "Epoch 92: loss improved from 0.37879 to 0.37751, saving model to ./model_PID__092_loss_0.378_vloss_0.409_acc_0.819_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3775 - accuracy: 0.8190 - val_loss: 0.4094 - val_accuracy: 0.8189\n",
            "Epoch 93/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.8216\n",
            "Epoch 93: loss did not improve from 0.37751\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3776 - accuracy: 0.8216 - val_loss: 0.4113 - val_accuracy: 0.8174\n",
            "Epoch 94/500\n",
            "1527/1558 [============================>.] - ETA: 0s - loss: 0.3757 - accuracy: 0.8212\n",
            "Epoch 94: loss improved from 0.37751 to 0.37570, saving model to ./model_PID__094_loss_0.376_vloss_0.410_acc_0.822_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3757 - accuracy: 0.8222 - val_loss: 0.4105 - val_accuracy: 0.8204\n",
            "Epoch 95/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3792 - accuracy: 0.8243\n",
            "Epoch 95: loss did not improve from 0.37570\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3767 - accuracy: 0.8261 - val_loss: 0.4116 - val_accuracy: 0.8189\n",
            "Epoch 96/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3774 - accuracy: 0.8236\n",
            "Epoch 96: loss did not improve from 0.37570\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3763 - accuracy: 0.8235 - val_loss: 0.4042 - val_accuracy: 0.8219\n",
            "Epoch 97/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3746 - accuracy: 0.8228\n",
            "Epoch 97: loss did not improve from 0.37570\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3759 - accuracy: 0.8216 - val_loss: 0.4094 - val_accuracy: 0.8159\n",
            "Epoch 98/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8215\n",
            "Epoch 98: loss did not improve from 0.37570\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3763 - accuracy: 0.8209 - val_loss: 0.4078 - val_accuracy: 0.8204\n",
            "Epoch 99/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8244\n",
            "Epoch 99: loss improved from 0.37570 to 0.37505, saving model to ./model_PID__099_loss_0.375_vloss_0.407_acc_0.823_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3751 - accuracy: 0.8235 - val_loss: 0.4067 - val_accuracy: 0.8189\n",
            "Epoch 100/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8239\n",
            "Epoch 100: loss did not improve from 0.37505\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3753 - accuracy: 0.8235 - val_loss: 0.4028 - val_accuracy: 0.8204\n",
            "Epoch 101/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3720 - accuracy: 0.8241\n",
            "Epoch 101: loss improved from 0.37505 to 0.37413, saving model to ./model_PID__101_loss_0.374_vloss_0.403_acc_0.822_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3741 - accuracy: 0.8222 - val_loss: 0.4026 - val_accuracy: 0.8159\n",
            "Epoch 102/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8235\n",
            "Epoch 102: loss did not improve from 0.37413\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3742 - accuracy: 0.8235 - val_loss: 0.4031 - val_accuracy: 0.8189\n",
            "Epoch 103/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8211\n",
            "Epoch 103: loss improved from 0.37413 to 0.37322, saving model to ./model_PID__103_loss_0.373_vloss_0.400_acc_0.822_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3732 - accuracy: 0.8216 - val_loss: 0.4001 - val_accuracy: 0.8234\n",
            "Epoch 104/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8174\n",
            "Epoch 104: loss improved from 0.37322 to 0.37238, saving model to ./model_PID__104_loss_0.372_vloss_0.403_acc_0.818_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3724 - accuracy: 0.8184 - val_loss: 0.4026 - val_accuracy: 0.8204\n",
            "Epoch 105/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8248\n",
            "Epoch 105: loss improved from 0.37238 to 0.37132, saving model to ./model_PID__105_loss_0.371_vloss_0.399_acc_0.826_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3713 - accuracy: 0.8261 - val_loss: 0.3991 - val_accuracy: 0.8249\n",
            "Epoch 106/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8272\n",
            "Epoch 106: loss improved from 0.37132 to 0.37126, saving model to ./model_PID__106_loss_0.371_vloss_0.399_acc_0.825_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3713 - accuracy: 0.8254 - val_loss: 0.3986 - val_accuracy: 0.8204\n",
            "Epoch 107/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8250\n",
            "Epoch 107: loss improved from 0.37126 to 0.36959, saving model to ./model_PID__107_loss_0.370_vloss_0.399_acc_0.824_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3696 - accuracy: 0.8241 - val_loss: 0.3986 - val_accuracy: 0.8234\n",
            "Epoch 108/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8259\n",
            "Epoch 108: loss did not improve from 0.36959\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3717 - accuracy: 0.8254 - val_loss: 0.4010 - val_accuracy: 0.8278\n",
            "Epoch 109/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8239\n",
            "Epoch 109: loss improved from 0.36959 to 0.36946, saving model to ./model_PID__109_loss_0.369_vloss_0.397_acc_0.824_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3695 - accuracy: 0.8241 - val_loss: 0.3974 - val_accuracy: 0.8219\n",
            "Epoch 110/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3714 - accuracy: 0.8219\n",
            "Epoch 110: loss did not improve from 0.36946\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3701 - accuracy: 0.8228 - val_loss: 0.3952 - val_accuracy: 0.8219\n",
            "Epoch 111/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3703 - accuracy: 0.8265\n",
            "Epoch 111: loss did not improve from 0.36946\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3705 - accuracy: 0.8261 - val_loss: 0.3952 - val_accuracy: 0.8234\n",
            "Epoch 112/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8273\n",
            "Epoch 112: loss improved from 0.36946 to 0.36792, saving model to ./model_PID__112_loss_0.368_vloss_0.401_acc_0.826_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3679 - accuracy: 0.8261 - val_loss: 0.4013 - val_accuracy: 0.8234\n",
            "Epoch 113/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8275\n",
            "Epoch 113: loss did not improve from 0.36792\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3687 - accuracy: 0.8273 - val_loss: 0.3950 - val_accuracy: 0.8249\n",
            "Epoch 114/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8306\n",
            "Epoch 114: loss improved from 0.36792 to 0.36666, saving model to ./model_PID__114_loss_0.367_vloss_0.394_acc_0.830_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3667 - accuracy: 0.8299 - val_loss: 0.3942 - val_accuracy: 0.8234\n",
            "Epoch 115/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3696 - accuracy: 0.8272\n",
            "Epoch 115: loss did not improve from 0.36666\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3674 - accuracy: 0.8286 - val_loss: 0.3923 - val_accuracy: 0.8219\n",
            "Epoch 116/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.8267\n",
            "Epoch 116: loss improved from 0.36666 to 0.36560, saving model to ./model_PID__116_loss_0.366_vloss_0.395_acc_0.827_vacc_0.828.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3656 - accuracy: 0.8267 - val_loss: 0.3954 - val_accuracy: 0.8278\n",
            "Epoch 117/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3681 - accuracy: 0.8266\n",
            "Epoch 117: loss did not improve from 0.36560\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3688 - accuracy: 0.8261 - val_loss: 0.3915 - val_accuracy: 0.8263\n",
            "Epoch 118/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8299\n",
            "Epoch 118: loss improved from 0.36560 to 0.36499, saving model to ./model_PID__118_loss_0.365_vloss_0.392_acc_0.828_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3650 - accuracy: 0.8280 - val_loss: 0.3916 - val_accuracy: 0.8263\n",
            "Epoch 119/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8241\n",
            "Epoch 119: loss improved from 0.36499 to 0.36471, saving model to ./model_PID__119_loss_0.365_vloss_0.391_acc_0.827_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3647 - accuracy: 0.8267 - val_loss: 0.3906 - val_accuracy: 0.8263\n",
            "Epoch 120/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3665 - accuracy: 0.8279\n",
            "Epoch 120: loss did not improve from 0.36471\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3665 - accuracy: 0.8280 - val_loss: 0.3894 - val_accuracy: 0.8249\n",
            "Epoch 121/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8255\n",
            "Epoch 121: loss improved from 0.36471 to 0.36327, saving model to ./model_PID__121_loss_0.363_vloss_0.397_acc_0.826_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3633 - accuracy: 0.8261 - val_loss: 0.3968 - val_accuracy: 0.8249\n",
            "Epoch 122/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8269\n",
            "Epoch 122: loss improved from 0.36327 to 0.36221, saving model to ./model_PID__122_loss_0.362_vloss_0.394_acc_0.826_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3622 - accuracy: 0.8261 - val_loss: 0.3944 - val_accuracy: 0.8249\n",
            "Epoch 123/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.8296\n",
            "Epoch 123: loss did not improve from 0.36221\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3644 - accuracy: 0.8299 - val_loss: 0.3917 - val_accuracy: 0.8263\n",
            "Epoch 124/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8303\n",
            "Epoch 124: loss improved from 0.36221 to 0.36178, saving model to ./model_PID__124_loss_0.362_vloss_0.390_acc_0.830_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3618 - accuracy: 0.8299 - val_loss: 0.3903 - val_accuracy: 0.8249\n",
            "Epoch 125/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8295\n",
            "Epoch 125: loss did not improve from 0.36178\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3636 - accuracy: 0.8273 - val_loss: 0.3881 - val_accuracy: 0.8234\n",
            "Epoch 126/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8326\n",
            "Epoch 126: loss improved from 0.36178 to 0.36050, saving model to ./model_PID__126_loss_0.361_vloss_0.391_acc_0.832_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3605 - accuracy: 0.8318 - val_loss: 0.3907 - val_accuracy: 0.8219\n",
            "Epoch 127/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3603 - accuracy: 0.8289\n",
            "Epoch 127: loss improved from 0.36050 to 0.36013, saving model to ./model_PID__127_loss_0.360_vloss_0.390_acc_0.829_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3601 - accuracy: 0.8293 - val_loss: 0.3902 - val_accuracy: 0.8219\n",
            "Epoch 128/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.8303\n",
            "Epoch 128: loss improved from 0.36013 to 0.35996, saving model to ./model_PID__128_loss_0.360_vloss_0.392_acc_0.830_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3600 - accuracy: 0.8299 - val_loss: 0.3925 - val_accuracy: 0.8219\n",
            "Epoch 129/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3578 - accuracy: 0.8278\n",
            "Epoch 129: loss did not improve from 0.35996\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3604 - accuracy: 0.8261 - val_loss: 0.3878 - val_accuracy: 0.8249\n",
            "Epoch 130/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.8282\n",
            "Epoch 130: loss improved from 0.35996 to 0.35960, saving model to ./model_PID__130_loss_0.360_vloss_0.387_acc_0.829_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3596 - accuracy: 0.8286 - val_loss: 0.3872 - val_accuracy: 0.8249\n",
            "Epoch 131/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.8288\n",
            "Epoch 131: loss improved from 0.35960 to 0.35923, saving model to ./model_PID__131_loss_0.359_vloss_0.389_acc_0.829_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3592 - accuracy: 0.8293 - val_loss: 0.3887 - val_accuracy: 0.8234\n",
            "Epoch 132/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8269\n",
            "Epoch 132: loss improved from 0.35923 to 0.35865, saving model to ./model_PID__132_loss_0.359_vloss_0.387_acc_0.827_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3587 - accuracy: 0.8267 - val_loss: 0.3873 - val_accuracy: 0.8249\n",
            "Epoch 133/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8286\n",
            "Epoch 133: loss did not improve from 0.35865\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3593 - accuracy: 0.8280 - val_loss: 0.3889 - val_accuracy: 0.8249\n",
            "Epoch 134/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3563 - accuracy: 0.8290\n",
            "Epoch 134: loss improved from 0.35865 to 0.35649, saving model to ./model_PID__134_loss_0.356_vloss_0.389_acc_0.829_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3565 - accuracy: 0.8293 - val_loss: 0.3889 - val_accuracy: 0.8249\n",
            "Epoch 135/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8299\n",
            "Epoch 135: loss did not improve from 0.35649\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3570 - accuracy: 0.8299 - val_loss: 0.3918 - val_accuracy: 0.8204\n",
            "Epoch 136/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3562 - accuracy: 0.8326\n",
            "Epoch 136: loss did not improve from 0.35649\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3567 - accuracy: 0.8318 - val_loss: 0.3875 - val_accuracy: 0.8234\n",
            "Epoch 137/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8326\n",
            "Epoch 137: loss improved from 0.35649 to 0.35437, saving model to ./model_PID__137_loss_0.354_vloss_0.391_acc_0.833_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3544 - accuracy: 0.8331 - val_loss: 0.3905 - val_accuracy: 0.8263\n",
            "Epoch 138/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3540 - accuracy: 0.8288\n",
            "Epoch 138: loss did not improve from 0.35437\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3553 - accuracy: 0.8280 - val_loss: 0.3882 - val_accuracy: 0.8249\n",
            "Epoch 139/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3556 - accuracy: 0.8267\n",
            "Epoch 139: loss did not improve from 0.35437\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3548 - accuracy: 0.8273 - val_loss: 0.3869 - val_accuracy: 0.8263\n",
            "Epoch 140/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8286\n",
            "Epoch 140: loss improved from 0.35437 to 0.35418, saving model to ./model_PID__140_loss_0.354_vloss_0.388_acc_0.829_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3542 - accuracy: 0.8293 - val_loss: 0.3883 - val_accuracy: 0.8263\n",
            "Epoch 141/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.8312\n",
            "Epoch 141: loss improved from 0.35418 to 0.35316, saving model to ./model_PID__141_loss_0.353_vloss_0.387_acc_0.831_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3532 - accuracy: 0.8312 - val_loss: 0.3873 - val_accuracy: 0.8263\n",
            "Epoch 142/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8318\n",
            "Epoch 142: loss improved from 0.35316 to 0.35243, saving model to ./model_PID__142_loss_0.352_vloss_0.386_acc_0.832_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3524 - accuracy: 0.8318 - val_loss: 0.3864 - val_accuracy: 0.8204\n",
            "Epoch 143/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8341\n",
            "Epoch 143: loss improved from 0.35243 to 0.35145, saving model to ./model_PID__143_loss_0.351_vloss_0.383_acc_0.831_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3515 - accuracy: 0.8312 - val_loss: 0.3831 - val_accuracy: 0.8263\n",
            "Epoch 144/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8317\n",
            "Epoch 144: loss did not improve from 0.35145\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3525 - accuracy: 0.8318 - val_loss: 0.3862 - val_accuracy: 0.8234\n",
            "Epoch 145/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3522 - accuracy: 0.8344\n",
            "Epoch 145: loss improved from 0.35145 to 0.35134, saving model to ./model_PID__145_loss_0.351_vloss_0.386_acc_0.835_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3513 - accuracy: 0.8350 - val_loss: 0.3858 - val_accuracy: 0.8249\n",
            "Epoch 146/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3494 - accuracy: 0.8313\n",
            "Epoch 146: loss improved from 0.35134 to 0.34986, saving model to ./model_PID__146_loss_0.350_vloss_0.387_acc_0.831_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3499 - accuracy: 0.8312 - val_loss: 0.3867 - val_accuracy: 0.8263\n",
            "Epoch 147/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8325\n",
            "Epoch 147: loss did not improve from 0.34986\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3506 - accuracy: 0.8325 - val_loss: 0.3842 - val_accuracy: 0.8293\n",
            "Epoch 148/500\n",
            "1526/1558 [============================>.] - ETA: 0s - loss: 0.3485 - accuracy: 0.8316\n",
            "Epoch 148: loss improved from 0.34986 to 0.34879, saving model to ./model_PID__148_loss_0.349_vloss_0.386_acc_0.832_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3488 - accuracy: 0.8318 - val_loss: 0.3858 - val_accuracy: 0.8263\n",
            "Epoch 149/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3477 - accuracy: 0.8320\n",
            "Epoch 149: loss improved from 0.34879 to 0.34860, saving model to ./model_PID__149_loss_0.349_vloss_0.386_acc_0.831_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3486 - accuracy: 0.8312 - val_loss: 0.3864 - val_accuracy: 0.8234\n",
            "Epoch 150/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3457 - accuracy: 0.8324\n",
            "Epoch 150: loss improved from 0.34860 to 0.34813, saving model to ./model_PID__150_loss_0.348_vloss_0.384_acc_0.832_vacc_0.828.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3481 - accuracy: 0.8318 - val_loss: 0.3837 - val_accuracy: 0.8278\n",
            "Epoch 151/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3457 - accuracy: 0.8349\n",
            "Epoch 151: loss improved from 0.34813 to 0.34807, saving model to ./model_PID__151_loss_0.348_vloss_0.386_acc_0.833_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3481 - accuracy: 0.8331 - val_loss: 0.3860 - val_accuracy: 0.8263\n",
            "Epoch 152/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8313\n",
            "Epoch 152: loss improved from 0.34807 to 0.34751, saving model to ./model_PID__152_loss_0.348_vloss_0.385_acc_0.832_vacc_0.829.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3475 - accuracy: 0.8318 - val_loss: 0.3853 - val_accuracy: 0.8293\n",
            "Epoch 153/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8363\n",
            "Epoch 153: loss improved from 0.34751 to 0.34693, saving model to ./model_PID__153_loss_0.347_vloss_0.386_acc_0.834_vacc_0.826.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3469 - accuracy: 0.8338 - val_loss: 0.3856 - val_accuracy: 0.8263\n",
            "Epoch 154/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8353\n",
            "Epoch 154: loss improved from 0.34693 to 0.34630, saving model to ./model_PID__154_loss_0.346_vloss_0.392_acc_0.836_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3463 - accuracy: 0.8357 - val_loss: 0.3923 - val_accuracy: 0.8249\n",
            "Epoch 155/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.8341\n",
            "Epoch 155: loss did not improve from 0.34630\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3465 - accuracy: 0.8338 - val_loss: 0.3837 - val_accuracy: 0.8278\n",
            "Epoch 156/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.8361\n",
            "Epoch 156: loss improved from 0.34630 to 0.34468, saving model to ./model_PID__156_loss_0.345_vloss_0.386_acc_0.837_vacc_0.828.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3447 - accuracy: 0.8370 - val_loss: 0.3861 - val_accuracy: 0.8278\n",
            "Epoch 157/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.8343\n",
            "Epoch 157: loss did not improve from 0.34468\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3451 - accuracy: 0.8344 - val_loss: 0.3879 - val_accuracy: 0.8263\n",
            "Epoch 158/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3442 - accuracy: 0.8352\n",
            "Epoch 158: loss improved from 0.34468 to 0.34452, saving model to ./model_PID__158_loss_0.345_vloss_0.390_acc_0.835_vacc_0.829.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3445 - accuracy: 0.8350 - val_loss: 0.3897 - val_accuracy: 0.8293\n",
            "Epoch 159/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8316\n",
            "Epoch 159: loss did not improve from 0.34452\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3457 - accuracy: 0.8318 - val_loss: 0.3885 - val_accuracy: 0.8263\n",
            "Epoch 160/500\n",
            "1526/1558 [============================>.] - ETA: 0s - loss: 0.3447 - accuracy: 0.8336\n",
            "Epoch 160: loss improved from 0.34452 to 0.34311, saving model to ./model_PID__160_loss_0.343_vloss_0.390_acc_0.834_vacc_0.828.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3431 - accuracy: 0.8344 - val_loss: 0.3902 - val_accuracy: 0.8278\n",
            "Epoch 161/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8369\n",
            "Epoch 161: loss did not improve from 0.34311\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3439 - accuracy: 0.8370 - val_loss: 0.3928 - val_accuracy: 0.8249\n",
            "Epoch 162/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3428 - accuracy: 0.8342\n",
            "Epoch 162: loss improved from 0.34311 to 0.34190, saving model to ./model_PID__162_loss_0.342_vloss_0.389_acc_0.835_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3419 - accuracy: 0.8350 - val_loss: 0.3887 - val_accuracy: 0.8234\n",
            "Epoch 163/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.8390\n",
            "Epoch 163: loss improved from 0.34190 to 0.34135, saving model to ./model_PID__163_loss_0.341_vloss_0.394_acc_0.838_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3414 - accuracy: 0.8376 - val_loss: 0.3939 - val_accuracy: 0.8219\n",
            "Epoch 164/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3414 - accuracy: 0.8359\n",
            "Epoch 164: loss did not improve from 0.34135\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3427 - accuracy: 0.8357 - val_loss: 0.3974 - val_accuracy: 0.8293\n",
            "Epoch 165/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.8387\n",
            "Epoch 165: loss improved from 0.34135 to 0.34084, saving model to ./model_PID__165_loss_0.341_vloss_0.390_acc_0.838_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3408 - accuracy: 0.8376 - val_loss: 0.3903 - val_accuracy: 0.8234\n",
            "Epoch 166/500\n",
            "1528/1558 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8318\n",
            "Epoch 166: loss improved from 0.34084 to 0.34075, saving model to ./model_PID__166_loss_0.341_vloss_0.390_acc_0.835_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3407 - accuracy: 0.8350 - val_loss: 0.3896 - val_accuracy: 0.8249\n",
            "Epoch 167/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8369\n",
            "Epoch 167: loss did not improve from 0.34075\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3416 - accuracy: 0.8376 - val_loss: 0.3902 - val_accuracy: 0.8249\n",
            "Epoch 168/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8382\n",
            "Epoch 168: loss improved from 0.34075 to 0.34036, saving model to ./model_PID__168_loss_0.340_vloss_0.391_acc_0.837_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3404 - accuracy: 0.8370 - val_loss: 0.3912 - val_accuracy: 0.8219\n",
            "Epoch 169/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3390 - accuracy: 0.8382\n",
            "Epoch 169: loss improved from 0.34036 to 0.33896, saving model to ./model_PID__169_loss_0.339_vloss_0.393_acc_0.838_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3390 - accuracy: 0.8383 - val_loss: 0.3927 - val_accuracy: 0.8234\n",
            "Epoch 170/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8364\n",
            "Epoch 170: loss did not improve from 0.33896\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3399 - accuracy: 0.8363 - val_loss: 0.3951 - val_accuracy: 0.8219\n",
            "Epoch 171/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3390 - accuracy: 0.8421\n",
            "Epoch 171: loss improved from 0.33896 to 0.33862, saving model to ./model_PID__171_loss_0.339_vloss_0.395_acc_0.842_vacc_0.825.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3386 - accuracy: 0.8421 - val_loss: 0.3946 - val_accuracy: 0.8249\n",
            "Epoch 172/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8373\n",
            "Epoch 172: loss did not improve from 0.33862\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3391 - accuracy: 0.8376 - val_loss: 0.3956 - val_accuracy: 0.8249\n",
            "Epoch 173/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3402 - accuracy: 0.8359\n",
            "Epoch 173: loss did not improve from 0.33862\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3391 - accuracy: 0.8370 - val_loss: 0.3962 - val_accuracy: 0.8234\n",
            "Epoch 174/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3380 - accuracy: 0.8409\n",
            "Epoch 174: loss did not improve from 0.33862\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3388 - accuracy: 0.8408 - val_loss: 0.3956 - val_accuracy: 0.8204\n",
            "Epoch 175/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8382\n",
            "Epoch 175: loss improved from 0.33862 to 0.33822, saving model to ./model_PID__175_loss_0.338_vloss_0.397_acc_0.838_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3382 - accuracy: 0.8376 - val_loss: 0.3969 - val_accuracy: 0.8234\n",
            "Epoch 176/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.8343\n",
            "Epoch 176: loss improved from 0.33822 to 0.33807, saving model to ./model_PID__176_loss_0.338_vloss_0.398_acc_0.834_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3381 - accuracy: 0.8344 - val_loss: 0.3980 - val_accuracy: 0.8219\n",
            "Epoch 177/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8404\n",
            "Epoch 177: loss improved from 0.33807 to 0.33622, saving model to ./model_PID__177_loss_0.336_vloss_0.399_acc_0.840_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3362 - accuracy: 0.8402 - val_loss: 0.3989 - val_accuracy: 0.8189\n",
            "Epoch 178/500\n",
            "1527/1558 [============================>.] - ETA: 0s - loss: 0.3377 - accuracy: 0.8409\n",
            "Epoch 178: loss improved from 0.33622 to 0.33502, saving model to ./model_PID__178_loss_0.335_vloss_0.409_acc_0.843_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3350 - accuracy: 0.8427 - val_loss: 0.4087 - val_accuracy: 0.8114\n",
            "Epoch 179/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3383 - accuracy: 0.8397\n",
            "Epoch 179: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3383 - accuracy: 0.8395 - val_loss: 0.3973 - val_accuracy: 0.8189\n",
            "Epoch 180/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8403\n",
            "Epoch 180: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3368 - accuracy: 0.8408 - val_loss: 0.4025 - val_accuracy: 0.8189\n",
            "Epoch 181/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3367 - accuracy: 0.8421\n",
            "Epoch 181: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3364 - accuracy: 0.8415 - val_loss: 0.4014 - val_accuracy: 0.8189\n",
            "Epoch 182/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3351 - accuracy: 0.8412\n",
            "Epoch 182: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3360 - accuracy: 0.8415 - val_loss: 0.4007 - val_accuracy: 0.8219\n",
            "Epoch 183/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8448\n",
            "Epoch 183: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3369 - accuracy: 0.8447 - val_loss: 0.4002 - val_accuracy: 0.8174\n",
            "Epoch 184/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3361 - accuracy: 0.8414\n",
            "Epoch 184: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3364 - accuracy: 0.8415 - val_loss: 0.4019 - val_accuracy: 0.8174\n",
            "Epoch 185/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3372 - accuracy: 0.8344\n",
            "Epoch 185: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3366 - accuracy: 0.8344 - val_loss: 0.4029 - val_accuracy: 0.8174\n",
            "Epoch 186/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3335 - accuracy: 0.8418\n",
            "Epoch 186: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3354 - accuracy: 0.8402 - val_loss: 0.4068 - val_accuracy: 0.8174\n",
            "Epoch 187/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.8407\n",
            "Epoch 187: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3356 - accuracy: 0.8408 - val_loss: 0.4104 - val_accuracy: 0.8204\n",
            "Epoch 188/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.8438\n",
            "Epoch 188: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3354 - accuracy: 0.8440 - val_loss: 0.4097 - val_accuracy: 0.8174\n",
            "Epoch 189/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.8425\n",
            "Epoch 189: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3353 - accuracy: 0.8427 - val_loss: 0.4082 - val_accuracy: 0.8144\n",
            "Epoch 190/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.8414\n",
            "Epoch 190: loss did not improve from 0.33502\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3357 - accuracy: 0.8421 - val_loss: 0.4090 - val_accuracy: 0.8144\n",
            "Epoch 191/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8403\n",
            "Epoch 191: loss improved from 0.33502 to 0.33430, saving model to ./model_PID__191_loss_0.334_vloss_0.409_acc_0.841_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3343 - accuracy: 0.8408 - val_loss: 0.4090 - val_accuracy: 0.8189\n",
            "Epoch 192/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8444\n",
            "Epoch 192: loss did not improve from 0.33430\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3349 - accuracy: 0.8447 - val_loss: 0.4092 - val_accuracy: 0.8144\n",
            "Epoch 193/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8436\n",
            "Epoch 193: loss improved from 0.33430 to 0.33419, saving model to ./model_PID__193_loss_0.334_vloss_0.412_acc_0.842_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3342 - accuracy: 0.8421 - val_loss: 0.4115 - val_accuracy: 0.8174\n",
            "Epoch 194/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8417\n",
            "Epoch 194: loss did not improve from 0.33419\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3342 - accuracy: 0.8434 - val_loss: 0.4152 - val_accuracy: 0.8204\n",
            "Epoch 195/500\n",
            "1532/1558 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8414\n",
            "Epoch 195: loss did not improve from 0.33419\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3350 - accuracy: 0.8383 - val_loss: 0.4125 - val_accuracy: 0.8159\n",
            "Epoch 196/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8382\n",
            "Epoch 196: loss improved from 0.33419 to 0.33415, saving model to ./model_PID__196_loss_0.334_vloss_0.414_acc_0.840_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3342 - accuracy: 0.8395 - val_loss: 0.4144 - val_accuracy: 0.8204\n",
            "Epoch 197/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.8427\n",
            "Epoch 197: loss improved from 0.33415 to 0.33404, saving model to ./model_PID__197_loss_0.334_vloss_0.416_acc_0.843_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3340 - accuracy: 0.8434 - val_loss: 0.4160 - val_accuracy: 0.8159\n",
            "Epoch 198/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8430\n",
            "Epoch 198: loss improved from 0.33404 to 0.33369, saving model to ./model_PID__198_loss_0.334_vloss_0.421_acc_0.843_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3337 - accuracy: 0.8427 - val_loss: 0.4212 - val_accuracy: 0.8189\n",
            "Epoch 199/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8412\n",
            "Epoch 199: loss did not improve from 0.33369\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3341 - accuracy: 0.8415 - val_loss: 0.4179 - val_accuracy: 0.8159\n",
            "Epoch 200/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8460\n",
            "Epoch 200: loss improved from 0.33369 to 0.33344, saving model to ./model_PID__200_loss_0.333_vloss_0.417_acc_0.845_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3334 - accuracy: 0.8447 - val_loss: 0.4168 - val_accuracy: 0.8159\n",
            "Epoch 201/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8419\n",
            "Epoch 201: loss improved from 0.33344 to 0.33172, saving model to ./model_PID__201_loss_0.332_vloss_0.428_acc_0.842_vacc_0.823.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3317 - accuracy: 0.8421 - val_loss: 0.4281 - val_accuracy: 0.8234\n",
            "Epoch 202/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8424\n",
            "Epoch 202: loss did not improve from 0.33172\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3328 - accuracy: 0.8427 - val_loss: 0.4199 - val_accuracy: 0.8099\n",
            "Epoch 203/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3338 - accuracy: 0.8400\n",
            "Epoch 203: loss did not improve from 0.33172\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3335 - accuracy: 0.8402 - val_loss: 0.4205 - val_accuracy: 0.8129\n",
            "Epoch 204/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.8441\n",
            "Epoch 204: loss did not improve from 0.33172\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3331 - accuracy: 0.8440 - val_loss: 0.4232 - val_accuracy: 0.8129\n",
            "Epoch 205/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3314 - accuracy: 0.8445\n",
            "Epoch 205: loss did not improve from 0.33172\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3330 - accuracy: 0.8434 - val_loss: 0.4215 - val_accuracy: 0.8144\n",
            "Epoch 206/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3322 - accuracy: 0.8421\n",
            "Epoch 206: loss did not improve from 0.33172\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3322 - accuracy: 0.8421 - val_loss: 0.4234 - val_accuracy: 0.8159\n",
            "Epoch 207/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.8389\n",
            "Epoch 207: loss did not improve from 0.33172\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3327 - accuracy: 0.8402 - val_loss: 0.4267 - val_accuracy: 0.8189\n",
            "Epoch 208/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3321 - accuracy: 0.8465\n",
            "Epoch 208: loss improved from 0.33172 to 0.33125, saving model to ./model_PID__208_loss_0.331_vloss_0.422_acc_0.847_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3312 - accuracy: 0.8466 - val_loss: 0.4219 - val_accuracy: 0.8129\n",
            "Epoch 209/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3343 - accuracy: 0.8392\n",
            "Epoch 209: loss did not improve from 0.33125\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3333 - accuracy: 0.8402 - val_loss: 0.4233 - val_accuracy: 0.8144\n",
            "Epoch 210/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.8462\n",
            "Epoch 210: loss did not improve from 0.33125\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3331 - accuracy: 0.8447 - val_loss: 0.4257 - val_accuracy: 0.8129\n",
            "Epoch 211/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.8459\n",
            "Epoch 211: loss did not improve from 0.33125\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3316 - accuracy: 0.8447 - val_loss: 0.4260 - val_accuracy: 0.8114\n",
            "Epoch 212/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8444\n",
            "Epoch 212: loss did not improve from 0.33125\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3319 - accuracy: 0.8434 - val_loss: 0.4245 - val_accuracy: 0.8114\n",
            "Epoch 213/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8453\n",
            "Epoch 213: loss improved from 0.33125 to 0.33027, saving model to ./model_PID__213_loss_0.330_vloss_0.423_acc_0.846_vacc_0.810.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3303 - accuracy: 0.8460 - val_loss: 0.4235 - val_accuracy: 0.8099\n",
            "Epoch 214/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.8432\n",
            "Epoch 214: loss did not improve from 0.33027\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3312 - accuracy: 0.8440 - val_loss: 0.4290 - val_accuracy: 0.8174\n",
            "Epoch 215/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.8480\n",
            "Epoch 215: loss improved from 0.33027 to 0.32987, saving model to ./model_PID__215_loss_0.330_vloss_0.429_acc_0.847_vacc_0.808.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3299 - accuracy: 0.8466 - val_loss: 0.4287 - val_accuracy: 0.8084\n",
            "Epoch 216/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8446\n",
            "Epoch 216: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3318 - accuracy: 0.8440 - val_loss: 0.4263 - val_accuracy: 0.8114\n",
            "Epoch 217/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.8453\n",
            "Epoch 217: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3308 - accuracy: 0.8453 - val_loss: 0.4263 - val_accuracy: 0.8144\n",
            "Epoch 218/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3297 - accuracy: 0.8452\n",
            "Epoch 218: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3314 - accuracy: 0.8440 - val_loss: 0.4314 - val_accuracy: 0.8144\n",
            "Epoch 219/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8482\n",
            "Epoch 219: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3306 - accuracy: 0.8492 - val_loss: 0.4292 - val_accuracy: 0.8084\n",
            "Epoch 220/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8442\n",
            "Epoch 220: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3303 - accuracy: 0.8440 - val_loss: 0.4259 - val_accuracy: 0.8174\n",
            "Epoch 221/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8443\n",
            "Epoch 221: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3306 - accuracy: 0.8447 - val_loss: 0.4286 - val_accuracy: 0.8174\n",
            "Epoch 222/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.8427\n",
            "Epoch 222: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3317 - accuracy: 0.8427 - val_loss: 0.4288 - val_accuracy: 0.8159\n",
            "Epoch 223/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.8495\n",
            "Epoch 223: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3300 - accuracy: 0.8485 - val_loss: 0.4311 - val_accuracy: 0.8114\n",
            "Epoch 224/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8434\n",
            "Epoch 224: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3310 - accuracy: 0.8440 - val_loss: 0.4323 - val_accuracy: 0.8174\n",
            "Epoch 225/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3306 - accuracy: 0.8490\n",
            "Epoch 225: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3304 - accuracy: 0.8492 - val_loss: 0.4342 - val_accuracy: 0.8129\n",
            "Epoch 226/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8456\n",
            "Epoch 226: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3299 - accuracy: 0.8453 - val_loss: 0.4307 - val_accuracy: 0.8129\n",
            "Epoch 227/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8459\n",
            "Epoch 227: loss did not improve from 0.32987\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3303 - accuracy: 0.8466 - val_loss: 0.4316 - val_accuracy: 0.8129\n",
            "Epoch 228/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3299 - accuracy: 0.8473\n",
            "Epoch 228: loss improved from 0.32987 to 0.32904, saving model to ./model_PID__228_loss_0.329_vloss_0.440_acc_0.848_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3290 - accuracy: 0.8479 - val_loss: 0.4397 - val_accuracy: 0.8144\n",
            "Epoch 229/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.8510\n",
            "Epoch 229: loss did not improve from 0.32904\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3296 - accuracy: 0.8504 - val_loss: 0.4313 - val_accuracy: 0.8129\n",
            "Epoch 230/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.8439\n",
            "Epoch 230: loss did not improve from 0.32904\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3297 - accuracy: 0.8460 - val_loss: 0.4346 - val_accuracy: 0.8144\n",
            "Epoch 231/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.8448\n",
            "Epoch 231: loss did not improve from 0.32904\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3293 - accuracy: 0.8434 - val_loss: 0.4311 - val_accuracy: 0.8159\n",
            "Epoch 232/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3300 - accuracy: 0.8434\n",
            "Epoch 232: loss did not improve from 0.32904\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3300 - accuracy: 0.8434 - val_loss: 0.4322 - val_accuracy: 0.8159\n",
            "Epoch 233/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8431\n",
            "Epoch 233: loss improved from 0.32904 to 0.32863, saving model to ./model_PID__233_loss_0.329_vloss_0.436_acc_0.843_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3286 - accuracy: 0.8434 - val_loss: 0.4363 - val_accuracy: 0.8129\n",
            "Epoch 234/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3296 - accuracy: 0.8456\n",
            "Epoch 234: loss did not improve from 0.32863\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3291 - accuracy: 0.8460 - val_loss: 0.4339 - val_accuracy: 0.8144\n",
            "Epoch 235/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.8434\n",
            "Epoch 235: loss did not improve from 0.32863\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3289 - accuracy: 0.8440 - val_loss: 0.4365 - val_accuracy: 0.8084\n",
            "Epoch 236/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8422\n",
            "Epoch 236: loss improved from 0.32863 to 0.32774, saving model to ./model_PID__236_loss_0.328_vloss_0.439_acc_0.843_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3277 - accuracy: 0.8434 - val_loss: 0.4391 - val_accuracy: 0.8159\n",
            "Epoch 237/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8485\n",
            "Epoch 237: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3300 - accuracy: 0.8485 - val_loss: 0.4370 - val_accuracy: 0.8114\n",
            "Epoch 238/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8423\n",
            "Epoch 238: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3291 - accuracy: 0.8427 - val_loss: 0.4348 - val_accuracy: 0.8144\n",
            "Epoch 239/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8448\n",
            "Epoch 239: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3283 - accuracy: 0.8440 - val_loss: 0.4324 - val_accuracy: 0.8129\n",
            "Epoch 240/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.8485\n",
            "Epoch 240: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3286 - accuracy: 0.8472 - val_loss: 0.4364 - val_accuracy: 0.8084\n",
            "Epoch 241/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.8506\n",
            "Epoch 241: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3288 - accuracy: 0.8511 - val_loss: 0.4345 - val_accuracy: 0.8084\n",
            "Epoch 242/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8476\n",
            "Epoch 242: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3284 - accuracy: 0.8485 - val_loss: 0.4357 - val_accuracy: 0.8114\n",
            "Epoch 243/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8480\n",
            "Epoch 243: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3291 - accuracy: 0.8492 - val_loss: 0.4349 - val_accuracy: 0.8144\n",
            "Epoch 244/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.8487\n",
            "Epoch 244: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3288 - accuracy: 0.8492 - val_loss: 0.4348 - val_accuracy: 0.8159\n",
            "Epoch 245/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.8466\n",
            "Epoch 245: loss did not improve from 0.32774\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3285 - accuracy: 0.8466 - val_loss: 0.4336 - val_accuracy: 0.8099\n",
            "Epoch 246/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.8461\n",
            "Epoch 246: loss improved from 0.32774 to 0.32743, saving model to ./model_PID__246_loss_0.327_vloss_0.434_acc_0.847_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3274 - accuracy: 0.8466 - val_loss: 0.4343 - val_accuracy: 0.8159\n",
            "Epoch 247/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8449\n",
            "Epoch 247: loss did not improve from 0.32743\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3283 - accuracy: 0.8447 - val_loss: 0.4395 - val_accuracy: 0.8129\n",
            "Epoch 248/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8434\n",
            "Epoch 248: loss did not improve from 0.32743\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3275 - accuracy: 0.8434 - val_loss: 0.4467 - val_accuracy: 0.8144\n",
            "Epoch 249/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8447\n",
            "Epoch 249: loss did not improve from 0.32743\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3281 - accuracy: 0.8440 - val_loss: 0.4393 - val_accuracy: 0.8144\n",
            "Epoch 250/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8443\n",
            "Epoch 250: loss did not improve from 0.32743\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3275 - accuracy: 0.8440 - val_loss: 0.4413 - val_accuracy: 0.8114\n",
            "Epoch 251/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8466\n",
            "Epoch 251: loss did not improve from 0.32743\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3287 - accuracy: 0.8472 - val_loss: 0.4402 - val_accuracy: 0.8129\n",
            "Epoch 252/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3291 - accuracy: 0.8451\n",
            "Epoch 252: loss improved from 0.32743 to 0.32735, saving model to ./model_PID__252_loss_0.327_vloss_0.437_acc_0.847_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3273 - accuracy: 0.8466 - val_loss: 0.4368 - val_accuracy: 0.8114\n",
            "Epoch 253/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8507\n",
            "Epoch 253: loss improved from 0.32735 to 0.32653, saving model to ./model_PID__253_loss_0.327_vloss_0.435_acc_0.850_vacc_0.810.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3265 - accuracy: 0.8504 - val_loss: 0.4349 - val_accuracy: 0.8099\n",
            "Epoch 254/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8452\n",
            "Epoch 254: loss did not improve from 0.32653\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3297 - accuracy: 0.8453 - val_loss: 0.4383 - val_accuracy: 0.8159\n",
            "Epoch 255/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3267 - accuracy: 0.8473\n",
            "Epoch 255: loss did not improve from 0.32653\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3269 - accuracy: 0.8466 - val_loss: 0.4415 - val_accuracy: 0.8129\n",
            "Epoch 256/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8451\n",
            "Epoch 256: loss did not improve from 0.32653\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3280 - accuracy: 0.8453 - val_loss: 0.4426 - val_accuracy: 0.8144\n",
            "Epoch 257/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8415\n",
            "Epoch 257: loss improved from 0.32653 to 0.32637, saving model to ./model_PID__257_loss_0.326_vloss_0.441_acc_0.843_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3264 - accuracy: 0.8434 - val_loss: 0.4413 - val_accuracy: 0.8129\n",
            "Epoch 258/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3270 - accuracy: 0.8429\n",
            "Epoch 258: loss did not improve from 0.32637\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3267 - accuracy: 0.8427 - val_loss: 0.4446 - val_accuracy: 0.8099\n",
            "Epoch 259/500\n",
            "1528/1558 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8488\n",
            "Epoch 259: loss did not improve from 0.32637\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3270 - accuracy: 0.8485 - val_loss: 0.4417 - val_accuracy: 0.8129\n",
            "Epoch 260/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3275 - accuracy: 0.8476\n",
            "Epoch 260: loss did not improve from 0.32637\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3272 - accuracy: 0.8472 - val_loss: 0.4411 - val_accuracy: 0.8159\n",
            "Epoch 261/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8491\n",
            "Epoch 261: loss did not improve from 0.32637\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3270 - accuracy: 0.8492 - val_loss: 0.4455 - val_accuracy: 0.8099\n",
            "Epoch 262/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.8502\n",
            "Epoch 262: loss did not improve from 0.32637\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3272 - accuracy: 0.8511 - val_loss: 0.4452 - val_accuracy: 0.8114\n",
            "Epoch 263/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8470\n",
            "Epoch 263: loss did not improve from 0.32637\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3268 - accuracy: 0.8472 - val_loss: 0.4436 - val_accuracy: 0.8129\n",
            "Epoch 264/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.8491\n",
            "Epoch 264: loss improved from 0.32637 to 0.32622, saving model to ./model_PID__264_loss_0.326_vloss_0.443_acc_0.849_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3262 - accuracy: 0.8485 - val_loss: 0.4426 - val_accuracy: 0.8159\n",
            "Epoch 265/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3285 - accuracy: 0.8459\n",
            "Epoch 265: loss did not improve from 0.32622\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3265 - accuracy: 0.8472 - val_loss: 0.4419 - val_accuracy: 0.8144\n",
            "Epoch 266/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.8453\n",
            "Epoch 266: loss improved from 0.32622 to 0.32563, saving model to ./model_PID__266_loss_0.326_vloss_0.446_acc_0.846_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3256 - accuracy: 0.8460 - val_loss: 0.4465 - val_accuracy: 0.8159\n",
            "Epoch 267/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.8438\n",
            "Epoch 267: loss did not improve from 0.32563\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3276 - accuracy: 0.8447 - val_loss: 0.4428 - val_accuracy: 0.8114\n",
            "Epoch 268/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8489\n",
            "Epoch 268: loss did not improve from 0.32563\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3256 - accuracy: 0.8485 - val_loss: 0.4418 - val_accuracy: 0.8159\n",
            "Epoch 269/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.8487\n",
            "Epoch 269: loss did not improve from 0.32563\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3262 - accuracy: 0.8485 - val_loss: 0.4427 - val_accuracy: 0.8099\n",
            "Epoch 270/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8481\n",
            "Epoch 270: loss did not improve from 0.32563\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3262 - accuracy: 0.8479 - val_loss: 0.4421 - val_accuracy: 0.8144\n",
            "Epoch 271/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8485\n",
            "Epoch 271: loss did not improve from 0.32563\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3259 - accuracy: 0.8498 - val_loss: 0.4425 - val_accuracy: 0.8189\n",
            "Epoch 272/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8511\n",
            "Epoch 272: loss did not improve from 0.32563\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3260 - accuracy: 0.8498 - val_loss: 0.4428 - val_accuracy: 0.8159\n",
            "Epoch 273/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.8474\n",
            "Epoch 273: loss improved from 0.32563 to 0.32507, saving model to ./model_PID__273_loss_0.325_vloss_0.444_acc_0.848_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3251 - accuracy: 0.8479 - val_loss: 0.4445 - val_accuracy: 0.8144\n",
            "Epoch 274/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3267 - accuracy: 0.8441\n",
            "Epoch 274: loss did not improve from 0.32507\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3258 - accuracy: 0.8447 - val_loss: 0.4450 - val_accuracy: 0.8159\n",
            "Epoch 275/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8458\n",
            "Epoch 275: loss did not improve from 0.32507\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3267 - accuracy: 0.8460 - val_loss: 0.4434 - val_accuracy: 0.8144\n",
            "Epoch 276/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8509\n",
            "Epoch 276: loss improved from 0.32507 to 0.32489, saving model to ./model_PID__276_loss_0.325_vloss_0.443_acc_0.851_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3249 - accuracy: 0.8511 - val_loss: 0.4431 - val_accuracy: 0.8159\n",
            "Epoch 277/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8486\n",
            "Epoch 277: loss did not improve from 0.32489\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3254 - accuracy: 0.8479 - val_loss: 0.4458 - val_accuracy: 0.8114\n",
            "Epoch 278/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8494\n",
            "Epoch 278: loss did not improve from 0.32489\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3256 - accuracy: 0.8479 - val_loss: 0.4424 - val_accuracy: 0.8159\n",
            "Epoch 279/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8444\n",
            "Epoch 279: loss did not improve from 0.32489\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3268 - accuracy: 0.8453 - val_loss: 0.4443 - val_accuracy: 0.8144\n",
            "Epoch 280/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.8460\n",
            "Epoch 280: loss did not improve from 0.32489\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3252 - accuracy: 0.8453 - val_loss: 0.4476 - val_accuracy: 0.8144\n",
            "Epoch 281/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8439\n",
            "Epoch 281: loss improved from 0.32489 to 0.32457, saving model to ./model_PID__281_loss_0.325_vloss_0.444_acc_0.845_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3246 - accuracy: 0.8453 - val_loss: 0.4444 - val_accuracy: 0.8144\n",
            "Epoch 282/500\n",
            "1532/1558 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8486\n",
            "Epoch 282: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3255 - accuracy: 0.8472 - val_loss: 0.4451 - val_accuracy: 0.8144\n",
            "Epoch 283/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.8492\n",
            "Epoch 283: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3248 - accuracy: 0.8492 - val_loss: 0.4452 - val_accuracy: 0.8114\n",
            "Epoch 284/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8460\n",
            "Epoch 284: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3249 - accuracy: 0.8472 - val_loss: 0.4457 - val_accuracy: 0.8129\n",
            "Epoch 285/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8471\n",
            "Epoch 285: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3248 - accuracy: 0.8479 - val_loss: 0.4515 - val_accuracy: 0.8084\n",
            "Epoch 286/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8481\n",
            "Epoch 286: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3257 - accuracy: 0.8479 - val_loss: 0.4502 - val_accuracy: 0.8114\n",
            "Epoch 287/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8456\n",
            "Epoch 287: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3263 - accuracy: 0.8460 - val_loss: 0.4469 - val_accuracy: 0.8144\n",
            "Epoch 288/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8470\n",
            "Epoch 288: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3248 - accuracy: 0.8472 - val_loss: 0.4487 - val_accuracy: 0.8144\n",
            "Epoch 289/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8449\n",
            "Epoch 289: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3252 - accuracy: 0.8453 - val_loss: 0.4447 - val_accuracy: 0.8114\n",
            "Epoch 290/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8479\n",
            "Epoch 290: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3247 - accuracy: 0.8492 - val_loss: 0.4466 - val_accuracy: 0.8159\n",
            "Epoch 291/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3248 - accuracy: 0.8485\n",
            "Epoch 291: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3248 - accuracy: 0.8485 - val_loss: 0.4445 - val_accuracy: 0.8174\n",
            "Epoch 292/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8493\n",
            "Epoch 292: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3253 - accuracy: 0.8492 - val_loss: 0.4432 - val_accuracy: 0.8189\n",
            "Epoch 293/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8443\n",
            "Epoch 293: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3262 - accuracy: 0.8440 - val_loss: 0.4494 - val_accuracy: 0.8159\n",
            "Epoch 294/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8461\n",
            "Epoch 294: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3252 - accuracy: 0.8460 - val_loss: 0.4449 - val_accuracy: 0.8129\n",
            "Epoch 295/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8478\n",
            "Epoch 295: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3257 - accuracy: 0.8472 - val_loss: 0.4526 - val_accuracy: 0.8129\n",
            "Epoch 296/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8508\n",
            "Epoch 296: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3246 - accuracy: 0.8511 - val_loss: 0.4563 - val_accuracy: 0.8114\n",
            "Epoch 297/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8457\n",
            "Epoch 297: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3264 - accuracy: 0.8460 - val_loss: 0.4442 - val_accuracy: 0.8144\n",
            "Epoch 298/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8457\n",
            "Epoch 298: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3251 - accuracy: 0.8460 - val_loss: 0.4469 - val_accuracy: 0.8159\n",
            "Epoch 299/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8458\n",
            "Epoch 299: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3258 - accuracy: 0.8466 - val_loss: 0.4478 - val_accuracy: 0.8159\n",
            "Epoch 300/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8446\n",
            "Epoch 300: loss did not improve from 0.32457\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3247 - accuracy: 0.8447 - val_loss: 0.4470 - val_accuracy: 0.8174\n",
            "Epoch 301/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8509\n",
            "Epoch 301: loss improved from 0.32457 to 0.32410, saving model to ./model_PID__301_loss_0.324_vloss_0.448_acc_0.850_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3241 - accuracy: 0.8504 - val_loss: 0.4482 - val_accuracy: 0.8114\n",
            "Epoch 302/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8437\n",
            "Epoch 302: loss did not improve from 0.32410\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3250 - accuracy: 0.8440 - val_loss: 0.4463 - val_accuracy: 0.8189\n",
            "Epoch 303/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8449\n",
            "Epoch 303: loss did not improve from 0.32410\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3242 - accuracy: 0.8447 - val_loss: 0.4454 - val_accuracy: 0.8189\n",
            "Epoch 304/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3254 - accuracy: 0.8475\n",
            "Epoch 304: loss did not improve from 0.32410\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3251 - accuracy: 0.8479 - val_loss: 0.4491 - val_accuracy: 0.8159\n",
            "Epoch 305/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8460\n",
            "Epoch 305: loss did not improve from 0.32410\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3245 - accuracy: 0.8472 - val_loss: 0.4473 - val_accuracy: 0.8204\n",
            "Epoch 306/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8483\n",
            "Epoch 306: loss did not improve from 0.32410\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3243 - accuracy: 0.8479 - val_loss: 0.4498 - val_accuracy: 0.8174\n",
            "Epoch 307/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8481\n",
            "Epoch 307: loss improved from 0.32410 to 0.32386, saving model to ./model_PID__307_loss_0.324_vloss_0.447_acc_0.849_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3239 - accuracy: 0.8485 - val_loss: 0.4474 - val_accuracy: 0.8174\n",
            "Epoch 308/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8464\n",
            "Epoch 308: loss did not improve from 0.32386\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3243 - accuracy: 0.8466 - val_loss: 0.4470 - val_accuracy: 0.8204\n",
            "Epoch 309/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8473\n",
            "Epoch 309: loss improved from 0.32386 to 0.32360, saving model to ./model_PID__309_loss_0.324_vloss_0.451_acc_0.849_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3236 - accuracy: 0.8485 - val_loss: 0.4509 - val_accuracy: 0.8144\n",
            "Epoch 310/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8503\n",
            "Epoch 310: loss improved from 0.32360 to 0.32309, saving model to ./model_PID__310_loss_0.323_vloss_0.452_acc_0.851_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 3s 2ms/step - loss: 0.3231 - accuracy: 0.8511 - val_loss: 0.4518 - val_accuracy: 0.8114\n",
            "Epoch 311/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8479\n",
            "Epoch 311: loss did not improve from 0.32309\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3250 - accuracy: 0.8479 - val_loss: 0.4477 - val_accuracy: 0.8174\n",
            "Epoch 312/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8490\n",
            "Epoch 312: loss did not improve from 0.32309\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3241 - accuracy: 0.8498 - val_loss: 0.4487 - val_accuracy: 0.8189\n",
            "Epoch 313/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8444\n",
            "Epoch 313: loss did not improve from 0.32309\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3254 - accuracy: 0.8440 - val_loss: 0.4493 - val_accuracy: 0.8189\n",
            "Epoch 314/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8462\n",
            "Epoch 314: loss improved from 0.32309 to 0.32259, saving model to ./model_PID__314_loss_0.323_vloss_0.452_acc_0.846_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3226 - accuracy: 0.8460 - val_loss: 0.4515 - val_accuracy: 0.8129\n",
            "Epoch 315/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8475\n",
            "Epoch 315: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3247 - accuracy: 0.8472 - val_loss: 0.4521 - val_accuracy: 0.8189\n",
            "Epoch 316/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8472\n",
            "Epoch 316: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3240 - accuracy: 0.8472 - val_loss: 0.4521 - val_accuracy: 0.8189\n",
            "Epoch 317/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8511\n",
            "Epoch 317: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3234 - accuracy: 0.8492 - val_loss: 0.4509 - val_accuracy: 0.8174\n",
            "Epoch 318/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8497\n",
            "Epoch 318: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3244 - accuracy: 0.8479 - val_loss: 0.4522 - val_accuracy: 0.8129\n",
            "Epoch 319/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8511\n",
            "Epoch 319: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3237 - accuracy: 0.8492 - val_loss: 0.4518 - val_accuracy: 0.8159\n",
            "Epoch 320/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8482\n",
            "Epoch 320: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3234 - accuracy: 0.8479 - val_loss: 0.4522 - val_accuracy: 0.8159\n",
            "Epoch 321/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8504\n",
            "Epoch 321: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3229 - accuracy: 0.8504 - val_loss: 0.4537 - val_accuracy: 0.8219\n",
            "Epoch 322/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8474\n",
            "Epoch 322: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3241 - accuracy: 0.8479 - val_loss: 0.4489 - val_accuracy: 0.8174\n",
            "Epoch 323/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.8492\n",
            "Epoch 323: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3241 - accuracy: 0.8492 - val_loss: 0.4533 - val_accuracy: 0.8174\n",
            "Epoch 324/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8476\n",
            "Epoch 324: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3239 - accuracy: 0.8479 - val_loss: 0.4571 - val_accuracy: 0.8129\n",
            "Epoch 325/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.8524\n",
            "Epoch 325: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3228 - accuracy: 0.8524 - val_loss: 0.4492 - val_accuracy: 0.8174\n",
            "Epoch 326/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8447\n",
            "Epoch 326: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3242 - accuracy: 0.8447 - val_loss: 0.4557 - val_accuracy: 0.8144\n",
            "Epoch 327/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8471\n",
            "Epoch 327: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3238 - accuracy: 0.8472 - val_loss: 0.4547 - val_accuracy: 0.8159\n",
            "Epoch 328/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8530\n",
            "Epoch 328: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3231 - accuracy: 0.8511 - val_loss: 0.4595 - val_accuracy: 0.8159\n",
            "Epoch 329/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8531\n",
            "Epoch 329: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3235 - accuracy: 0.8517 - val_loss: 0.4575 - val_accuracy: 0.8069\n",
            "Epoch 330/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8466\n",
            "Epoch 330: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3250 - accuracy: 0.8466 - val_loss: 0.4503 - val_accuracy: 0.8159\n",
            "Epoch 331/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8486\n",
            "Epoch 331: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3239 - accuracy: 0.8485 - val_loss: 0.4550 - val_accuracy: 0.8159\n",
            "Epoch 332/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.8504\n",
            "Epoch 332: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3236 - accuracy: 0.8504 - val_loss: 0.4509 - val_accuracy: 0.8159\n",
            "Epoch 333/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8476\n",
            "Epoch 333: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3233 - accuracy: 0.8479 - val_loss: 0.4536 - val_accuracy: 0.8114\n",
            "Epoch 334/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.8484\n",
            "Epoch 334: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3229 - accuracy: 0.8492 - val_loss: 0.4548 - val_accuracy: 0.8159\n",
            "Epoch 335/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8446\n",
            "Epoch 335: loss did not improve from 0.32259\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3227 - accuracy: 0.8453 - val_loss: 0.4536 - val_accuracy: 0.8174\n",
            "Epoch 336/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8505\n",
            "Epoch 336: loss improved from 0.32259 to 0.32245, saving model to ./model_PID__336_loss_0.322_vloss_0.451_acc_0.850_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3225 - accuracy: 0.8504 - val_loss: 0.4511 - val_accuracy: 0.8189\n",
            "Epoch 337/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.8504\n",
            "Epoch 337: loss did not improve from 0.32245\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3230 - accuracy: 0.8504 - val_loss: 0.4582 - val_accuracy: 0.8114\n",
            "Epoch 338/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.8499\n",
            "Epoch 338: loss did not improve from 0.32245\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3231 - accuracy: 0.8498 - val_loss: 0.4536 - val_accuracy: 0.8129\n",
            "Epoch 339/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8468\n",
            "Epoch 339: loss did not improve from 0.32245\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3232 - accuracy: 0.8472 - val_loss: 0.4551 - val_accuracy: 0.8129\n",
            "Epoch 340/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.8535\n",
            "Epoch 340: loss did not improve from 0.32245\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3228 - accuracy: 0.8537 - val_loss: 0.4542 - val_accuracy: 0.8144\n",
            "Epoch 341/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3229 - accuracy: 0.8505\n",
            "Epoch 341: loss did not improve from 0.32245\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3230 - accuracy: 0.8504 - val_loss: 0.4569 - val_accuracy: 0.8114\n",
            "Epoch 342/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8500\n",
            "Epoch 342: loss did not improve from 0.32245\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3233 - accuracy: 0.8492 - val_loss: 0.4512 - val_accuracy: 0.8144\n",
            "Epoch 343/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8502\n",
            "Epoch 343: loss did not improve from 0.32245\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3227 - accuracy: 0.8485 - val_loss: 0.4557 - val_accuracy: 0.8129\n",
            "Epoch 344/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.8480\n",
            "Epoch 344: loss improved from 0.32245 to 0.32153, saving model to ./model_PID__344_loss_0.322_vloss_0.456_acc_0.848_vacc_0.810.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3215 - accuracy: 0.8479 - val_loss: 0.4556 - val_accuracy: 0.8099\n",
            "Epoch 345/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.8511\n",
            "Epoch 345: loss did not improve from 0.32153\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3224 - accuracy: 0.8511 - val_loss: 0.4514 - val_accuracy: 0.8174\n",
            "Epoch 346/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8505\n",
            "Epoch 346: loss did not improve from 0.32153\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3227 - accuracy: 0.8504 - val_loss: 0.4521 - val_accuracy: 0.8159\n",
            "Epoch 347/500\n",
            "1532/1558 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8538\n",
            "Epoch 347: loss improved from 0.32153 to 0.32105, saving model to ./model_PID__347_loss_0.321_vloss_0.457_acc_0.852_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3210 - accuracy: 0.8524 - val_loss: 0.4566 - val_accuracy: 0.8129\n",
            "Epoch 348/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8461\n",
            "Epoch 348: loss did not improve from 0.32105\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3223 - accuracy: 0.8466 - val_loss: 0.4556 - val_accuracy: 0.8084\n",
            "Epoch 349/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8490\n",
            "Epoch 349: loss did not improve from 0.32105\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3214 - accuracy: 0.8479 - val_loss: 0.4607 - val_accuracy: 0.8144\n",
            "Epoch 350/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.8477\n",
            "Epoch 350: loss did not improve from 0.32105\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3224 - accuracy: 0.8485 - val_loss: 0.4559 - val_accuracy: 0.8129\n",
            "Epoch 351/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8470\n",
            "Epoch 351: loss improved from 0.32105 to 0.32068, saving model to ./model_PID__351_loss_0.321_vloss_0.454_acc_0.849_vacc_0.819.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3207 - accuracy: 0.8485 - val_loss: 0.4538 - val_accuracy: 0.8189\n",
            "Epoch 352/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8491\n",
            "Epoch 352: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3242 - accuracy: 0.8479 - val_loss: 0.4538 - val_accuracy: 0.8189\n",
            "Epoch 353/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.8520\n",
            "Epoch 353: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3221 - accuracy: 0.8530 - val_loss: 0.4590 - val_accuracy: 0.8174\n",
            "Epoch 354/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8513\n",
            "Epoch 354: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3228 - accuracy: 0.8517 - val_loss: 0.4520 - val_accuracy: 0.8159\n",
            "Epoch 355/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.8495\n",
            "Epoch 355: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3208 - accuracy: 0.8492 - val_loss: 0.4568 - val_accuracy: 0.8189\n",
            "Epoch 356/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8477\n",
            "Epoch 356: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3226 - accuracy: 0.8498 - val_loss: 0.4536 - val_accuracy: 0.8114\n",
            "Epoch 357/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8480\n",
            "Epoch 357: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3220 - accuracy: 0.8479 - val_loss: 0.4569 - val_accuracy: 0.8129\n",
            "Epoch 358/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8477\n",
            "Epoch 358: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3221 - accuracy: 0.8479 - val_loss: 0.4542 - val_accuracy: 0.8144\n",
            "Epoch 359/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8524\n",
            "Epoch 359: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3225 - accuracy: 0.8517 - val_loss: 0.4542 - val_accuracy: 0.8174\n",
            "Epoch 360/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8502\n",
            "Epoch 360: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3217 - accuracy: 0.8498 - val_loss: 0.4534 - val_accuracy: 0.8114\n",
            "Epoch 361/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8491\n",
            "Epoch 361: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3208 - accuracy: 0.8479 - val_loss: 0.4548 - val_accuracy: 0.8129\n",
            "Epoch 362/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.8499\n",
            "Epoch 362: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3216 - accuracy: 0.8498 - val_loss: 0.4513 - val_accuracy: 0.8114\n",
            "Epoch 363/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8491\n",
            "Epoch 363: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3214 - accuracy: 0.8492 - val_loss: 0.4536 - val_accuracy: 0.8204\n",
            "Epoch 364/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8493\n",
            "Epoch 364: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3220 - accuracy: 0.8498 - val_loss: 0.4529 - val_accuracy: 0.8159\n",
            "Epoch 365/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3210 - accuracy: 0.8498\n",
            "Epoch 365: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3210 - accuracy: 0.8498 - val_loss: 0.4495 - val_accuracy: 0.8144\n",
            "Epoch 366/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8474\n",
            "Epoch 366: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3220 - accuracy: 0.8479 - val_loss: 0.4522 - val_accuracy: 0.8114\n",
            "Epoch 367/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8475\n",
            "Epoch 367: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3210 - accuracy: 0.8479 - val_loss: 0.4509 - val_accuracy: 0.8159\n",
            "Epoch 368/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8463\n",
            "Epoch 368: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3214 - accuracy: 0.8472 - val_loss: 0.4548 - val_accuracy: 0.8159\n",
            "Epoch 369/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8513\n",
            "Epoch 369: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3209 - accuracy: 0.8504 - val_loss: 0.4593 - val_accuracy: 0.8144\n",
            "Epoch 370/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8482\n",
            "Epoch 370: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3218 - accuracy: 0.8492 - val_loss: 0.4543 - val_accuracy: 0.8189\n",
            "Epoch 371/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8506\n",
            "Epoch 371: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3213 - accuracy: 0.8498 - val_loss: 0.4591 - val_accuracy: 0.8159\n",
            "Epoch 372/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8518\n",
            "Epoch 372: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3213 - accuracy: 0.8517 - val_loss: 0.4532 - val_accuracy: 0.8189\n",
            "Epoch 373/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8503\n",
            "Epoch 373: loss did not improve from 0.32068\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3220 - accuracy: 0.8498 - val_loss: 0.4571 - val_accuracy: 0.8159\n",
            "Epoch 374/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8481\n",
            "Epoch 374: loss improved from 0.32068 to 0.32059, saving model to ./model_PID__374_loss_0.321_vloss_0.459_acc_0.849_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3206 - accuracy: 0.8485 - val_loss: 0.4594 - val_accuracy: 0.8144\n",
            "Epoch 375/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8491\n",
            "Epoch 375: loss did not improve from 0.32059\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3212 - accuracy: 0.8485 - val_loss: 0.4568 - val_accuracy: 0.8159\n",
            "Epoch 376/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8523\n",
            "Epoch 376: loss did not improve from 0.32059\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3214 - accuracy: 0.8517 - val_loss: 0.4595 - val_accuracy: 0.8144\n",
            "Epoch 377/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8515\n",
            "Epoch 377: loss improved from 0.32059 to 0.32010, saving model to ./model_PID__377_loss_0.320_vloss_0.459_acc_0.852_vacc_0.813.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3201 - accuracy: 0.8517 - val_loss: 0.4588 - val_accuracy: 0.8129\n",
            "Epoch 378/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8484\n",
            "Epoch 378: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3214 - accuracy: 0.8485 - val_loss: 0.4588 - val_accuracy: 0.8159\n",
            "Epoch 379/500\n",
            "1530/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8490\n",
            "Epoch 379: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3209 - accuracy: 0.8498 - val_loss: 0.4570 - val_accuracy: 0.8159\n",
            "Epoch 380/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.8499\n",
            "Epoch 380: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3207 - accuracy: 0.8498 - val_loss: 0.4583 - val_accuracy: 0.8204\n",
            "Epoch 381/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8504\n",
            "Epoch 381: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3213 - accuracy: 0.8511 - val_loss: 0.4605 - val_accuracy: 0.8204\n",
            "Epoch 382/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8524\n",
            "Epoch 382: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3209 - accuracy: 0.8517 - val_loss: 0.4591 - val_accuracy: 0.8159\n",
            "Epoch 383/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8509\n",
            "Epoch 383: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3209 - accuracy: 0.8504 - val_loss: 0.4540 - val_accuracy: 0.8144\n",
            "Epoch 384/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8498\n",
            "Epoch 384: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3224 - accuracy: 0.8511 - val_loss: 0.4523 - val_accuracy: 0.8144\n",
            "Epoch 385/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8505\n",
            "Epoch 385: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3208 - accuracy: 0.8517 - val_loss: 0.4556 - val_accuracy: 0.8114\n",
            "Epoch 386/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8464\n",
            "Epoch 386: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3214 - accuracy: 0.8472 - val_loss: 0.4598 - val_accuracy: 0.8189\n",
            "Epoch 387/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.8482\n",
            "Epoch 387: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3205 - accuracy: 0.8485 - val_loss: 0.4596 - val_accuracy: 0.8144\n",
            "Epoch 388/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8529\n",
            "Epoch 388: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3205 - accuracy: 0.8517 - val_loss: 0.4581 - val_accuracy: 0.8174\n",
            "Epoch 389/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8494\n",
            "Epoch 389: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3217 - accuracy: 0.8498 - val_loss: 0.4563 - val_accuracy: 0.8129\n",
            "Epoch 390/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8473\n",
            "Epoch 390: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3212 - accuracy: 0.8472 - val_loss: 0.4577 - val_accuracy: 0.8129\n",
            "Epoch 391/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8501\n",
            "Epoch 391: loss did not improve from 0.32010\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3214 - accuracy: 0.8504 - val_loss: 0.4563 - val_accuracy: 0.8129\n",
            "Epoch 392/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8535\n",
            "Epoch 392: loss improved from 0.32010 to 0.32006, saving model to ./model_PID__392_loss_0.320_vloss_0.456_acc_0.852_vacc_0.817.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3201 - accuracy: 0.8517 - val_loss: 0.4556 - val_accuracy: 0.8174\n",
            "Epoch 393/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8530\n",
            "Epoch 393: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3224 - accuracy: 0.8530 - val_loss: 0.4569 - val_accuracy: 0.8159\n",
            "Epoch 394/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8519\n",
            "Epoch 394: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3210 - accuracy: 0.8530 - val_loss: 0.4564 - val_accuracy: 0.8144\n",
            "Epoch 395/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8489\n",
            "Epoch 395: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3212 - accuracy: 0.8479 - val_loss: 0.4585 - val_accuracy: 0.8144\n",
            "Epoch 396/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8564\n",
            "Epoch 396: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3207 - accuracy: 0.8569 - val_loss: 0.4585 - val_accuracy: 0.8129\n",
            "Epoch 397/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8480\n",
            "Epoch 397: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3206 - accuracy: 0.8472 - val_loss: 0.4625 - val_accuracy: 0.8129\n",
            "Epoch 398/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8475\n",
            "Epoch 398: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3207 - accuracy: 0.8472 - val_loss: 0.4553 - val_accuracy: 0.8144\n",
            "Epoch 399/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8495\n",
            "Epoch 399: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3208 - accuracy: 0.8498 - val_loss: 0.4589 - val_accuracy: 0.8114\n",
            "Epoch 400/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8485\n",
            "Epoch 400: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3236 - accuracy: 0.8485 - val_loss: 0.4546 - val_accuracy: 0.8144\n",
            "Epoch 401/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8508\n",
            "Epoch 401: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3210 - accuracy: 0.8504 - val_loss: 0.4538 - val_accuracy: 0.8144\n",
            "Epoch 402/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8533\n",
            "Epoch 402: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3209 - accuracy: 0.8530 - val_loss: 0.4567 - val_accuracy: 0.8174\n",
            "Epoch 403/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8474\n",
            "Epoch 403: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3212 - accuracy: 0.8466 - val_loss: 0.4553 - val_accuracy: 0.8159\n",
            "Epoch 404/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8503\n",
            "Epoch 404: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3216 - accuracy: 0.8498 - val_loss: 0.4537 - val_accuracy: 0.8204\n",
            "Epoch 405/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.8497\n",
            "Epoch 405: loss did not improve from 0.32006\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3206 - accuracy: 0.8511 - val_loss: 0.4543 - val_accuracy: 0.8129\n",
            "Epoch 406/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8491\n",
            "Epoch 406: loss improved from 0.32006 to 0.31996, saving model to ./model_PID__406_loss_0.320_vloss_0.455_acc_0.850_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3200 - accuracy: 0.8504 - val_loss: 0.4549 - val_accuracy: 0.8114\n",
            "Epoch 407/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8489\n",
            "Epoch 407: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3218 - accuracy: 0.8492 - val_loss: 0.4574 - val_accuracy: 0.8159\n",
            "Epoch 408/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8455\n",
            "Epoch 408: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3216 - accuracy: 0.8460 - val_loss: 0.4579 - val_accuracy: 0.8129\n",
            "Epoch 409/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8515\n",
            "Epoch 409: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3201 - accuracy: 0.8524 - val_loss: 0.4603 - val_accuracy: 0.8174\n",
            "Epoch 410/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8506\n",
            "Epoch 410: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3212 - accuracy: 0.8485 - val_loss: 0.4565 - val_accuracy: 0.8129\n",
            "Epoch 411/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8486\n",
            "Epoch 411: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3208 - accuracy: 0.8498 - val_loss: 0.4560 - val_accuracy: 0.8159\n",
            "Epoch 412/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8492\n",
            "Epoch 412: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3213 - accuracy: 0.8492 - val_loss: 0.4578 - val_accuracy: 0.8159\n",
            "Epoch 413/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8479\n",
            "Epoch 413: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3211 - accuracy: 0.8479 - val_loss: 0.4548 - val_accuracy: 0.8159\n",
            "Epoch 414/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8488\n",
            "Epoch 414: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3216 - accuracy: 0.8492 - val_loss: 0.4582 - val_accuracy: 0.8174\n",
            "Epoch 415/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8520\n",
            "Epoch 415: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3207 - accuracy: 0.8524 - val_loss: 0.4590 - val_accuracy: 0.8159\n",
            "Epoch 416/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8518\n",
            "Epoch 416: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3200 - accuracy: 0.8511 - val_loss: 0.4575 - val_accuracy: 0.8159\n",
            "Epoch 417/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8506\n",
            "Epoch 417: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3211 - accuracy: 0.8511 - val_loss: 0.4649 - val_accuracy: 0.8144\n",
            "Epoch 418/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8508\n",
            "Epoch 418: loss did not improve from 0.31996\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3203 - accuracy: 0.8504 - val_loss: 0.4616 - val_accuracy: 0.8159\n",
            "Epoch 419/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8490\n",
            "Epoch 419: loss improved from 0.31996 to 0.31974, saving model to ./model_PID__419_loss_0.320_vloss_0.467_acc_0.848_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3197 - accuracy: 0.8479 - val_loss: 0.4672 - val_accuracy: 0.8159\n",
            "Epoch 420/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.8511\n",
            "Epoch 420: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3207 - accuracy: 0.8511 - val_loss: 0.4643 - val_accuracy: 0.8189\n",
            "Epoch 421/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8532\n",
            "Epoch 421: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3208 - accuracy: 0.8524 - val_loss: 0.4633 - val_accuracy: 0.8144\n",
            "Epoch 422/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8542\n",
            "Epoch 422: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3203 - accuracy: 0.8543 - val_loss: 0.4646 - val_accuracy: 0.8144\n",
            "Epoch 423/500\n",
            "1528/1558 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.8508\n",
            "Epoch 423: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3215 - accuracy: 0.8511 - val_loss: 0.4675 - val_accuracy: 0.8159\n",
            "Epoch 424/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.8514\n",
            "Epoch 424: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3206 - accuracy: 0.8504 - val_loss: 0.4636 - val_accuracy: 0.8144\n",
            "Epoch 425/500\n",
            "1532/1558 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8525\n",
            "Epoch 425: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3209 - accuracy: 0.8530 - val_loss: 0.4653 - val_accuracy: 0.8159\n",
            "Epoch 426/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8531\n",
            "Epoch 426: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3198 - accuracy: 0.8524 - val_loss: 0.4666 - val_accuracy: 0.8129\n",
            "Epoch 427/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8494\n",
            "Epoch 427: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3210 - accuracy: 0.8504 - val_loss: 0.4635 - val_accuracy: 0.8129\n",
            "Epoch 428/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8556\n",
            "Epoch 428: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3203 - accuracy: 0.8549 - val_loss: 0.4689 - val_accuracy: 0.8099\n",
            "Epoch 429/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8472\n",
            "Epoch 429: loss did not improve from 0.31974\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3228 - accuracy: 0.8479 - val_loss: 0.4612 - val_accuracy: 0.8174\n",
            "Epoch 430/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8494\n",
            "Epoch 430: loss improved from 0.31974 to 0.31910, saving model to ./model_PID__430_loss_0.319_vloss_0.470_acc_0.849_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3191 - accuracy: 0.8492 - val_loss: 0.4703 - val_accuracy: 0.8114\n",
            "Epoch 431/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8507\n",
            "Epoch 431: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3217 - accuracy: 0.8498 - val_loss: 0.4649 - val_accuracy: 0.8174\n",
            "Epoch 432/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8523\n",
            "Epoch 432: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3195 - accuracy: 0.8524 - val_loss: 0.4663 - val_accuracy: 0.8129\n",
            "Epoch 433/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3229 - accuracy: 0.8488\n",
            "Epoch 433: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3224 - accuracy: 0.8492 - val_loss: 0.4623 - val_accuracy: 0.8174\n",
            "Epoch 434/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.8504\n",
            "Epoch 434: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3208 - accuracy: 0.8504 - val_loss: 0.4636 - val_accuracy: 0.8159\n",
            "Epoch 435/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8529\n",
            "Epoch 435: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3195 - accuracy: 0.8517 - val_loss: 0.4667 - val_accuracy: 0.8174\n",
            "Epoch 436/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8508\n",
            "Epoch 436: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3206 - accuracy: 0.8511 - val_loss: 0.4658 - val_accuracy: 0.8159\n",
            "Epoch 437/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8494\n",
            "Epoch 437: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3207 - accuracy: 0.8485 - val_loss: 0.4663 - val_accuracy: 0.8144\n",
            "Epoch 438/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8509\n",
            "Epoch 438: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3197 - accuracy: 0.8504 - val_loss: 0.4638 - val_accuracy: 0.8144\n",
            "Epoch 439/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8522\n",
            "Epoch 439: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3209 - accuracy: 0.8511 - val_loss: 0.4685 - val_accuracy: 0.8144\n",
            "Epoch 440/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8511\n",
            "Epoch 440: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3200 - accuracy: 0.8511 - val_loss: 0.4626 - val_accuracy: 0.8114\n",
            "Epoch 441/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8521\n",
            "Epoch 441: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3204 - accuracy: 0.8517 - val_loss: 0.4673 - val_accuracy: 0.8099\n",
            "Epoch 442/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8490\n",
            "Epoch 442: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3207 - accuracy: 0.8485 - val_loss: 0.4654 - val_accuracy: 0.8204\n",
            "Epoch 443/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3183 - accuracy: 0.8516\n",
            "Epoch 443: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3207 - accuracy: 0.8517 - val_loss: 0.4649 - val_accuracy: 0.8099\n",
            "Epoch 444/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8474\n",
            "Epoch 444: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3197 - accuracy: 0.8479 - val_loss: 0.4618 - val_accuracy: 0.8174\n",
            "Epoch 445/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8486\n",
            "Epoch 445: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3227 - accuracy: 0.8498 - val_loss: 0.4633 - val_accuracy: 0.8144\n",
            "Epoch 446/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8493\n",
            "Epoch 446: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3194 - accuracy: 0.8498 - val_loss: 0.4629 - val_accuracy: 0.8129\n",
            "Epoch 447/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8520\n",
            "Epoch 447: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3205 - accuracy: 0.8517 - val_loss: 0.4638 - val_accuracy: 0.8114\n",
            "Epoch 448/500\n",
            "1529/1558 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8502\n",
            "Epoch 448: loss did not improve from 0.31910\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3211 - accuracy: 0.8485 - val_loss: 0.4637 - val_accuracy: 0.8144\n",
            "Epoch 449/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8487\n",
            "Epoch 449: loss improved from 0.31910 to 0.31860, saving model to ./model_PID__449_loss_0.319_vloss_0.467_acc_0.849_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3186 - accuracy: 0.8492 - val_loss: 0.4665 - val_accuracy: 0.8159\n",
            "Epoch 450/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3178 - accuracy: 0.8505\n",
            "Epoch 450: loss did not improve from 0.31860\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3209 - accuracy: 0.8498 - val_loss: 0.4693 - val_accuracy: 0.8144\n",
            "Epoch 451/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8527\n",
            "Epoch 451: loss improved from 0.31860 to 0.31853, saving model to ./model_PID__451_loss_0.319_vloss_0.464_acc_0.853_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3185 - accuracy: 0.8530 - val_loss: 0.4644 - val_accuracy: 0.8144\n",
            "Epoch 452/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8491\n",
            "Epoch 452: loss did not improve from 0.31853\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3236 - accuracy: 0.8498 - val_loss: 0.4611 - val_accuracy: 0.8129\n",
            "Epoch 453/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8475\n",
            "Epoch 453: loss did not improve from 0.31853\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3201 - accuracy: 0.8485 - val_loss: 0.4665 - val_accuracy: 0.8159\n",
            "Epoch 454/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.8473\n",
            "Epoch 454: loss did not improve from 0.31853\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3232 - accuracy: 0.8479 - val_loss: 0.4626 - val_accuracy: 0.8174\n",
            "Epoch 455/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8503\n",
            "Epoch 455: loss did not improve from 0.31853\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3203 - accuracy: 0.8504 - val_loss: 0.4644 - val_accuracy: 0.8144\n",
            "Epoch 456/500\n",
            "1532/1558 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8505\n",
            "Epoch 456: loss did not improve from 0.31853\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3216 - accuracy: 0.8511 - val_loss: 0.4612 - val_accuracy: 0.8144\n",
            "Epoch 457/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8504\n",
            "Epoch 457: loss did not improve from 0.31853\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3208 - accuracy: 0.8504 - val_loss: 0.4597 - val_accuracy: 0.8174\n",
            "Epoch 458/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8519\n",
            "Epoch 458: loss did not improve from 0.31853\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3202 - accuracy: 0.8517 - val_loss: 0.4625 - val_accuracy: 0.8144\n",
            "Epoch 459/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8564\n",
            "Epoch 459: loss improved from 0.31853 to 0.31791, saving model to ./model_PID__459_loss_0.318_vloss_0.462_acc_0.856_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3179 - accuracy: 0.8562 - val_loss: 0.4617 - val_accuracy: 0.8159\n",
            "Epoch 460/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8483\n",
            "Epoch 460: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3209 - accuracy: 0.8485 - val_loss: 0.4626 - val_accuracy: 0.8144\n",
            "Epoch 461/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8493\n",
            "Epoch 461: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3206 - accuracy: 0.8492 - val_loss: 0.4647 - val_accuracy: 0.8159\n",
            "Epoch 462/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8515\n",
            "Epoch 462: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3200 - accuracy: 0.8524 - val_loss: 0.4636 - val_accuracy: 0.8159\n",
            "Epoch 463/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3183 - accuracy: 0.8537\n",
            "Epoch 463: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3200 - accuracy: 0.8517 - val_loss: 0.4663 - val_accuracy: 0.8174\n",
            "Epoch 464/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8511\n",
            "Epoch 464: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3206 - accuracy: 0.8511 - val_loss: 0.4647 - val_accuracy: 0.8174\n",
            "Epoch 465/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8489\n",
            "Epoch 465: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3209 - accuracy: 0.8485 - val_loss: 0.4618 - val_accuracy: 0.8159\n",
            "Epoch 466/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8530\n",
            "Epoch 466: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3201 - accuracy: 0.8524 - val_loss: 0.4623 - val_accuracy: 0.8174\n",
            "Epoch 467/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8481\n",
            "Epoch 467: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3211 - accuracy: 0.8485 - val_loss: 0.4676 - val_accuracy: 0.8159\n",
            "Epoch 468/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.8496\n",
            "Epoch 468: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3193 - accuracy: 0.8498 - val_loss: 0.4677 - val_accuracy: 0.8144\n",
            "Epoch 469/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8475\n",
            "Epoch 469: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3207 - accuracy: 0.8472 - val_loss: 0.4616 - val_accuracy: 0.8144\n",
            "Epoch 470/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.8525\n",
            "Epoch 470: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3203 - accuracy: 0.8511 - val_loss: 0.4655 - val_accuracy: 0.8174\n",
            "Epoch 471/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8495\n",
            "Epoch 471: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3208 - accuracy: 0.8504 - val_loss: 0.4622 - val_accuracy: 0.8159\n",
            "Epoch 472/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8488\n",
            "Epoch 472: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3203 - accuracy: 0.8492 - val_loss: 0.4658 - val_accuracy: 0.8174\n",
            "Epoch 473/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8497\n",
            "Epoch 473: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3208 - accuracy: 0.8498 - val_loss: 0.4649 - val_accuracy: 0.8174\n",
            "Epoch 474/500\n",
            "1531/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8511\n",
            "Epoch 474: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3194 - accuracy: 0.8517 - val_loss: 0.4646 - val_accuracy: 0.8159\n",
            "Epoch 475/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8487\n",
            "Epoch 475: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3199 - accuracy: 0.8492 - val_loss: 0.4654 - val_accuracy: 0.8144\n",
            "Epoch 476/500\n",
            "1534/1558 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.8501\n",
            "Epoch 476: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3197 - accuracy: 0.8498 - val_loss: 0.4661 - val_accuracy: 0.8159\n",
            "Epoch 477/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8516\n",
            "Epoch 477: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3211 - accuracy: 0.8517 - val_loss: 0.4662 - val_accuracy: 0.8159\n",
            "Epoch 478/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8491\n",
            "Epoch 478: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3204 - accuracy: 0.8498 - val_loss: 0.4660 - val_accuracy: 0.8159\n",
            "Epoch 479/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8502\n",
            "Epoch 479: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3197 - accuracy: 0.8517 - val_loss: 0.4640 - val_accuracy: 0.8159\n",
            "Epoch 480/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8518\n",
            "Epoch 480: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3201 - accuracy: 0.8517 - val_loss: 0.4659 - val_accuracy: 0.8159\n",
            "Epoch 481/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8514\n",
            "Epoch 481: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3193 - accuracy: 0.8524 - val_loss: 0.4621 - val_accuracy: 0.8159\n",
            "Epoch 482/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8524\n",
            "Epoch 482: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3191 - accuracy: 0.8524 - val_loss: 0.4644 - val_accuracy: 0.8174\n",
            "Epoch 483/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8504\n",
            "Epoch 483: loss did not improve from 0.31791\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3210 - accuracy: 0.8498 - val_loss: 0.4624 - val_accuracy: 0.8159\n",
            "Epoch 484/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.8556\n",
            "Epoch 484: loss improved from 0.31791 to 0.31765, saving model to ./model_PID__484_loss_0.318_vloss_0.465_acc_0.856_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3177 - accuracy: 0.8556 - val_loss: 0.4654 - val_accuracy: 0.8159\n",
            "Epoch 485/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8520\n",
            "Epoch 485: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3197 - accuracy: 0.8517 - val_loss: 0.4629 - val_accuracy: 0.8174\n",
            "Epoch 486/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8489\n",
            "Epoch 486: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3197 - accuracy: 0.8498 - val_loss: 0.4600 - val_accuracy: 0.8144\n",
            "Epoch 487/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8523\n",
            "Epoch 487: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3203 - accuracy: 0.8524 - val_loss: 0.4661 - val_accuracy: 0.8144\n",
            "Epoch 488/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8503\n",
            "Epoch 488: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3204 - accuracy: 0.8498 - val_loss: 0.4642 - val_accuracy: 0.8159\n",
            "Epoch 489/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8557\n",
            "Epoch 489: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3193 - accuracy: 0.8556 - val_loss: 0.4674 - val_accuracy: 0.8144\n",
            "Epoch 490/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8543\n",
            "Epoch 490: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3194 - accuracy: 0.8543 - val_loss: 0.4634 - val_accuracy: 0.8189\n",
            "Epoch 491/500\n",
            "1533/1558 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8526\n",
            "Epoch 491: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3195 - accuracy: 0.8524 - val_loss: 0.4639 - val_accuracy: 0.8174\n",
            "Epoch 492/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8501\n",
            "Epoch 492: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3191 - accuracy: 0.8504 - val_loss: 0.4667 - val_accuracy: 0.8144\n",
            "Epoch 493/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8493\n",
            "Epoch 493: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3200 - accuracy: 0.8485 - val_loss: 0.4670 - val_accuracy: 0.8159\n",
            "Epoch 494/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8505\n",
            "Epoch 494: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3204 - accuracy: 0.8498 - val_loss: 0.4667 - val_accuracy: 0.8189\n",
            "Epoch 495/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3190 - accuracy: 0.8496\n",
            "Epoch 495: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3195 - accuracy: 0.8492 - val_loss: 0.4679 - val_accuracy: 0.8144\n",
            "Epoch 496/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8475\n",
            "Epoch 496: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3190 - accuracy: 0.8472 - val_loss: 0.4715 - val_accuracy: 0.8189\n",
            "Epoch 497/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8478\n",
            "Epoch 497: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3189 - accuracy: 0.8485 - val_loss: 0.4639 - val_accuracy: 0.8174\n",
            "Epoch 498/500\n",
            "1535/1558 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8502\n",
            "Epoch 498: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3195 - accuracy: 0.8504 - val_loss: 0.4611 - val_accuracy: 0.8174\n",
            "Epoch 499/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8487\n",
            "Epoch 499: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 3ms/step - loss: 0.3183 - accuracy: 0.8492 - val_loss: 0.4634 - val_accuracy: 0.8174\n",
            "Epoch 500/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8529\n",
            "Epoch 500: loss did not improve from 0.31765\n",
            "1558/1558 [==============================] - 4s 2ms/step - loss: 0.3200 - accuracy: 0.8511 - val_loss: 0.4685 - val_accuracy: 0.8174\n"
          ]
        }
      ],
      "source": [
        "if __learning__: \n",
        "    history = model.fit(X_train, y_train, epochs=500, batch_size=1, validation_data=(X_test, y_test),verbose=1,callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgzklVywoNmk"
      },
      "execution_count": 687,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 688,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwcWQ94IpDFu",
        "outputId": "7b579161-0de3-4a91-8a98-6a2c609aefab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": 689,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": 690,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bina_transformer=Binarizer(threshold=0.5)\n",
        "y_pred_transform=bina_transformer.fit_transform(y_pred)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H"
      },
      "execution_count": 691,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 692,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_test,y_pred_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Z69kCq3T-pMo",
        "outputId": "cff0024f-4944-4f3a-802e-009183203570"
      },
      "execution_count": 693,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 694,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nZ0rmkNsBGnl",
        "outputId": "070acf09-a1b4-4de3-fc49-15533c959f37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fpr, tpr, _ = roc_curve( y_pred_transform,y_test,pos_label=1)\n",
        "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "auc = roc_auc_score(y_test, y_pred_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMRd5eGU9ASA",
        "outputId": "a635d20d-cef9-4caa-e4de-5c9c742c7cda"
      },
      "execution_count": 695,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8193547229399942"
            ]
          },
          "metadata": {},
          "execution_count": 695
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzlQR00e8Miq",
        "outputId": "192c6369-859a-47f2-8e15-15bc8fba4329"
      },
      "execution_count": 696,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.22849462, 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 696
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(fpr,tpr)\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZRlw4piW7_uW",
        "outputId": "3c3f6c4a-7e92-43c5-f306-31aa4d3a5b00"
      },
      "execution_count": 697,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": 698,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_df=pd.DataFrame({\"epoch\":history.epoch, \"loss\":history.history[\"loss\"],\"val_loss\":history.history[\"val_loss\"]})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl"
      },
      "execution_count": 699,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "4ENvDCA-0U1g",
        "outputId": "e40702a0-71d2-43ee-8a67-39140a6010c0"
      },
      "execution_count": 700,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Val_Loss\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"d48668a2-2908-462a-8111-9cd4018fb32d\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d48668a2-2908-462a-8111-9cd4018fb32d\")) {                    Plotly.newPlot(                        \"d48668a2-2908-462a-8111-9cd4018fb32d\",                        [{\"line\":{\"color\":\"rgba(193,99,99,0.8)\"},\"name\":\"Val_Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499],\"y\":[0.6594204306602478,0.6031413078308105,0.5559513568878174,0.5207397937774658,0.5060045123100281,0.49910956621170044,0.4872208535671234,0.4845896363258362,0.48181626200675964,0.4778633415699005,0.47728431224823,0.4740460216999054,0.4760732650756836,0.4726676642894745,0.47079503536224365,0.4706955850124359,0.4684862196445465,0.4681575894355774,0.46629297733306885,0.4659680724143982,0.462404727935791,0.46051135659217834,0.45760977268218994,0.45385879278182983,0.45912325382232666,0.45080190896987915,0.455615371465683,0.4510818421840668,0.4526451826095581,0.4507802128791809,0.4473382532596588,0.44541579484939575,0.4465256333351135,0.4421607255935669,0.44794172048568726,0.44053107500076294,0.44120222330093384,0.44860804080963135,0.43437862396240234,0.43995848298072815,0.43567973375320435,0.4288346469402313,0.4320783317089081,0.42464396357536316,0.43017417192459106,0.4334491193294525,0.42093291878700256,0.4230675995349884,0.429332435131073,0.4231683909893036,0.43096667528152466,0.42820775508880615,0.4238772690296173,0.42555588483810425,0.42842626571655273,0.42841285467147827,0.4330328702926636,0.42461642622947693,0.42077818512916565,0.4260816276073456,0.421308308839798,0.42281538248062134,0.42182543873786926,0.42134490609169006,0.4227399230003357,0.42011675238609314,0.42101722955703735,0.4240296483039856,0.4273725152015686,0.42321160435676575,0.4218781888484955,0.42209893465042114,0.4249865710735321,0.4161972105503082,0.4239181578159332,0.4233333468437195,0.41728636622428894,0.42074137926101685,0.42283567786216736,0.41641268134117126,0.4174652099609375,0.41710877418518066,0.41598647832870483,0.4136318862438202,0.41755926609039307,0.41128799319267273,0.4139426648616791,0.410715788602829,0.41529324650764465,0.4129220247268677,0.4116774797439575,0.40941908955574036,0.4113273024559021,0.41045793890953064,0.41161397099494934,0.40419572591781616,0.409402459859848,0.4078056514263153,0.4067167639732361,0.4027915894985199,0.4026467204093933,0.4030517339706421,0.40007472038269043,0.40257391333580017,0.39912644028663635,0.3985785245895386,0.3985697031021118,0.4009835124015808,0.3973853886127472,0.39521706104278564,0.3952186405658722,0.401334673166275,0.39500892162323,0.394199937582016,0.3923344314098358,0.39541032910346985,0.3914793133735657,0.391584575176239,0.3905826508998871,0.389419823884964,0.3967821002006531,0.3943725526332855,0.39174050092697144,0.3903074860572815,0.38810643553733826,0.39071592688560486,0.39015480875968933,0.39248308539390564,0.387793630361557,0.38720130920410156,0.3886668384075165,0.38727807998657227,0.3888751268386841,0.38893669843673706,0.39183303713798523,0.3875020146369934,0.3905337154865265,0.38819319009780884,0.3868843913078308,0.3882947564125061,0.38732045888900757,0.38639259338378906,0.3831106722354889,0.3862365484237671,0.38583478331565857,0.3866663873195648,0.3841882050037384,0.3858397603034973,0.38637781143188477,0.38365283608436584,0.38595154881477356,0.3852541446685791,0.3856153190135956,0.392315536737442,0.383680522441864,0.38607558608055115,0.3878745436668396,0.38968154788017273,0.38852590322494507,0.390229731798172,0.39276817440986633,0.3887324929237366,0.3938915729522705,0.39735379815101624,0.3902675211429596,0.3895965814590454,0.39024078845977783,0.39117446541786194,0.3926878273487091,0.3950909972190857,0.3945775628089905,0.39564308524131775,0.3961876630783081,0.3956161439418793,0.39685434103012085,0.39795932173728943,0.3989455997943878,0.40868642926216125,0.3973380923271179,0.40245598554611206,0.401354044675827,0.40074166655540466,0.4001995623111725,0.4018893539905548,0.4029020071029663,0.4068392515182495,0.4104185700416565,0.40973952412605286,0.40818682312965393,0.40900859236717224,0.40903088450431824,0.40921035408973694,0.4115060567855835,0.41516318917274475,0.412485808134079,0.4143764078617096,0.41595590114593506,0.421170175075531,0.4178629517555237,0.4167706072330475,0.4280504286289215,0.4199391305446625,0.4205440580844879,0.42318195104599,0.42154282331466675,0.4233969449996948,0.42672714591026306,0.4219413697719574,0.42330771684646606,0.4257028102874756,0.4259587824344635,0.4244949519634247,0.42348822951316833,0.4289587438106537,0.42865094542503357,0.4262792766094208,0.4262513518333435,0.4314062297344208,0.4291895925998688,0.42591652274131775,0.4285637438297272,0.4287874102592468,0.4311296343803406,0.43227094411849976,0.4341641664505005,0.4306717813014984,0.43164390325546265,0.4396587908267975,0.43133744597435,0.4346103370189667,0.4310671389102936,0.43216782808303833,0.4362536072731018,0.43394461274147034,0.43652474880218506,0.43908125162124634,0.4370116591453552,0.4348028004169464,0.4324188828468323,0.4364187717437744,0.4344898760318756,0.43571943044662476,0.43487024307250977,0.43483904004096985,0.4336032569408417,0.43429890275001526,0.43947097659111023,0.44667816162109375,0.4393380880355835,0.4413090646266937,0.44018450379371643,0.43676862120628357,0.4348600506782532,0.43825823068618774,0.44153347611427307,0.44259050488471985,0.4412965178489685,0.4446042776107788,0.4416821300983429,0.44109800457954407,0.4454766511917114,0.4452308714389801,0.44363054633140564,0.4425904154777527,0.44186529517173767,0.4464956820011139,0.44281089305877686,0.44182953238487244,0.4427025616168976,0.44206923246383667,0.4424607455730438,0.44281303882598877,0.444457083940506,0.4450078308582306,0.4433971643447876,0.44305524230003357,0.4457520842552185,0.4424009621143341,0.4442582130432129,0.4476455748081207,0.4444146454334259,0.4451053738594055,0.4451862871646881,0.445723295211792,0.4514836370944977,0.450177401304245,0.446920782327652,0.4487292468547821,0.444715291261673,0.4466032385826111,0.4444736838340759,0.4432479441165924,0.4493562579154968,0.44489288330078125,0.4526416063308716,0.4562925696372986,0.44415444135665894,0.44690629839897156,0.4477936923503876,0.446987122297287,0.4481816589832306,0.4462827444076538,0.44536092877388,0.4490700960159302,0.44732537865638733,0.4498385190963745,0.44740185141563416,0.44695883989334106,0.4509197771549225,0.45181968808174133,0.44770804047584534,0.4487497806549072,0.4493003785610199,0.4515063762664795,0.45205920934677124,0.45206835865974426,0.4509357213973999,0.4521680474281311,0.4518464207649231,0.4521883726119995,0.4537496566772461,0.44887322187423706,0.45329397916793823,0.45709487795829773,0.44921255111694336,0.455678254365921,0.4546896517276764,0.4594946801662445,0.45751747488975525,0.45033255219459534,0.45499101281166077,0.4509361982345581,0.4536137282848358,0.4548037350177765,0.45359039306640625,0.451088011264801,0.45819950103759766,0.4535585641860962,0.4550571143627167,0.4541949927806854,0.4568946063518524,0.45123931765556335,0.45573046803474426,0.455632746219635,0.4514334797859192,0.452088326215744,0.4566460847854614,0.4556175768375397,0.4606510102748871,0.4559313952922821,0.453758180141449,0.45377296209335327,0.45899254083633423,0.45200732350349426,0.45675939321517944,0.4535999298095703,0.4569242596626282,0.4541756212711334,0.4541510343551636,0.45335155725479126,0.4547937214374542,0.4512960612773895,0.4536261558532715,0.4529344141483307,0.4495096206665039,0.45218321681022644,0.4508858919143677,0.4548492431640625,0.45927777886390686,0.454332172870636,0.4591253101825714,0.4532307982444763,0.45705193281173706,0.4594292640686035,0.45678967237472534,0.4595317244529724,0.45882681012153625,0.4588436186313629,0.45695874094963074,0.45831137895584106,0.46049994230270386,0.4590526819229126,0.4539856016635895,0.45234960317611694,0.4555869996547699,0.45983466506004333,0.4595668613910675,0.4581175446510315,0.45631143450737,0.4576818645000458,0.45626401901245117,0.4555891156196594,0.4569481611251831,0.4563615918159485,0.45852506160736084,0.45850449800491333,0.4624530076980591,0.455288827419281,0.45894262194633484,0.45458880066871643,0.45380035042762756,0.45665472745895386,0.45525825023651123,0.4536703824996948,0.4542744755744934,0.4548954665660858,0.4574296772480011,0.45790818333625793,0.46031585335731506,0.45648667216300964,0.4560222625732422,0.457786500453949,0.45479515194892883,0.4581747055053711,0.4589703381061554,0.4574573040008545,0.46487700939178467,0.46156397461891174,0.4672412574291229,0.4643118381500244,0.4633033871650696,0.46463483572006226,0.4675441384315491,0.4635777175426483,0.46531447768211365,0.46662506461143494,0.4634750783443451,0.46891629695892334,0.46120330691337585,0.470255583524704,0.46494361758232117,0.466300904750824,0.4623086750507355,0.4635695517063141,0.4667087495326996,0.4657887816429138,0.46625518798828125,0.463773250579834,0.46854090690612793,0.4625517725944519,0.4673053026199341,0.46537894010543823,0.4648621082305908,0.4617747366428375,0.46330106258392334,0.4629082679748535,0.4637930393218994,0.46372950077056885,0.4665420353412628,0.4692573845386505,0.46439018845558167,0.46106481552124023,0.4664883315563202,0.462587833404541,0.4643532335758209,0.46118882298469543,0.45969459414482117,0.46246251463890076,0.46172985434532166,0.4625522494316101,0.4647444784641266,0.4635666608810425,0.46629858016967773,0.4647256135940552,0.46176233887672424,0.4623114764690399,0.467634379863739,0.46770745515823364,0.4616355299949646,0.4655236303806305,0.46220552921295166,0.4657672345638275,0.464948832988739,0.46455445885658264,0.4653772711753845,0.46610593795776367,0.4661751985549927,0.46596816182136536,0.4640445411205292,0.46590638160705566,0.4621255397796631,0.4643540382385254,0.462442547082901,0.46539178490638733,0.4628877341747284,0.4600377678871155,0.4660951495170593,0.464179664850235,0.4674243628978729,0.4633576571941376,0.4639321565628052,0.4666539132595062,0.46702447533607483,0.46667489409446716,0.46794575452804565,0.47149163484573364,0.4638975262641907,0.46114274859428406,0.46339544653892517,0.468530535697937],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"rgba(35,128,132,0.8)\"},\"name\":\"Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499],\"y\":[0.6840522289276123,0.647208571434021,0.604408323764801,0.5673767924308777,0.542603611946106,0.525372326374054,0.5123338103294373,0.5030768513679504,0.49502167105674744,0.4897814989089966,0.4832661747932434,0.48014363646507263,0.4753270745277405,0.4726116955280304,0.4686003029346466,0.466274231672287,0.46302613615989685,0.4604433476924896,0.45911192893981934,0.45763835310935974,0.4543876647949219,0.45358961820602417,0.44985103607177734,0.44739630818367004,0.4434322118759155,0.44173139333724976,0.43982240557670593,0.4381231963634491,0.4358183741569519,0.4334307014942169,0.43137505650520325,0.4282337725162506,0.42916157841682434,0.4252764880657196,0.42284923791885376,0.4217231571674347,0.4201548397541046,0.41750648617744446,0.4174536466598511,0.41524818539619446,0.4140003025531769,0.4136403501033783,0.4114997386932373,0.410013347864151,0.40896379947662354,0.4076773226261139,0.40604159235954285,0.4056170582771301,0.4057961404323578,0.40371251106262207,0.40157195925712585,0.40156084299087524,0.4010160565376282,0.3997047543525696,0.39881354570388794,0.39757776260375977,0.39673084020614624,0.3968435823917389,0.3957981467247009,0.39606520533561707,0.39497190713882446,0.3934668302536011,0.3933551013469696,0.3914417326450348,0.3927360475063324,0.39092859625816345,0.39140018820762634,0.3911554217338562,0.38983529806137085,0.3912509083747864,0.38913649320602417,0.38913673162460327,0.38771358132362366,0.38832923769950867,0.38756027817726135,0.3863355815410614,0.38751399517059326,0.38498762249946594,0.3852134346961975,0.3843473792076111,0.3838827311992645,0.3829196095466614,0.38320931792259216,0.38272881507873535,0.38174983859062195,0.3810264468193054,0.3818773329257965,0.38092485070228577,0.3792027235031128,0.3802447021007538,0.3787879943847656,0.37751200795173645,0.37761300802230835,0.3757009506225586,0.3766670227050781,0.37632086873054504,0.37587445974349976,0.37627193331718445,0.3750531077384949,0.3753407597541809,0.37413081526756287,0.3742147982120514,0.37321987748146057,0.3723791241645813,0.3713214099407196,0.37125715613365173,0.3695889413356781,0.37167635560035706,0.36946481466293335,0.3701346516609192,0.3705315887928009,0.36791619658470154,0.368696928024292,0.36666420102119446,0.36736375093460083,0.36560332775115967,0.36880284547805786,0.36498552560806274,0.36471131443977356,0.3664550483226776,0.36326920986175537,0.3622131645679474,0.3644322156906128,0.3617762625217438,0.3635876476764679,0.3605048656463623,0.3601283133029938,0.3599621057510376,0.36044302582740784,0.3596031963825226,0.35923251509666443,0.3586510419845581,0.3592911660671234,0.356489896774292,0.357019305229187,0.35673171281814575,0.35436901450157166,0.3552994430065155,0.35477977991104126,0.35418230295181274,0.3531639873981476,0.35243433713912964,0.35145479440689087,0.35247156023979187,0.351336270570755,0.34986400604248047,0.35055702924728394,0.34878775477409363,0.34860214591026306,0.34813323616981506,0.34807080030441284,0.3475118577480316,0.34692618250846863,0.3462976813316345,0.3465063273906708,0.3446844220161438,0.3450990319252014,0.34452101588249207,0.34567636251449585,0.343111127614975,0.34385013580322266,0.3418973982334137,0.3413540720939636,0.34269359707832336,0.34083667397499084,0.3407476544380188,0.3415752947330475,0.3403569459915161,0.3389640152454376,0.33986178040504456,0.3386242091655731,0.3390897512435913,0.3391040861606598,0.3388464152812958,0.3382154703140259,0.3380652964115143,0.3362194001674652,0.33502212166786194,0.33830320835113525,0.33684584498405457,0.3364023268222809,0.335999995470047,0.33694830536842346,0.33639082312583923,0.3365708887577057,0.33539339900016785,0.3355778455734253,0.33539992570877075,0.33529776334762573,0.33571600914001465,0.3342956602573395,0.33490708470344543,0.33418580889701843,0.33419039845466614,0.3349820077419281,0.3341512084007263,0.33403655886650085,0.33368828892707825,0.3341052234172821,0.33344051241874695,0.3317241668701172,0.33277562260627747,0.33352574706077576,0.33312660455703735,0.33299848437309265,0.3322084844112396,0.3326702415943146,0.33124640583992004,0.3332965075969696,0.33308157324790955,0.33157894015312195,0.331850528717041,0.33026644587516785,0.3312402069568634,0.3298740088939667,0.3317854106426239,0.33079713582992554,0.33141693472862244,0.3306223750114441,0.33027979731559753,0.3306489884853363,0.3316526710987091,0.3300291895866394,0.3310127258300781,0.3304007053375244,0.3299010097980499,0.33028551936149597,0.3290395736694336,0.32959941029548645,0.32965973019599915,0.32926586270332336,0.3299619257450104,0.32862821221351624,0.32907241582870483,0.3288780152797699,0.32774168252944946,0.33000126481056213,0.3291277289390564,0.32829686999320984,0.3285607695579529,0.3287932276725769,0.32843536138534546,0.32911065220832825,0.3287922441959381,0.32845374941825867,0.3274340331554413,0.32829833030700684,0.32749783992767334,0.32812342047691345,0.32749101519584656,0.32870933413505554,0.3273475170135498,0.3265311121940613,0.3296798765659332,0.3268948197364807,0.32795098423957825,0.3263741135597229,0.3266506493091583,0.3269749581813812,0.3271871507167816,0.3270415961742401,0.32716184854507446,0.3268151581287384,0.3262158930301666,0.3264632225036621,0.32563263177871704,0.3275986909866333,0.32563692331314087,0.32619795203208923,0.32618942856788635,0.3258529007434845,0.3260156512260437,0.32507264614105225,0.32577481865882874,0.32668060064315796,0.32489264011383057,0.325364887714386,0.3256201446056366,0.3267763555049896,0.3251523971557617,0.3245663046836853,0.325541615486145,0.32477033138275146,0.3248642086982727,0.3248407244682312,0.32572993636131287,0.32626160979270935,0.3247913420200348,0.32516148686408997,0.32468074560165405,0.3247586190700531,0.32525160908699036,0.3262457847595215,0.32520177960395813,0.3256917893886566,0.3245994448661804,0.3263624906539917,0.32505637407302856,0.3258324861526489,0.32471510767936707,0.32409611344337463,0.32497894763946533,0.324160099029541,0.32510969042778015,0.324528306722641,0.3243313133716583,0.3238576054573059,0.324311763048172,0.32359975576400757,0.3230879604816437,0.3249691128730774,0.3240717947483063,0.325435996055603,0.32259124517440796,0.32470712065696716,0.32397815585136414,0.3233933448791504,0.32443931698799133,0.3237263560295105,0.3233727216720581,0.32294943928718567,0.3241233229637146,0.32412442564964294,0.323949933052063,0.3228262960910797,0.3241567611694336,0.32376712560653687,0.3230677843093872,0.32351619005203247,0.32502055168151855,0.32389578223228455,0.32355278730392456,0.3233024477958679,0.32288113236427307,0.32268819212913513,0.32245147228240967,0.32299402356147766,0.323086678981781,0.3231782019138336,0.32279038429260254,0.32303619384765625,0.32332056760787964,0.3226742744445801,0.32152917981147766,0.32242903113365173,0.32274162769317627,0.3210494816303253,0.32233741879463196,0.3213954567909241,0.32236766815185547,0.3206808269023895,0.324196994304657,0.32206445932388306,0.32280126214027405,0.32075124979019165,0.3226267099380493,0.3220217823982239,0.32206419110298157,0.3224712610244751,0.32170724868774414,0.32078802585601807,0.3215828239917755,0.32142847776412964,0.32202738523483276,0.3210334777832031,0.32202136516571045,0.3210228979587555,0.32135453820228577,0.3209390938282013,0.32178524136543274,0.3213358521461487,0.32130664587020874,0.32200804352760315,0.3205937445163727,0.32116055488586426,0.3214201331138611,0.3201042711734772,0.3214322030544281,0.3208586573600769,0.3207305073738098,0.3212689459323883,0.32093191146850586,0.32086241245269775,0.32237687706947327,0.3208305239677429,0.32142019271850586,0.3205417990684509,0.3205246925354004,0.3216956555843353,0.32122302055358887,0.3214176297187805,0.32005661725997925,0.3223744034767151,0.3209804892539978,0.32119277119636536,0.32074806094169617,0.3206234574317932,0.3207232654094696,0.32082098722457886,0.32356759905815125,0.3210228383541107,0.32091817259788513,0.32122063636779785,0.32162919640541077,0.32064375281333923,0.3199557662010193,0.3217763304710388,0.3215518295764923,0.32012680172920227,0.321162611246109,0.32076290249824524,0.32126185297966003,0.3210940361022949,0.321574330329895,0.320697546005249,0.31997066736221313,0.3210599720478058,0.320289671421051,0.3197404742240906,0.3206900656223297,0.3208404779434204,0.32029956579208374,0.3214567005634308,0.3205837607383728,0.3208642899990082,0.3198215663433075,0.3209618628025055,0.32027071714401245,0.32282114028930664,0.31909728050231934,0.32165244221687317,0.31947648525238037,0.32237157225608826,0.32084354758262634,0.31950464844703674,0.3206268846988678,0.3206689953804016,0.31967005133628845,0.3208574950695038,0.3200303912162781,0.3204057812690735,0.3206976652145386,0.320674329996109,0.31967949867248535,0.3227332532405853,0.3194486200809479,0.32053226232528687,0.32110917568206787,0.31859663128852844,0.3208898603916168,0.31852832436561584,0.32364651560783386,0.3201362192630768,0.3232123851776123,0.32026243209838867,0.3216458857059479,0.32083866000175476,0.32021012902259827,0.31791484355926514,0.320858359336853,0.3205536901950836,0.32003775238990784,0.3200049102306366,0.32061922550201416,0.32089054584503174,0.32009589672088623,0.3211347460746765,0.31930088996887207,0.3206900954246521,0.3202531933784485,0.32083359360694885,0.3202737271785736,0.3208373785018921,0.3193572759628296,0.31992441415786743,0.31965503096580505,0.32110798358917236,0.32036352157592773,0.3197130262851715,0.32010534405708313,0.31934410333633423,0.3191198408603668,0.32101869583129883,0.31765216588974,0.3197203576564789,0.3196794390678406,0.32028794288635254,0.32035738229751587,0.3192877471446991,0.31943798065185547,0.3195076882839203,0.31913745403289795,0.31996408104896545,0.32041725516319275,0.3195028007030487,0.3189728558063507,0.3189007341861725,0.319466233253479,0.31831425428390503,0.32003873586654663],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.94]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"<b>Loss Val_Loss</b>\"}},\"yaxis2\":{\"anchor\":\"x\",\"overlaying\":\"y\",\"side\":\"right\"},\"title\":{\"text\":\"Loss Val_Loss\"},\"autosize\":false,\"width\":1200,\"height\":600,\"paper_bgcolor\":\"rgb(200,200,200)\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d48668a2-2908-462a-8111-9cd4018fb32d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPyhVN5EamnoWvc/65pIVL",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "642f61ee5dc94d54a92bb346546e9bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70034f1b2077439b9c2af5de9fa05f0e",
              "IPY_MODEL_0cfaaa59b5c847b8aa3bbef853ec4cf4"
            ],
            "layout": "IPY_MODEL_43042fe567794fee82df4410fc6220a7"
          }
        },
        "70034f1b2077439b9c2af5de9fa05f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62f49f504e2144e199ada685b4c14b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_a18efbedd141492f8d138ad062814eaf",
            "value": "0.001 MB of 0.031 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "0cfaaa59b5c847b8aa3bbef853ec4cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2afd1668bcf440e385b76bfa3d368be3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_417e20d634db42b6b477d5fd159a74b6",
            "value": 0.018282445121572042
          }
        },
        "43042fe567794fee82df4410fc6220a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62f49f504e2144e199ada685b4c14b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a18efbedd141492f8d138ad062814eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2afd1668bcf440e385b76bfa3d368be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "417e20d634db42b6b477d5fd159a74b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}