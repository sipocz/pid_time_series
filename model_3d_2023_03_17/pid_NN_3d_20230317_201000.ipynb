{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/model_3d_2023_03_17/pid_NN_3d_20230317_201000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tNYnFR-6Xh",
        "outputId": "221f17d7-2a4e-431c-8165-f446ca89ce30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "31242421-b243-4802-ee61-66395d18ab90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint3\"\n",
        "data_source=\"SINT3_415/3D_transposed\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_3D_part\"\n",
        "fname_bad = \"415_SC_3D_part\"\n",
        "fname_good_ext=[\"1.csv\",\"3.csv\",\"5.csv\",\"7.csv\"]\n",
        "fname_bad_ext=[\"2.csv\",\"4.csv\",\"6.csv\",\"8.csv\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good \n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "73716730-8630-498e-89c0-a6c9a685efa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/SINT3_415/3D_transposed/415_SC_3D_part\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/SINT3_415/3D_transposed/415_SC_3D_part\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols=[str(i) for i in range(60)]"
      ],
      "metadata": {
        "id": "SpsF7dVwl7bH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vUcMjZAGKvtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb4b334-d6cf-4ed2-d2ff-d5736391dba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 60 columns]\n"
          ]
        }
      ],
      "source": [
        "df_good=pd.DataFrame(columns=cols)\n",
        "df_bad=pd.DataFrame(columns=cols)\n",
        "print(df_good.head())\n",
        "for i,goods in enumerate(fname_good_ext):\n",
        "    df_good_tmp = pd.read_csv(path_good+goods,usecols=None )\n",
        "    df_good_tmp.columns=cols\n",
        "    df_good=pd.concat([df_good,df_good_tmp],axis=0,)\n",
        "    df_bad_tmp = pd.read_csv(path_bad+fname_bad_ext[i],usecols=None)\n",
        "    df_bad_tmp.columns=cols\n",
        "    df_bad=pd.concat([df_bad,df_bad_tmp],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYuDXKraLOt4",
        "outputId": "be07413b-5f6e-43ca-ee96-c2d458155b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(df_good.isnull().values.any())\n",
        "print(df_bad.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "d2d7b2a9-e1fe-4dca-b768-2bec41f46d30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0     1     2     3     4     5     6     7     8     9  ...  \\\n",
              "569  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "570  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "571  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "572  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "573  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "\n",
              "           50        51        52        53        54        55        56  \\\n",
              "569  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "570  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "571  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "572  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "573  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "\n",
              "           57        58        59  \n",
              "569  0.216297  0.216297  0.216297  \n",
              "570  0.216297  0.216297  0.216297  \n",
              "571  0.216297  0.216297  0.216297  \n",
              "572  0.216297  0.216297  0.163654  \n",
              "573  0.216297  0.163654  0.216297  \n",
              "\n",
              "[5 rows x 60 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee055487-1fb4-4089-a179-21a072578114\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.163654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.163654</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 60 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee055487-1fb4-4089-a179-21a072578114')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee055487-1fb4-4089-a179-21a072578114 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee055487-1fb4-4089-a179-21a072578114');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df_good.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "df_ = df_good\n",
        "\n",
        "# You must normalize the data before applying the fit method\n",
        "df_good_normalized=(df_ - df_.mean()) / df_.std()\n",
        "\n",
        "# Normalize bad data with the good data parameters\n",
        "df_bad_normalized=(df_bad - df_.mean()) / df_.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": [
        "df_good_normalized[\"state\"]=0\n",
        "df_bad_normalized[\"state\"]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3W5mi70VM6hL"
      },
      "outputs": [],
      "source": [
        "df_good_normalized=df_good_normalized.reindex()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_good_normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Vg9QgProrxU_",
        "outputId": "acc42a5a-c653-4754-b74e-c7284baaa748"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "1   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "2   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "3   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "4   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "569 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "570 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "571 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "572 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "573 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "\n",
              "            7         8         9  ...        51        52        53  \\\n",
              "0   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "1   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "2   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "3   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "4   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "569 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "570 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "571 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "572 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "573 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "\n",
              "           54        55        56        57        58        59  state  \n",
              "0   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "1   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "2   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "3   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "4   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "..        ...       ...       ...       ...       ...       ...    ...  \n",
              "569 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.025578      0  \n",
              "570 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.025578      0  \n",
              "571 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.025578      0  \n",
              "572 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.041168      0  \n",
              "573 -0.025480 -0.025480 -0.025479 -0.025479 -0.041063 -0.025578      0  \n",
              "\n",
              "[2646 rows x 61 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d0c30e0-7757-4d6d-b528-9a4aad8de0d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.041168</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.041063</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2646 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d0c30e0-7757-4d6d-b528-9a4aad8de0d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d0c30e0-7757-4d6d-b528-9a4aad8de0d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d0c30e0-7757-4d6d-b528-9a4aad8de0d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_normalized=pd.concat([df_good_normalized,df_bad_normalized],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "fe052977-1ec7-49d5-d46f-680902602856"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "1   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "2   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "3   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "4   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "515 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "516 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "517 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "518 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "519 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "\n",
              "            7         8         9  ...        51        52        53  \\\n",
              "0   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "1   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "2   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "3   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "4   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "515 -0.318939 -0.320116 -0.321295  ... -0.370495  2.344881  3.525656   \n",
              "516 -0.318939 -0.320116 -0.321295  ...  2.344881  3.525656  1.690100   \n",
              "517 -0.318939 -0.320116 -0.321295  ...  3.525656  1.690100 -2.078618   \n",
              "518 -0.318939 -0.320116 -0.321295  ...  1.690100 -2.078619 -2.669684   \n",
              "519 -0.318939 -0.320116 -0.321295  ... -2.078619 -2.669684  0.108052   \n",
              "\n",
              "           54        55        56        57        58        59  state  \n",
              "0   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "1   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "2   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "3   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "4   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "..        ...       ...       ...       ...       ...       ...    ...  \n",
              "515  1.690100 -2.078618 -2.669683  0.108053  2.199832  2.951400      1  \n",
              "516 -2.078618 -2.669683  0.108053  2.199826  2.951541  3.221172      1  \n",
              "517 -2.669683  0.108053  2.199826  2.951536  3.221317  1.193817      1  \n",
              "518  0.108052  2.199826  2.951536  3.221312  1.193936 -2.382354      1  \n",
              "519  2.199826  2.951536  3.221312  1.193931 -2.382277  0.507865      1  \n",
              "\n",
              "[5658 rows x 61 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53cd31e7-46fc-4e36-a6de-928e7eae932e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.370495</td>\n",
              "      <td>2.344881</td>\n",
              "      <td>3.525656</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078618</td>\n",
              "      <td>-2.669683</td>\n",
              "      <td>0.108053</td>\n",
              "      <td>2.199832</td>\n",
              "      <td>2.951400</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>2.344881</td>\n",
              "      <td>3.525656</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078618</td>\n",
              "      <td>-2.669683</td>\n",
              "      <td>0.108053</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951541</td>\n",
              "      <td>3.221172</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>3.525656</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078618</td>\n",
              "      <td>-2.669683</td>\n",
              "      <td>0.108053</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951536</td>\n",
              "      <td>3.221317</td>\n",
              "      <td>1.193817</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078619</td>\n",
              "      <td>-2.669684</td>\n",
              "      <td>0.108052</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951536</td>\n",
              "      <td>3.221312</td>\n",
              "      <td>1.193936</td>\n",
              "      <td>-2.382354</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.078619</td>\n",
              "      <td>-2.669684</td>\n",
              "      <td>0.108052</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951536</td>\n",
              "      <td>3.221312</td>\n",
              "      <td>1.193931</td>\n",
              "      <td>-2.382277</td>\n",
              "      <td>0.507865</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5658 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53cd31e7-46fc-4e36-a6de-928e7eae932e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-53cd31e7-46fc-4e36-a6de-928e7eae932e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-53cd31e7-46fc-4e36-a6de-928e7eae932e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df_all_normalized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "nVvhP84S_F1y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_N1_=1100 #70  #700\n",
        "_N2_=118 #12  #120\n",
        "_lr_=0.001\n",
        "_batch_size_=32\n",
        "_drop1_=0.5\n",
        "_drop2_=0.7\n",
        "_epochs_=9500\n"
      ],
      "metadata": {
        "id": "XC5_bGE0iyi4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"lr\": _lr_, \"batch_size\": _batch_size_,\"architecture\": \"NN\", \n",
        "          \"depth\": 2,\n",
        "          \"layer1\":_N1_,  \"layer2\":_N2_, \n",
        "          \"drop1\":_drop1_,\"drop2\":_drop2_,\n",
        "          \"epochs\":_epochs_\n",
        "          \n",
        "          \n",
        "          }\n",
        "\n",
        "wandb.init(project=\"pid_3d\", entity=\"sipoczlaszlo\",config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539,
          "referenced_widgets": [
            "8eaf1b3a27cc408cb4b8789b9f4aff17",
            "9366987909ac43908f85ad8eb238b1ba",
            "006ba89529b545d889bc1119c27a264e",
            "8f6114bf2dfc43bdb0e2b8399b8b7b90",
            "b5ac96620577491ba1ea0b6c6d5598fb",
            "02be6f6f014b43188ef6f5f86f3c64c8",
            "cd5e404223e745d49094876aa2e79fd0",
            "7608f25433df4b2dbe5c31520b2fade0"
          ]
        },
        "id": "nOtKllcviuoj",
        "outputId": "b67051b2-1fd1-4698-9696-abf6a426e0c2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2p3g9i1k) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8eaf1b3a27cc408cb4b8789b9f4aff17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▂▄▄▄▅▅▅▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇██▇███████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>epoch/loss</td><td>█▆▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████████████████</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▄▄▄▅▆▅▆▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.85808</td></tr><tr><td>epoch/epoch</td><td>869</td></tr><tr><td>epoch/learning_rate</td><td>0.002</td></tr><tr><td>epoch/loss</td><td>0.32055</td></tr><tr><td>epoch/lr</td><td>0.002</td></tr><tr><td>epoch/val_accuracy</td><td>0.86926</td></tr><tr><td>epoch/val_loss</td><td>0.26481</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">divine-brook-24</strong> at: <a href='https://wandb.ai/sipoczlaszlo/pid_3d/runs/2p3g9i1k' target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_3d/runs/2p3g9i1k</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_180249-2p3g9i1k/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2p3g9i1k). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_180519-20gqzqia</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sipoczlaszlo/pid_3d/runs/20gqzqia' target=\"_blank\">deep-aardvark-25</a></strong> to <a href='https://wandb.ai/sipoczlaszlo/pid_3d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sipoczlaszlo/pid_3d' target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_3d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sipoczlaszlo/pid_3d/runs/20gqzqia' target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_3d/runs/20gqzqia</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sipoczlaszlo/pid_3d/runs/20gqzqia?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f6fc3851700>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=60\n",
        "\n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "l1_out=Dense(_N1_,activation=\"swish\",kernel_initializer='glorot_uniform',)(input1) # kernel_initializer='lecun_normal'\n",
        "l2_out=Dropout(_drop1_)(l1_out)\n",
        "\n",
        "\n",
        "l3_out=Dense(_N2_,activation=\"swish\",kernel_initializer='glorot_uniform',)(l2_out) #kernel_initializer='lecun_normal',\n",
        "l4_out=Dropout(_drop2_)(l3_out)\n",
        "\n",
        "pred=Dense(1, activation=\"sigmoid\",)(l4_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=_lr_,) #\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# 35 5 1 relu relu sigmoid SGD 0.01 loss: 0.1402 - accuracy: 0.9435 - val_loss: 0.7302 - val_accuracy: 0.8548\n",
        "# 35 12 1 relu relu sigmoid SGD 0.01 loss 0.1162 94.6% test : 85%\n",
        "# 17 5 1 relu relu sigmoid SGD 0.01  loss: 0.1714 - accuracy: 0.9300 - val_loss: 0.9535 - val_accuracy: 0.8503\n",
        "# 35 5 1 relu relu sigmoid Adam 0.01 loss: 0.1238 - accuracy: 0.9467 - val_loss: 5.7545 - val_accuracy: 0.8653\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.01 loss: 0.1184 - accuracy: 0.9525 - val_loss: 3.5327 - val_accuracy: 0.8428\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1185 - accuracy: 0.9525 - val_loss: 2.3218 - val_accuracy: 0.8593\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1041 - accuracy: 0.9576 - val_loss: 5.1465 - val_accuracy: 0.8353  +1300 epoch \n",
        "# 135 15 1 swish swish sigmoid Adamax 0.001 batch size:1 epoch 100 loss: 0.1707 - accuracy: 0.9352 - val_loss: 0.8066 - val_accuracy: 0.8892   **** egész jó\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=cols\n",
        "X_NN=df_all_normalized[prediktorok][:]  # \n",
        "y_NN=df_all_normalized[\"state\"][:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\"\n",
        "#model_file=\"model_PID__94_loss_0.116_vloss_0.115_acc_0.950_vacc_0.966.hdf5\"\n",
        "model_file=\"model_PID__4491_loss_0.115_vloss_0.679_acc_0.954_vacc_0.880.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False"
      ],
      "metadata": {
        "id": "UIxI3AS6Yw3S"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=0.9,shuffle=True,random_state=33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "jJfOOTfGfDXi"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate_corrector(epoch,lr):\n",
        "    if epoch > 4000:\n",
        "        lr = 0.001\n",
        "        return lr\n",
        "    if epoch > 3000:\n",
        "        lr = 0.002\n",
        "        return lr\n",
        "    if epoch > 2000:\n",
        "        lr = 0.005\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 500:\n",
        "        lr = 0.002\n",
        "        return lr\n",
        "    return lr\n",
        "    "
      ],
      "metadata": {
        "id": "A-Kv8ORiEfub"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.keras import WandbMetricsLogger\n",
        "fname=\"./model_PID_3D\"\n",
        "callbacks = [\n",
        "        LearningRateScheduler(learning_rate_corrector,verbose=1),\n",
        "        WandbMetricsLogger(),       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch:04.0f}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{accuracy:.3f}_vacc_{val_accuracy:.3f}.hdf5\", monitor='loss',\n",
        "                        verbose=2, save_best_only=True, mode='min')]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__learning__=True"
      ],
      "metadata": {
        "id": "O6ofy0moderd"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z3Z4q14D7eC"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "9Ol0mW6WRlkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bfcedbf-9245-4319-d7d6-cdb0d71913f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA streamkimeneten csak az utolsó 5000 sor látható.\u001b[0m\n",
            "\n",
            "Epoch 8251: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8251: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8252: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8252: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8253: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8253: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8254: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8254: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8255: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8255: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8256: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8256: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8257: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8257: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8258: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8258: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8259: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8259: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8260: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8260: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8261: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8261: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8262: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8262: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8263: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8263: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8264: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8264: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8265: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8265: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8266: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8266: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8267: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8267: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8268: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8268: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8269: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8269: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8270: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8270: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8271: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8271: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8272: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8272: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8273: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8273: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8274: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8274: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8275: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8275: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8276: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8276: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8277: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8277: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8278: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8278: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8279: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8279: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8280: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8280: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8281: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8281: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8282: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8282: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8283: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8283: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8284: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8284: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8285: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8285: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8286: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8286: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8287: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8287: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8288: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8288: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8289: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8289: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8290: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8290: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8291: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8291: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8292: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8292: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8293: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8293: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8294: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8294: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8295: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8295: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8296: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8296: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8297: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8297: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8298: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8298: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8299: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8299: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8300: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8300: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8301: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8301: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8302: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8302: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8303: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8303: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8304: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8304: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8305: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8305: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8306: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8306: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8307: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8307: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8308: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8308: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8309: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8309: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8310: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8310: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8311: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8311: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8312: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8312: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8313: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8313: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8314: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8314: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8315: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8315: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8316: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8316: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8317: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8317: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8318: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8318: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8319: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8319: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8320: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8320: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8321: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8321: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8322: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8322: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8323: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8323: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8324: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8324: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8325: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8325: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8326: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8326: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8327: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8327: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8328: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8328: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8329: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8329: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8330: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8330: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8331: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8331: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8332: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8332: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8333: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8333: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8334: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8334: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8335: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8335: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8336: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8336: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8337: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8337: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8338: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8338: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8339: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8339: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8340: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8340: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8341: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8341: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8342: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8342: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8343: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8343: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8344: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8344: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8345: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8345: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8346: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8346: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8347: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8347: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8348: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8348: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8349: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8349: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8350: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8350: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8351: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8351: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8352: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8352: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8353: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8353: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8354: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8354: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8355: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8355: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8356: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8356: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8357: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8357: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8358: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8358: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8359: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8359: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8360: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8360: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8361: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8361: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8362: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8362: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8363: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8363: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8364: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8364: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8365: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8365: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8366: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8366: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8367: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8367: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8368: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8368: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8369: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8369: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8370: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8370: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8371: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8371: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8372: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8372: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8373: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8373: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8374: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8374: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8375: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8375: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8376: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8376: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8377: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8377: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8378: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8378: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8379: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8379: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8380: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8380: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8381: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8381: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8382: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8382: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8383: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8383: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8384: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8384: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8385: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8385: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8386: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8386: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8387: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8387: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8388: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8388: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8389: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8389: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8390: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8390: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8391: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8391: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8392: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8392: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8393: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8393: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8394: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8394: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8395: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8395: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8396: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8396: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8397: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8397: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8398: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8398: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8399: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8399: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8400: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8400: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8401: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8401: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8402: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8402: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8403: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8403: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8404: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8404: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8405: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8405: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8406: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8406: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8407: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8407: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8408: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8408: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8409: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8409: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8410: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8410: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8411: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8411: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8412: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8412: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8413: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8413: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8414: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8414: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8415: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8415: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8416: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8416: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8417: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8417: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8418: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8418: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8419: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8419: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8420: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8420: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8421: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8421: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8422: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8422: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8423: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8423: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8424: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8424: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8425: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8425: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8426: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8426: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8427: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8427: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8428: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8428: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8429: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8429: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8430: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8430: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8431: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8431: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8432: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8432: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8433: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8433: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8434: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8434: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8435: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8435: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8436: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8436: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8437: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8437: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8438: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8438: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8439: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8439: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8440: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8440: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8441: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8441: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8442: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8442: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8443: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8443: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8444: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8444: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8445: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8445: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8446: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8446: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8447: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8447: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8448: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8448: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8449: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8449: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8450: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8450: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8451: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8451: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8452: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8452: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8453: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8453: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8454: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8454: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8455: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8455: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8456: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8456: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8457: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8457: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8458: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8458: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8459: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8459: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8460: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8460: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8461: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8461: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8462: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8462: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8463: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8463: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8464: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8464: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8465: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8465: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8466: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8466: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8467: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8467: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8468: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8468: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8469: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8469: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8470: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8470: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8471: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8471: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8472: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8472: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8473: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8473: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8474: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8474: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8475: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8475: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8476: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8476: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8477: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8477: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8478: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8478: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8479: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8479: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8480: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8480: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8481: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8481: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8482: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8482: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8483: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8483: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8484: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8484: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8485: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8485: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8486: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8486: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8487: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8487: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8488: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8488: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8489: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8489: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8490: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8490: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8491: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8491: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8492: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8492: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8493: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8493: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8494: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8494: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8495: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8495: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8496: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8496: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8497: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8497: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8498: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8498: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8499: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8499: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8500: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8500: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8501: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8501: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8502: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8502: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8503: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8503: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8504: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8504: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8505: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8505: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8506: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8506: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8507: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8507: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8508: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8508: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8509: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8509: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8510: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8510: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8511: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8511: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8512: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8512: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8513: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8513: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8514: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8514: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8515: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8515: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8516: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8516: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8517: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8517: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8518: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8518: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8519: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8519: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8520: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8520: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8521: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8521: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8522: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8522: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8523: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8523: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8524: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8524: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8525: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8525: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8526: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8526: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8527: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8527: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8528: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8528: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8529: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8529: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8530: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8530: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8531: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8531: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8532: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8532: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8533: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8533: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8534: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8534: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8535: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8535: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8536: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8536: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8537: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8537: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8538: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8538: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8539: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8539: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8540: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8540: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8541: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8541: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8542: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8542: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8543: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8543: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8544: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8544: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8545: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8545: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8546: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8546: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8547: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8547: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8548: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8548: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8549: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8549: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8550: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8550: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8551: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8551: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8552: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8552: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8553: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8553: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8554: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8554: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8555: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8555: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8556: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8556: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8557: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8557: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8558: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8558: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8559: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8559: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8560: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8560: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8561: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8561: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8562: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8562: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8563: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8563: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8564: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8564: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8565: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8565: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8566: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8566: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8567: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8567: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8568: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8568: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8569: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8569: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8570: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8570: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8571: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8571: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8572: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8572: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8573: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8573: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8574: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8574: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8575: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8575: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8576: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8576: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8577: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8577: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8578: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8578: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8579: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8579: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8580: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8580: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8581: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8581: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8582: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8582: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8583: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8583: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8584: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8584: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8585: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8585: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8586: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8586: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8587: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8587: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8588: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8588: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8589: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8589: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8590: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8590: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8591: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8591: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8592: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8592: loss did not improve from 0.08408\n",
            "\n",
            "Epoch 8593: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8593: loss improved from 0.08408 to 0.08163, saving model to ./model_PID_3D_8593_loss_0.082_vloss_0.150_acc_0.970_vacc_0.965.hdf5\n",
            "\n",
            "Epoch 8594: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8594: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8595: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8595: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8596: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8596: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8597: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8597: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8598: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8598: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8599: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8599: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8600: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8600: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8601: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8601: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8602: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8602: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8603: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8603: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8604: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8604: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8605: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8605: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8606: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8606: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8607: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8607: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8608: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8608: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8609: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8609: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8610: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8610: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8611: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8611: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8612: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8612: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8613: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8613: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8614: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8614: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8615: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8615: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8616: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8616: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8617: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8617: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8618: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8618: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8619: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8619: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8620: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8620: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8621: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8621: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8622: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8622: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8623: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8623: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8624: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8624: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8625: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8625: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8626: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8626: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8627: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8627: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8628: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8628: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8629: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8629: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8630: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8630: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8631: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8631: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8632: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8632: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8633: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8633: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8634: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8634: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8635: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8635: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8636: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8636: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8637: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8637: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8638: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8638: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8639: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8639: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8640: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8640: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8641: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8641: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8642: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8642: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8643: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8643: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8644: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8644: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8645: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8645: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8646: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8646: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8647: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8647: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8648: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8648: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8649: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8649: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8650: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8650: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8651: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8651: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8652: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8652: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8653: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8653: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8654: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8654: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8655: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8655: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8656: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8656: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8657: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8657: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8658: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8658: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8659: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8659: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8660: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8660: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8661: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8661: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8662: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8662: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8663: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8663: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8664: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8664: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8665: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8665: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8666: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8666: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8667: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8667: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8668: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8668: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8669: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8669: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8670: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8670: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8671: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8671: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8672: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8672: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8673: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8673: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8674: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8674: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8675: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8675: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8676: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8676: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8677: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8677: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8678: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8678: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8679: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8679: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8680: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8680: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8681: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8681: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8682: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8682: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8683: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8683: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8684: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8684: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8685: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8685: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8686: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8686: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8687: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8687: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8688: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8688: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8689: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8689: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8690: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8690: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8691: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8691: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8692: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8692: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8693: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8693: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8694: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8694: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8695: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8695: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8696: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8696: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8697: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8697: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8698: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8698: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8699: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8699: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8700: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8700: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8701: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8701: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8702: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8702: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8703: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8703: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8704: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8704: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8705: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8705: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8706: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8706: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8707: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8707: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8708: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8708: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8709: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8709: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8710: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8710: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8711: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8711: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8712: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8712: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8713: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8713: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8714: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8714: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8715: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8715: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8716: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8716: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8717: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8717: loss did not improve from 0.08163\n",
            "\n",
            "Epoch 8718: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8718: loss improved from 0.08163 to 0.07993, saving model to ./model_PID_3D_8718_loss_0.080_vloss_0.143_acc_0.968_vacc_0.970.hdf5\n",
            "\n",
            "Epoch 8719: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8719: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8720: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8720: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8721: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8721: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8722: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8722: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8723: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8723: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8724: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8724: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8725: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8725: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8726: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8726: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8727: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8727: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8728: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8728: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8729: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8729: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8730: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8730: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8731: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8731: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8732: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8732: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8733: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8733: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8734: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8734: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8735: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8735: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8736: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8736: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8737: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8737: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8738: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8738: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8739: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8739: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8740: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8740: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8741: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8741: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8742: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8742: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8743: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8743: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8744: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8744: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8745: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8745: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8746: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8746: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8747: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8747: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8748: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8748: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8749: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8749: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8750: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8750: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8751: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8751: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8752: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8752: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8753: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8753: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8754: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8754: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8755: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8755: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8756: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8756: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8757: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8757: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8758: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8758: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8759: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8759: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8760: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8760: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8761: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8761: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8762: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8762: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8763: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8763: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8764: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8764: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8765: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8765: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8766: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8766: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8767: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8767: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8768: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8768: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8769: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8769: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8770: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8770: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8771: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8771: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8772: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8772: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8773: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8773: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8774: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8774: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8775: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8775: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8776: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8776: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8777: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8777: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8778: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8778: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8779: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8779: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8780: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8780: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8781: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8781: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8782: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8782: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8783: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8783: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8784: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8784: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8785: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8785: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8786: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8786: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8787: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8787: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8788: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8788: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8789: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8789: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8790: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8790: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8791: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8791: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8792: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8792: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8793: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8793: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8794: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8794: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8795: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8795: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8796: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8796: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8797: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8797: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8798: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8798: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8799: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8799: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8800: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8800: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8801: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8801: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8802: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8802: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8803: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8803: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8804: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8804: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8805: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8805: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8806: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8806: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8807: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8807: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8808: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8808: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8809: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8809: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8810: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8810: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8811: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8811: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8812: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8812: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8813: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8813: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8814: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8814: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8815: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8815: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8816: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8816: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8817: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8817: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8818: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8818: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8819: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8819: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8820: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8820: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8821: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8821: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8822: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8822: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8823: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8823: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8824: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8824: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8825: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8825: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8826: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8826: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8827: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8827: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8828: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8828: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8829: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8829: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8830: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8830: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8831: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8831: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8832: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8832: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8833: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8833: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8834: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8834: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8835: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8835: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8836: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8836: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8837: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8837: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8838: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8838: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8839: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8839: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8840: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8840: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8841: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8841: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8842: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8842: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8843: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8843: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8844: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8844: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8845: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8845: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8846: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8846: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8847: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8847: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8848: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8848: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8849: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8849: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8850: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8850: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8851: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8851: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8852: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8852: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8853: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8853: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8854: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8854: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8855: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8855: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8856: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8856: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8857: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8857: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8858: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8858: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8859: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8859: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8860: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8860: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8861: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8861: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8862: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8862: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8863: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8863: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8864: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8864: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8865: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8865: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8866: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8866: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8867: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8867: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8868: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8868: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8869: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8869: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8870: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8870: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8871: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8871: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8872: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8872: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8873: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8873: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8874: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8874: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8875: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8875: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8876: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8876: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8877: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8877: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8878: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8878: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8879: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8879: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8880: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8880: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8881: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8881: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8882: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8882: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8883: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8883: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8884: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8884: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8885: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8885: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8886: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8886: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8887: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8887: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8888: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8888: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8889: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8889: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8890: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8890: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8891: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8891: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8892: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8892: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8893: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8893: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8894: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8894: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8895: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8895: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8896: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8896: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8897: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8897: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8898: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8898: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8899: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8899: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8900: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8900: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8901: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8901: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8902: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8902: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8903: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8903: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8904: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8904: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8905: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8905: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8906: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8906: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8907: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8907: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8908: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8908: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8909: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8909: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8910: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8910: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8911: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8911: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8912: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8912: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8913: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8913: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8914: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8914: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8915: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8915: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8916: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8916: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8917: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8917: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8918: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8918: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8919: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8919: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8920: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8920: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8921: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8921: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8922: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8922: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8923: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8923: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8924: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8924: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8925: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8925: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8926: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8926: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8927: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8927: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8928: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8928: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8929: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8929: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8930: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8930: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8931: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8931: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8932: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8932: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8933: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8933: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8934: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8934: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8935: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8935: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8936: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8936: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8937: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8937: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8938: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8938: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8939: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8939: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8940: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8940: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8941: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8941: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8942: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8942: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8943: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8943: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8944: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8944: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8945: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8945: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8946: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8946: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8947: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8947: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8948: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8948: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8949: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8949: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8950: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8950: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8951: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8951: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8952: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8952: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8953: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8953: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8954: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8954: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8955: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8955: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8956: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8956: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8957: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8957: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8958: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8958: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8959: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8959: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8960: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8960: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8961: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8961: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8962: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8962: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8963: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8963: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8964: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8964: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8965: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8965: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8966: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8966: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8967: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8967: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8968: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8968: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8969: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8969: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8970: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8970: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8971: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8971: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8972: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8972: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8973: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8973: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8974: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8974: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8975: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8975: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8976: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8976: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8977: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8977: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8978: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8978: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8979: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8979: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8980: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8980: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8981: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8981: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8982: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8982: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8983: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8983: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8984: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8984: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8985: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8985: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8986: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8986: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8987: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8987: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8988: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8988: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8989: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8989: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8990: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8990: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8991: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8991: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8992: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8992: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8993: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8993: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8994: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8994: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8995: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8995: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8996: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8996: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8997: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8997: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8998: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8998: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 8999: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 8999: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9000: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9000: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9001: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9001: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9002: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9002: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9003: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9003: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9004: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9004: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9005: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9005: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9006: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9006: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9007: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9007: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9008: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9008: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9009: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9009: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9010: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9010: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9011: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9011: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9012: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9012: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9013: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9013: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9014: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9014: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9015: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9015: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9016: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9016: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9017: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9017: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9018: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9018: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9019: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9019: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9020: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9020: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9021: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9021: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9022: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9022: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9023: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9023: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9024: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9024: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9025: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9025: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9026: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9026: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9027: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9027: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9028: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9028: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9029: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9029: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9030: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9030: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9031: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9031: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9032: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9032: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9033: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9033: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9034: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9034: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9035: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9035: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9036: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9036: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9037: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9037: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9038: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9038: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9039: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9039: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9040: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9040: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9041: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9041: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9042: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9042: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9043: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9043: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9044: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9044: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9045: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9045: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9046: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9046: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9047: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9047: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9048: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9048: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9049: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9049: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9050: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9050: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9051: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9051: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9052: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9052: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9053: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9053: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9054: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9054: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9055: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9055: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9056: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9056: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9057: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9057: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9058: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9058: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9059: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9059: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9060: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9060: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9061: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9061: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9062: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9062: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9063: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9063: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9064: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9064: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9065: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9065: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9066: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9066: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9067: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9067: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9068: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9068: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9069: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9069: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9070: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9070: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9071: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9071: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9072: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9072: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9073: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9073: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9074: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9074: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9075: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9075: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9076: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9076: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9077: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9077: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9078: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9078: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9079: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9079: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9080: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9080: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9081: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9081: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9082: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9082: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9083: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9083: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9084: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9084: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9085: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9085: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9086: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9086: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9087: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9087: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9088: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9088: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9089: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9089: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9090: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9090: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9091: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9091: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9092: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9092: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9093: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9093: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9094: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9094: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9095: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9095: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9096: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9096: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9097: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9097: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9098: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9098: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9099: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9099: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9100: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9100: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9101: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9101: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9102: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9102: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9103: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9103: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9104: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9104: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9105: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9105: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9106: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9106: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9107: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9107: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9108: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9108: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9109: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9109: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9110: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9110: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9111: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9111: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9112: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9112: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9113: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9113: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9114: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9114: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9115: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9115: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9116: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9116: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9117: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9117: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9118: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9118: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9119: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9119: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9120: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9120: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9121: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9121: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9122: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9122: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9123: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9123: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9124: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9124: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9125: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9125: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9126: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9126: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9127: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9127: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9128: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9128: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9129: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9129: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9130: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9130: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9131: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9131: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9132: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9132: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9133: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9133: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9134: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9134: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9135: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9135: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9136: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9136: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9137: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9137: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9138: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9138: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9139: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9139: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9140: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9140: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9141: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9141: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9142: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9142: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9143: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9143: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9144: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9144: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9145: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9145: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9146: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9146: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9147: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9147: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9148: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9148: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9149: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9149: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9150: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9150: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9151: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9151: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9152: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9152: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9153: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9153: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9154: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9154: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9155: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9155: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9156: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9156: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9157: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9157: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9158: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9158: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9159: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9159: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9160: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9160: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9161: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9161: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9162: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9162: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9163: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9163: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9164: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9164: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9165: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9165: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9166: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9166: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9167: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9167: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9168: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9168: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9169: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9169: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9170: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9170: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9171: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9171: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9172: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9172: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9173: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9173: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9174: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9174: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9175: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9175: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9176: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9176: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9177: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9177: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9178: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9178: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9179: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9179: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9180: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9180: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9181: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9181: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9182: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9182: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9183: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9183: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9184: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9184: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9185: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9185: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9186: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9186: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9187: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9187: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9188: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9188: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9189: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9189: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9190: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9190: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9191: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9191: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9192: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9192: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9193: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9193: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9194: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9194: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9195: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9195: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9196: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9196: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9197: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9197: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9198: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9198: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9199: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9199: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9200: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9200: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9201: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9201: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9202: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9202: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9203: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9203: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9204: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9204: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9205: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9205: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9206: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9206: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9207: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9207: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9208: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9208: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9209: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9209: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9210: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9210: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9211: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9211: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9212: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9212: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9213: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9213: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9214: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9214: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9215: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9215: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9216: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9216: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9217: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9217: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9218: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9218: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9219: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9219: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9220: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9220: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9221: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9221: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9222: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9222: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9223: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9223: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9224: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9224: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9225: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9225: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9226: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9226: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9227: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9227: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9228: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9228: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9229: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9229: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9230: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9230: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9231: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9231: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9232: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9232: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9233: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9233: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9234: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9234: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9235: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9235: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9236: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9236: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9237: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9237: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9238: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9238: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9239: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9239: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9240: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9240: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9241: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9241: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9242: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9242: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9243: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9243: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9244: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9244: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9245: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9245: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9246: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9246: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9247: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9247: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9248: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9248: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9249: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9249: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9250: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9250: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9251: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9251: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9252: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9252: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9253: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9253: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9254: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9254: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9255: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9255: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9256: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9256: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9257: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9257: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9258: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9258: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9259: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9259: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9260: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9260: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9261: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9261: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9262: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9262: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9263: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9263: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9264: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9264: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9265: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9265: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9266: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9266: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9267: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9267: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9268: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9268: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9269: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9269: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9270: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9270: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9271: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9271: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9272: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9272: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9273: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9273: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9274: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9274: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9275: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9275: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9276: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9276: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9277: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9277: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9278: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9278: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9279: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9279: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9280: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9280: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9281: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9281: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9282: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9282: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9283: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9283: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9284: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9284: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9285: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9285: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9286: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9286: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9287: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9287: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9288: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9288: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9289: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9289: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9290: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9290: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9291: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9291: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9292: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9292: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9293: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9293: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9294: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9294: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9295: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9295: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9296: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9296: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9297: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9297: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9298: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9298: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9299: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9299: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9300: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9300: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9301: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9301: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9302: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9302: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9303: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9303: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9304: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9304: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9305: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9305: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9306: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9306: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9307: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9307: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9308: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9308: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9309: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9309: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9310: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9310: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9311: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9311: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9312: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9312: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9313: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9313: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9314: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9314: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9315: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9315: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9316: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9316: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9317: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9317: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9318: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9318: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9319: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9319: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9320: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9320: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9321: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9321: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9322: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9322: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9323: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9323: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9324: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9324: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9325: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9325: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9326: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9326: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9327: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9327: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9328: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9328: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9329: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9329: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9330: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9330: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9331: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9331: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9332: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9332: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9333: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9333: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9334: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9334: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9335: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9335: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9336: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9336: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9337: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9337: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9338: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9338: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9339: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9339: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9340: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9340: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9341: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9341: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9342: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9342: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9343: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9343: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9344: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9344: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9345: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9345: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9346: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9346: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9347: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9347: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9348: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9348: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9349: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9349: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9350: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9350: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9351: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9351: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9352: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9352: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9353: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9353: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9354: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9354: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9355: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9355: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9356: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9356: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9357: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9357: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9358: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9358: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9359: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9359: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9360: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9360: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9361: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9361: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9362: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9362: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9363: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9363: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9364: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9364: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9365: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9365: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9366: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9366: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9367: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9367: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9368: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9368: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9369: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9369: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9370: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9370: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9371: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9371: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9372: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9372: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9373: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9373: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9374: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9374: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9375: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9375: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9376: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9376: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9377: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9377: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9378: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9378: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9379: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9379: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9380: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9380: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9381: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9381: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9382: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9382: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9383: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9383: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9384: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9384: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9385: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9385: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9386: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9386: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9387: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9387: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9388: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9388: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9389: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9389: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9390: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9390: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9391: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9391: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9392: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9392: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9393: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9393: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9394: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9394: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9395: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9395: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9396: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9396: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9397: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9397: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9398: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9398: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9399: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9399: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9400: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9400: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9401: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9401: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9402: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9402: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9403: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9403: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9404: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9404: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9405: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9405: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9406: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9406: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9407: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9407: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9408: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9408: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9409: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9409: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9410: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9410: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9411: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9411: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9412: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9412: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9413: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9413: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9414: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9414: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9415: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9415: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9416: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9416: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9417: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9417: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9418: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9418: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9419: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9419: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9420: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9420: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9421: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9421: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9422: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9422: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9423: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9423: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9424: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9424: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9425: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9425: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9426: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9426: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9427: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9427: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9428: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9428: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9429: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9429: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9430: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9430: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9431: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9431: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9432: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9432: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9433: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9433: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9434: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9434: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9435: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9435: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9436: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9436: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9437: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9437: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9438: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9438: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9439: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9439: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9440: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9440: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9441: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9441: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9442: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9442: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9443: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9443: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9444: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9444: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9445: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9445: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9446: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9446: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9447: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9447: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9448: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9448: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9449: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9449: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9450: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9450: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9451: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9451: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9452: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9452: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9453: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9453: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9454: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9454: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9455: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9455: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9456: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9456: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9457: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9457: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9458: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9458: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9459: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9459: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9460: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9460: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9461: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9461: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9462: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9462: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9463: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9463: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9464: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9464: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9465: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9465: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9466: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9466: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9467: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9467: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9468: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9468: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9469: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9469: loss did not improve from 0.07993\n",
            "\n",
            "Epoch 9470: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9470: loss improved from 0.07993 to 0.07947, saving model to ./model_PID_3D_9470_loss_0.079_vloss_0.150_acc_0.966_vacc_0.972.hdf5\n",
            "\n",
            "Epoch 9471: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9471: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9472: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9472: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9473: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9473: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9474: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9474: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9475: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9475: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9476: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9476: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9477: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9477: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9478: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9478: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9479: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9479: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9480: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9480: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9481: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9481: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9482: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9482: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9483: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9483: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9484: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9484: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9485: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9485: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9486: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9486: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9487: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9487: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9488: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9488: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9489: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9489: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9490: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9490: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9491: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9491: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9492: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9492: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9493: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9493: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9494: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9494: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9495: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9495: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9496: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9496: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9497: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9497: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9498: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9498: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9499: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9499: loss did not improve from 0.07947\n",
            "\n",
            "Epoch 9500: LearningRateScheduler setting learning rate to 0.001.\n",
            "\n",
            "Epoch 9500: loss did not improve from 0.07947\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __learning__: \n",
        "    history = model.fit(X_train, y_train, epochs=_epochs_, batch_size=_epochs_, validation_data=(X_test, y_test),verbose=0,callbacks=callbacks,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blabla"
      ],
      "metadata": {
        "id": "pSNV4K_HDX5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "3d5db873-7520-42e5-b6e2-85a7dfe9adbd"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-e1a9de5dc7f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblabla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'blabla' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=True\n",
        "model_file=\"model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5\"\n",
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "EGg1PjCJDTKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "JgzklVywoNmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwcWQ94IpDFu"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bina_transformer=Binarizer(threshold=0.5)\n",
        "y_pred_transform=bina_transformer.fit_transform(y_pred)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_test,y_pred_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "id": "Z69kCq3T-pMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ0rmkNsBGnl"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve( y_pred_transform,y_test,pos_label=1)\n",
        "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "auc = roc_auc_score(y_test, y_pred_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_normalized"
      ],
      "metadata": {
        "id": "sNIc1l6vF6Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_changer(arr):\n",
        "    o=[\"r\" if i>0.5 else \"g\" for i in arr]\n",
        "    return o"
      ],
      "metadata": {
        "id": "YFJoZO8TG1ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotgraf(df_in, predicted):\n",
        "    xkoordinata=[i for i in range(len(df_in[\"0\"]))]\n",
        "    plot.figure(figsize=(12,6))\n",
        "    col_ch=color_changer(predicted)\n",
        "    plot.scatter(xkoordinata,df_in[\"0\"],c=col_ch,marker=\".\",alpha=0.3)\n",
        "    plot.ylabel('értékek')\n",
        "    plot.xlabel('index')\n",
        "    plot\n",
        "    plot.show()"
      ],
      "metadata": {
        "id": "YMHy-wbZGeqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_all_predict"
      ],
      "metadata": {
        "id": "HuL1OutGHHEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_all_predict=model.predict(df_all_normalized[prediktorok])"
      ],
      "metadata": {
        "id": "4aXpzheKE7bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_predict=df_y_all_predict.round()"
      ],
      "metadata": {
        "id": "70M96KLSdCHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_predict"
      ],
      "metadata": {
        "id": "e72-bHKzdZve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotgraf(df_all_normalized[400:600],df_y_predict[400:600])"
      ],
      "metadata": {
        "id": "BO3xdrHVGXhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "id": "WMRd5eGU9ASA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_df=pd.DataFrame({\"epoch\":history.epoch, \"loss\":history.history[\"loss\"],\"val_loss\":history.history[\"val_loss\"]})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "id": "4ENvDCA-0U1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3PcktnfNXK0MqTcwimnoc",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8eaf1b3a27cc408cb4b8789b9f4aff17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9366987909ac43908f85ad8eb238b1ba",
              "IPY_MODEL_006ba89529b545d889bc1119c27a264e"
            ],
            "layout": "IPY_MODEL_8f6114bf2dfc43bdb0e2b8399b8b7b90"
          }
        },
        "9366987909ac43908f85ad8eb238b1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5ac96620577491ba1ea0b6c6d5598fb",
            "placeholder": "​",
            "style": "IPY_MODEL_02be6f6f014b43188ef6f5f86f3c64c8",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "006ba89529b545d889bc1119c27a264e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd5e404223e745d49094876aa2e79fd0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7608f25433df4b2dbe5c31520b2fade0",
            "value": 1
          }
        },
        "8f6114bf2dfc43bdb0e2b8399b8b7b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ac96620577491ba1ea0b6c6d5598fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02be6f6f014b43188ef6f5f86f3c64c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd5e404223e745d49094876aa2e79fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7608f25433df4b2dbe5c31520b2fade0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}