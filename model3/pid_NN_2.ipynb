{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/model3/pid_NN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tNYnFR-6Xh",
        "outputId": "e58ab8bc-9a56-47a3-d8d0-047866ed40e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.7-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.12.0-py2.py3-none-any.whl (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 57.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 49.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 57.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 54.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 48.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 49.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 49.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 54.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 38.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 52.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 50.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 16.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 44.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=10f131277eaa80551f9f32620c17c3166d8e55721b4ed50f74d4cdbb6e917d8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "962094c4-605a-4597-d6d2-e0ce66e86526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint1\"\n",
        "data_source=\"5\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_error_part1.csv\"\n",
        "fname_bad = \"415_SC_error_part2.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "6e2448fa-226c-41e2-a4a5-c6b56c9310da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part1.csv\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part2.csv\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vUcMjZAGKvtt"
      },
      "outputs": [],
      "source": [
        "df_good = pd.read_csv(path_good,usecols=None)\n",
        "df_bad = pd.read_csv(path_bad,usecols=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYuDXKraLOt4",
        "outputId": "6eb8358c-521d-41c8-e757-37084ab80e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(df_good.isnull().values.any())\n",
        "print(df_bad.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "3b59fc2c-63d7-4e90-e332-9da6bce3e921"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0          1          2          3          4        5         6  \\\n",
              "0 -54.810024 -80.342186 -60.770203 -41.081482 -21.779583 -3.82353 -0.806820   \n",
              "1 -80.342186 -60.770203 -41.081482 -21.779583  -3.823530 -0.80682  0.220875   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f564f1b-01d4-4591-a88d-10b192b7282c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-54.810024</td>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.82353</td>\n",
              "      <td>-0.806820</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.823530</td>\n",
              "      <td>-0.80682</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f564f1b-01d4-4591-a88d-10b192b7282c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f564f1b-01d4-4591-a88d-10b192b7282c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f564f1b-01d4-4591-a88d-10b192b7282c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df_good.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "df_ = df_good\n",
        "\n",
        "# You must normalize the data before applying the fit method\n",
        "df_good_normalized=(df_ - df_.mean()) / df_.std()\n",
        "\n",
        "# Normalize bad data with the good data parameters\n",
        "df_bad_normalized=(df_bad - df_.mean()) / df_.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": [
        "df_good_normalized[\"state\"]=0\n",
        "df_bad_normalized[\"state\"]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3W5mi70VM6hL",
        "outputId": "32bc5b0c-e167-45dc-eaa7-def9808c49dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0          1          2          3         4         5  \\\n",
              "0    -10.681306 -16.586266 -14.612051 -11.087981 -6.293341 -1.192618   \n",
              "1    -15.654548 -12.549683  -9.889987  -5.905180 -1.164099 -0.314249   \n",
              "2    -11.842250  -8.489023  -5.260696  -1.083756 -0.302359 -0.015017   \n",
              "3     -8.007214  -4.508142  -0.954188  -0.273732 -0.008793 -0.015017   \n",
              "4     -4.247524  -0.804833  -0.230672   0.002217 -0.008793 -0.015017   \n",
              "...         ...        ...        ...        ...       ...       ...   \n",
              "1053   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1054   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1055   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1056   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1057   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "\n",
              "             6         7         8         9  ...        11        12  \\\n",
              "0    -0.315574 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "2    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "3    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "4    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...  ...       ...       ...   \n",
              "1053 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1054 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1055 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1056 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1057 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "\n",
              "            13        14        15        16        17        18        19  \\\n",
              "0    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "2    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "3    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "4    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1053 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1054 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1055 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1056 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1057 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "\n",
              "      state  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  \n",
              "...     ...  \n",
              "1053      0  \n",
              "1054      0  \n",
              "1055      0  \n",
              "1056      0  \n",
              "1057      0  \n",
              "\n",
              "[1058 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e74737f-7197-453c-9619-4672eb4a379c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10.681306</td>\n",
              "      <td>-16.586266</td>\n",
              "      <td>-14.612051</td>\n",
              "      <td>-11.087981</td>\n",
              "      <td>-6.293341</td>\n",
              "      <td>-1.192618</td>\n",
              "      <td>-0.315574</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-15.654548</td>\n",
              "      <td>-12.549683</td>\n",
              "      <td>-9.889987</td>\n",
              "      <td>-5.905180</td>\n",
              "      <td>-1.164099</td>\n",
              "      <td>-0.314249</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-11.842250</td>\n",
              "      <td>-8.489023</td>\n",
              "      <td>-5.260696</td>\n",
              "      <td>-1.083756</td>\n",
              "      <td>-0.302359</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-8.007214</td>\n",
              "      <td>-4.508142</td>\n",
              "      <td>-0.954188</td>\n",
              "      <td>-0.273732</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.247524</td>\n",
              "      <td>-0.804833</td>\n",
              "      <td>-0.230672</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1058 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e74737f-7197-453c-9619-4672eb4a379c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e74737f-7197-453c-9619-4672eb4a379c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e74737f-7197-453c-9619-4672eb4a379c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df_good_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_normalized=pd.concat([df_good_normalized,df_bad_normalized],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "d724b008-e71c-4440-e049-0942d9675c6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "1263  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1264  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1265  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1266  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1267  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "1263 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1264 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1265 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1266 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1267 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "1263 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1264 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1265 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1266 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1267 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dba84349-73de-4ddc-9054-f276d1e6e44b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1263</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1265</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dba84349-73de-4ddc-9054-f276d1e6e44b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dba84349-73de-4ddc-9054-f276d1e6e44b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dba84349-73de-4ddc-9054-f276d1e6e44b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_all_normalized.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "nVvhP84S_F1y"
      },
      "execution_count": 419,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_N1_=234\n",
        "_N2_=40\n",
        "_lr_=0.001\n",
        "_batch_size_=3\n",
        "_drop1_=0.3\n",
        "_drop2_=0.1\n",
        "_epochs_=5500\n"
      ],
      "metadata": {
        "id": "XC5_bGE0iyi4"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"lr\": _lr_, \"batch_size\": _batch_size_,\"architecture\": \"NN\", \n",
        "          \"depth\": 2,\n",
        "          \"layer1\":_N1_,  \"layer2\":_N2_, \n",
        "          \"drop1\":_drop1_,\"drop2\":_drop2_,\n",
        "          \"epochs\":_epochs_\n",
        "          \n",
        "          \n",
        "          }\n",
        "\n",
        "wandb.init(project=\"pid_1\", entity=\"sipoczlaszlo\",config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "nOtKllcviuoj",
        "outputId": "aa8e9736-1bfe-491d-e8c4-9e012cc74711"
      },
      "execution_count": 421,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2lk2ytkt) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▃▅▆▇▇▇▇▇██████▇██▇██████████████████▇█▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>epoch/lr</td><td>████▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▇▇█▇▇▆▇▇▆▇▆▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇█</td></tr><tr><td>epoch/val_loss</td><td>▁▁▂▃▄▄▅▆▅▆▆▆▇▇█▇████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.95186</td></tr><tr><td>epoch/epoch</td><td>5499</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.12502</td></tr><tr><td>epoch/lr</td><td>0.0001</td></tr><tr><td>epoch/val_accuracy</td><td>0.87425</td></tr><tr><td>epoch/val_loss</td><td>0.70322</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vocal-deluge-36</strong>: <a href=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/2lk2ytkt\" target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_1/runs/2lk2ytkt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221218_175102-2lk2ytkt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2lk2ytkt). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221218_193046-2buow5cl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/2buow5cl\" target=\"_blank\">bright-haze-37</a></strong> to <a href=\"https://wandb.ai/sipoczlaszlo/pid_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/2buow5cl?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f3b9968ce50>"
            ]
          },
          "metadata": {},
          "execution_count": 421
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=20\n",
        "\n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "l1_out=Dense(_N1_,activation=\"swish\",kernel_initializer='glorot_uniform',)(input1) # kernel_initializer='lecun_normal'\n",
        "l2_out=Dropout(_drop1_)(l1_out)\n",
        "\n",
        "\n",
        "l3_out=Dense(_N2_,activation=\"swish\",kernel_initializer='glorot_uniform',)(l2_out) #kernel_initializer='lecun_normal',\n",
        "l4_out=Dropout(_drop2_)(l3_out)\n",
        "\n",
        "pred=Dense(1, activation=\"sigmoid\",)(l4_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=_lr_,) #\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# 35 5 1 relu relu sigmoid SGD 0.01 loss: 0.1402 - accuracy: 0.9435 - val_loss: 0.7302 - val_accuracy: 0.8548\n",
        "# 35 12 1 relu relu sigmoid SGD 0.01 loss 0.1162 94.6% test : 85%\n",
        "# 17 5 1 relu relu sigmoid SGD 0.01  loss: 0.1714 - accuracy: 0.9300 - val_loss: 0.9535 - val_accuracy: 0.8503\n",
        "# 35 5 1 relu relu sigmoid Adam 0.01 loss: 0.1238 - accuracy: 0.9467 - val_loss: 5.7545 - val_accuracy: 0.8653\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.01 loss: 0.1184 - accuracy: 0.9525 - val_loss: 3.5327 - val_accuracy: 0.8428\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1185 - accuracy: 0.9525 - val_loss: 2.3218 - val_accuracy: 0.8593\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1041 - accuracy: 0.9576 - val_loss: 5.1465 - val_accuracy: 0.8353  +1300 epoch \n",
        "# 135 15 1 swish swish sigmoid Adamax 0.001 batch size:1 epoch 100 loss: 0.1707 - accuracy: 0.9352 - val_loss: 0.8066 - val_accuracy: 0.8892   **** egész jó\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\"]\n",
        "X_NN=df_all_normalized[prediktorok][:-100]  # \n",
        "y_NN=df_all_normalized[\"state\"][:-100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\"\n",
        "#model_file=\"model_PID__94_loss_0.116_vloss_0.115_acc_0.950_vacc_0.966.hdf5\"\n",
        "model_file=\"model_PID__4491_loss_0.115_vloss_0.679_acc_0.954_vacc_0.880.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False"
      ],
      "metadata": {
        "id": "UIxI3AS6Yw3S"
      },
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=0.7,shuffle=True,random_state=33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "jJfOOTfGfDXi"
      },
      "execution_count": 517,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate_corrector(epoch,lr):\n",
        "    if epoch > 4000:\n",
        "        lr = 0.01\n",
        "        return lr\n",
        "    if epoch > 3000:\n",
        "        lr = 0.05\n",
        "        return lr\n",
        "    if epoch > 2000:\n",
        "        lr = 0.01\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 500:\n",
        "        lr = 0.005\n",
        "        return lr\n",
        "    return lr\n",
        "    "
      ],
      "metadata": {
        "id": "A-Kv8ORiEfub"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.keras import WandbMetricsLogger\n",
        "fname=\"./model_PID_\"\n",
        "callbacks = [\n",
        "        LearningRateScheduler(learning_rate_corrector,verbose=1),\n",
        "        WandbMetricsLogger(),       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch:04.0f}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{accuracy:.3f}_vacc_{val_accuracy:.3f}.hdf5\", monitor='loss',\n",
        "                        verbose=2, save_best_only=True, mode='min')]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__learning__=True"
      ],
      "metadata": {
        "id": "O6ofy0moderd"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z3Z4q14D7eC"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {
        "id": "9Ol0mW6WRlkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f79c28-0ab2-400d-af5a-20393cf582c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA streamkimeneten csak az utolsó 5000 sor látható.\u001b[0m\n",
            "Epoch 4667: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0735 - accuracy: 0.9724 - val_loss: 2.2284 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4668: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4668/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9698\n",
            "Epoch 4668: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0741 - accuracy: 0.9698 - val_loss: 2.2280 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4669: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4669/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9724\n",
            "Epoch 4669: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0692 - accuracy: 0.9724 - val_loss: 2.2276 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4670: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4670/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9718\n",
            "Epoch 4670: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0728 - accuracy: 0.9718 - val_loss: 2.2286 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4671: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4671/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9698\n",
            "Epoch 4671: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0714 - accuracy: 0.9698 - val_loss: 2.2287 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4672: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4672/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9730\n",
            "Epoch 4672: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0693 - accuracy: 0.9730 - val_loss: 2.2332 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4673: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4673/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9698\n",
            "Epoch 4673: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0762 - accuracy: 0.9698 - val_loss: 2.2378 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4674: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4674/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9750\n",
            "Epoch 4674: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0671 - accuracy: 0.9750 - val_loss: 2.2453 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4675: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4675/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9698\n",
            "Epoch 4675: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0722 - accuracy: 0.9698 - val_loss: 2.2482 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4676: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4676/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9730\n",
            "Epoch 4676: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0675 - accuracy: 0.9730 - val_loss: 2.2523 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4677: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4677/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9647\n",
            "Epoch 4677: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0763 - accuracy: 0.9647 - val_loss: 2.2521 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4678: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4678/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9705\n",
            "Epoch 4678: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0766 - accuracy: 0.9705 - val_loss: 2.2470 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4679: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4679/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9705\n",
            "Epoch 4679: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0689 - accuracy: 0.9705 - val_loss: 2.2410 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4680: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4680/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9705\n",
            "Epoch 4680: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0737 - accuracy: 0.9705 - val_loss: 2.2359 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4681: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4681/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9698\n",
            "Epoch 4681: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0766 - accuracy: 0.9698 - val_loss: 2.2307 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4682: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4682/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9718\n",
            "Epoch 4682: loss did not improve from 0.06566\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0684 - accuracy: 0.9718 - val_loss: 2.2293 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4683: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4683/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9775\n",
            "Epoch 4683: loss improved from 0.06566 to 0.06542, saving model to ./model_PID__4683_loss_0.065_vloss_2.232_acc_0.978_vacc_0.882.hdf5\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0654 - accuracy: 0.9775 - val_loss: 2.2316 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4684: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4684/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9673\n",
            "Epoch 4684: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0752 - accuracy: 0.9673 - val_loss: 2.2356 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4685: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4685/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9679\n",
            "Epoch 4685: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0724 - accuracy: 0.9679 - val_loss: 2.2382 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4686: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4686/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9705\n",
            "Epoch 4686: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0717 - accuracy: 0.9705 - val_loss: 2.2396 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4687: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4687/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9692\n",
            "Epoch 4687: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0756 - accuracy: 0.9692 - val_loss: 2.2384 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4688: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4688/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9660\n",
            "Epoch 4688: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0713 - accuracy: 0.9660 - val_loss: 2.2374 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4689: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4689/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9705\n",
            "Epoch 4689: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0783 - accuracy: 0.9705 - val_loss: 2.2359 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4690: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4690/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9724\n",
            "Epoch 4690: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0696 - accuracy: 0.9724 - val_loss: 2.2357 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4691: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4691/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9711\n",
            "Epoch 4691: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0719 - accuracy: 0.9711 - val_loss: 2.2354 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4692: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4692/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9698\n",
            "Epoch 4692: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0760 - accuracy: 0.9698 - val_loss: 2.2354 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4693: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4693/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9685\n",
            "Epoch 4693: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0757 - accuracy: 0.9685 - val_loss: 2.2361 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4694: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4694/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9705\n",
            "Epoch 4694: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0715 - accuracy: 0.9705 - val_loss: 2.2386 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4695: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4695/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9692\n",
            "Epoch 4695: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0759 - accuracy: 0.9692 - val_loss: 2.2442 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4696: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4696/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9705\n",
            "Epoch 4696: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0704 - accuracy: 0.9705 - val_loss: 2.2522 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4697: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4697/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9711\n",
            "Epoch 4697: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0720 - accuracy: 0.9711 - val_loss: 2.2578 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4698: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4698/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9698\n",
            "Epoch 4698: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0748 - accuracy: 0.9698 - val_loss: 2.2595 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4699: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4699/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9679\n",
            "Epoch 4699: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0768 - accuracy: 0.9679 - val_loss: 2.2543 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4700: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4700/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9698\n",
            "Epoch 4700: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0733 - accuracy: 0.9698 - val_loss: 2.2458 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4701: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4701/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9724\n",
            "Epoch 4701: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0707 - accuracy: 0.9724 - val_loss: 2.2393 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4702: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4702/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9698\n",
            "Epoch 4702: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0748 - accuracy: 0.9698 - val_loss: 2.2342 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4703: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4703/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9679\n",
            "Epoch 4703: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0780 - accuracy: 0.9679 - val_loss: 2.2291 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4704: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4704/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9692\n",
            "Epoch 4704: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0775 - accuracy: 0.9692 - val_loss: 2.2264 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4705: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4705/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9718\n",
            "Epoch 4705: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0788 - accuracy: 0.9718 - val_loss: 2.2266 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4706: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4706/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9724\n",
            "Epoch 4706: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0716 - accuracy: 0.9724 - val_loss: 2.2323 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4707: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4707/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9692\n",
            "Epoch 4707: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0704 - accuracy: 0.9692 - val_loss: 2.2387 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4708: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4708/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9679\n",
            "Epoch 4708: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0734 - accuracy: 0.9679 - val_loss: 2.2438 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4709: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4709/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9653\n",
            "Epoch 4709: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0788 - accuracy: 0.9653 - val_loss: 2.2432 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4710: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4710/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9685\n",
            "Epoch 4710: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0885 - accuracy: 0.9685 - val_loss: 2.2389 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4711: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4711/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9692\n",
            "Epoch 4711: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0725 - accuracy: 0.9692 - val_loss: 2.2348 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4712: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4712/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9711\n",
            "Epoch 4712: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0708 - accuracy: 0.9711 - val_loss: 2.2311 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4713: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4713/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9705\n",
            "Epoch 4713: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0709 - accuracy: 0.9705 - val_loss: 2.2306 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4714: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4714/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9711\n",
            "Epoch 4714: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0735 - accuracy: 0.9711 - val_loss: 2.2332 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4715: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4715/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9724\n",
            "Epoch 4715: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0716 - accuracy: 0.9724 - val_loss: 2.2379 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4716: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4716/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9711\n",
            "Epoch 4716: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0757 - accuracy: 0.9711 - val_loss: 2.2436 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4717: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4717/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9705\n",
            "Epoch 4717: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0707 - accuracy: 0.9705 - val_loss: 2.2493 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4718: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4718/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9711\n",
            "Epoch 4718: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0696 - accuracy: 0.9711 - val_loss: 2.2544 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4719: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4719/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9711\n",
            "Epoch 4719: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0699 - accuracy: 0.9711 - val_loss: 2.2578 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4720: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4720/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9737\n",
            "Epoch 4720: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0756 - accuracy: 0.9737 - val_loss: 2.2596 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4721: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4721/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9673\n",
            "Epoch 4721: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0803 - accuracy: 0.9673 - val_loss: 2.2590 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4722: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4722/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9705\n",
            "Epoch 4722: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0776 - accuracy: 0.9705 - val_loss: 2.2542 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4723: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4723/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9685\n",
            "Epoch 4723: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0768 - accuracy: 0.9685 - val_loss: 2.2510 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4724: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4724/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9685\n",
            "Epoch 4724: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0740 - accuracy: 0.9685 - val_loss: 2.2490 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4725: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4725/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9718\n",
            "Epoch 4725: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0761 - accuracy: 0.9718 - val_loss: 2.2501 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4726: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4726/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9724\n",
            "Epoch 4726: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0760 - accuracy: 0.9724 - val_loss: 2.2516 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4727: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4727/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9698\n",
            "Epoch 4727: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0705 - accuracy: 0.9698 - val_loss: 2.2544 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4728: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4728/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9685\n",
            "Epoch 4728: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0758 - accuracy: 0.9685 - val_loss: 2.2569 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4729: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4729/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9660\n",
            "Epoch 4729: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0751 - accuracy: 0.9660 - val_loss: 2.2603 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4730: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4730/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9718\n",
            "Epoch 4730: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0729 - accuracy: 0.9718 - val_loss: 2.2633 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4731: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4731/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9711\n",
            "Epoch 4731: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0713 - accuracy: 0.9711 - val_loss: 2.2635 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4732: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4732/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9705\n",
            "Epoch 4732: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0707 - accuracy: 0.9705 - val_loss: 2.2664 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4733: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4733/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9679\n",
            "Epoch 4733: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0759 - accuracy: 0.9679 - val_loss: 2.2642 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4734: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4734/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9692\n",
            "Epoch 4734: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0810 - accuracy: 0.9692 - val_loss: 2.2586 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4735: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4735/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9724\n",
            "Epoch 4735: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0707 - accuracy: 0.9724 - val_loss: 2.2556 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4736: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4736/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9724\n",
            "Epoch 4736: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0700 - accuracy: 0.9724 - val_loss: 2.2539 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4737: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4737/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9692\n",
            "Epoch 4737: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0715 - accuracy: 0.9692 - val_loss: 2.2543 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4738: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4738/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9705\n",
            "Epoch 4738: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0700 - accuracy: 0.9705 - val_loss: 2.2557 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4739: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4739/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9705\n",
            "Epoch 4739: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0692 - accuracy: 0.9705 - val_loss: 2.2594 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4740: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4740/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9711\n",
            "Epoch 4740: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0732 - accuracy: 0.9711 - val_loss: 2.2648 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4741: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4741/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9692\n",
            "Epoch 4741: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0801 - accuracy: 0.9692 - val_loss: 2.2696 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4742: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4742/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9698\n",
            "Epoch 4742: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0754 - accuracy: 0.9698 - val_loss: 2.2675 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4743: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4743/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9730\n",
            "Epoch 4743: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0749 - accuracy: 0.9730 - val_loss: 2.2631 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4744: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4744/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9711\n",
            "Epoch 4744: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0706 - accuracy: 0.9711 - val_loss: 2.2577 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4745: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4745/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9698\n",
            "Epoch 4745: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0858 - accuracy: 0.9698 - val_loss: 2.2555 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4746: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4746/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9692\n",
            "Epoch 4746: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0712 - accuracy: 0.9692 - val_loss: 2.2565 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4747: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4747/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9737\n",
            "Epoch 4747: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0695 - accuracy: 0.9737 - val_loss: 2.2612 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4748: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4748/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9685\n",
            "Epoch 4748: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0813 - accuracy: 0.9685 - val_loss: 2.2661 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4749: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4749/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9711\n",
            "Epoch 4749: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0749 - accuracy: 0.9711 - val_loss: 2.2692 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4750: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4750/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9711\n",
            "Epoch 4750: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0681 - accuracy: 0.9711 - val_loss: 2.2708 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4751: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4751/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9692\n",
            "Epoch 4751: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0701 - accuracy: 0.9692 - val_loss: 2.2706 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4752: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4752/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9692\n",
            "Epoch 4752: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0672 - accuracy: 0.9692 - val_loss: 2.2707 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4753: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4753/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9698\n",
            "Epoch 4753: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0721 - accuracy: 0.9698 - val_loss: 2.2687 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4754: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4754/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9705\n",
            "Epoch 4754: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0726 - accuracy: 0.9705 - val_loss: 2.2661 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4755: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4755/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9692\n",
            "Epoch 4755: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.0709 - accuracy: 0.9692 - val_loss: 2.2625 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4756: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4756/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9718\n",
            "Epoch 4756: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0719 - accuracy: 0.9718 - val_loss: 2.2591 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4757: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4757/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9763\n",
            "Epoch 4757: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0701 - accuracy: 0.9763 - val_loss: 2.2601 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4758: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4758/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9730\n",
            "Epoch 4758: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0719 - accuracy: 0.9730 - val_loss: 2.2616 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4759: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4759/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9711\n",
            "Epoch 4759: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0733 - accuracy: 0.9711 - val_loss: 2.2636 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4760: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4760/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9685\n",
            "Epoch 4760: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0718 - accuracy: 0.9685 - val_loss: 2.2647 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4761: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4761/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9692\n",
            "Epoch 4761: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0673 - accuracy: 0.9692 - val_loss: 2.2635 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4762: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4762/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9730\n",
            "Epoch 4762: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0685 - accuracy: 0.9730 - val_loss: 2.2634 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4763: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4763/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9724\n",
            "Epoch 4763: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0703 - accuracy: 0.9724 - val_loss: 2.2625 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4764: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4764/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9698\n",
            "Epoch 4764: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0688 - accuracy: 0.9698 - val_loss: 2.2629 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4765: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4765/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9692\n",
            "Epoch 4765: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0676 - accuracy: 0.9692 - val_loss: 2.2635 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4766: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4766/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9705\n",
            "Epoch 4766: loss did not improve from 0.06542\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0753 - accuracy: 0.9705 - val_loss: 2.2651 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4767: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4767/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9730\n",
            "Epoch 4767: loss improved from 0.06542 to 0.06537, saving model to ./model_PID__4767_loss_0.065_vloss_2.268_acc_0.973_vacc_0.883.hdf5\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0654 - accuracy: 0.9730 - val_loss: 2.2681 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4768: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4768/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9692\n",
            "Epoch 4768: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0749 - accuracy: 0.9692 - val_loss: 2.2691 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4769: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4769/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9692\n",
            "Epoch 4769: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0798 - accuracy: 0.9692 - val_loss: 2.2681 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4770: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4770/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9698\n",
            "Epoch 4770: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0818 - accuracy: 0.9698 - val_loss: 2.2665 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4771: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4771/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9705\n",
            "Epoch 4771: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0768 - accuracy: 0.9705 - val_loss: 2.2649 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4772: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4772/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9724\n",
            "Epoch 4772: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0673 - accuracy: 0.9724 - val_loss: 2.2660 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4773: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4773/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9724\n",
            "Epoch 4773: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0673 - accuracy: 0.9724 - val_loss: 2.2705 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4774: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4774/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9685\n",
            "Epoch 4774: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0731 - accuracy: 0.9685 - val_loss: 2.2747 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4775: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4775/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9673\n",
            "Epoch 4775: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0743 - accuracy: 0.9673 - val_loss: 2.2752 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4776: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4776/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9711\n",
            "Epoch 4776: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0719 - accuracy: 0.9711 - val_loss: 2.2718 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4777: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4777/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9737\n",
            "Epoch 4777: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0849 - accuracy: 0.9737 - val_loss: 2.2682 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4778: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4778/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9705\n",
            "Epoch 4778: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0701 - accuracy: 0.9705 - val_loss: 2.2665 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4779: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4779/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9698\n",
            "Epoch 4779: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0849 - accuracy: 0.9698 - val_loss: 2.2657 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4780: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4780/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9685\n",
            "Epoch 4780: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0775 - accuracy: 0.9685 - val_loss: 2.2643 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4781: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4781/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9711\n",
            "Epoch 4781: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0712 - accuracy: 0.9711 - val_loss: 2.2635 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4782: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4782/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9698\n",
            "Epoch 4782: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0760 - accuracy: 0.9698 - val_loss: 2.2634 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4783: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4783/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9634\n",
            "Epoch 4783: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0886 - accuracy: 0.9634 - val_loss: 2.2654 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4784: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4784/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9679\n",
            "Epoch 4784: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0782 - accuracy: 0.9679 - val_loss: 2.2695 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4785: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4785/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9711\n",
            "Epoch 4785: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0781 - accuracy: 0.9711 - val_loss: 2.2725 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4786: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4786/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9692\n",
            "Epoch 4786: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0773 - accuracy: 0.9692 - val_loss: 2.2715 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4787: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4787/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9730\n",
            "Epoch 4787: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0684 - accuracy: 0.9730 - val_loss: 2.2690 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4788: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4788/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9705\n",
            "Epoch 4788: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0701 - accuracy: 0.9705 - val_loss: 2.2678 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4789: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4789/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9679\n",
            "Epoch 4789: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0962 - accuracy: 0.9679 - val_loss: 2.2684 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4790: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4790/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9711\n",
            "Epoch 4790: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0936 - accuracy: 0.9711 - val_loss: 2.2683 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4791: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4791/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9705\n",
            "Epoch 4791: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0719 - accuracy: 0.9705 - val_loss: 2.2687 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4792: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4792/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9685\n",
            "Epoch 4792: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0698 - accuracy: 0.9685 - val_loss: 2.2679 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4793: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4793/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9679\n",
            "Epoch 4793: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0717 - accuracy: 0.9679 - val_loss: 2.2691 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4794: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4794/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9724\n",
            "Epoch 4794: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0682 - accuracy: 0.9724 - val_loss: 2.2699 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4795: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4795/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9705\n",
            "Epoch 4795: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0758 - accuracy: 0.9705 - val_loss: 2.2667 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4796: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4796/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9743\n",
            "Epoch 4796: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0654 - accuracy: 0.9743 - val_loss: 2.2653 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4797: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4797/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9730\n",
            "Epoch 4797: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0706 - accuracy: 0.9730 - val_loss: 2.2663 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4798: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4798/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9730\n",
            "Epoch 4798: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0717 - accuracy: 0.9730 - val_loss: 2.2701 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4799: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4799/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9679\n",
            "Epoch 4799: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0762 - accuracy: 0.9679 - val_loss: 2.2729 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4800: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4800/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9679\n",
            "Epoch 4800: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0736 - accuracy: 0.9679 - val_loss: 2.2740 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4801: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4801/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9705\n",
            "Epoch 4801: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0672 - accuracy: 0.9705 - val_loss: 2.2774 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4802: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4802/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9705\n",
            "Epoch 4802: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0723 - accuracy: 0.9705 - val_loss: 2.2789 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4803: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4803/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9673\n",
            "Epoch 4803: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0725 - accuracy: 0.9673 - val_loss: 2.2790 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4804: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4804/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9685\n",
            "Epoch 4804: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0719 - accuracy: 0.9685 - val_loss: 2.2777 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4805: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4805/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9711\n",
            "Epoch 4805: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0721 - accuracy: 0.9711 - val_loss: 2.2741 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4806: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4806/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9679\n",
            "Epoch 4806: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0732 - accuracy: 0.9679 - val_loss: 2.2693 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4807: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4807/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9711\n",
            "Epoch 4807: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0699 - accuracy: 0.9711 - val_loss: 2.2670 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4808: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4808/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9698\n",
            "Epoch 4808: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0752 - accuracy: 0.9698 - val_loss: 2.2665 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4809: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4809/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9698\n",
            "Epoch 4809: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0720 - accuracy: 0.9698 - val_loss: 2.2690 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4810: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4810/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9730\n",
            "Epoch 4810: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0695 - accuracy: 0.9730 - val_loss: 2.2732 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4811: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4811/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9718\n",
            "Epoch 4811: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0721 - accuracy: 0.9718 - val_loss: 2.2763 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4812: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4812/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9685\n",
            "Epoch 4812: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0771 - accuracy: 0.9685 - val_loss: 2.2823 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4813: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4813/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9705\n",
            "Epoch 4813: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0728 - accuracy: 0.9705 - val_loss: 2.2885 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4814: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4814/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9698\n",
            "Epoch 4814: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0719 - accuracy: 0.9698 - val_loss: 2.2921 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4815: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4815/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9679\n",
            "Epoch 4815: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0785 - accuracy: 0.9679 - val_loss: 2.2931 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4816: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4816/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9718\n",
            "Epoch 4816: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0717 - accuracy: 0.9718 - val_loss: 2.2907 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4817: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4817/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9705\n",
            "Epoch 4817: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0727 - accuracy: 0.9705 - val_loss: 2.2885 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4818: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4818/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9724\n",
            "Epoch 4818: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0708 - accuracy: 0.9724 - val_loss: 2.2874 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4819: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4819/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9685\n",
            "Epoch 4819: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0729 - accuracy: 0.9685 - val_loss: 2.2879 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4820: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4820/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9698\n",
            "Epoch 4820: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0708 - accuracy: 0.9698 - val_loss: 2.2905 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4821: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4821/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9692\n",
            "Epoch 4821: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0753 - accuracy: 0.9692 - val_loss: 2.2936 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4822: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4822/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9730\n",
            "Epoch 4822: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0677 - accuracy: 0.9730 - val_loss: 2.2976 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4823: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4823/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9679\n",
            "Epoch 4823: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0750 - accuracy: 0.9679 - val_loss: 2.2994 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4824: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4824/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9641\n",
            "Epoch 4824: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0876 - accuracy: 0.9641 - val_loss: 2.2984 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4825: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4825/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9698\n",
            "Epoch 4825: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0673 - accuracy: 0.9698 - val_loss: 2.2982 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4826: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4826/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9679\n",
            "Epoch 4826: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0762 - accuracy: 0.9679 - val_loss: 2.2976 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4827: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4827/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9705\n",
            "Epoch 4827: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0703 - accuracy: 0.9705 - val_loss: 2.2977 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4828: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4828/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9711\n",
            "Epoch 4828: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0729 - accuracy: 0.9711 - val_loss: 2.2974 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4829: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4829/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9685\n",
            "Epoch 4829: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0721 - accuracy: 0.9685 - val_loss: 2.2988 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4830: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4830/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9711\n",
            "Epoch 4830: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0708 - accuracy: 0.9711 - val_loss: 2.3009 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4831: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4831/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9692\n",
            "Epoch 4831: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0783 - accuracy: 0.9692 - val_loss: 2.3014 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4832: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4832/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9660\n",
            "Epoch 4832: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0759 - accuracy: 0.9660 - val_loss: 2.3014 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4833: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4833/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9685\n",
            "Epoch 4833: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0826 - accuracy: 0.9685 - val_loss: 2.2987 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4834: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4834/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9705\n",
            "Epoch 4834: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0704 - accuracy: 0.9705 - val_loss: 2.2961 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4835: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4835/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9679\n",
            "Epoch 4835: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0757 - accuracy: 0.9679 - val_loss: 2.2947 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4836: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4836/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9705\n",
            "Epoch 4836: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0730 - accuracy: 0.9705 - val_loss: 2.2936 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4837: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4837/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9711\n",
            "Epoch 4837: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0711 - accuracy: 0.9711 - val_loss: 2.2927 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4838: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4838/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9711\n",
            "Epoch 4838: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0759 - accuracy: 0.9711 - val_loss: 2.2920 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4839: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4839/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9718\n",
            "Epoch 4839: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0736 - accuracy: 0.9718 - val_loss: 2.2940 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4840: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4840/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9685\n",
            "Epoch 4840: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0731 - accuracy: 0.9685 - val_loss: 2.2941 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4841: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4841/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9698\n",
            "Epoch 4841: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0734 - accuracy: 0.9698 - val_loss: 2.2942 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4842: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4842/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9711\n",
            "Epoch 4842: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0707 - accuracy: 0.9711 - val_loss: 2.2950 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4843: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4843/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9724\n",
            "Epoch 4843: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0755 - accuracy: 0.9724 - val_loss: 2.2927 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4844: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4844/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9724\n",
            "Epoch 4844: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0801 - accuracy: 0.9724 - val_loss: 2.2903 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4845: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4845/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9685\n",
            "Epoch 4845: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0723 - accuracy: 0.9685 - val_loss: 2.2859 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4846: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4846/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9711\n",
            "Epoch 4846: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0712 - accuracy: 0.9711 - val_loss: 2.2818 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4847: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4847/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9711\n",
            "Epoch 4847: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0694 - accuracy: 0.9711 - val_loss: 2.2776 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4848: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4848/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9692\n",
            "Epoch 4848: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0809 - accuracy: 0.9692 - val_loss: 2.2742 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4849: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4849/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9685\n",
            "Epoch 4849: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0735 - accuracy: 0.9685 - val_loss: 2.2705 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4850: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4850/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9705\n",
            "Epoch 4850: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0825 - accuracy: 0.9705 - val_loss: 2.2655 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4851: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4851/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9743\n",
            "Epoch 4851: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0681 - accuracy: 0.9743 - val_loss: 2.2629 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4852: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4852/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9692\n",
            "Epoch 4852: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0712 - accuracy: 0.9692 - val_loss: 2.2629 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4853: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4853/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9692\n",
            "Epoch 4853: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0723 - accuracy: 0.9692 - val_loss: 2.2648 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4854: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4854/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9692\n",
            "Epoch 4854: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0740 - accuracy: 0.9692 - val_loss: 2.2670 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4855: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4855/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9698\n",
            "Epoch 4855: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0698 - accuracy: 0.9698 - val_loss: 2.2708 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4856: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4856/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9698\n",
            "Epoch 4856: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0763 - accuracy: 0.9698 - val_loss: 2.2731 - val_accuracy: 0.8847 - lr: 0.0100\n",
            "\n",
            "Epoch 4857: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4857/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9711\n",
            "Epoch 4857: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0661 - accuracy: 0.9711 - val_loss: 2.2741 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4858: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4858/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9692\n",
            "Epoch 4858: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0710 - accuracy: 0.9692 - val_loss: 2.2750 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4859: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4859/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9718\n",
            "Epoch 4859: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0734 - accuracy: 0.9718 - val_loss: 2.2758 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4860: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4860/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9711\n",
            "Epoch 4860: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0749 - accuracy: 0.9711 - val_loss: 2.2785 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4861: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4861/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9698\n",
            "Epoch 4861: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0775 - accuracy: 0.9698 - val_loss: 2.2795 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4862: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4862/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9705\n",
            "Epoch 4862: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0754 - accuracy: 0.9705 - val_loss: 2.2786 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4863: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4863/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9718\n",
            "Epoch 4863: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0749 - accuracy: 0.9718 - val_loss: 2.2822 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4864: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4864/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9724\n",
            "Epoch 4864: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0735 - accuracy: 0.9724 - val_loss: 2.2879 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4865: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4865/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9724\n",
            "Epoch 4865: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0698 - accuracy: 0.9724 - val_loss: 2.2927 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4866: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4866/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9705\n",
            "Epoch 4866: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0695 - accuracy: 0.9705 - val_loss: 2.2945 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4867: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4867/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9724\n",
            "Epoch 4867: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0687 - accuracy: 0.9724 - val_loss: 2.2949 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4868: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4868/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9711\n",
            "Epoch 4868: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0752 - accuracy: 0.9711 - val_loss: 2.2950 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4869: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4869/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9724\n",
            "Epoch 4869: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0715 - accuracy: 0.9724 - val_loss: 2.2941 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4870: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4870/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9698\n",
            "Epoch 4870: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0788 - accuracy: 0.9698 - val_loss: 2.2939 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4871: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4871/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9724\n",
            "Epoch 4871: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0698 - accuracy: 0.9724 - val_loss: 2.2951 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4872: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4872/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9698\n",
            "Epoch 4872: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0724 - accuracy: 0.9698 - val_loss: 2.2967 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4873: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4873/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9724\n",
            "Epoch 4873: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0730 - accuracy: 0.9724 - val_loss: 2.2999 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4874: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4874/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9692\n",
            "Epoch 4874: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0742 - accuracy: 0.9692 - val_loss: 2.3047 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4875: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4875/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9724\n",
            "Epoch 4875: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0687 - accuracy: 0.9724 - val_loss: 2.3100 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4876: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4876/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9692\n",
            "Epoch 4876: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0825 - accuracy: 0.9692 - val_loss: 2.3150 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4877: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4877/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9724\n",
            "Epoch 4877: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0686 - accuracy: 0.9724 - val_loss: 2.3191 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4878: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4878/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9692\n",
            "Epoch 4878: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0684 - accuracy: 0.9692 - val_loss: 2.3211 - val_accuracy: 0.8832 - lr: 0.0100\n",
            "\n",
            "Epoch 4879: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4879/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9724\n",
            "Epoch 4879: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0720 - accuracy: 0.9724 - val_loss: 2.3229 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4880: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4880/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9698\n",
            "Epoch 4880: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0732 - accuracy: 0.9698 - val_loss: 2.3234 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4881: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4881/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9724\n",
            "Epoch 4881: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0696 - accuracy: 0.9724 - val_loss: 2.3239 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4882: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4882/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9698\n",
            "Epoch 4882: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0714 - accuracy: 0.9698 - val_loss: 2.3234 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4883: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4883/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9711\n",
            "Epoch 4883: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0741 - accuracy: 0.9711 - val_loss: 2.3223 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4884: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4884/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9705\n",
            "Epoch 4884: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0755 - accuracy: 0.9705 - val_loss: 2.3276 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4885: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4885/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9711\n",
            "Epoch 4885: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0703 - accuracy: 0.9711 - val_loss: 2.3334 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4886: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4886/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9705\n",
            "Epoch 4886: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0740 - accuracy: 0.9705 - val_loss: 2.3402 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4887: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4887/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9692\n",
            "Epoch 4887: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0708 - accuracy: 0.9692 - val_loss: 2.3475 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4888: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4888/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9685\n",
            "Epoch 4888: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0840 - accuracy: 0.9685 - val_loss: 2.3552 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4889: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4889/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9660\n",
            "Epoch 4889: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0763 - accuracy: 0.9660 - val_loss: 2.3606 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4890: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4890/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9685\n",
            "Epoch 4890: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0755 - accuracy: 0.9685 - val_loss: 2.3639 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4891: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4891/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9698\n",
            "Epoch 4891: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0665 - accuracy: 0.9698 - val_loss: 2.3674 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4892: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4892/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9724\n",
            "Epoch 4892: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0729 - accuracy: 0.9724 - val_loss: 2.3699 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4893: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4893/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9711\n",
            "Epoch 4893: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0714 - accuracy: 0.9711 - val_loss: 2.3704 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4894: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4894/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9737\n",
            "Epoch 4894: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0696 - accuracy: 0.9737 - val_loss: 2.3734 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4895: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4895/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9685\n",
            "Epoch 4895: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0740 - accuracy: 0.9685 - val_loss: 2.3765 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4896: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4896/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9724\n",
            "Epoch 4896: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0669 - accuracy: 0.9724 - val_loss: 2.3810 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4897: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4897/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9730\n",
            "Epoch 4897: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0706 - accuracy: 0.9730 - val_loss: 2.3842 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4898: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4898/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9705\n",
            "Epoch 4898: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0730 - accuracy: 0.9705 - val_loss: 2.3888 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4899: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4899/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9724\n",
            "Epoch 4899: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0680 - accuracy: 0.9724 - val_loss: 2.3925 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4900: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4900/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9730\n",
            "Epoch 4900: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0681 - accuracy: 0.9730 - val_loss: 2.3944 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4901: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4901/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9730\n",
            "Epoch 4901: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0693 - accuracy: 0.9730 - val_loss: 2.3957 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4902: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4902/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9724\n",
            "Epoch 4902: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0720 - accuracy: 0.9724 - val_loss: 2.3947 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4903: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4903/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9692\n",
            "Epoch 4903: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0815 - accuracy: 0.9692 - val_loss: 2.3897 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 4904: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4904/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9724\n",
            "Epoch 4904: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0705 - accuracy: 0.9724 - val_loss: 2.3843 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4905: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4905/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9724\n",
            "Epoch 4905: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0675 - accuracy: 0.9724 - val_loss: 2.3820 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4906: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4906/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9718\n",
            "Epoch 4906: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0731 - accuracy: 0.9718 - val_loss: 2.3822 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4907: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4907/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9730\n",
            "Epoch 4907: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0690 - accuracy: 0.9730 - val_loss: 2.3869 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4908: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4908/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9724\n",
            "Epoch 4908: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0695 - accuracy: 0.9724 - val_loss: 2.3940 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4909: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4909/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9724\n",
            "Epoch 4909: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0676 - accuracy: 0.9724 - val_loss: 2.4015 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4910: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4910/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9679\n",
            "Epoch 4910: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0709 - accuracy: 0.9679 - val_loss: 2.4073 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4911: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4911/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9711\n",
            "Epoch 4911: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0799 - accuracy: 0.9711 - val_loss: 2.4098 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4912: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4912/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9692\n",
            "Epoch 4912: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0714 - accuracy: 0.9692 - val_loss: 2.4087 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4913: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4913/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9724\n",
            "Epoch 4913: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0789 - accuracy: 0.9724 - val_loss: 2.4054 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4914: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4914/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9698\n",
            "Epoch 4914: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0779 - accuracy: 0.9698 - val_loss: 2.4013 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 4915: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4915/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9705\n",
            "Epoch 4915: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0750 - accuracy: 0.9705 - val_loss: 2.3993 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 4916: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4916/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9756\n",
            "Epoch 4916: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0705 - accuracy: 0.9756 - val_loss: 2.4010 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 4917: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4917/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9692\n",
            "Epoch 4917: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0793 - accuracy: 0.9692 - val_loss: 2.4005 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4918: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4918/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9724\n",
            "Epoch 4918: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0841 - accuracy: 0.9724 - val_loss: 2.4005 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4919: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4919/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9724\n",
            "Epoch 4919: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0710 - accuracy: 0.9724 - val_loss: 2.4013 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4920: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4920/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9711\n",
            "Epoch 4920: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0777 - accuracy: 0.9711 - val_loss: 2.4025 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4921: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4921/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9711\n",
            "Epoch 4921: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0704 - accuracy: 0.9711 - val_loss: 2.4019 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4922: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4922/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9692\n",
            "Epoch 4922: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0852 - accuracy: 0.9692 - val_loss: 2.4000 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4923: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4923/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9737\n",
            "Epoch 4923: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0665 - accuracy: 0.9737 - val_loss: 2.3984 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4924: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4924/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9705\n",
            "Epoch 4924: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0697 - accuracy: 0.9705 - val_loss: 2.3941 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4925: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4925/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9685\n",
            "Epoch 4925: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0728 - accuracy: 0.9685 - val_loss: 2.3889 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4926: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4926/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9692\n",
            "Epoch 4926: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0745 - accuracy: 0.9692 - val_loss: 2.3845 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4927: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4927/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9718\n",
            "Epoch 4927: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0745 - accuracy: 0.9718 - val_loss: 2.3813 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4928: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4928/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9679\n",
            "Epoch 4928: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0811 - accuracy: 0.9679 - val_loss: 2.3807 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4929: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4929/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9705\n",
            "Epoch 4929: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0836 - accuracy: 0.9705 - val_loss: 2.3812 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4930: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4930/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9692\n",
            "Epoch 4930: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0791 - accuracy: 0.9692 - val_loss: 2.3826 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4931: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4931/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9666\n",
            "Epoch 4931: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0751 - accuracy: 0.9666 - val_loss: 2.3840 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4932: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4932/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9698\n",
            "Epoch 4932: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0708 - accuracy: 0.9698 - val_loss: 2.3853 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4933: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4933/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9711\n",
            "Epoch 4933: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0687 - accuracy: 0.9711 - val_loss: 2.3854 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4934: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4934/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9718\n",
            "Epoch 4934: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0714 - accuracy: 0.9718 - val_loss: 2.3862 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4935: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4935/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9705\n",
            "Epoch 4935: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0797 - accuracy: 0.9705 - val_loss: 2.3857 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4936: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4936/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9711\n",
            "Epoch 4936: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0699 - accuracy: 0.9711 - val_loss: 2.3858 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4937: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4937/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9679\n",
            "Epoch 4937: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0751 - accuracy: 0.9679 - val_loss: 2.3855 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4938: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4938/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9724\n",
            "Epoch 4938: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0711 - accuracy: 0.9724 - val_loss: 2.3867 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4939: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4939/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9718\n",
            "Epoch 4939: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0701 - accuracy: 0.9718 - val_loss: 2.3889 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 4940: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4940/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9724\n",
            "Epoch 4940: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0711 - accuracy: 0.9724 - val_loss: 2.3927 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 4941: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4941/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9685\n",
            "Epoch 4941: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0691 - accuracy: 0.9685 - val_loss: 2.3993 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4942: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4942/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9730\n",
            "Epoch 4942: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0775 - accuracy: 0.9730 - val_loss: 2.4070 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4943: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4943/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9718\n",
            "Epoch 4943: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0715 - accuracy: 0.9718 - val_loss: 2.4120 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4944: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4944/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9705\n",
            "Epoch 4944: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0735 - accuracy: 0.9705 - val_loss: 2.4120 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4945: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4945/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9711\n",
            "Epoch 4945: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0738 - accuracy: 0.9711 - val_loss: 2.4056 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4946: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4946/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
            "Epoch 4946: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 2.3977 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4947: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4947/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9685\n",
            "Epoch 4947: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0755 - accuracy: 0.9685 - val_loss: 2.3896 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4948: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4948/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9698\n",
            "Epoch 4948: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0830 - accuracy: 0.9698 - val_loss: 2.3798 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4949: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4949/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9711\n",
            "Epoch 4949: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0724 - accuracy: 0.9711 - val_loss: 2.3723 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4950: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4950/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9743\n",
            "Epoch 4950: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0699 - accuracy: 0.9743 - val_loss: 2.3691 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4951: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4951/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9730\n",
            "Epoch 4951: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0715 - accuracy: 0.9730 - val_loss: 2.3696 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4952: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4952/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9685\n",
            "Epoch 4952: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0768 - accuracy: 0.9685 - val_loss: 2.3728 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4953: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4953/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9711\n",
            "Epoch 4953: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0738 - accuracy: 0.9711 - val_loss: 2.3774 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4954: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4954/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9673\n",
            "Epoch 4954: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0707 - accuracy: 0.9673 - val_loss: 2.3809 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4955: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4955/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9666\n",
            "Epoch 4955: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0745 - accuracy: 0.9666 - val_loss: 2.3823 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4956: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4956/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9692\n",
            "Epoch 4956: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0684 - accuracy: 0.9692 - val_loss: 2.3828 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4957: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4957/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9698\n",
            "Epoch 4957: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0725 - accuracy: 0.9698 - val_loss: 2.3811 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4958: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4958/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9705\n",
            "Epoch 4958: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0746 - accuracy: 0.9705 - val_loss: 2.3784 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4959: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4959/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9698\n",
            "Epoch 4959: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0809 - accuracy: 0.9698 - val_loss: 2.3746 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4960: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4960/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9705\n",
            "Epoch 4960: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0720 - accuracy: 0.9705 - val_loss: 2.3727 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4961: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4961/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9730\n",
            "Epoch 4961: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0728 - accuracy: 0.9730 - val_loss: 2.3704 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4962: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4962/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9718\n",
            "Epoch 4962: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0714 - accuracy: 0.9718 - val_loss: 2.3709 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4963: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4963/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9750\n",
            "Epoch 4963: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0682 - accuracy: 0.9750 - val_loss: 2.3733 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4964: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4964/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9685\n",
            "Epoch 4964: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0748 - accuracy: 0.9685 - val_loss: 2.3750 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4965: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4965/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9730\n",
            "Epoch 4965: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0678 - accuracy: 0.9730 - val_loss: 2.3749 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4966: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4966/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9724\n",
            "Epoch 4966: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0701 - accuracy: 0.9724 - val_loss: 2.3723 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4967: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4967/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9705\n",
            "Epoch 4967: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0752 - accuracy: 0.9705 - val_loss: 2.3701 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4968: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4968/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9698\n",
            "Epoch 4968: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0711 - accuracy: 0.9698 - val_loss: 2.3676 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4969: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4969/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9718\n",
            "Epoch 4969: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0720 - accuracy: 0.9718 - val_loss: 2.3659 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4970: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4970/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9698\n",
            "Epoch 4970: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0697 - accuracy: 0.9698 - val_loss: 2.3649 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4971: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4971/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9737\n",
            "Epoch 4971: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0686 - accuracy: 0.9737 - val_loss: 2.3653 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4972: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4972/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9705\n",
            "Epoch 4972: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0694 - accuracy: 0.9705 - val_loss: 2.3664 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4973: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4973/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9698\n",
            "Epoch 4973: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0870 - accuracy: 0.9698 - val_loss: 2.3640 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4974: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4974/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9724\n",
            "Epoch 4974: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0699 - accuracy: 0.9724 - val_loss: 2.3629 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4975: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4975/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9718\n",
            "Epoch 4975: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0702 - accuracy: 0.9718 - val_loss: 2.3634 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4976: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4976/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9705\n",
            "Epoch 4976: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0722 - accuracy: 0.9705 - val_loss: 2.3650 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4977: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4977/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9698\n",
            "Epoch 4977: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0728 - accuracy: 0.9698 - val_loss: 2.3666 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4978: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4978/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9679\n",
            "Epoch 4978: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0757 - accuracy: 0.9679 - val_loss: 2.3699 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4979: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4979/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9724\n",
            "Epoch 4979: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0716 - accuracy: 0.9724 - val_loss: 2.3727 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 4980: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4980/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9685\n",
            "Epoch 4980: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0715 - accuracy: 0.9685 - val_loss: 2.3740 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4981: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4981/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9692\n",
            "Epoch 4981: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0781 - accuracy: 0.9692 - val_loss: 2.3747 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 4982: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4982/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9730\n",
            "Epoch 4982: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0703 - accuracy: 0.9730 - val_loss: 2.3752 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4983: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4983/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9692\n",
            "Epoch 4983: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0719 - accuracy: 0.9692 - val_loss: 2.3749 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4984: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4984/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9679\n",
            "Epoch 4984: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0786 - accuracy: 0.9679 - val_loss: 2.3704 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4985: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4985/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9724\n",
            "Epoch 4985: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0706 - accuracy: 0.9724 - val_loss: 2.3692 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4986: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4986/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9692\n",
            "Epoch 4986: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0788 - accuracy: 0.9692 - val_loss: 2.3654 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 4987: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4987/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9705\n",
            "Epoch 4987: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0682 - accuracy: 0.9705 - val_loss: 2.3621 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 4988: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4988/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9685\n",
            "Epoch 4988: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0748 - accuracy: 0.9685 - val_loss: 2.3606 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4989: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4989/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9724\n",
            "Epoch 4989: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0706 - accuracy: 0.9724 - val_loss: 2.3607 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4990: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4990/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9692\n",
            "Epoch 4990: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0753 - accuracy: 0.9692 - val_loss: 2.3638 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4991: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4991/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9692\n",
            "Epoch 4991: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0750 - accuracy: 0.9692 - val_loss: 2.3681 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4992: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4992/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9692\n",
            "Epoch 4992: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0710 - accuracy: 0.9692 - val_loss: 2.3731 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4993: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4993/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9685\n",
            "Epoch 4993: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0808 - accuracy: 0.9685 - val_loss: 2.3763 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4994: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4994/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9679\n",
            "Epoch 4994: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0807 - accuracy: 0.9679 - val_loss: 2.3769 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4995: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4995/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9692\n",
            "Epoch 4995: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0693 - accuracy: 0.9692 - val_loss: 2.3760 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 4996: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4996/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9718\n",
            "Epoch 4996: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0700 - accuracy: 0.9718 - val_loss: 2.3713 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 4997: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4997/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9698\n",
            "Epoch 4997: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0757 - accuracy: 0.9698 - val_loss: 2.3663 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4998: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4998/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9666\n",
            "Epoch 4998: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0779 - accuracy: 0.9666 - val_loss: 2.3581 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 4999: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4999/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9724\n",
            "Epoch 4999: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0693 - accuracy: 0.9724 - val_loss: 2.3540 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5000: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5000/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9711\n",
            "Epoch 5000: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0726 - accuracy: 0.9711 - val_loss: 2.3558 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5001: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5001/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9724\n",
            "Epoch 5001: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0740 - accuracy: 0.9724 - val_loss: 2.3607 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5002: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5002/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9666\n",
            "Epoch 5002: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0820 - accuracy: 0.9666 - val_loss: 2.3652 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5003: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5003/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9705\n",
            "Epoch 5003: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0745 - accuracy: 0.9705 - val_loss: 2.3659 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5004: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5004/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9724\n",
            "Epoch 5004: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0713 - accuracy: 0.9724 - val_loss: 2.3642 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5005: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5005/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9737\n",
            "Epoch 5005: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0703 - accuracy: 0.9737 - val_loss: 2.3592 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5006: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5006/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9718\n",
            "Epoch 5006: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0712 - accuracy: 0.9718 - val_loss: 2.3533 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5007: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5007/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9718\n",
            "Epoch 5007: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0695 - accuracy: 0.9718 - val_loss: 2.3486 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5008: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5008/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9724\n",
            "Epoch 5008: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0691 - accuracy: 0.9724 - val_loss: 2.3474 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5009: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5009/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9679\n",
            "Epoch 5009: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0750 - accuracy: 0.9679 - val_loss: 2.3482 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5010: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5010/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9724\n",
            "Epoch 5010: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0901 - accuracy: 0.9724 - val_loss: 2.3504 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5011: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5011/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9660\n",
            "Epoch 5011: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0721 - accuracy: 0.9660 - val_loss: 2.3544 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5012: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5012/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9692\n",
            "Epoch 5012: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0804 - accuracy: 0.9692 - val_loss: 2.3578 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5013: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5013/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9673\n",
            "Epoch 5013: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0732 - accuracy: 0.9673 - val_loss: 2.3553 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5014: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5014/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9685\n",
            "Epoch 5014: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0790 - accuracy: 0.9685 - val_loss: 2.3528 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5015: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5015/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9718\n",
            "Epoch 5015: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0688 - accuracy: 0.9718 - val_loss: 2.3525 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5016: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5016/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9685\n",
            "Epoch 5016: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0758 - accuracy: 0.9685 - val_loss: 2.3515 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5017: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5017/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9718\n",
            "Epoch 5017: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0741 - accuracy: 0.9718 - val_loss: 2.3507 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5018: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5018/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9711\n",
            "Epoch 5018: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0708 - accuracy: 0.9711 - val_loss: 2.3489 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5019: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5019/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9718\n",
            "Epoch 5019: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0713 - accuracy: 0.9718 - val_loss: 2.3489 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5020: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5020/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9705\n",
            "Epoch 5020: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0716 - accuracy: 0.9705 - val_loss: 2.3521 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5021: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5021/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9685\n",
            "Epoch 5021: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0689 - accuracy: 0.9685 - val_loss: 2.3551 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5022: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5022/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9666\n",
            "Epoch 5022: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0762 - accuracy: 0.9666 - val_loss: 2.3565 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5023: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5023/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9711\n",
            "Epoch 5023: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0719 - accuracy: 0.9711 - val_loss: 2.3548 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5024: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5024/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9705\n",
            "Epoch 5024: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0712 - accuracy: 0.9705 - val_loss: 2.3534 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5025: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5025/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9685\n",
            "Epoch 5025: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0804 - accuracy: 0.9685 - val_loss: 2.3491 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5026: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5026/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9698\n",
            "Epoch 5026: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0688 - accuracy: 0.9698 - val_loss: 2.3438 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5027: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5027/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9705\n",
            "Epoch 5027: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0734 - accuracy: 0.9705 - val_loss: 2.3396 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5028: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5028/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9653\n",
            "Epoch 5028: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0739 - accuracy: 0.9653 - val_loss: 2.3396 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5029: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5029/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9730\n",
            "Epoch 5029: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0687 - accuracy: 0.9730 - val_loss: 2.3396 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5030: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5030/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9698\n",
            "Epoch 5030: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0767 - accuracy: 0.9698 - val_loss: 2.3433 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5031: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5031/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9730\n",
            "Epoch 5031: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0699 - accuracy: 0.9730 - val_loss: 2.3488 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5032: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5032/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9698\n",
            "Epoch 5032: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0730 - accuracy: 0.9698 - val_loss: 2.3535 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5033: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5033/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9692\n",
            "Epoch 5033: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0842 - accuracy: 0.9692 - val_loss: 2.3544 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5034: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5034/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9692\n",
            "Epoch 5034: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 2.3550 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5035: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5035/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9730\n",
            "Epoch 5035: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0787 - accuracy: 0.9730 - val_loss: 2.3556 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5036: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5036/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9698\n",
            "Epoch 5036: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0695 - accuracy: 0.9698 - val_loss: 2.3586 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5037: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5037/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9724\n",
            "Epoch 5037: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0683 - accuracy: 0.9724 - val_loss: 2.3630 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5038: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5038/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9711\n",
            "Epoch 5038: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0686 - accuracy: 0.9711 - val_loss: 2.3701 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5039: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5039/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9685\n",
            "Epoch 5039: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0780 - accuracy: 0.9685 - val_loss: 2.3782 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5040: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5040/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9698\n",
            "Epoch 5040: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0732 - accuracy: 0.9698 - val_loss: 2.3824 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5041: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5041/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9692\n",
            "Epoch 5041: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0721 - accuracy: 0.9692 - val_loss: 2.3828 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5042: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5042/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9685\n",
            "Epoch 5042: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0705 - accuracy: 0.9685 - val_loss: 2.3818 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5043: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5043/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9685\n",
            "Epoch 5043: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0743 - accuracy: 0.9685 - val_loss: 2.3805 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5044: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5044/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9698\n",
            "Epoch 5044: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0779 - accuracy: 0.9698 - val_loss: 2.3795 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5045: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5045/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9705\n",
            "Epoch 5045: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0742 - accuracy: 0.9705 - val_loss: 2.3793 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5046: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5046/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9698\n",
            "Epoch 5046: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0717 - accuracy: 0.9698 - val_loss: 2.3798 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5047: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5047/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9685\n",
            "Epoch 5047: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0768 - accuracy: 0.9685 - val_loss: 2.3807 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5048: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5048/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9718\n",
            "Epoch 5048: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0689 - accuracy: 0.9718 - val_loss: 2.3815 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5049: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5049/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9685\n",
            "Epoch 5049: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0730 - accuracy: 0.9685 - val_loss: 2.3836 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5050: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5050/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9711\n",
            "Epoch 5050: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0726 - accuracy: 0.9711 - val_loss: 2.3852 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5051: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5051/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9660\n",
            "Epoch 5051: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0784 - accuracy: 0.9660 - val_loss: 2.3868 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5052: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5052/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9685\n",
            "Epoch 5052: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0753 - accuracy: 0.9685 - val_loss: 2.3836 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5053: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5053/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9750\n",
            "Epoch 5053: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0689 - accuracy: 0.9750 - val_loss: 2.3820 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5054: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5054/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9737\n",
            "Epoch 5054: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0711 - accuracy: 0.9737 - val_loss: 2.3827 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5055: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5055/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9705\n",
            "Epoch 5055: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0785 - accuracy: 0.9705 - val_loss: 2.3837 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5056: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5056/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9724\n",
            "Epoch 5056: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0679 - accuracy: 0.9724 - val_loss: 2.3855 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5057: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5057/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9685\n",
            "Epoch 5057: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0872 - accuracy: 0.9685 - val_loss: 2.3855 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5058: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5058/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9718\n",
            "Epoch 5058: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0693 - accuracy: 0.9718 - val_loss: 2.3831 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5059: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5059/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9698\n",
            "Epoch 5059: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0731 - accuracy: 0.9698 - val_loss: 2.3800 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5060: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5060/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9692\n",
            "Epoch 5060: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0790 - accuracy: 0.9692 - val_loss: 2.3751 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5061: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5061/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9698\n",
            "Epoch 5061: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0666 - accuracy: 0.9698 - val_loss: 2.3717 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5062: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5062/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9679\n",
            "Epoch 5062: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0727 - accuracy: 0.9679 - val_loss: 2.3711 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5063: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5063/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9698\n",
            "Epoch 5063: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0703 - accuracy: 0.9698 - val_loss: 2.3708 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5064: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5064/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9673\n",
            "Epoch 5064: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0684 - accuracy: 0.9673 - val_loss: 2.3760 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5065: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5065/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9705\n",
            "Epoch 5065: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0696 - accuracy: 0.9705 - val_loss: 2.3823 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5066: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5066/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9711\n",
            "Epoch 5066: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0690 - accuracy: 0.9711 - val_loss: 2.3890 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5067: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5067/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9679\n",
            "Epoch 5067: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0759 - accuracy: 0.9679 - val_loss: 2.3956 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5068: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5068/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9698\n",
            "Epoch 5068: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0730 - accuracy: 0.9698 - val_loss: 2.3982 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5069: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5069/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9685\n",
            "Epoch 5069: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0746 - accuracy: 0.9685 - val_loss: 2.3966 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5070: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5070/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9711\n",
            "Epoch 5070: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0675 - accuracy: 0.9711 - val_loss: 2.3957 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5071: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5071/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9647\n",
            "Epoch 5071: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0849 - accuracy: 0.9647 - val_loss: 2.3926 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5072: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5072/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9685\n",
            "Epoch 5072: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0739 - accuracy: 0.9685 - val_loss: 2.3888 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5073: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5073/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9730\n",
            "Epoch 5073: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0702 - accuracy: 0.9730 - val_loss: 2.3866 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5074: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5074/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9705\n",
            "Epoch 5074: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0688 - accuracy: 0.9705 - val_loss: 2.3849 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5075: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5075/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9679\n",
            "Epoch 5075: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.0805 - accuracy: 0.9679 - val_loss: 2.3858 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5076: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5076/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9673\n",
            "Epoch 5076: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0750 - accuracy: 0.9673 - val_loss: 2.3853 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5077: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5077/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9692\n",
            "Epoch 5077: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0710 - accuracy: 0.9692 - val_loss: 2.3848 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5078: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5078/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9698\n",
            "Epoch 5078: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0700 - accuracy: 0.9698 - val_loss: 2.3849 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5079: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5079/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9685\n",
            "Epoch 5079: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0750 - accuracy: 0.9685 - val_loss: 2.3874 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5080: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5080/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9692\n",
            "Epoch 5080: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0719 - accuracy: 0.9692 - val_loss: 2.3893 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5081: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5081/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9718\n",
            "Epoch 5081: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0769 - accuracy: 0.9718 - val_loss: 2.3910 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5082: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5082/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9692\n",
            "Epoch 5082: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0690 - accuracy: 0.9692 - val_loss: 2.3933 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5083: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5083/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9718\n",
            "Epoch 5083: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0703 - accuracy: 0.9718 - val_loss: 2.3946 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5084: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5084/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9692\n",
            "Epoch 5084: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0959 - accuracy: 0.9692 - val_loss: 2.3953 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5085: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5085/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9692\n",
            "Epoch 5085: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0741 - accuracy: 0.9692 - val_loss: 2.3938 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5086: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5086/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9724\n",
            "Epoch 5086: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0673 - accuracy: 0.9724 - val_loss: 2.3915 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5087: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5087/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9711\n",
            "Epoch 5087: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0741 - accuracy: 0.9711 - val_loss: 2.3892 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5088: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5088/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9666\n",
            "Epoch 5088: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0768 - accuracy: 0.9666 - val_loss: 2.3871 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5089: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5089/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9685\n",
            "Epoch 5089: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0713 - accuracy: 0.9685 - val_loss: 2.3858 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5090: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5090/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9685\n",
            "Epoch 5090: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0719 - accuracy: 0.9685 - val_loss: 2.3852 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5091: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5091/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9698\n",
            "Epoch 5091: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0707 - accuracy: 0.9698 - val_loss: 2.3843 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5092: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5092/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9705\n",
            "Epoch 5092: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0725 - accuracy: 0.9705 - val_loss: 2.3823 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5093: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5093/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9698\n",
            "Epoch 5093: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0720 - accuracy: 0.9698 - val_loss: 2.3825 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5094: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5094/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9673\n",
            "Epoch 5094: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0692 - accuracy: 0.9673 - val_loss: 2.3848 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5095: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5095/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9711\n",
            "Epoch 5095: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0689 - accuracy: 0.9711 - val_loss: 2.3885 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5096: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5096/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9705\n",
            "Epoch 5096: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0801 - accuracy: 0.9705 - val_loss: 2.3914 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5097: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5097/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9698\n",
            "Epoch 5097: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0722 - accuracy: 0.9698 - val_loss: 2.3945 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5098: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5098/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9724\n",
            "Epoch 5098: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0850 - accuracy: 0.9724 - val_loss: 2.3945 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5099: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5099/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9692\n",
            "Epoch 5099: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0844 - accuracy: 0.9692 - val_loss: 2.3888 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5100: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5100/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9718\n",
            "Epoch 5100: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0705 - accuracy: 0.9718 - val_loss: 2.3825 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5101: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5101/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9692\n",
            "Epoch 5101: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0727 - accuracy: 0.9692 - val_loss: 2.3785 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5102: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5102/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9666\n",
            "Epoch 5102: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0792 - accuracy: 0.9666 - val_loss: 2.3767 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5103: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5103/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9711\n",
            "Epoch 5103: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0687 - accuracy: 0.9711 - val_loss: 2.3768 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5104: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5104/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9692\n",
            "Epoch 5104: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0710 - accuracy: 0.9692 - val_loss: 2.3791 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5105: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5105/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9718\n",
            "Epoch 5105: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0734 - accuracy: 0.9718 - val_loss: 2.3831 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5106: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5106/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9685\n",
            "Epoch 5106: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0714 - accuracy: 0.9685 - val_loss: 2.3876 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5107: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5107/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9685\n",
            "Epoch 5107: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0729 - accuracy: 0.9685 - val_loss: 2.3904 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5108: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5108/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9698\n",
            "Epoch 5108: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0734 - accuracy: 0.9698 - val_loss: 2.3919 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5109: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5109/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9730\n",
            "Epoch 5109: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0668 - accuracy: 0.9730 - val_loss: 2.3918 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5110: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5110/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9685\n",
            "Epoch 5110: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0718 - accuracy: 0.9685 - val_loss: 2.3922 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5111: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5111/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9692\n",
            "Epoch 5111: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0680 - accuracy: 0.9692 - val_loss: 2.3922 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5112: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5112/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9679\n",
            "Epoch 5112: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0788 - accuracy: 0.9679 - val_loss: 2.3892 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5113: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5113/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9718\n",
            "Epoch 5113: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0684 - accuracy: 0.9718 - val_loss: 2.3875 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5114: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5114/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9685\n",
            "Epoch 5114: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0743 - accuracy: 0.9685 - val_loss: 2.3857 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5115: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5115/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9718\n",
            "Epoch 5115: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0811 - accuracy: 0.9718 - val_loss: 2.3853 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5116: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5116/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9711\n",
            "Epoch 5116: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0850 - accuracy: 0.9711 - val_loss: 2.3874 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5117: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5117/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9692\n",
            "Epoch 5117: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 2.3873 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5118: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5118/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9692\n",
            "Epoch 5118: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0886 - accuracy: 0.9692 - val_loss: 2.3890 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5119: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5119/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9730\n",
            "Epoch 5119: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0691 - accuracy: 0.9730 - val_loss: 2.3905 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5120: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5120/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9711\n",
            "Epoch 5120: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0738 - accuracy: 0.9711 - val_loss: 2.3903 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5121: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5121/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9711\n",
            "Epoch 5121: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0897 - accuracy: 0.9711 - val_loss: 2.3877 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5122: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5122/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9724\n",
            "Epoch 5122: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0687 - accuracy: 0.9724 - val_loss: 2.3873 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5123: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5123/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9724\n",
            "Epoch 5123: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0803 - accuracy: 0.9724 - val_loss: 2.3886 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5124: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5124/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9692\n",
            "Epoch 5124: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0714 - accuracy: 0.9692 - val_loss: 2.3883 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5125: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5125/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9711\n",
            "Epoch 5125: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0732 - accuracy: 0.9711 - val_loss: 2.3877 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5126: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5126/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9730\n",
            "Epoch 5126: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0673 - accuracy: 0.9730 - val_loss: 2.3878 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5127: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5127/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9724\n",
            "Epoch 5127: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0740 - accuracy: 0.9724 - val_loss: 2.3903 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5128: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5128/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9705\n",
            "Epoch 5128: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0682 - accuracy: 0.9705 - val_loss: 2.3937 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5129: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5129/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9660\n",
            "Epoch 5129: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0738 - accuracy: 0.9660 - val_loss: 2.3971 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5130: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5130/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9730\n",
            "Epoch 5130: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0707 - accuracy: 0.9730 - val_loss: 2.3974 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5131: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5131/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9711\n",
            "Epoch 5131: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0741 - accuracy: 0.9711 - val_loss: 2.3985 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5132: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5132/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9660\n",
            "Epoch 5132: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0773 - accuracy: 0.9660 - val_loss: 2.3984 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5133: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5133/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9660\n",
            "Epoch 5133: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0781 - accuracy: 0.9660 - val_loss: 2.3969 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5134: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5134/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9730\n",
            "Epoch 5134: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0683 - accuracy: 0.9730 - val_loss: 2.3978 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5135: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5135/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9705\n",
            "Epoch 5135: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0678 - accuracy: 0.9705 - val_loss: 2.4013 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5136: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5136/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9698\n",
            "Epoch 5136: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0986 - accuracy: 0.9698 - val_loss: 2.4018 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5137: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5137/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9679\n",
            "Epoch 5137: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0731 - accuracy: 0.9679 - val_loss: 2.4003 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5138: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5138/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9698\n",
            "Epoch 5138: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0730 - accuracy: 0.9698 - val_loss: 2.3962 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5139: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5139/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9692\n",
            "Epoch 5139: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0866 - accuracy: 0.9692 - val_loss: 2.3900 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5140: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5140/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9724\n",
            "Epoch 5140: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0694 - accuracy: 0.9724 - val_loss: 2.3845 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5141: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5141/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9685\n",
            "Epoch 5141: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0770 - accuracy: 0.9685 - val_loss: 2.3777 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5142: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5142/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9705\n",
            "Epoch 5142: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0738 - accuracy: 0.9705 - val_loss: 2.3736 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5143: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5143/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9660\n",
            "Epoch 5143: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0718 - accuracy: 0.9660 - val_loss: 2.3718 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5144: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5144/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9673\n",
            "Epoch 5144: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0727 - accuracy: 0.9673 - val_loss: 2.3723 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5145: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5145/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9698\n",
            "Epoch 5145: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0698 - accuracy: 0.9698 - val_loss: 2.3763 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5146: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5146/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9679\n",
            "Epoch 5146: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0721 - accuracy: 0.9679 - val_loss: 2.3800 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5147: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5147/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9705\n",
            "Epoch 5147: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0724 - accuracy: 0.9705 - val_loss: 2.3846 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5148: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5148/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9705\n",
            "Epoch 5148: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0678 - accuracy: 0.9705 - val_loss: 2.3884 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5149: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5149/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9711\n",
            "Epoch 5149: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0719 - accuracy: 0.9711 - val_loss: 2.3910 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5150: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5150/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9711\n",
            "Epoch 5150: loss did not improve from 0.06537\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0696 - accuracy: 0.9711 - val_loss: 2.3911 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5151: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5151/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9750\n",
            "Epoch 5151: loss improved from 0.06537 to 0.06451, saving model to ./model_PID__5151_loss_0.065_vloss_2.392_acc_0.975_vacc_0.879.hdf5\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.0645 - accuracy: 0.9750 - val_loss: 2.3916 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5152: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5152/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9698\n",
            "Epoch 5152: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0700 - accuracy: 0.9698 - val_loss: 2.3899 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5153: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5153/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9698\n",
            "Epoch 5153: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0860 - accuracy: 0.9698 - val_loss: 2.3845 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5154: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5154/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9705\n",
            "Epoch 5154: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0723 - accuracy: 0.9705 - val_loss: 2.3802 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5155: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5155/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9718\n",
            "Epoch 5155: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0705 - accuracy: 0.9718 - val_loss: 2.3789 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5156: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5156/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9724\n",
            "Epoch 5156: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0683 - accuracy: 0.9724 - val_loss: 2.3790 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5157: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5157/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9711\n",
            "Epoch 5157: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0716 - accuracy: 0.9711 - val_loss: 2.3813 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5158: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5158/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9718\n",
            "Epoch 5158: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0697 - accuracy: 0.9718 - val_loss: 2.3841 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5159: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5159/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9660\n",
            "Epoch 5159: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0747 - accuracy: 0.9660 - val_loss: 2.3834 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5160: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5160/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9685\n",
            "Epoch 5160: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0697 - accuracy: 0.9685 - val_loss: 2.3827 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5161: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5161/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9692\n",
            "Epoch 5161: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0668 - accuracy: 0.9692 - val_loss: 2.3820 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5162: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5162/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9711\n",
            "Epoch 5162: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0715 - accuracy: 0.9711 - val_loss: 2.3815 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5163: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5163/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9705\n",
            "Epoch 5163: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0713 - accuracy: 0.9705 - val_loss: 2.3830 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5164: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5164/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9718\n",
            "Epoch 5164: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0736 - accuracy: 0.9718 - val_loss: 2.3858 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5165: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5165/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9718\n",
            "Epoch 5165: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0701 - accuracy: 0.9718 - val_loss: 2.3883 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5166: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5166/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9730\n",
            "Epoch 5166: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0727 - accuracy: 0.9730 - val_loss: 2.3892 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5167: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5167/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9705\n",
            "Epoch 5167: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0740 - accuracy: 0.9705 - val_loss: 2.3864 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5168: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5168/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9730\n",
            "Epoch 5168: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0664 - accuracy: 0.9730 - val_loss: 2.3832 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5169: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5169/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9705\n",
            "Epoch 5169: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0712 - accuracy: 0.9705 - val_loss: 2.3806 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5170: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5170/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9705\n",
            "Epoch 5170: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0709 - accuracy: 0.9705 - val_loss: 2.3818 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5171: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5171/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9685\n",
            "Epoch 5171: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0733 - accuracy: 0.9685 - val_loss: 2.3803 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5172: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5172/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9705\n",
            "Epoch 5172: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0720 - accuracy: 0.9705 - val_loss: 2.3772 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5173: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5173/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9679\n",
            "Epoch 5173: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0701 - accuracy: 0.9679 - val_loss: 2.3746 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5174: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5174/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9685\n",
            "Epoch 5174: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0708 - accuracy: 0.9685 - val_loss: 2.3730 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5175: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5175/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9724\n",
            "Epoch 5175: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0685 - accuracy: 0.9724 - val_loss: 2.3720 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5176: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5176/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9718\n",
            "Epoch 5176: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0784 - accuracy: 0.9718 - val_loss: 2.3731 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5177: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5177/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9718\n",
            "Epoch 5177: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0826 - accuracy: 0.9718 - val_loss: 2.3739 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5178: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5178/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9679\n",
            "Epoch 5178: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0759 - accuracy: 0.9679 - val_loss: 2.3766 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5179: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5179/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9718\n",
            "Epoch 5179: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0707 - accuracy: 0.9718 - val_loss: 2.3805 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5180: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5180/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9698\n",
            "Epoch 5180: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.0712 - accuracy: 0.9698 - val_loss: 2.3859 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5181: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5181/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9705\n",
            "Epoch 5181: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0680 - accuracy: 0.9705 - val_loss: 2.3907 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5182: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5182/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9666\n",
            "Epoch 5182: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0729 - accuracy: 0.9666 - val_loss: 2.3918 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5183: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5183/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9705\n",
            "Epoch 5183: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0734 - accuracy: 0.9705 - val_loss: 2.3912 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5184: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5184/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9705\n",
            "Epoch 5184: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.0850 - accuracy: 0.9705 - val_loss: 2.3889 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5185: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5185/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9698\n",
            "Epoch 5185: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0702 - accuracy: 0.9698 - val_loss: 2.3851 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5186: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5186/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9692\n",
            "Epoch 5186: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0700 - accuracy: 0.9692 - val_loss: 2.3790 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5187: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5187/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9705\n",
            "Epoch 5187: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0791 - accuracy: 0.9705 - val_loss: 2.3734 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5188: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5188/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9743\n",
            "Epoch 5188: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0676 - accuracy: 0.9743 - val_loss: 2.3716 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5189: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5189/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9705\n",
            "Epoch 5189: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0977 - accuracy: 0.9705 - val_loss: 2.3697 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5190: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5190/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9660\n",
            "Epoch 5190: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0810 - accuracy: 0.9660 - val_loss: 2.3712 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5191: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5191/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9673\n",
            "Epoch 5191: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0798 - accuracy: 0.9673 - val_loss: 2.3743 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5192: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5192/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9718\n",
            "Epoch 5192: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0730 - accuracy: 0.9718 - val_loss: 2.3795 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5193: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5193/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9698\n",
            "Epoch 5193: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0709 - accuracy: 0.9698 - val_loss: 2.3846 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5194: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5194/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9685\n",
            "Epoch 5194: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0800 - accuracy: 0.9685 - val_loss: 2.3881 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5195: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5195/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9634\n",
            "Epoch 5195: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.1103 - accuracy: 0.9634 - val_loss: 2.3855 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5196: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5196/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9724\n",
            "Epoch 5196: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0674 - accuracy: 0.9724 - val_loss: 2.3823 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5197: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5197/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9711\n",
            "Epoch 5197: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0744 - accuracy: 0.9711 - val_loss: 2.3785 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5198: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5198/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9698\n",
            "Epoch 5198: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0733 - accuracy: 0.9698 - val_loss: 2.3730 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5199: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5199/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9705\n",
            "Epoch 5199: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0690 - accuracy: 0.9705 - val_loss: 2.3712 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5200: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5200/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9711\n",
            "Epoch 5200: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.0745 - accuracy: 0.9711 - val_loss: 2.3710 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5201: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5201/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9685\n",
            "Epoch 5201: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0763 - accuracy: 0.9685 - val_loss: 2.3723 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5202: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5202/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9679\n",
            "Epoch 5202: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0753 - accuracy: 0.9679 - val_loss: 2.3752 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5203: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5203/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9705\n",
            "Epoch 5203: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0718 - accuracy: 0.9705 - val_loss: 2.3779 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5204: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5204/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9679\n",
            "Epoch 5204: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.1035 - accuracy: 0.9679 - val_loss: 2.3799 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5205: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5205/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9718\n",
            "Epoch 5205: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0764 - accuracy: 0.9718 - val_loss: 2.3824 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5206: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5206/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9692\n",
            "Epoch 5206: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0804 - accuracy: 0.9692 - val_loss: 2.3806 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5207: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5207/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9730\n",
            "Epoch 5207: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0705 - accuracy: 0.9730 - val_loss: 2.3787 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5208: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5208/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9698\n",
            "Epoch 5208: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0857 - accuracy: 0.9698 - val_loss: 2.3756 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5209: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5209/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9705\n",
            "Epoch 5209: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0768 - accuracy: 0.9705 - val_loss: 2.3718 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5210: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5210/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9698\n",
            "Epoch 5210: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0717 - accuracy: 0.9698 - val_loss: 2.3678 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5211: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5211/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9698\n",
            "Epoch 5211: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0728 - accuracy: 0.9698 - val_loss: 2.3668 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5212: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5212/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9666\n",
            "Epoch 5212: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0706 - accuracy: 0.9666 - val_loss: 2.3675 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5213: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5213/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9711\n",
            "Epoch 5213: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0692 - accuracy: 0.9711 - val_loss: 2.3698 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5214: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5214/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9705\n",
            "Epoch 5214: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0769 - accuracy: 0.9705 - val_loss: 2.3714 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5215: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5215/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9724\n",
            "Epoch 5215: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0675 - accuracy: 0.9724 - val_loss: 2.3733 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5216: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5216/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9692\n",
            "Epoch 5216: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0726 - accuracy: 0.9692 - val_loss: 2.3747 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5217: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5217/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9685\n",
            "Epoch 5217: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0749 - accuracy: 0.9685 - val_loss: 2.3750 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5218: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5218/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9705\n",
            "Epoch 5218: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0945 - accuracy: 0.9705 - val_loss: 2.3729 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5219: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5219/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9730\n",
            "Epoch 5219: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0664 - accuracy: 0.9730 - val_loss: 2.3713 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5220: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5220/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9724\n",
            "Epoch 5220: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0697 - accuracy: 0.9724 - val_loss: 2.3700 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5221: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5221/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9698\n",
            "Epoch 5221: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0732 - accuracy: 0.9698 - val_loss: 2.3667 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5222: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5222/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9718\n",
            "Epoch 5222: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0700 - accuracy: 0.9718 - val_loss: 2.3623 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5223: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5223/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9718\n",
            "Epoch 5223: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0659 - accuracy: 0.9718 - val_loss: 2.3609 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5224: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5224/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9698\n",
            "Epoch 5224: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0705 - accuracy: 0.9698 - val_loss: 2.3615 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5225: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5225/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9705\n",
            "Epoch 5225: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0717 - accuracy: 0.9705 - val_loss: 2.3638 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5226: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5226/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9698\n",
            "Epoch 5226: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0748 - accuracy: 0.9698 - val_loss: 2.3633 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5227: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5227/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9705\n",
            "Epoch 5227: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0677 - accuracy: 0.9705 - val_loss: 2.3611 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5228: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5228/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9730\n",
            "Epoch 5228: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0659 - accuracy: 0.9730 - val_loss: 2.3590 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5229: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5229/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9711\n",
            "Epoch 5229: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0718 - accuracy: 0.9711 - val_loss: 2.3563 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5230: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5230/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9724\n",
            "Epoch 5230: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0689 - accuracy: 0.9724 - val_loss: 2.3550 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5231: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5231/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9698\n",
            "Epoch 5231: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0700 - accuracy: 0.9698 - val_loss: 2.3553 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5232: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5232/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9692\n",
            "Epoch 5232: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0832 - accuracy: 0.9692 - val_loss: 2.3549 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5233: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5233/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9692\n",
            "Epoch 5233: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0729 - accuracy: 0.9692 - val_loss: 2.3541 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5234: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5234/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9730\n",
            "Epoch 5234: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0701 - accuracy: 0.9730 - val_loss: 2.3524 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5235: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5235/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9724\n",
            "Epoch 5235: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0696 - accuracy: 0.9724 - val_loss: 2.3527 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5236: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5236/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9692\n",
            "Epoch 5236: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0757 - accuracy: 0.9692 - val_loss: 2.3518 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5237: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5237/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9718\n",
            "Epoch 5237: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0693 - accuracy: 0.9718 - val_loss: 2.3505 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5238: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5238/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9718\n",
            "Epoch 5238: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0786 - accuracy: 0.9718 - val_loss: 2.3480 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5239: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5239/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9730\n",
            "Epoch 5239: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0705 - accuracy: 0.9730 - val_loss: 2.3490 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5240: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5240/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9705\n",
            "Epoch 5240: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0714 - accuracy: 0.9705 - val_loss: 2.3500 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5241: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5241/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9724\n",
            "Epoch 5241: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0740 - accuracy: 0.9724 - val_loss: 2.3528 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5242: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5242/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9692\n",
            "Epoch 5242: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0737 - accuracy: 0.9692 - val_loss: 2.3534 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5243: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5243/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9730\n",
            "Epoch 5243: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0704 - accuracy: 0.9730 - val_loss: 2.3554 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5244: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5244/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9724\n",
            "Epoch 5244: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0699 - accuracy: 0.9724 - val_loss: 2.3571 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5245: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5245/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9685\n",
            "Epoch 5245: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0772 - accuracy: 0.9685 - val_loss: 2.3585 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5246: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5246/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9705\n",
            "Epoch 5246: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0730 - accuracy: 0.9705 - val_loss: 2.3582 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5247: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5247/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9660\n",
            "Epoch 5247: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0886 - accuracy: 0.9660 - val_loss: 2.3584 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5248: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5248/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9698\n",
            "Epoch 5248: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0679 - accuracy: 0.9698 - val_loss: 2.3594 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5249: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5249/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9718\n",
            "Epoch 5249: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0693 - accuracy: 0.9718 - val_loss: 2.3594 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5250: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5250/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9705\n",
            "Epoch 5250: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0777 - accuracy: 0.9705 - val_loss: 2.3619 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5251: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5251/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9737\n",
            "Epoch 5251: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0717 - accuracy: 0.9737 - val_loss: 2.3651 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5252: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5252/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9718\n",
            "Epoch 5252: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0698 - accuracy: 0.9718 - val_loss: 2.3666 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5253: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5253/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9711\n",
            "Epoch 5253: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0711 - accuracy: 0.9711 - val_loss: 2.3690 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5254: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5254/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9718\n",
            "Epoch 5254: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0742 - accuracy: 0.9718 - val_loss: 2.3716 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5255: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5255/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9685\n",
            "Epoch 5255: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0805 - accuracy: 0.9685 - val_loss: 2.3707 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5256: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5256/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9679\n",
            "Epoch 5256: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0716 - accuracy: 0.9679 - val_loss: 2.3706 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5257: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5257/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9698\n",
            "Epoch 5257: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0725 - accuracy: 0.9698 - val_loss: 2.3687 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5258: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5258/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9673\n",
            "Epoch 5258: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0828 - accuracy: 0.9673 - val_loss: 2.3658 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5259: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5259/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9698\n",
            "Epoch 5259: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0672 - accuracy: 0.9698 - val_loss: 2.3645 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5260: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5260/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9711\n",
            "Epoch 5260: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0798 - accuracy: 0.9711 - val_loss: 2.3633 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5261: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5261/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9705\n",
            "Epoch 5261: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0715 - accuracy: 0.9705 - val_loss: 2.3609 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5262: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5262/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9756\n",
            "Epoch 5262: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0657 - accuracy: 0.9756 - val_loss: 2.3621 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5263: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5263/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9705\n",
            "Epoch 5263: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0744 - accuracy: 0.9705 - val_loss: 2.3622 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5264: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5264/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9730\n",
            "Epoch 5264: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0714 - accuracy: 0.9730 - val_loss: 2.3630 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5265: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5265/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9737\n",
            "Epoch 5265: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0727 - accuracy: 0.9737 - val_loss: 2.3642 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5266: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5266/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9718\n",
            "Epoch 5266: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0664 - accuracy: 0.9718 - val_loss: 2.3643 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5267: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5267/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9705\n",
            "Epoch 5267: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0736 - accuracy: 0.9705 - val_loss: 2.3608 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5268: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5268/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9718\n",
            "Epoch 5268: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0752 - accuracy: 0.9718 - val_loss: 2.3547 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5269: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5269/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9718\n",
            "Epoch 5269: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0732 - accuracy: 0.9718 - val_loss: 2.3459 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5270: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5270/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9711\n",
            "Epoch 5270: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0700 - accuracy: 0.9711 - val_loss: 2.3380 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5271: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5271/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9673\n",
            "Epoch 5271: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0746 - accuracy: 0.9673 - val_loss: 2.3335 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5272: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5272/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9692\n",
            "Epoch 5272: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0721 - accuracy: 0.9692 - val_loss: 2.3320 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5273: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5273/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9685\n",
            "Epoch 5273: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0728 - accuracy: 0.9685 - val_loss: 2.3333 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5274: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5274/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9705\n",
            "Epoch 5274: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0728 - accuracy: 0.9705 - val_loss: 2.3381 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5275: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5275/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9698\n",
            "Epoch 5275: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0738 - accuracy: 0.9698 - val_loss: 2.3431 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5276: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5276/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9743\n",
            "Epoch 5276: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0686 - accuracy: 0.9743 - val_loss: 2.3472 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5277: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5277/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9698\n",
            "Epoch 5277: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0695 - accuracy: 0.9698 - val_loss: 2.3465 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5278: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5278/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9698\n",
            "Epoch 5278: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0816 - accuracy: 0.9698 - val_loss: 2.3426 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5279: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5279/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9692\n",
            "Epoch 5279: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 2.3348 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5280: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5280/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9698\n",
            "Epoch 5280: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0695 - accuracy: 0.9698 - val_loss: 2.3283 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5281: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5281/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9718\n",
            "Epoch 5281: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0706 - accuracy: 0.9718 - val_loss: 2.3237 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5282: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5282/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9698\n",
            "Epoch 5282: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0683 - accuracy: 0.9698 - val_loss: 2.3216 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5283: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5283/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9763\n",
            "Epoch 5283: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0659 - accuracy: 0.9763 - val_loss: 2.3221 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5284: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5284/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9692\n",
            "Epoch 5284: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0711 - accuracy: 0.9692 - val_loss: 2.3222 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5285: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5285/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9718\n",
            "Epoch 5285: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0702 - accuracy: 0.9718 - val_loss: 2.3232 - val_accuracy: 0.8817 - lr: 0.0100\n",
            "\n",
            "Epoch 5286: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5286/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9673\n",
            "Epoch 5286: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0701 - accuracy: 0.9673 - val_loss: 2.3220 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5287: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5287/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9698\n",
            "Epoch 5287: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0792 - accuracy: 0.9698 - val_loss: 2.3207 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5288: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5288/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9679\n",
            "Epoch 5288: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0725 - accuracy: 0.9679 - val_loss: 2.3203 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5289: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5289/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9673\n",
            "Epoch 5289: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0826 - accuracy: 0.9673 - val_loss: 2.3196 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5290: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5290/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9711\n",
            "Epoch 5290: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0755 - accuracy: 0.9711 - val_loss: 2.3200 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5291: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5291/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9692\n",
            "Epoch 5291: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0719 - accuracy: 0.9692 - val_loss: 2.3205 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5292: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5292/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9698\n",
            "Epoch 5292: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0943 - accuracy: 0.9698 - val_loss: 2.3200 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5293: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5293/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9698\n",
            "Epoch 5293: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0750 - accuracy: 0.9698 - val_loss: 2.3200 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5294: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5294/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9711\n",
            "Epoch 5294: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0727 - accuracy: 0.9711 - val_loss: 2.3208 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5295: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5295/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9730\n",
            "Epoch 5295: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0668 - accuracy: 0.9730 - val_loss: 2.3225 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5296: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5296/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9679\n",
            "Epoch 5296: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0730 - accuracy: 0.9679 - val_loss: 2.3222 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5297: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5297/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9705\n",
            "Epoch 5297: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0717 - accuracy: 0.9705 - val_loss: 2.3214 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5298: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5298/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9685\n",
            "Epoch 5298: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0731 - accuracy: 0.9685 - val_loss: 2.3191 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5299: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5299/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9679\n",
            "Epoch 5299: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0793 - accuracy: 0.9679 - val_loss: 2.3165 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5300: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5300/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9705\n",
            "Epoch 5300: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0764 - accuracy: 0.9705 - val_loss: 2.3161 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5301: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5301/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9685\n",
            "Epoch 5301: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0743 - accuracy: 0.9685 - val_loss: 2.3176 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5302: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5302/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9705\n",
            "Epoch 5302: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0741 - accuracy: 0.9705 - val_loss: 2.3222 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5303: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5303/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9705\n",
            "Epoch 5303: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0872 - accuracy: 0.9705 - val_loss: 2.3261 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5304: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5304/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9673\n",
            "Epoch 5304: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0777 - accuracy: 0.9673 - val_loss: 2.3252 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5305: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5305/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9705\n",
            "Epoch 5305: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0881 - accuracy: 0.9705 - val_loss: 2.3239 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5306: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5306/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9724\n",
            "Epoch 5306: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0729 - accuracy: 0.9724 - val_loss: 2.3211 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5307: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5307/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9718\n",
            "Epoch 5307: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0702 - accuracy: 0.9718 - val_loss: 2.3185 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5308: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5308/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9692\n",
            "Epoch 5308: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0887 - accuracy: 0.9692 - val_loss: 2.3206 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5309: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5309/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9718\n",
            "Epoch 5309: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0672 - accuracy: 0.9718 - val_loss: 2.3236 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5310: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5310/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9724\n",
            "Epoch 5310: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0691 - accuracy: 0.9724 - val_loss: 2.3254 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5311: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5311/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9692\n",
            "Epoch 5311: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0785 - accuracy: 0.9692 - val_loss: 2.3246 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5312: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5312/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9730\n",
            "Epoch 5312: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0703 - accuracy: 0.9730 - val_loss: 2.3249 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5313: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5313/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9705\n",
            "Epoch 5313: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0735 - accuracy: 0.9705 - val_loss: 2.3247 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5314: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5314/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9692\n",
            "Epoch 5314: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0697 - accuracy: 0.9692 - val_loss: 2.3242 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5315: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5315/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9737\n",
            "Epoch 5315: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0681 - accuracy: 0.9737 - val_loss: 2.3245 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5316: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5316/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9724\n",
            "Epoch 5316: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0752 - accuracy: 0.9724 - val_loss: 2.3237 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5317: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5317/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9724\n",
            "Epoch 5317: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0729 - accuracy: 0.9724 - val_loss: 2.3246 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5318: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5318/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9730\n",
            "Epoch 5318: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0692 - accuracy: 0.9730 - val_loss: 2.3287 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5319: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5319/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9711\n",
            "Epoch 5319: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0745 - accuracy: 0.9711 - val_loss: 2.3322 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5320: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5320/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9698\n",
            "Epoch 5320: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0710 - accuracy: 0.9698 - val_loss: 2.3352 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5321: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5321/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9711\n",
            "Epoch 5321: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0667 - accuracy: 0.9711 - val_loss: 2.3383 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5322: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5322/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9705\n",
            "Epoch 5322: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0829 - accuracy: 0.9705 - val_loss: 2.3410 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5323: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5323/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9724\n",
            "Epoch 5323: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0745 - accuracy: 0.9724 - val_loss: 2.3402 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5324: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5324/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9685\n",
            "Epoch 5324: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0720 - accuracy: 0.9685 - val_loss: 2.3395 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5325: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5325/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9711\n",
            "Epoch 5325: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0706 - accuracy: 0.9711 - val_loss: 2.3397 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5326: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5326/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9718\n",
            "Epoch 5326: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0715 - accuracy: 0.9718 - val_loss: 2.3399 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5327: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5327/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9679\n",
            "Epoch 5327: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0710 - accuracy: 0.9679 - val_loss: 2.3393 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5328: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5328/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9673\n",
            "Epoch 5328: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0767 - accuracy: 0.9673 - val_loss: 2.3411 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5329: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5329/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9692\n",
            "Epoch 5329: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0722 - accuracy: 0.9692 - val_loss: 2.3447 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5330: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5330/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9718\n",
            "Epoch 5330: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0684 - accuracy: 0.9718 - val_loss: 2.3503 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5331: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5331/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9711\n",
            "Epoch 5331: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0731 - accuracy: 0.9711 - val_loss: 2.3565 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5332: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5332/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9711\n",
            "Epoch 5332: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0721 - accuracy: 0.9711 - val_loss: 2.3629 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5333: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5333/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9685\n",
            "Epoch 5333: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0793 - accuracy: 0.9685 - val_loss: 2.3673 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5334: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5334/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9711\n",
            "Epoch 5334: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0756 - accuracy: 0.9711 - val_loss: 2.3690 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5335: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5335/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9692\n",
            "Epoch 5335: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0767 - accuracy: 0.9692 - val_loss: 2.3685 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5336: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5336/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9673\n",
            "Epoch 5336: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0707 - accuracy: 0.9673 - val_loss: 2.3670 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5337: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5337/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9698\n",
            "Epoch 5337: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0832 - accuracy: 0.9698 - val_loss: 2.3640 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5338: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5338/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9711\n",
            "Epoch 5338: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0787 - accuracy: 0.9711 - val_loss: 2.3620 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5339: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5339/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9724\n",
            "Epoch 5339: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0682 - accuracy: 0.9724 - val_loss: 2.3602 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5340: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5340/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9698\n",
            "Epoch 5340: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0870 - accuracy: 0.9698 - val_loss: 2.3601 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5341: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5341/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9724\n",
            "Epoch 5341: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0681 - accuracy: 0.9724 - val_loss: 2.3620 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5342: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5342/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9718\n",
            "Epoch 5342: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0674 - accuracy: 0.9718 - val_loss: 2.3657 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5343: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5343/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9711\n",
            "Epoch 5343: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0672 - accuracy: 0.9711 - val_loss: 2.3708 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5344: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5344/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9730\n",
            "Epoch 5344: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0719 - accuracy: 0.9730 - val_loss: 2.3742 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5345: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5345/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9718\n",
            "Epoch 5345: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0698 - accuracy: 0.9718 - val_loss: 2.3759 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5346: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5346/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9692\n",
            "Epoch 5346: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0881 - accuracy: 0.9692 - val_loss: 2.3739 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5347: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5347/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9705\n",
            "Epoch 5347: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0728 - accuracy: 0.9705 - val_loss: 2.3711 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5348: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5348/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9685\n",
            "Epoch 5348: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0706 - accuracy: 0.9685 - val_loss: 2.3657 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5349: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5349/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9692\n",
            "Epoch 5349: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0741 - accuracy: 0.9692 - val_loss: 2.3617 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5350: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5350/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9743\n",
            "Epoch 5350: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0718 - accuracy: 0.9743 - val_loss: 2.3578 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5351: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5351/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9698\n",
            "Epoch 5351: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0759 - accuracy: 0.9698 - val_loss: 2.3549 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5352: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5352/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9711\n",
            "Epoch 5352: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.0706 - accuracy: 0.9711 - val_loss: 2.3520 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5353: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5353/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9705\n",
            "Epoch 5353: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0694 - accuracy: 0.9705 - val_loss: 2.3530 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5354: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5354/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9705\n",
            "Epoch 5354: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0760 - accuracy: 0.9705 - val_loss: 2.3548 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5355: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5355/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9718\n",
            "Epoch 5355: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0700 - accuracy: 0.9718 - val_loss: 2.3543 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5356: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5356/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9711\n",
            "Epoch 5356: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0787 - accuracy: 0.9711 - val_loss: 2.3545 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5357: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5357/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9653\n",
            "Epoch 5357: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.0743 - accuracy: 0.9653 - val_loss: 2.3521 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5358: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5358/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9705\n",
            "Epoch 5358: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0728 - accuracy: 0.9705 - val_loss: 2.3496 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5359: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5359/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9698\n",
            "Epoch 5359: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0685 - accuracy: 0.9698 - val_loss: 2.3483 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5360: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5360/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9698\n",
            "Epoch 5360: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0761 - accuracy: 0.9698 - val_loss: 2.3447 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5361: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5361/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9711\n",
            "Epoch 5361: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0727 - accuracy: 0.9711 - val_loss: 2.3430 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5362: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5362/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9718\n",
            "Epoch 5362: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0700 - accuracy: 0.9718 - val_loss: 2.3432 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5363: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5363/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9724\n",
            "Epoch 5363: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0719 - accuracy: 0.9724 - val_loss: 2.3447 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5364: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5364/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9692\n",
            "Epoch 5364: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0704 - accuracy: 0.9692 - val_loss: 2.3479 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5365: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5365/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9685\n",
            "Epoch 5365: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0736 - accuracy: 0.9685 - val_loss: 2.3516 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5366: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5366/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9737\n",
            "Epoch 5366: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0692 - accuracy: 0.9737 - val_loss: 2.3555 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5367: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5367/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9692\n",
            "Epoch 5367: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0717 - accuracy: 0.9692 - val_loss: 2.3590 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5368: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5368/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9692\n",
            "Epoch 5368: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0732 - accuracy: 0.9692 - val_loss: 2.3600 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5369: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5369/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9673\n",
            "Epoch 5369: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.0738 - accuracy: 0.9673 - val_loss: 2.3611 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5370: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5370/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9679\n",
            "Epoch 5370: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0704 - accuracy: 0.9679 - val_loss: 2.3602 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5371: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5371/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9724\n",
            "Epoch 5371: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0720 - accuracy: 0.9724 - val_loss: 2.3594 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5372: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5372/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9730\n",
            "Epoch 5372: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0703 - accuracy: 0.9730 - val_loss: 2.3571 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5373: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5373/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9692\n",
            "Epoch 5373: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0734 - accuracy: 0.9692 - val_loss: 2.3553 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5374: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5374/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9705\n",
            "Epoch 5374: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0732 - accuracy: 0.9705 - val_loss: 2.3546 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5375: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5375/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9724\n",
            "Epoch 5375: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0774 - accuracy: 0.9724 - val_loss: 2.3545 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5376: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5376/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9685\n",
            "Epoch 5376: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0785 - accuracy: 0.9685 - val_loss: 2.3564 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5377: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5377/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9711\n",
            "Epoch 5377: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0715 - accuracy: 0.9711 - val_loss: 2.3578 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5378: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5378/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9711\n",
            "Epoch 5378: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0712 - accuracy: 0.9711 - val_loss: 2.3611 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5379: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5379/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9698\n",
            "Epoch 5379: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0700 - accuracy: 0.9698 - val_loss: 2.3664 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5380: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5380/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9666\n",
            "Epoch 5380: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0724 - accuracy: 0.9666 - val_loss: 2.3709 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5381: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5381/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9711\n",
            "Epoch 5381: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0705 - accuracy: 0.9711 - val_loss: 2.3745 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5382: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5382/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9698\n",
            "Epoch 5382: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0711 - accuracy: 0.9698 - val_loss: 2.3758 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5383: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5383/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9718\n",
            "Epoch 5383: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0676 - accuracy: 0.9718 - val_loss: 2.3761 - val_accuracy: 0.8713 - lr: 0.0100\n",
            "\n",
            "Epoch 5384: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5384/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9705\n",
            "Epoch 5384: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0820 - accuracy: 0.9705 - val_loss: 2.3750 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "\n",
            "Epoch 5385: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5385/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9705\n",
            "Epoch 5385: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0690 - accuracy: 0.9705 - val_loss: 2.3748 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5386: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5386/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9711\n",
            "Epoch 5386: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0710 - accuracy: 0.9711 - val_loss: 2.3786 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5387: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5387/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9705\n",
            "Epoch 5387: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0741 - accuracy: 0.9705 - val_loss: 2.3802 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5388: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5388/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9718\n",
            "Epoch 5388: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0808 - accuracy: 0.9718 - val_loss: 2.3806 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5389: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5389/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9692\n",
            "Epoch 5389: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0740 - accuracy: 0.9692 - val_loss: 2.3809 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5390: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5390/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9724\n",
            "Epoch 5390: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0804 - accuracy: 0.9724 - val_loss: 2.3793 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5391: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5391/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9705\n",
            "Epoch 5391: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0739 - accuracy: 0.9705 - val_loss: 2.3764 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5392: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5392/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9724\n",
            "Epoch 5392: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0703 - accuracy: 0.9724 - val_loss: 2.3701 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5393: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5393/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9692\n",
            "Epoch 5393: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 2.3662 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5394: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5394/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9724\n",
            "Epoch 5394: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0778 - accuracy: 0.9724 - val_loss: 2.3635 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5395: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5395/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9711\n",
            "Epoch 5395: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0667 - accuracy: 0.9711 - val_loss: 2.3644 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5396: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5396/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9718\n",
            "Epoch 5396: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0690 - accuracy: 0.9718 - val_loss: 2.3679 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5397: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5397/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9730\n",
            "Epoch 5397: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0724 - accuracy: 0.9730 - val_loss: 2.3714 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5398: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5398/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9711\n",
            "Epoch 5398: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0750 - accuracy: 0.9711 - val_loss: 2.3761 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5399: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5399/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9692\n",
            "Epoch 5399: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0708 - accuracy: 0.9692 - val_loss: 2.3791 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5400: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5400/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9730\n",
            "Epoch 5400: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0700 - accuracy: 0.9730 - val_loss: 2.3752 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5401: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5401/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9679\n",
            "Epoch 5401: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0748 - accuracy: 0.9679 - val_loss: 2.3696 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5402: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5402/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9718\n",
            "Epoch 5402: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0714 - accuracy: 0.9718 - val_loss: 2.3627 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5403: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5403/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9692\n",
            "Epoch 5403: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0693 - accuracy: 0.9692 - val_loss: 2.3582 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5404: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5404/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9692\n",
            "Epoch 5404: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0770 - accuracy: 0.9692 - val_loss: 2.3568 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5405: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5405/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9718\n",
            "Epoch 5405: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0725 - accuracy: 0.9718 - val_loss: 2.3564 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5406: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5406/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9653\n",
            "Epoch 5406: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0705 - accuracy: 0.9653 - val_loss: 2.3588 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5407: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5407/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9724\n",
            "Epoch 5407: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0741 - accuracy: 0.9724 - val_loss: 2.3621 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5408: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5408/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9692\n",
            "Epoch 5408: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0704 - accuracy: 0.9692 - val_loss: 2.3668 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5409: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5409/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9692\n",
            "Epoch 5409: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0661 - accuracy: 0.9692 - val_loss: 2.3728 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5410: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5410/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9698\n",
            "Epoch 5410: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0702 - accuracy: 0.9698 - val_loss: 2.3791 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5411: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5411/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9705\n",
            "Epoch 5411: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0704 - accuracy: 0.9705 - val_loss: 2.3810 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5412: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5412/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9692\n",
            "Epoch 5412: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0719 - accuracy: 0.9692 - val_loss: 2.3820 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5413: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5413/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9724\n",
            "Epoch 5413: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0692 - accuracy: 0.9724 - val_loss: 2.3819 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5414: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5414/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9724\n",
            "Epoch 5414: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0666 - accuracy: 0.9724 - val_loss: 2.3811 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5415: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5415/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9698\n",
            "Epoch 5415: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0669 - accuracy: 0.9698 - val_loss: 2.3808 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5416: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5416/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9705\n",
            "Epoch 5416: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0735 - accuracy: 0.9705 - val_loss: 2.3803 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5417: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5417/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9679\n",
            "Epoch 5417: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0816 - accuracy: 0.9679 - val_loss: 2.3788 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5418: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5418/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9718\n",
            "Epoch 5418: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0704 - accuracy: 0.9718 - val_loss: 2.3786 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5419: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5419/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9685\n",
            "Epoch 5419: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0748 - accuracy: 0.9685 - val_loss: 2.3764 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5420: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5420/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9685\n",
            "Epoch 5420: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0718 - accuracy: 0.9685 - val_loss: 2.3732 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5421: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5421/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9718\n",
            "Epoch 5421: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0728 - accuracy: 0.9718 - val_loss: 2.3706 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5422: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5422/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9711\n",
            "Epoch 5422: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0870 - accuracy: 0.9711 - val_loss: 2.3680 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5423: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5423/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9698\n",
            "Epoch 5423: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0694 - accuracy: 0.9698 - val_loss: 2.3674 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5424: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5424/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9730\n",
            "Epoch 5424: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0678 - accuracy: 0.9730 - val_loss: 2.3696 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5425: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5425/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9705\n",
            "Epoch 5425: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0695 - accuracy: 0.9705 - val_loss: 2.3752 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5426: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5426/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9724\n",
            "Epoch 5426: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0676 - accuracy: 0.9724 - val_loss: 2.3804 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5427: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5427/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9724\n",
            "Epoch 5427: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0683 - accuracy: 0.9724 - val_loss: 2.3836 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5428: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5428/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9679\n",
            "Epoch 5428: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0815 - accuracy: 0.9679 - val_loss: 2.3817 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5429: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5429/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9692\n",
            "Epoch 5429: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0717 - accuracy: 0.9692 - val_loss: 2.3758 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5430: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5430/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9698\n",
            "Epoch 5430: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0826 - accuracy: 0.9698 - val_loss: 2.3688 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5431: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5431/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9673\n",
            "Epoch 5431: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0720 - accuracy: 0.9673 - val_loss: 2.3644 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5432: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5432/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9679\n",
            "Epoch 5432: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0719 - accuracy: 0.9679 - val_loss: 2.3612 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5433: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5433/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9724\n",
            "Epoch 5433: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0698 - accuracy: 0.9724 - val_loss: 2.3604 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5434: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5434/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9705\n",
            "Epoch 5434: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0696 - accuracy: 0.9705 - val_loss: 2.3640 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5435: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5435/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9705\n",
            "Epoch 5435: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0716 - accuracy: 0.9705 - val_loss: 2.3656 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5436: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5436/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9705\n",
            "Epoch 5436: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0751 - accuracy: 0.9705 - val_loss: 2.3643 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5437: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5437/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9698\n",
            "Epoch 5437: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0714 - accuracy: 0.9698 - val_loss: 2.3609 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5438: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5438/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9685\n",
            "Epoch 5438: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0767 - accuracy: 0.9685 - val_loss: 2.3584 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5439: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5439/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9718\n",
            "Epoch 5439: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0680 - accuracy: 0.9718 - val_loss: 2.3595 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5440: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5440/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9666\n",
            "Epoch 5440: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0787 - accuracy: 0.9666 - val_loss: 2.3593 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5441: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5441/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9641\n",
            "Epoch 5441: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0789 - accuracy: 0.9641 - val_loss: 2.3613 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5442: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5442/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9705\n",
            "Epoch 5442: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0737 - accuracy: 0.9705 - val_loss: 2.3630 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5443: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5443/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9724\n",
            "Epoch 5443: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0651 - accuracy: 0.9724 - val_loss: 2.3662 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5444: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5444/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9705\n",
            "Epoch 5444: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0688 - accuracy: 0.9705 - val_loss: 2.3697 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5445: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5445/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
            "Epoch 5445: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 2.3719 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5446: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5446/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9711\n",
            "Epoch 5446: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0734 - accuracy: 0.9711 - val_loss: 2.3726 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5447: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5447/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9666\n",
            "Epoch 5447: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0746 - accuracy: 0.9666 - val_loss: 2.3719 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5448: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5448/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9679\n",
            "Epoch 5448: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.0748 - accuracy: 0.9679 - val_loss: 2.3699 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5449: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5449/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9685\n",
            "Epoch 5449: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0822 - accuracy: 0.9685 - val_loss: 2.3652 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5450: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5450/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9679\n",
            "Epoch 5450: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0741 - accuracy: 0.9679 - val_loss: 2.3616 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5451: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5451/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9711\n",
            "Epoch 5451: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0699 - accuracy: 0.9711 - val_loss: 2.3592 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5452: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5452/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9718\n",
            "Epoch 5452: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0678 - accuracy: 0.9718 - val_loss: 2.3589 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5453: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5453/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9666\n",
            "Epoch 5453: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0791 - accuracy: 0.9666 - val_loss: 2.3620 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5454: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5454/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9724\n",
            "Epoch 5454: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0719 - accuracy: 0.9724 - val_loss: 2.3615 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5455: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5455/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9724\n",
            "Epoch 5455: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0679 - accuracy: 0.9724 - val_loss: 2.3622 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5456: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5456/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9685\n",
            "Epoch 5456: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0732 - accuracy: 0.9685 - val_loss: 2.3632 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5457: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5457/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9685\n",
            "Epoch 5457: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0762 - accuracy: 0.9685 - val_loss: 2.3606 - val_accuracy: 0.8802 - lr: 0.0100\n",
            "\n",
            "Epoch 5458: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5458/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9673\n",
            "Epoch 5458: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0762 - accuracy: 0.9673 - val_loss: 2.3564 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5459: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5459/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9679\n",
            "Epoch 5459: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0754 - accuracy: 0.9679 - val_loss: 2.3526 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5460: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5460/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9679\n",
            "Epoch 5460: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0717 - accuracy: 0.9679 - val_loss: 2.3509 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5461: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5461/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9724\n",
            "Epoch 5461: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.0693 - accuracy: 0.9724 - val_loss: 2.3500 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5462: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5462/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9705\n",
            "Epoch 5462: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0707 - accuracy: 0.9705 - val_loss: 2.3531 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5463: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5463/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9724\n",
            "Epoch 5463: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.0991 - accuracy: 0.9724 - val_loss: 2.3588 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5464: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5464/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9666\n",
            "Epoch 5464: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0735 - accuracy: 0.9666 - val_loss: 2.3620 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5465: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5465/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9673\n",
            "Epoch 5465: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0785 - accuracy: 0.9673 - val_loss: 2.3614 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5466: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5466/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9705\n",
            "Epoch 5466: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.0696 - accuracy: 0.9705 - val_loss: 2.3582 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5467: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5467/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9730\n",
            "Epoch 5467: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0839 - accuracy: 0.9730 - val_loss: 2.3531 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5468: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5468/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9711\n",
            "Epoch 5468: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0713 - accuracy: 0.9711 - val_loss: 2.3509 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5469: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5469/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9711\n",
            "Epoch 5469: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0659 - accuracy: 0.9711 - val_loss: 2.3529 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5470: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5470/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9679\n",
            "Epoch 5470: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0753 - accuracy: 0.9679 - val_loss: 2.3567 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5471: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5471/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9743\n",
            "Epoch 5471: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0672 - accuracy: 0.9743 - val_loss: 2.3631 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5472: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5472/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9711\n",
            "Epoch 5472: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0716 - accuracy: 0.9711 - val_loss: 2.3691 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5473: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5473/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9698\n",
            "Epoch 5473: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0716 - accuracy: 0.9698 - val_loss: 2.3697 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5474: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5474/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9698\n",
            "Epoch 5474: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0750 - accuracy: 0.9698 - val_loss: 2.3697 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5475: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5475/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9698\n",
            "Epoch 5475: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0938 - accuracy: 0.9698 - val_loss: 2.3655 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5476: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5476/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9724\n",
            "Epoch 5476: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0743 - accuracy: 0.9724 - val_loss: 2.3625 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5477: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5477/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9718\n",
            "Epoch 5477: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0721 - accuracy: 0.9718 - val_loss: 2.3597 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5478: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5478/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9705\n",
            "Epoch 5478: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0721 - accuracy: 0.9705 - val_loss: 2.3577 - val_accuracy: 0.8743 - lr: 0.0100\n",
            "\n",
            "Epoch 5479: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5479/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9705\n",
            "Epoch 5479: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0845 - accuracy: 0.9705 - val_loss: 2.3576 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5480: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5480/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9711\n",
            "Epoch 5480: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0771 - accuracy: 0.9711 - val_loss: 2.3614 - val_accuracy: 0.8757 - lr: 0.0100\n",
            "\n",
            "Epoch 5481: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5481/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9705\n",
            "Epoch 5481: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0704 - accuracy: 0.9705 - val_loss: 2.3646 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5482: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5482/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9705\n",
            "Epoch 5482: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0727 - accuracy: 0.9705 - val_loss: 2.3641 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5483: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5483/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9692\n",
            "Epoch 5483: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0735 - accuracy: 0.9692 - val_loss: 2.3623 - val_accuracy: 0.8772 - lr: 0.0100\n",
            "\n",
            "Epoch 5484: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5484/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9724\n",
            "Epoch 5484: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0703 - accuracy: 0.9724 - val_loss: 2.3618 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5485: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5485/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9692\n",
            "Epoch 5485: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0714 - accuracy: 0.9692 - val_loss: 2.3598 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5486: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5486/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9679\n",
            "Epoch 5486: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0709 - accuracy: 0.9679 - val_loss: 2.3587 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5487: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5487/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9698\n",
            "Epoch 5487: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0789 - accuracy: 0.9698 - val_loss: 2.3515 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5488: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5488/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9673\n",
            "Epoch 5488: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0707 - accuracy: 0.9673 - val_loss: 2.3486 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5489: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5489/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9660\n",
            "Epoch 5489: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0716 - accuracy: 0.9660 - val_loss: 2.3464 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5490: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5490/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9718\n",
            "Epoch 5490: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0724 - accuracy: 0.9718 - val_loss: 2.3474 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5491: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5491/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9718\n",
            "Epoch 5491: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0703 - accuracy: 0.9718 - val_loss: 2.3514 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5492: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5492/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9718\n",
            "Epoch 5492: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0739 - accuracy: 0.9718 - val_loss: 2.3558 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5493: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5493/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9730\n",
            "Epoch 5493: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0646 - accuracy: 0.9730 - val_loss: 2.3610 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5494: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5494/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9685\n",
            "Epoch 5494: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0712 - accuracy: 0.9685 - val_loss: 2.3624 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5495: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5495/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9730\n",
            "Epoch 5495: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0675 - accuracy: 0.9730 - val_loss: 2.3628 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5496: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5496/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9685\n",
            "Epoch 5496: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0787 - accuracy: 0.9685 - val_loss: 2.3611 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5497: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5497/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9718\n",
            "Epoch 5497: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0706 - accuracy: 0.9718 - val_loss: 2.3576 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5498: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5498/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9692\n",
            "Epoch 5498: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0727 - accuracy: 0.9692 - val_loss: 2.3536 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5499: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5499/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9698\n",
            "Epoch 5499: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0738 - accuracy: 0.9698 - val_loss: 2.3501 - val_accuracy: 0.8787 - lr: 0.0100\n",
            "\n",
            "Epoch 5500: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5500/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9705\n",
            "Epoch 5500: loss did not improve from 0.06451\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0710 - accuracy: 0.9705 - val_loss: 2.3478 - val_accuracy: 0.8787 - lr: 0.0100\n"
          ]
        }
      ],
      "source": [
        "if __learning__: \n",
        "    history = model.fit(X_train, y_train, epochs=_epochs_, batch_size=_epochs_, validation_data=(X_test, y_test),verbose=1,callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgzklVywoNmk"
      },
      "execution_count": 532,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 546,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwcWQ94IpDFu",
        "outputId": "eee44001-8804-4b2b-b330-955b392cb81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": 547,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": 548,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bina_transformer=Binarizer(threshold=0.5)\n",
        "y_pred_transform=bina_transformer.fit_transform(y_pred)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H"
      },
      "execution_count": 549,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 550,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_test,y_pred_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Z69kCq3T-pMo",
        "outputId": "bdf1673e-25ad-420a-b721-f66548d55df7"
      },
      "execution_count": 551,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 552,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nZ0rmkNsBGnl",
        "outputId": "8bca8436-4d86-4ecc-d0c0-9d1e99a68202"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fpr, tpr, _ = roc_curve( y_pred_transform,y_test,pos_label=1)\n",
        "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "auc = roc_auc_score(y_test, y_pred_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMRd5eGU9ASA",
        "outputId": "5bf37908-59b1-44cc-ccee-57a0d5b1ccc9"
      },
      "execution_count": 553,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8788522943350194"
            ]
          },
          "metadata": {},
          "execution_count": 553
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzlQR00e8Miq",
        "outputId": "4f364080-702f-4979-b78a-5eb4c3054f47"
      },
      "execution_count": 554,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.12990937, 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 554
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(fpr,tpr)\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZRlw4piW7_uW",
        "outputId": "ea61628c-86a1-4184-d104-0ecb47f25036"
      },
      "execution_count": 555,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": 556,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_df=pd.DataFrame({\"epoch\":history.epoch, \"loss\":history.history[\"loss\"],\"val_loss\":history.history[\"val_loss\"]})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl"
      },
      "execution_count": 557,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "4ENvDCA-0U1g",
        "outputId": "85b8f1d7-642f-40d0-8a9b-7495b6117295"
      },
      "execution_count": 558,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Val_Loss\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"8fa36b59-2405-4072-a0c6-059379748714\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8fa36b59-2405-4072-a0c6-059379748714\")) {                    Plotly.newPlot(                        \"8fa36b59-2405-4072-a0c6-059379748714\",                        [{\"line\":{\"color\":\"rgba(193,99,99,0.8)\"},\"name\":\"Val_Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499],\"y\":[2.0268259048461914,2.0283236503601074,2.029909133911133,2.0317368507385254,2.0338568687438965,2.035712480545044,2.038912534713745,2.0366456508636475,2.034297227859497,2.032517910003662,2.030717134475708,2.0297460556030273,2.029681444168091,2.032574415206909,2.0357155799865723,2.040276527404785,2.043160915374756,2.0459132194519043,2.045370101928711,2.0443973541259766,2.0457870960235596,2.0477850437164307,2.050673723220825,2.053004503250122,2.055695056915283,2.0547871589660645,2.050083875656128,2.044193744659424,2.036358594894409,2.029345750808716,2.0259878635406494,2.0260605812072754,2.0294623374938965,2.0345044136047363,2.0386881828308105,2.042433738708496,2.045989513397217,2.0451042652130127,2.041808605194092,2.038983106613159,2.03649640083313,2.0379204750061035,2.0419230461120605,2.0443122386932373,2.0494585037231445,2.052640438079834,2.0536179542541504,2.052013397216797,2.050379514694214,2.0515360832214355,2.052849292755127,2.0523383617401123,2.053828716278076,2.05637264251709,2.060323476791382,2.0655906200408936,2.0698466300964355,2.071171760559082,2.0719709396362305,2.069554090499878,2.0670664310455322,2.065763473510742,2.064596176147461,2.064739227294922,2.0661301612854004,2.068638563156128,2.0721614360809326,2.0779895782470703,2.081632614135742,2.0849609375,2.0847952365875244,2.08427357673645,2.081420660018921,2.0776472091674805,2.071258306503296,2.0656416416168213,2.0642433166503906,2.0665409564971924,2.071105480194092,2.075329542160034,2.0776782035827637,2.0776329040527344,2.075389862060547,2.071096658706665,2.0672054290771484,2.065013885498047,2.066338062286377,2.0687096118927,2.073922872543335,2.077169418334961,2.0787177085876465,2.079439163208008,2.078935384750366,2.076799154281616,2.076141595840454,2.074753761291504,2.0729525089263916,2.072075128555298,2.0707318782806396,2.0710179805755615,2.065293312072754,2.060833215713501,2.059148073196411,2.060870885848999,2.0653369426727295,2.0714733600616455,2.0769898891448975,2.080437183380127,2.0831618309020996,2.0833568572998047,2.0818405151367188,2.078134059906006,2.0743212699890137,2.071185827255249,2.0714662075042725,2.0719618797302246,2.074042558670044,2.0735909938812256,2.0718443393707275,2.069147825241089,2.0680108070373535,2.067946434020996,2.068857431411743,2.0713136196136475,2.0737359523773193,2.0757663249969482,2.07598876953125,2.074544906616211,2.073622703552246,2.07243013381958,2.0722551345825195,2.0730435848236084,2.0725016593933105,2.0733258724212646,2.0767152309417725,2.077350616455078,2.076366662979126,2.0760324001312256,2.076643705368042,2.0772154331207275,2.0733089447021484,2.068948745727539,2.064711093902588,2.0627002716064453,2.0635311603546143,2.065747022628784,2.067492723464966,2.070819854736328,2.0756776332855225,2.078486204147339,2.077737331390381,2.076176166534424,2.0750789642333984,2.0738513469696045,2.072258949279785,2.071147918701172,2.07083797454834,2.0704336166381836,2.0729031562805176,2.0746302604675293,2.074890613555908,2.0773541927337646,2.081921339035034,2.086808681488037,2.0914764404296875,2.0908074378967285,2.0909056663513184,2.092735528945923,2.093332529067993,2.093233823776245,2.092498302459717,2.0909600257873535,2.0916366577148438,2.093339681625366,2.096853733062744,2.0987744331359863,2.101148843765259,2.104539632797241,2.1071503162384033,2.1091468334198,2.108529567718506,2.100087881088257,2.093306541442871,2.0884382724761963,2.0863037109375,2.0863492488861084,2.089663505554199,2.093647003173828,2.1005938053131104,2.106766700744629,2.1119749546051025,2.1105377674102783,2.1055943965911865,2.100316286087036,2.095694065093994,2.09053111076355,2.086883783340454,2.0854313373565674,2.085146188735962,2.0852508544921875,2.085354804992676,2.0871083736419678,2.0885870456695557,2.0928256511688232,2.0969038009643555,2.101414680480957,2.105468273162842,2.107923746109009,2.103579044342041,2.0953433513641357,2.084200859069824,2.0757949352264404,2.07025408744812,2.0665230751037598,2.065236806869507,2.067081928253174,2.0709338188171387,2.0772769451141357,2.084068536758423,2.0884058475494385,2.090813159942627,2.0922651290893555,2.091595411300659,2.0904805660247803,2.0865767002105713,2.079192638397217,2.0723958015441895,2.067047595977783,2.0652079582214355,2.063631772994995,2.0629258155822754,2.063774585723877,2.06650710105896,2.0697720050811768,2.073342800140381,2.0775537490844727,2.0790698528289795,2.0802628993988037,2.079667806625366,2.075634717941284,2.0729024410247803,2.072169303894043,2.0745561122894287,2.0776431560516357,2.080899238586426,2.0835824012756348,2.0864195823669434,2.0902140140533447,2.0933825969696045,2.098050832748413,2.100475311279297,2.101245403289795,2.101536989212036,2.1027984619140625,2.104443311691284,2.103900671005249,2.102607250213623,2.100043296813965,2.098562717437744,2.097700357437134,2.096165895462036,2.0959627628326416,2.0991578102111816,2.099933624267578,2.099489688873291,2.1001198291778564,2.100088596343994,2.097855567932129,2.0957939624786377,2.094130277633667,2.092769145965576,2.0914199352264404,2.0910773277282715,2.0937588214874268,2.0988552570343018,2.1035332679748535,2.1067492961883545,2.1082301139831543,2.105280876159668,2.105928421020508,2.104799270629883,2.1033973693847656,2.101052761077881,2.098430633544922,2.0967376232147217,2.0974690914154053,2.102538824081421,2.1083550453186035,2.111539840698242,2.111762046813965,2.108659267425537,2.10524582862854,2.1016650199890137,2.099518299102783,2.0983424186706543,2.098021984100342,2.101173162460327,2.104931592941284,2.108107566833496,2.1081464290618896,2.1043944358825684,2.1004960536956787,2.0978357791900635,2.098043918609619,2.0996904373168945,2.0995304584503174,2.0983142852783203,2.098021984100342,2.0991032123565674,2.0988729000091553,2.102144956588745,2.1069440841674805,2.111217737197876,2.117063522338867,2.1174609661102295,2.1163179874420166,2.114320755004883,2.112109422683716,2.11085844039917,2.111748695373535,2.1144585609436035,2.120023727416992,2.1255695819854736,2.130331039428711,2.1315996646881104,2.132617235183716,2.1319284439086914,2.132038116455078,2.1310553550720215,2.1296653747558594,2.1266727447509766,2.124573230743408,2.124086618423462,2.123180389404297,2.124660015106201,2.128629684448242,2.1316449642181396,2.134641647338867,2.1356089115142822,2.136127471923828,2.137367010116577,2.1365768909454346,2.1334781646728516,2.127990484237671,2.126474618911743,2.127195358276367,2.127545118331909,2.129270553588867,2.1280786991119385,2.1260855197906494,2.124189615249634,2.122749090194702,2.1234171390533447,2.126739978790283,2.1312079429626465,2.1335909366607666,2.1323089599609375,2.126227855682373,2.121973991394043,2.1190993785858154,2.116778612136841,2.113814115524292,2.112677574157715,2.1097710132598877,2.108851909637451,2.111229419708252,2.114981174468994,2.1199214458465576,2.1236443519592285,2.1256117820739746,2.125697135925293,2.1222193241119385,2.115809440612793,2.1101858615875244,2.106926202774048,2.106250047683716,2.1071555614471436,2.110731363296509,2.115802049636841,2.121474266052246,2.1254889965057373,2.120978355407715,2.1133174896240234,2.1049129962921143,2.1018900871276855,2.100710868835449,2.101832866668701,2.1024510860443115,2.1040725708007812,2.105506658554077,2.103090286254883,2.099868059158325,2.0976715087890625,2.095350742340088,2.0942718982696533,2.094210147857666,2.0973761081695557,2.101372718811035,2.105848789215088,2.1090967655181885,2.110304594039917,2.109879493713379,2.108447313308716,2.106426954269409,2.1075196266174316,2.1090941429138184,2.112481117248535,2.114316463470459,2.118394613265991,2.121851682662964,2.125007390975952,2.1263585090637207,2.123499631881714,2.1177942752838135,2.1106085777282715,2.1045758724212646,2.1035654544830322,2.1074485778808594,2.1135454177856445,2.1204960346221924,2.126046657562256,2.1301956176757812,2.128668785095215,2.124156951904297,2.11777925491333,2.1136748790740967,2.113582134246826,2.116000175476074,2.1204309463500977,2.127563714981079,2.1330795288085938,2.1352689266204834,2.134786367416382,2.1303350925445557,2.126275062561035,2.123901128768921,2.123411178588867,2.1240241527557373,2.1263091564178467,2.1294405460357666,2.132526397705078,2.1349451541900635,2.1318411827087402,2.128180742263794,2.1251840591430664,2.1213595867156982,2.116745710372925,2.1144115924835205,2.1154067516326904,2.1167497634887695,2.12133526802063,2.128077745437622,2.1341919898986816,2.137424945831299,2.136030435562134,2.131298065185547,2.1235101222991943,2.11651873588562,2.111984968185425,2.1094586849212646,2.110626220703125,2.1149635314941406,2.120558500289917,2.1257925033569336,2.131286382675171,2.132600784301758,2.131826162338257,2.130194902420044,2.128455877304077,2.1250338554382324,2.123802661895752,2.1218953132629395,2.122210741043091,2.123800277709961,2.123826503753662,2.122973680496216,2.120094060897827,2.1154820919036865,2.1101832389831543,2.1034064292907715,2.0966007709503174,2.0925421714782715,2.0914719104766846,2.092670202255249,2.097454071044922,2.1039841175079346,2.1108055114746094,2.1167757511138916,2.120527744293213,2.1216347217559814,2.123706102371216,2.123825788497925,2.122462034225464,2.1231069564819336,2.1264679431915283,2.1315314769744873,2.139173746109009,2.1463282108306885,2.1522185802459717,2.1542842388153076,2.153089761734009,2.1509878635406494,2.1477863788604736,2.1442036628723145,2.1391236782073975,2.134223699569702,2.1304423809051514,2.128666639328003,2.129176139831543,2.1302096843719482,2.132826089859009,2.135385036468506,2.138791084289551,2.1409661769866943,2.1424732208251953,2.1433799266815186,2.144766330718994,2.144969940185547,2.144897937774658,2.1442131996154785,2.142427921295166,2.140371084213257,2.13901686668396,2.1368796825408936,2.1347713470458984,2.1341934204101562,2.1354591846466064,2.1359903812408447,2.135479688644409,2.135000705718994,2.136068820953369,2.1368603706359863,2.1370604038238525,2.136029005050659,2.134718179702759,2.1327738761901855,2.1305952072143555,2.128541946411133,2.127466917037964,2.126847267150879,2.12507700920105,2.1256721019744873,2.124873638153076,2.1243252754211426,2.124168634414673,2.1242504119873047,2.124642848968506,2.126354932785034,2.126603841781616,2.127992630004883,2.1295793056488037,2.1303212642669678,2.1305038928985596,2.1308324337005615,2.1310558319091797,2.1305487155914307,2.129406690597534,2.1274943351745605,2.1260130405426025,2.124504566192627,2.122174024581909,2.1207191944122314,2.121375322341919,2.1228349208831787,2.124795436859131,2.1275224685668945,2.129909038543701,2.1324596405029297,2.1341092586517334,2.1359028816223145,2.1386377811431885,2.140058755874634,2.1388349533081055,2.1371660232543945,2.1355249881744385,2.1350343227386475,2.1344263553619385,2.134989023208618,2.135383367538452,2.1372299194335938,2.139996290206909,2.143298387527466,2.1447842121124268,2.1469004154205322,2.1479103565216064,2.146953821182251,2.1448452472686768,2.140752077102661,2.1361851692199707,2.1323060989379883,2.12921142578125,2.126493453979492,2.1241495609283447,2.122515916824341,2.1221299171447754,2.123878002166748,2.127202272415161,2.129920244216919,2.131822109222412,2.1328136920928955,2.1326608657836914,2.1326682567596436,2.130889415740967,2.1285343170166016,2.125396251678467,2.1229679584503174,2.1225454807281494,2.1228995323181152,2.124783515930176,2.12725567817688,2.130429744720459,2.1329941749572754,2.132220506668091,2.1302175521850586,2.127415418624878,2.124227285385132,2.1203877925872803,2.116450309753418,2.1145431995391846,2.1141929626464844,2.113712787628174,2.114093780517578,2.1144492626190186,2.1149044036865234,2.115295886993408,2.1161580085754395,2.117938756942749,2.1188113689422607,2.1193065643310547,2.1198580265045166,2.12121319770813,2.1224493980407715,2.1240367889404297,2.1254777908325195,2.125066041946411,2.124077558517456,2.122194528579712,2.12115740776062,2.1199440956115723,2.1186299324035645,2.1173107624053955,2.1164777278900146,2.116929769515991,2.1167006492614746,2.1172163486480713,2.1191534996032715,2.1212337017059326,2.122652292251587,2.1245012283325195,2.1252241134643555,2.124293088912964,2.123098134994507,2.1207263469696045,2.1184470653533936,2.116302251815796,2.1150622367858887,2.1154048442840576,2.1164779663085938,2.118385076522827,2.120473623275757,2.122408151626587,2.1248393058776855,2.126682758331299,2.127793788909912,2.1292150020599365,2.131499767303467,2.132899045944214,2.1344172954559326,2.1351072788238525,2.1343138217926025,2.133253335952759,2.1312623023986816,2.1314308643341064,2.1319642066955566,2.131957769393921,2.132601261138916,2.1335883140563965,2.134800672531128,2.1361711025238037,2.1371819972991943,2.138206958770752,2.138848304748535,2.1401782035827637,2.14223313331604,2.1424853801727295,2.1429038047790527,2.143507719039917,2.1434688568115234,2.142944574356079,2.1421051025390625,2.140516757965088,2.1366896629333496,2.1324329376220703,2.1293649673461914,2.127345085144043,2.126091241836548,2.1255717277526855,2.127086639404297,2.128365993499756,2.129757881164551,2.132431745529175,2.1361470222473145,2.13865065574646,2.1406214237213135,2.142216444015503,2.1425936222076416,2.1429214477539062,2.1415457725524902,2.139432191848755,2.1372146606445312,2.1351542472839355,2.133098840713501,2.130018472671509,2.126786470413208,2.1247079372406006,2.124315023422241,2.1253092288970947,2.126934766769409,2.1285574436187744,2.1299076080322266,2.13154935836792,2.132993698120117,2.1337218284606934,2.1339190006256104,2.1320343017578125,2.1299619674682617,2.127504587173462,2.124453544616699,2.122741222381592,2.1222116947174072,2.1229586601257324,2.124452829360962,2.126790761947632,2.1293559074401855,2.130852699279785,2.1312406063079834,2.1304266452789307,2.129768133163452,2.128533363342285,2.127215623855591,2.1267244815826416,2.126957654953003,2.1259994506835938,2.1249101161956787,2.1241345405578613,2.1244382858276367,2.1254401206970215,2.127129316329956,2.1289682388305664,2.130457639694214,2.1314852237701416,2.1321160793304443,2.1325466632843018,2.1318864822387695,2.1316959857940674,2.131568670272827,2.1327672004699707,2.13362717628479,2.135014057159424,2.1372432708740234,2.138871908187866,2.1413304805755615,2.143479108810425,2.144883871078491,2.1440131664276123,2.140655994415283,2.1379573345184326,2.1370718479156494,2.136611223220825,2.1367642879486084,2.137077569961548,2.1375417709350586,2.1389994621276855,2.1396095752716064,2.1402764320373535,2.14136004447937,2.140153646469116,2.1378345489501953,2.1353678703308105,2.133570432662964,2.1332099437713623,2.1343021392822266,2.1356303691864014,2.1362521648406982,2.1362245082855225,2.1346662044525146,2.134068250656128,2.133481740951538,2.133260726928711,2.1326334476470947,2.131704092025757,2.130728244781494,2.130459785461426,2.13106369972229,2.131887674331665,2.1318306922912598,2.131344795227051,2.1315319538116455,2.1319425106048584,2.1327719688415527,2.133369207382202,2.13423228263855,2.1353979110717773,2.1373050212860107,2.138173818588257,2.1386349201202393,2.138014793395996,2.1355299949645996,2.133592367172241,2.132753372192383,2.1331048011779785,2.1343770027160645,2.1368629932403564,2.1399850845336914,2.1428887844085693,2.1465795040130615,2.149036407470703,2.1521642208099365,2.153642177581787,2.1548142433166504,2.1556222438812256,2.1562769412994385,2.155710220336914,2.1543257236480713,2.152022361755371,2.148991823196411,2.146559715270996,2.1454811096191406,2.1452550888061523,2.146017074584961,2.1470112800598145,2.1470677852630615,2.1472978591918945,2.1476545333862305,2.147921562194824,2.148577928543091,2.1496241092681885,2.150423288345337,2.1509506702423096,2.1507067680358887,2.1499578952789307,2.148529529571533,2.1468091011047363,2.1456148624420166,2.1449694633483887,2.1459062099456787,2.1474363803863525,2.149700403213501,2.1517317295074463,2.1526148319244385,2.1529457569122314,2.1523640155792236,2.152153730392456,2.151853084564209,2.1503098011016846,2.1479711532592773,2.146190643310547,2.144461154937744,2.1421453952789307,2.139989137649536,2.1388325691223145,2.1379308700561523,2.13802170753479,2.139862298965454,2.1424753665924072,2.145537853240967,2.148322343826294,2.149954080581665,2.1521122455596924,2.152369976043701,2.1521053314208984,2.151085138320923,2.1489791870117188,2.1460349559783936,2.143357992172241,2.142240524291992,2.141522169113159,2.1424691677093506,2.1436896324157715,2.145596981048584,2.1474533081054688,2.1479430198669434,2.1475393772125244,2.1458277702331543,2.1440536975860596,2.14209246635437,2.14056658744812,2.1394248008728027,2.1389269828796387,2.1384077072143555,2.139019012451172,2.1402506828308105,2.142137289047241,2.1436853408813477,2.1460981369018555,2.14772367477417,2.1493844985961914,2.149445056915283,2.1480462551116943,2.1457502841949463,2.1436312198638916,2.1430952548980713,2.142979860305786,2.1425116062164307,2.14296293258667,2.1436023712158203,2.1438283920288086,2.1450679302215576,2.1453874111175537,2.145169973373413,2.1455957889556885,2.1473875045776367,2.149038314819336,2.149498462677002,2.1507647037506104,2.1511619091033936,2.1503262519836426,2.1494688987731934,2.149070978164673,2.149878740310669,2.149866819381714,2.148543357849121,2.1465706825256348,2.143742799758911,2.1410388946533203,2.140507698059082,2.140925645828247,2.143017530441284,2.1454250812530518,2.1478941440582275,2.1502907276153564,2.1514158248901367,2.151048421859741,2.15065336227417,2.150362730026245,2.1492581367492676,2.147930860519409,2.147660732269287,2.148012638092041,2.1481635570526123,2.149405002593994,2.1502833366394043,2.1520626544952393,2.1530187129974365,2.1532235145568848,2.152812957763672,2.1518566608428955,2.1513540744781494,2.1514031887054443,2.15164852142334,2.1523454189300537,2.1530041694641113,2.1535658836364746,2.153640031814575,2.1550843715667725,2.15640926361084,2.1587276458740234,2.1595799922943115,2.1603455543518066,2.1616666316986084,2.161771774291992,2.161625385284424,2.1598148345947266,2.1582393646240234,2.156465768814087,2.1549673080444336,2.1528306007385254,2.151425838470459,2.1503868103027344,2.1491620540618896,2.148451089859009,2.149118661880493,2.148912191390991,2.1494176387786865,2.150006055831909,2.15082049369812,2.1519627571105957,2.1535158157348633,2.154630661010742,2.15303111076355,2.150049924850464,2.146637439727783,2.144662380218506,2.1433656215667725,2.1424508094787598,2.1429972648620605,2.144090414047241,2.145834445953369,2.1499409675598145,2.154906988143921,2.159572124481201,2.162899971008301,2.165576696395874,2.166598081588745,2.1645054817199707,2.162559986114502,2.1608288288116455,2.1597442626953125,2.159376859664917,2.1600983142852783,2.1609432697296143,2.163599967956543,2.1661946773529053,2.1674129962921143,2.1661922931671143,2.1659066677093506,2.1649954319000244,2.1627278327941895,2.1606242656707764,2.158703327178955,2.158142566680908,2.1579489707946777,2.1591601371765137,2.1603763103485107,2.164181709289551,2.1681060791015625,2.171865940093994,2.174839496612549,2.1774210929870605,2.17910099029541,2.180152654647827,2.180685043334961,2.181098699569702,2.180058002471924,2.179119110107422,2.177618980407715,2.175097942352295,2.1726746559143066,2.171452045440674,2.1700379848480225,2.1695117950439453,2.1696929931640625,2.170910358428955,2.1729133129119873,2.175161838531494,2.1777334213256836,2.18053936958313,2.1821084022521973,2.181889057159424,2.1805152893066406,2.17941951751709,2.1772191524505615,2.174938201904297,2.173365354537964,2.1715750694274902,2.169982433319092,2.169611692428589,2.169597625732422,2.1712002754211426,2.17215633392334,2.1737313270568848,2.1733551025390625,2.173001766204834,2.174459218978882,2.175638437271118,2.1768081188201904,2.178781032562256,2.1797120571136475,2.182004928588867,2.183086633682251,2.183086395263672,2.182483196258545,2.1812663078308105,2.17997670173645,2.178579092025757,2.176654815673828,2.174555540084839,2.172865152359009,2.171114444732666,2.169269561767578,2.1673600673675537,2.166057586669922,2.1651039123535156,2.165360927581787,2.165956974029541,2.1677849292755127,2.170684576034546,2.17435359954834,2.176203966140747,2.176800489425659,2.1774163246154785,2.1777589321136475,2.17846417427063,2.179511070251465,2.1806399822235107,2.18144154548645,2.1808433532714844,2.180697441101074,2.1803762912750244,2.180881977081299,2.1821112632751465,2.1826727390289307,2.184948682785034,2.186554431915283,2.187364101409912,2.1867690086364746,2.186168909072876,2.1845524311065674,2.1823811531066895,2.1819097995758057,2.1830921173095703,2.184824228286743,2.185689926147461,2.1865932941436768,2.186735153198242,2.1862680912017822,2.1853787899017334,2.185161828994751,2.184791088104248,2.1848881244659424,2.1851584911346436,2.185317039489746,2.1849541664123535,2.18621563911438,2.1870193481445312,2.186962604522705,2.1851067543029785,2.1835246086120605,2.1828746795654297,2.181342840194702,2.1805927753448486,2.1808414459228516,2.1825761795043945,2.184044599533081,2.185723304748535,2.186393976211548,2.187297821044922,2.1879494190216064,2.187462091445923,2.1878304481506348,2.189490556716919,2.1893250942230225,2.188544988632202,2.18929123878479,2.1887481212615967,2.187494993209839,2.1863865852355957,2.185920476913452,2.1864044666290283,2.1867716312408447,2.186074733734131,2.185842990875244,2.1848132610321045,2.184236764907837,2.181835412979126,2.180812358856201,2.180647611618042,2.18204402923584,2.1844730377197266,2.1847214698791504,2.1855762004852295,2.185155153274536,2.184710741043091,2.1833033561706543,2.1802592277526855,2.178049325942993,2.1752102375030518,2.173729419708252,2.1712536811828613,2.1691677570343018,2.166691303253174,2.1663873195648193,2.1683292388916016,2.169188976287842,2.17053484916687,2.1710915565490723,2.170984983444214,2.1696178913116455,2.1669459342956543,2.164301633834839,2.1632235050201416,2.162808656692505,2.1628925800323486,2.1630547046661377,2.164266586303711,2.165524482727051,2.168308734893799,2.171259880065918,2.1726315021514893,2.171921968460083,2.171401262283325,2.170621871948242,2.170632839202881,2.1716578006744385,2.174159049987793,2.1764655113220215,2.178457498550415,2.1803321838378906,2.1800785064697266,2.1783688068389893,2.1752982139587402,2.172586679458618,2.169903516769409,2.169025421142578,2.169111490249634,2.1693003177642822,2.169992685317993,2.1702141761779785,2.171109199523926,2.170957326889038,2.171320676803589,2.173797130584717,2.176321506500244,2.178478479385376,2.179387331008911,2.1801745891571045,2.181652069091797,2.182648181915283,2.1837642192840576,2.1839449405670166,2.1830973625183105,2.1830410957336426,2.1829748153686523,2.182762861251831,2.182178258895874,2.1816816329956055,2.181243419647217,2.1808929443359375,2.1811957359313965,2.1825506687164307,2.184288740158081,2.1858458518981934,2.1857404708862305,2.1857855319976807,2.1872270107269287,2.1873931884765625,2.187206983566284,2.188509702682495,2.188559055328369,2.1880862712860107,2.1880924701690674,2.1876792907714844,2.187821388244629,2.1876637935638428,2.186973810195923,2.1853785514831543,2.1854512691497803,2.1846349239349365,2.1842691898345947,2.184408187866211,2.184455394744873,2.184363842010498,2.1846587657928467,2.184488296508789,2.1842575073242188,2.18332576751709,2.1827757358551025,2.181370973587036,2.1793580055236816,2.178811550140381,2.17913818359375,2.1798593997955322,2.1817736625671387,2.184893846511841,2.18705677986145,2.1896893978118896,2.192349910736084,2.1945576667785645,2.1971957683563232,2.1977152824401855,2.1969146728515625,2.1961300373077393,2.1942527294158936,2.1937599182128906,2.1930532455444336,2.191922426223755,2.1915318965911865,2.1908586025238037,2.1895384788513184,2.1900525093078613,2.1901257038116455,2.1894917488098145,2.1908764839172363,2.192462682723999,2.1936373710632324,2.195091724395752,2.1949501037597656,2.193652391433716,2.1914637088775635,2.1893608570098877,2.187329053878784,2.184234380722046,2.182339906692505,2.180468797683716,2.180788993835449,2.1814301013946533,2.181851625442505,2.182722330093384,2.184100389480591,2.185120105743408,2.1850380897521973,2.1845149993896484,2.183087110519409,2.1832640171051025,2.181657075881958,2.180302143096924,2.1800413131713867,2.181547164916992,2.183109760284424,2.1853184700012207,2.1877686977386475,2.190735101699829,2.1925230026245117,2.194434642791748,2.195474147796631,2.1969122886657715,2.196465492248535,2.196512460708618,2.1946964263916016,2.1915528774261475,2.1898531913757324,2.1882975101470947,2.1871776580810547,2.1871371269226074,2.188450336456299,2.1909265518188477,2.194002866744995,2.1975743770599365,2.2009501457214355,2.2040154933929443,2.206648111343384,2.2071614265441895,2.2060110569000244,2.202169895172119,2.1972897052764893,2.1923024654388428,2.188814640045166,2.1857969760894775,2.184206485748291,2.184030294418335,2.1852705478668213,2.188516855239868,2.193010091781616,2.198833465576172,2.204441547393799,2.2090539932250977,2.211240768432617,2.211333751678467,2.2090442180633545,2.20609188079834,2.2030558586120605,2.200777769088745,2.1995084285736084,2.1994752883911133,2.1990888118743896,2.20034122467041,2.2015159130096436,2.203829050064087,2.207078695297241,2.209329128265381,2.2115774154663086,2.2144908905029297,2.218003988265991,2.2208027839660645,2.222188949584961,2.2221527099609375,2.222226858139038,2.222363233566284,2.222728967666626,2.2231884002685547,2.2251334190368652,2.22786545753479,2.230138063430786,2.232224464416504,2.235151767730713,2.237306594848633,2.2380757331848145,2.2370247840881348,2.236330032348633,2.234104633331299,2.2312510013580322,2.229233503341675,2.2269585132598877,2.2250683307647705,2.2239179611206055,2.222540855407715,2.223701000213623,2.2252442836761475,2.227609634399414,2.2303004264831543,2.233999729156494,2.2368788719177246,2.23856258392334,2.239441394805908,2.239938259124756,2.239370107650757,2.2388947010040283,2.237637519836426,2.235511302947998,2.2326550483703613,2.230524778366089,2.2267515659332275,2.2228639125823975,2.220215320587158,2.218189239501953,2.217294931411743,2.2172586917877197,2.218921661376953,2.2204601764678955,2.2218284606933594,2.2231767177581787,2.223367691040039,2.222465753555298,2.2208356857299805,2.219320058822632,2.2179646492004395,2.2177059650421143,2.217695474624634,2.2180466651916504,2.2184183597564697,2.216008424758911,2.2131967544555664,2.210615873336792,2.208966016769409,2.207379102706909,2.2051453590393066,2.20322322845459,2.2022716999053955,2.2027711868286133,2.203125,2.2033655643463135,2.203138589859009,2.202099561691284,2.2020299434661865,2.201523542404175,2.2015702724456787,2.2009997367858887,2.2012832164764404,2.2033839225769043,2.204127073287964,2.2067129611968994,2.2092363834381104,2.21301531791687,2.2141077518463135,2.214031219482422,2.2129106521606445,2.211029529571533,2.2100112438201904,2.210376262664795,2.212312698364258,2.2150304317474365,2.217174530029297,2.2210681438446045,2.2251508235931396,2.230156421661377,2.2315523624420166,2.232396364212036,2.231660842895508,2.230478286743164,2.229722738265991,2.2282803058624268,2.2281956672668457,2.2284932136535645,2.2290194034576416,2.2302401065826416,2.2304835319519043,2.229975461959839,2.229898691177368,2.2277472019195557,2.2263970375061035,2.225612163543701,2.2257256507873535,2.2243874073028564,2.2227559089660645,2.222426652908325,2.2234325408935547,2.2248544692993164,2.2270634174346924,2.228813409805298,2.230254650115967,2.231189250946045,2.232558488845825,2.2343459129333496,2.2365176677703857,2.2391090393066406,2.2403945922851562,2.239873170852661,2.2384917736053467,2.23750638961792,2.2368736267089844,2.237420082092285,2.2372844219207764,2.2367308139801025,2.236602306365967,2.2374267578125,2.238107442855835,2.238734006881714,2.238286018371582,2.2363996505737305,2.2351577281951904,2.233961820602417,2.2318944931030273,2.228443145751953,2.224088191986084,2.2202203273773193,2.2180933952331543,2.217843532562256,2.2182252407073975,2.2193057537078857,2.220953941345215,2.2219300270080566,2.223468780517578,2.2249884605407715,2.2251343727111816,2.2240865230560303,2.2223849296569824,2.2215704917907715,2.221822500228882,2.2238094806671143,2.227051019668579,2.229703664779663,2.2312216758728027,2.2335453033447266,2.2341368198394775,2.232182264328003,2.2290210723876953,2.226925849914551,2.22593355178833,2.2249605655670166,2.225407838821411,2.225236177444458,2.225153684616089,2.2243945598602295,2.2243828773498535,2.224391460418701,2.2255167961120605,2.226389169692993,2.2266788482666016,2.227149724960327,2.2281746864318848,2.229046583175659,2.2299611568450928,2.2299463748931885,2.230572462081909,2.2303311824798584,2.229593515396118,2.2283847332000732,2.228229284286499,2.228394031524658,2.2293527126312256,2.2294986248016357,2.228710889816284,2.2277936935424805,2.2276294231414795,2.2270469665527344,2.2271337509155273,2.2271385192871094,2.227806329727173,2.2290825843811035,2.231086492538452,2.2329370975494385,2.2349343299865723,2.236325740814209,2.2384798526763916,2.240535259246826,2.241713523864746,2.242664337158203,2.2421646118164062,2.2417116165161133,2.239774703979492,2.237851858139038,2.2359490394592285,2.2331128120422363,2.2312824726104736,2.2281646728515625,2.226059675216675,2.22475266456604,2.2237279415130615,2.223489284515381,2.2245147228240967,2.225348472595215,2.224409580230713,2.222726345062256,2.2204174995422363,2.2191648483276367,2.217569589614868,2.217283248901367,2.2178332805633545,2.218461751937866,2.2184956073760986,2.2185237407684326,2.2165157794952393,2.2117815017700195,2.20705246925354,2.203481435775757,2.201474666595459,2.200840473175049,2.2016892433166504,2.2025554180145264,2.2038440704345703,2.2052831649780273,2.206536293029785,2.2082736492156982,2.2096238136291504,2.2119345664978027,2.2128586769104004,2.213278293609619,2.213273286819458,2.210925817489624,2.208134174346924,2.205906629562378,2.20430850982666,2.203228712081909,2.200960159301758,2.2006406784057617,2.2015979290008545,2.202454090118408,2.2042860984802246,2.2063968181610107,2.2091405391693115,2.2130777835845947,2.215076446533203,2.215867757797241,2.215500593185425,2.214822292327881,2.213071346282959,2.2118494510650635,2.2102763652801514,2.2075347900390625,2.2069716453552246,2.207707405090332,2.209223747253418,2.2122244834899902,2.21539044380188,2.2179200649261475,2.220306873321533,2.2216925621032715,2.222079038619995,2.2209010124206543,2.2187716960906982,2.216993570327759,2.214235305786133,2.2112176418304443,2.2104380130767822,2.210606575012207,2.211514711380005,2.2141499519348145,2.2170820236206055,2.218820095062256,2.2201106548309326,2.2198903560638428,2.219836473464966,2.218374013900757,2.2157862186431885,2.2147626876831055,2.2155184745788574,2.21663761138916,2.2180497646331787,2.2191152572631836,2.219050168991089,2.2182765007019043,2.217412233352661,2.2164344787597656,2.215601682662964,2.2153801918029785,2.216447591781616,2.2177162170410156,2.2189154624938965,2.2202887535095215,2.222167491912842,2.222202777862549,2.2215592861175537,2.219817638397217,2.216984987258911,2.2149877548217773,2.212852954864502,2.21174693107605,2.211697578430176,2.2127809524536133,2.214698553085327,2.217353343963623,2.2199032306671143,2.22182559967041,2.223273754119873,2.2259087562561035,2.226882219314575,2.228736400604248,2.229612112045288,2.228989601135254,2.2291533946990967,2.2270874977111816,2.2250378131866455,2.223783493041992,2.222720146179199,2.2207133769989014,2.2186684608459473,2.2168333530426025,2.215832233428955,2.2143547534942627,2.2126946449279785,2.2116622924804688,2.2112293243408203,2.2106478214263916,2.2103052139282227,2.2080719470977783,2.206287145614624,2.2057433128356934,2.205841541290283,2.2063918113708496,2.207082748413086,2.207514524459839,2.2081499099731445,2.208933115005493,2.210031747817993,2.2112791538238525,2.2113966941833496,2.210179090499878,2.210073709487915,2.209179162979126,2.207578659057617,2.206400156021118,2.2063162326812744,2.2062740325927734,2.2067623138427734,2.20731782913208,2.207688808441162,2.2068424224853516,2.206188201904297,2.2047290802001953,2.2038021087646484,2.200929880142212,2.1979784965515137,2.19541335105896,2.1950161457061768,2.194854259490967,2.195478677749634,2.1959517002105713,2.1959638595581055,2.195188045501709,2.1954309940338135,2.196035385131836,2.1950879096984863,2.194239616394043,2.193279266357422,2.192551612854004,2.191051959991455,2.1903717517852783,2.189577579498291,2.1896138191223145,2.191784620285034,2.193635940551758,2.195966958999634,2.1984426975250244,2.2014288902282715,2.2036492824554443,2.206437826156616,2.2085306644439697,2.208411693572998,2.2070531845092773,2.2033259868621826,2.1991450786590576,2.195286273956299,2.19183349609375,2.19118595123291,2.190950870513916,2.1920113563537598,2.193641424179077,2.1945900917053223,2.1961417198181152,2.196448802947998,2.196481704711914,2.1965556144714355,2.1958470344543457,2.195578098297119,2.1953206062316895,2.1964383125305176,2.196059465408325,2.195563316345215,2.1951160430908203,2.1936047077178955,2.191598415374756,2.1907835006713867,2.190308094024658,2.190056800842285,2.191803455352783,2.1943106651306152,2.196427583694458,2.1976430416107178,2.19693660736084,2.1950652599334717,2.19260311126709,2.1897802352905273,2.1867775917053223,2.183631658554077,2.1806540489196777,2.178922414779663,2.1796681880950928,2.1824963092803955,2.1833667755126953,2.1849865913391113,2.185650110244751,2.1857645511627197,2.1869311332702637,2.187779664993286,2.187716007232666,2.1873257160186768,2.18656063079834,2.1854724884033203,2.185258388519287,2.1854352951049805,2.185800552368164,2.1858181953430176,2.1863296031951904,2.18562388420105,2.185732841491699,2.1856889724731445,2.1858065128326416,2.1856698989868164,2.1858890056610107,2.185699701309204,2.1852569580078125,2.1861112117767334,2.187368631362915,2.188610076904297,2.190490484237671,2.1909711360931396,2.1908180713653564,2.191352605819702,2.191681385040283,2.192868709564209,2.1945321559906006,2.1967549324035645,2.1988906860351562,2.2022452354431152,2.2057158946990967,2.2077367305755615,2.2083141803741455,2.20754075050354,2.2057747840881348,2.2022716999053955,2.199385166168213,2.196789026260376,2.1951282024383545,2.193758726119995,2.19366455078125,2.194566488265991,2.1955299377441406,2.1960253715515137,2.1981053352355957,2.199578046798706,2.200559377670288,2.2017059326171875,2.2031614780426025,2.2036080360412598,2.202920436859131,2.202430248260498,2.2015440464019775,2.2013251781463623,2.202143907546997,2.2031564712524414,2.2029778957366943,2.2017931938171387,2.200857162475586,2.200089693069458,2.1990063190460205,2.1984572410583496,2.197018623352051,2.1961658000946045,2.1973915100097656,2.2008869647979736,2.204270839691162,2.20745587348938,2.208801031112671,2.208746910095215,2.2097835540771484,2.209857225418091,2.2093822956085205,2.208953380584717,2.208853244781494,2.208228349685669,2.2070605754852295,2.2055375576019287,2.2045838832855225,2.2037875652313232,2.204498052597046,2.2051122188568115,2.205228328704834,2.2048227787017822,2.203195333480835,2.202706813812256,2.20377516746521,2.2047505378723145,2.2034735679626465,2.2014822959899902,2.1986677646636963,2.1972570419311523,2.195039987564087,2.194852828979492,2.196606397628784,2.1994855403900146,2.2022924423217773,2.205517530441284,2.208684206008911,2.209577798843384,2.207218647003174,2.204115152359009,2.2024874687194824,2.2002360820770264,2.2003090381622314,2.1990721225738525,2.199232816696167,2.1993157863616943,2.19976806640625,2.200427293777466,2.20084285736084,2.2000463008880615,2.201141834259033,2.201651096343994,2.2009055614471436,2.199517250061035,2.1983208656311035,2.197171449661255,2.1950485706329346,2.193192958831787,2.191948652267456,2.1913294792175293,2.19120192527771,2.1913626194000244,2.192488670349121,2.193793296813965,2.194122552871704,2.1952948570251465,2.196223020553589,2.1962063312530518,2.1954469680786133,2.194180965423584,2.191866636276245,2.188162326812744,2.1841418743133545,2.181213617324829,2.1800074577331543,2.180851459503174,2.183490037918091,2.1861894130706787,2.1891679763793945,2.19292950630188,2.197239398956299,2.2011985778808594,2.2046194076538086,2.2060370445251465,2.2056310176849365,2.202147960662842,2.1994478702545166,2.1979422569274902,2.196197986602783,2.195822238922119,2.1938891410827637,2.192692995071411,2.192596197128296,2.1926300525665283,2.193429470062256,2.195099115371704,2.1968994140625,2.197233200073242,2.196307420730591,2.1965105533599854,2.198375701904297,2.2001471519470215,2.2022244930267334,2.205737352371216,2.2097625732421875,2.2135262489318848,2.215329647064209,2.214853525161743,2.212716579437256,2.209620237350464,2.206218719482422,2.201773166656494,2.1968209743499756,2.194065570831299,2.193026304244995,2.1931443214416504,2.1950843334198,2.196460485458374,2.1986823081970215,2.20165753364563,2.2046985626220703,2.2069971561431885,2.2070114612579346,2.2070441246032715,2.207634687423706,2.209548234939575,2.2132086753845215,2.2158122062683105,2.2175424098968506,2.2184674739837646,2.218609094619751,2.217733860015869,2.2147927284240723,2.2120747566223145,2.2096426486968994,2.208198308944702,2.2078239917755127,2.208052396774292,2.208387613296509,2.2083096504211426,2.208803176879883,2.2109527587890625,2.213484048843384,2.2161574363708496,2.217057704925537,2.2183473110198975,2.2192561626434326,2.2201766967773438,2.2223823070526123,2.219789981842041,2.2183008193969727,2.2148900032043457,2.212441921234131,2.2094383239746094,2.205838680267334,2.2041571140289307,2.204925060272217,2.204800844192505,2.205191135406494,2.2071592807769775,2.2077667713165283,2.2052805423736572,2.2038607597351074,2.20100998878479,2.2013139724731445,2.204265594482422,2.2068941593170166,2.206364870071411,2.2044925689697266,2.2049765586853027,2.206359386444092,2.2081546783447266,2.208918333053589,2.2119247913360596,2.212489366531372,2.2122178077697754,2.212735414505005,2.2146902084350586,2.2141172885894775,2.2120771408081055,2.212656259536743,2.21055006980896,2.207796812057495,2.206057071685791,2.205613136291504,2.202568769454956,2.2000961303710938,2.199615240097046,2.2019147872924805,2.2018961906433105,2.200944662094116,2.1983091831207275,2.19752836227417,2.1978142261505127,2.198198080062866,2.1977908611297607,2.1942951679229736,2.1890788078308105,2.1842408180236816,2.183574676513672,2.185701370239258,2.1906819343566895,2.195301055908203,2.2004780769348145,2.206688404083252,2.211308717727661,2.210306167602539,2.2089345455169678,2.2090935707092285,2.2103612422943115,2.2123820781707764,2.213683605194092,2.213860511779785,2.215449810028076,2.2193307876586914,2.224378824234009,2.226287603378296,2.2263295650482178,2.2246792316436768,2.2191500663757324,2.2170827388763428,2.218357563018799,2.2196574211120605,2.223320960998535,2.225315570831299,2.2236013412475586,2.2211413383483887,2.2159807682037354,2.2121243476867676,2.2083001136779785,2.208144426345825,2.2109830379486084,2.2175090312957764,2.2238903045654297,2.226391553878784,2.2233123779296875,2.217231035232544,2.210371732711792,2.2067806720733643,2.2064261436462402,2.210230588912964,2.215662956237793,2.219226360321045,2.220203161239624,2.2208526134490967,2.2221832275390625,2.224696159362793,2.22529673576355,2.223386764526367,2.2207984924316406,2.2202889919281006,2.2190051078796387,2.219135046005249,2.2178642749786377,2.2154674530029297,2.2129197120666504,2.209968328475952,2.208897829055786,2.2092270851135254,2.2109317779541016,2.213752508163452,2.217794179916382,2.2210655212402344,2.2264139652252197,2.227782726287842,2.2306623458862305,2.2323930263519287,2.2334182262420654,2.232949733734131,2.232618570327759,2.2328908443450928,2.230808973312378,2.2285850048065186,2.2307302951812744,2.2316431999206543,2.230177640914917,2.230347156524658,2.229241371154785,2.2267327308654785,2.2260472774505615,2.225834608078003,2.2282679080963135,2.2286109924316406,2.226933002471924,2.22422456741333,2.220871686935425,2.218411684036255,2.2160136699676514,2.2169079780578613,2.2192580699920654,2.2251200675964355,2.2322723865509033,2.2410619258880615,2.2455196380615234,2.2455639839172363,2.243220090866089,2.2384636402130127,2.2313292026519775,2.2257087230682373,2.221273183822632,2.2206830978393555,2.223918914794922,2.2284891605377197,2.235187530517578,2.2414028644561768,2.2443344593048096,2.246760845184326,2.241175889968872,2.237013101577759,2.2309410572052,2.2282662391662598,2.226088285446167,2.2248804569244385,2.225346088409424,2.226351499557495,2.2276418209075928,2.229485273361206,2.2303576469421387,2.227980852127075,2.225910186767578,2.2256650924682617,2.223229169845581,2.223991632461548,2.2254087924957275,2.227184295654297,2.2286651134490967,2.2298712730407715,2.229401111602783,2.228252410888672,2.2275447845458984,2.2277069091796875,2.22577166557312,2.224456310272217,2.2244062423706055,2.22410249710083,2.2202720642089844,2.2169876098632812,2.2121598720550537,2.2097079753875732,2.2089791297912598,2.2147443294525146,2.220872402191162,2.2249438762664795,2.2269556522369385,2.2250559329986572,2.222898006439209,2.2180991172790527,2.213411569595337,2.2098095417022705,2.2072789669036865,2.2089409828186035,2.212064027786255,2.2142515182495117,2.215787887573242,2.2146430015563965,2.2125771045684814,2.2105753421783447,2.2077064514160156,2.205536365509033,2.2046115398406982,2.207754611968994,2.2096970081329346,2.2098639011383057,2.2087948322296143,2.20855450630188,2.20538067817688,2.202862501144409,2.2015557289123535,2.202500820159912,2.2056477069854736,2.209987163543701,2.215649127960205,2.2193033695220947,2.222383499145508,2.2237720489501953,2.2279765605926514,2.230983257293701,2.235133409500122,2.235342502593994,2.239870071411133,2.2457849979400635,2.2512998580932617,2.251756191253662,2.2508604526519775,2.2471694946289062,2.244670867919922,2.2452218532562256,2.248871088027954,2.2542736530303955,2.2620930671691895,2.26753830909729,2.2720417976379395,2.273953914642334,2.2762322425842285,2.277834177017212,2.276545286178589,2.278212785720825,2.281764030456543,2.2857298851013184,2.2892937660217285,2.2911083698272705,2.284667491912842,2.275372266769409,2.267900228500366,2.262006998062134,2.2590432167053223,2.261157989501953,2.264942169189453,2.265983819961548,2.264921188354492,2.26052188873291,2.255464553833008,2.25057053565979,2.249133348464966,2.2515480518341064,2.254112720489502,2.256579637527466,2.257169008255005,2.2526025772094727,2.2466397285461426,2.238520383834839,2.2310612201690674,2.225513458251953,2.2235116958618164,2.2237722873687744,2.2257914543151855,2.2291224002838135,2.23276424407959,2.2353293895721436,2.235658645629883,2.2351696491241455,2.233602285385132,2.230771541595459,2.2310822010040283,2.2328431606292725,2.232755661010742,2.2335357666015625,2.2357614040374756,2.2385952472686768,2.2407379150390625,2.2405080795288086,2.2369492053985596,2.2326157093048096,2.225651741027832,2.220578670501709,2.21797513961792,2.2185490131378174,2.220935821533203,2.2252702713012695,2.233330726623535,2.240816831588745,2.2461178302764893,2.245992422103882,2.242743492126465,2.240779161453247,2.237868547439575,2.2363336086273193,2.2362327575683594,2.2373552322387695,2.240208148956299,2.2412450313568115,2.2445085048675537,2.245551824569702,2.2436890602111816,2.241042375564575,2.2371985912323,2.2338271141052246,2.2306814193725586,2.2303080558776855,2.229386568069458,2.2313578128814697,2.2357919216156006,2.239392042160034,2.2411272525787354,2.241865873336792,2.242274761199951,2.2423553466796875,2.242629289627075,2.243748188018799,2.2448060512542725,2.2436578273773193,2.2412376403808594,2.2377381324768066,2.2370123863220215,2.235562562942505,2.2338411808013916,2.2350847721099854,2.2372257709503174,2.242640733718872,2.249230146408081,2.2527053356170654,2.25433611869812,2.253098726272583,2.24912166595459,2.2443294525146484,2.240334987640381,2.239471912384033,2.239759683609009,2.241049289703369,2.2433412075042725,2.2467095851898193,2.249087333679199,2.247826099395752,2.245074987411499,2.2407774925231934,2.237048864364624,2.2340357303619385,2.2340450286865234,2.2381622791290283,2.2410144805908203,2.244762659072876,2.2474141120910645,2.2487688064575195,2.2484209537506104,2.2495577335357666,2.2514610290527344,2.252960205078125,2.255218505859375,2.257877826690674,2.2609078884124756,2.262904644012451,2.2643847465515137,2.266263484954834,2.266294002532959,2.26644229888916,2.2631094455718994,2.260805130004883,2.258580446243286,2.2594809532165527,2.2605745792388916,2.261488437652588,2.2632410526275635,2.2645559310913086,2.265580892562866,2.2639288902282715,2.2606899738311768,2.2552671432495117,2.2525761127471924,2.252707004547119,2.254857063293457,2.252875328063965,2.2476446628570557,2.2445976734161377,2.241421699523926,2.2389001846313477,2.2394957542419434,2.2425732612609863,2.246212959289551,2.2485997676849365,2.249178647994995,2.2476718425750732,2.2446532249450684,2.243297576904297,2.2374050617218018,2.232872724533081,2.2293920516967773,2.231267213821411,2.2345945835113525,2.2394723892211914,2.2444067001342773,2.2507424354553223,2.2563273906707764,2.254985809326172,2.2488853931427,2.2423970699310303,2.2366247177124023,2.234100580215454,2.2354025840759277,2.2434639930725098,2.2516446113586426,2.258204936981201,2.2605745792388916,2.256760358810425,2.248762369155884,2.2376134395599365,2.2264904975891113,2.2203543186187744,2.219236373901367,2.22050404548645,2.2243897914886475,2.2292640209198,2.2345216274261475,2.239151954650879,2.2422640323638916,2.241678476333618,2.2388551235198975,2.233934164047241,2.229499101638794,2.227532148361206,2.227973222732544,2.2322022914886475,2.2358224391937256,2.238300323486328,2.241036891937256,2.2412943840026855,2.2420408725738525,2.2430896759033203,2.242633104324341,2.2379486560821533,2.233861207962036,2.229804515838623,2.2311453819274902,2.236570119857788,2.243187427520752,2.2505698204040527,2.254626989364624,2.2568094730377197,2.2579104900360107,2.2598021030426025,2.261747121810913,2.2663209438323975,2.269904136657715,2.2741804122924805,2.277750253677368,2.281040668487549,2.283705234527588,2.284288167953491,2.2847530841827393,2.286184549331665,2.287039279937744,2.29036283493042,2.2968909740448,2.299098014831543,2.300980567932129,2.3012959957122803,2.3044960498809814,2.304616689682007,2.303719997406006,2.3004202842712402,2.297531843185425,2.294182538986206,2.293084144592285,2.291985273361206,2.289846897125244,2.289407730102539,2.2900259494781494,2.2884507179260254,2.2864460945129395,2.2847108840942383,2.287627696990967,2.293497323989868,2.2981700897216797,2.300783395767212,2.303171157836914,2.304992198944092,2.3041486740112305,2.303375244140625,2.303173065185547,2.300936460494995,2.296123743057251,2.293443441390991,2.291206121444702,2.2929763793945312,2.2973997592926025,2.3014421463012695,2.2996630668640137,2.2978765964508057,2.293797016143799,2.2859625816345215,2.2774083614349365,2.2669990062713623,2.259049415588379,2.2545676231384277,2.2532529830932617,2.254605770111084,2.257288932800293,2.2611002922058105,2.26397442817688,2.2646520137786865,2.260397434234619,2.2577526569366455,2.255567789077759,2.252737522125244,2.2510054111480713,2.251556873321533,2.2551493644714355,2.2595510482788086,2.263780355453491,2.2677361965179443,2.268328905105591,2.267556667327881,2.265162229537964,2.2636022567749023,2.262521505355835,2.260899782180786,2.2588207721710205,2.257326126098633,2.256279945373535,2.254711627960205,2.2538163661956787,2.25627064704895,2.258730173110962,2.2600741386413574,2.25809383392334,2.2545933723449707,2.2503323554992676,2.2488274574279785,2.248192548751831,2.2472875118255615,2.25051212310791,2.2521960735321045,2.2526886463165283,2.252969264984131,2.2555079460144043,2.257457733154297,2.260788679122925,2.26241397857666,2.266439199447632,2.2683308124542236,2.273298501968384,2.2755932807922363,2.271364212036133,2.2654571533203125,2.261255979537964,2.2607882022857666,2.2652437686920166,2.2733843326568604,2.279507875442505,2.283621072769165,2.287707567214966,2.291252374649048,2.2921693325042725,2.2919692993164062,2.29131817817688,2.2899088859558105,2.2894227504730225,2.2904551029205322,2.2938711643218994,2.297375440597534,2.2983033657073975,2.297597646713257,2.2947373390197754,2.2911343574523926,2.2885324954986572,2.2900238037109375,2.2927417755126953,2.2960710525512695,2.296473741531372,2.2919769287109375,2.286339044570923,2.280719518661499,2.2739062309265137,2.2698402404785156,2.2699944972991943,2.272372245788574,2.2740771770477295,2.277034282684326,2.2788519859313965,2.281675100326538,2.282810688018799,2.2842464447021484,2.283489227294922,2.279203414916992,2.2709150314331055,2.262030601501465,2.2598276138305664,2.2615585327148438,2.265554189682007,2.270714044570923,2.275973081588745,2.2782437801361084,2.283231258392334,2.287834405899048,2.288132429122925,2.288270950317383,2.284355401992798,2.280488967895508,2.275491952896118,2.271456241607666,2.2689120769500732,2.2687456607818604,2.270580291748047,2.2717208862304688,2.2746663093566895,2.2743282318115234,2.272740364074707,2.273560047149658,2.2735657691955566,2.274526596069336,2.2774641513824463,2.279550313949585,2.278024196624756,2.275442600250244,2.272491693496704,2.270503520965576,2.2701404094696045,2.268523693084717,2.268118381500244,2.2662546634674072,2.2649481296539307,2.263949155807495,2.2606077194213867,2.2573001384735107,2.2517690658569336,2.2455978393554688,2.2431390285491943,2.242513656616211,2.241547107696533,2.243725061416626,2.2475547790527344,2.251688241958618,2.2511842250823975,2.247999429702759,2.2457473278045654,2.242271661758423,2.237272262573242,2.235140800476074,2.2348055839538574,2.2359559535980225,2.2395429611206055,2.2460010051727295,2.2490029335021973,2.2481014728546143,2.244771957397461,2.240032434463501,2.236532688140869,2.2370498180389404,2.2383477687835693,2.2412197589874268,2.244849920272827,2.2484183311462402,2.249102830886841,2.249764919281006,2.2500834465026855,2.2518744468688965,2.2543957233428955,2.258101463317871,2.26055645942688,2.2644593715667725,2.269378423690796,2.2733118534088135,2.2738964557647705,2.2721943855285645,2.268383741378784,2.266087293624878,2.264552593231201,2.2646963596343994,2.2655701637268066,2.269094228744507,2.271497964859009,2.269343614578247,2.2657723426818848,2.2620432376861572,2.257354259490967,2.25408673286438,2.254145383834839,2.2577600479125977,2.2636752128601074,2.269355535507202,2.2717208862304688,2.270373821258545,2.2693963050842285,2.267188787460327,2.2662711143493652,2.268437385559082,2.269787549972534,2.2714667320251465,2.274172782897949,2.2769923210144043,2.273603916168213,2.2683167457580566,2.263300895690918,2.260610342025757,2.258404016494751,2.2575478553771973,2.2568724155426025,2.257322072982788,2.259162664413452,2.263294219970703,2.266075849533081,2.2654013633728027,2.2625210285186768,2.260462760925293,2.2563350200653076,2.252924919128418,2.2503154277801514,2.250519275665283,2.2506964206695557,2.253535032272339,2.2565696239471436,2.258761405944824,2.2604429721832275,2.263188362121582,2.267987012863159,2.269150495529175,2.2685582637786865,2.2653703689575195,2.2621705532073975,2.2629008293151855,2.2624921798706055,2.265693187713623,2.264899730682373,2.26578426361084,2.2624642848968506,2.2590885162353516,2.2576920986175537,2.256110429763794,2.253478527069092,2.250883102416992,2.24729061126709,2.24151349067688,2.23942232131958,2.237046957015991,2.234436511993408,2.234023094177246,2.234569549560547,2.2356748580932617,2.236187696456909,2.238715410232544,2.239290952682495,2.240885019302368,2.2411015033721924,2.2395946979522705,2.2345170974731445,2.2264177799224854,2.220080614089966,2.2173211574554443,2.21863055229187,2.2228376865386963,2.2293219566345215,2.2371253967285156,2.244636297225952,2.2474560737609863,2.245786666870117,2.2404983043670654,2.232823610305786,2.2279815673828125,2.226240396499634,2.226363182067871,2.230008125305176,2.234966993331909,2.2410736083984375,2.249678611755371,2.2556240558624268,2.263105630874634,2.2667629718780518,2.2697103023529053,2.2675840854644775,2.2635183334350586,2.2606427669525146,2.2586073875427246,2.257112503051758,2.2575650215148926,2.2595653533935547,2.261507987976074,2.263624668121338,2.2644853591918945,2.266505718231201,2.268669605255127,2.2703797817230225,2.273381471633911,2.277719497680664,2.282022476196289,2.283113479614258,2.2812716960906982,2.2805280685424805,2.2792887687683105,2.2798309326171875,2.281024694442749,2.2850866317749023,2.2907495498657227,2.299128770828247,2.3076717853546143,2.3144443035125732,2.317627429962158,2.316526412963867,2.3130943775177,2.307701826095581,2.3026673793792725,2.297602415084839,2.2970659732818604,2.2989561557769775,2.300476551055908,2.3014914989471436,2.3013291358947754,2.301022529602051,2.3007895946502686,2.3015220165252686,2.304614305496216,2.307781219482422,2.3112871646881104,2.3147830963134766,2.3174076080322266,2.3183066844940186,2.317847728729248,2.314702033996582,2.310783624649048,2.308166742324829,2.304661750793457,2.3060736656188965,2.3075122833251953,2.308887243270874,2.31015682220459,2.310412645339966,2.30914044380188,2.3122522830963135,2.312417507171631,2.313585042953491,2.313849925994873,2.3138790130615234,2.3146910667419434,2.3123719692230225,2.3128275871276855,2.3147315979003906,2.3174755573272705,2.317438840866089,2.3154544830322266,2.3172552585601807,2.31828236579895,2.31797194480896,2.323232889175415,2.3300585746765137,2.335299015045166,2.338449478149414,2.3379156589508057,2.3380534648895264,2.3384487628936768,2.335953950881958,2.334044933319092,2.332380771636963,2.332180976867676,2.332477569580078,2.3301827907562256,2.325308322906494,2.316561222076416,2.307537317276001,2.2989368438720703,2.293132781982422,2.2917001247406006,2.2928011417388916,2.294907331466675,2.2974119186401367,2.3002657890319824,2.3024520874023438,2.304645538330078,2.3065197467803955,2.3070173263549805,2.305537223815918,2.30426287651062,2.297090530395508,2.2905895709991455,2.2871127128601074,2.283966541290283,2.279680013656616,2.2735981941223145,2.268580198287964,2.265666961669922,2.2663495540618896,2.271480083465576,2.278925657272339,2.28279185295105,2.287522554397583,2.290715456008911,2.2858142852783203,2.279507875442505,2.272035598754883,2.2670693397521973,2.26701283454895,2.271314859390259,2.277818441390991,2.283884286880493,2.287574052810669,2.289687395095825,2.2923858165740967,2.2914745807647705,2.289933204650879,2.2873294353485107,2.2878494262695312,2.2902894020080566,2.293891668319702,2.298816204071045,2.302868604660034,2.3068811893463135,2.30843448638916,2.308145046234131,2.308462619781494,2.3109869956970215,2.312950372695923,2.312229633331299,2.3074827194213867,2.3052308559417725,2.304079055786133,2.305936098098755,2.3083364963531494,2.31233286857605,2.315581798553467,2.3194420337677,2.32149600982666,2.3228418827056885,2.3225395679473877,2.3230957984924316,2.3268940448760986,2.330960512161255,2.33632493019104,2.33895206451416,2.3397533893585205,2.3372437953948975,2.3361976146698,2.33791446685791,2.339961528778076,2.343747854232788,2.3447675704956055,2.345986843109131,2.343327522277832,2.3357040882110596,2.3258068561553955,2.318234920501709,2.3158161640167236,2.3204185962677,2.3253278732299805,2.328904151916504,2.331737756729126,2.336392402648926,2.3418831825256348,2.347395181655884,2.351222515106201,2.348747491836548,2.345863103866577,2.339193820953369,2.332629680633545,2.3301167488098145,2.3312785625457764,2.3328492641448975,2.335613965988159,2.3368000984191895,2.3364243507385254,2.3356130123138428,2.335986375808716,2.332331895828247,2.3257296085357666,2.320807695388794,2.3170528411865234,2.3152425289154053,2.316588878631592,2.3203020095825195,2.324611186981201,2.328146457672119,2.3299965858459473,2.3280441761016846,2.3226675987243652,2.318309783935547,2.312835693359375,2.30987286567688,2.3108737468719482,2.3126840591430664,2.331744909286499,2.326800584793091,2.3126718997955322,2.3108670711517334,2.3532183170318604,2.381361722946167,2.3554527759552,2.344040632247925,2.3374130725860596,2.335789680480957,2.3420932292938232,2.366102933883667,2.3795242309570312,2.369675397872925,2.3608479499816895,2.3679370880126953,2.3716723918914795,2.3697381019592285,2.35632586479187,2.33764386177063,2.331209182739258,2.3367562294006348,2.353776693344116,2.349872589111328,2.3220603466033936,2.2886087894439697,2.2739288806915283,2.2691104412078857,2.266497850418091,2.264902353286743,2.2750563621520996,2.2778854370117188,2.272083282470703,2.25939679145813,2.248135566711426,2.247411012649536,2.277998447418213,2.269648313522339,2.2512001991271973,2.2423288822174072,2.2477262020111084,2.2599003314971924,2.2815866470336914,2.30499005317688,2.3209662437438965,2.320235013961792,2.307072162628174,2.2790751457214355,2.256659984588623,2.253113031387329,2.2745442390441895,2.312431573867798,2.3148348331451416,2.2965333461761475,2.2748003005981445,2.2730650901794434,2.2841739654541016,2.2937164306640625,2.328690767288208,2.3235814571380615,2.285959482192993,2.250680446624756,2.233973264694214,2.237119436264038,2.2540881633758545,2.2794277667999268,2.3179736137390137,2.3513638973236084,2.368896007537842,2.375206708908081,2.379608154296875,2.381516456604004,2.389211893081665,2.4058632850646973,2.4303722381591797,2.4717206954956055,2.511320114135742,2.521955966949463,2.5212016105651855,2.50730299949646,2.496649980545044,2.494154214859009,2.5090174674987793,2.504185914993286,2.493008613586426,2.468963146209717,2.446167230606079,2.429835081100464,2.4278697967529297,2.445807456970215,2.441394329071045,2.43041729927063,2.4130043983459473,2.40201473236084,2.4012012481689453,2.392461061477661,2.3900396823883057,2.380283832550049,2.3768808841705322,2.3696701526641846,2.3601903915405273,2.356567621231079,2.3424925804138184,2.3443799018859863,2.3599941730499268,2.373645067214966,2.382120370864868,2.3657422065734863,2.3438520431518555,2.318694591522217,2.314114809036255,2.3303890228271484,2.347310781478882,2.3314409255981445,2.3121819496154785,2.306715726852417,2.304941415786743,2.3189525604248047,2.312570095062256,2.2860970497131348,2.264014720916748,2.2568352222442627,2.264768600463867,2.2681944370269775,2.2682414054870605,2.2733190059661865,2.274214029312134,2.28157114982605,2.290762186050415,2.2941718101501465,2.3068907260894775,2.3294146060943604,2.3401589393615723,2.332556962966919,2.31933856010437,2.306436061859131,2.3013389110565186,2.3053972721099854,2.309391736984253,2.306288003921509,2.302408218383789,2.293721914291382,2.301424264907837,2.318805456161499,2.3250508308410645,2.3286476135253906,2.310666561126709,2.294487953186035,2.2894861698150635,2.3011634349823,2.3270821571350098,2.3364861011505127,2.3389339447021484,2.3172950744628906,2.309501886367798,2.2996268272399902,2.3013203144073486,2.3180668354034424,2.321089029312134,2.32180118560791,2.319403648376465,2.329468250274658,2.315060615539551,2.294297933578491,2.2826740741729736,2.284196615219116,2.280024528503418,2.2662160396575928,2.2560441493988037,2.235588312149048,2.2402760982513428,2.25209641456604,2.2481844425201416,2.2426035404205322,2.2347869873046875,2.253387451171875,2.2718653678894043,2.268711805343628,2.2359087467193604,2.2053468227386475,2.2061822414398193,2.2262935638427734,2.225080966949463,2.2239696979522705,2.216477632522583,2.2189981937408447,2.221599817276001,2.210251569747925,2.2009828090667725,2.2008092403411865,2.2031586170196533,2.202307939529419,2.1988587379455566,2.20302414894104,2.2121474742889404,2.223057985305786,2.2249040603637695,2.232867956161499,2.2281930446624756,2.225445508956909,2.232839345932007,2.232898235321045,2.224316358566284,2.2087812423706055,2.2096898555755615,2.2154459953308105,2.1936166286468506,2.156248092651367,2.1428561210632324,2.1464271545410156,2.1512348651885986,2.1475934982299805,2.138307809829712,2.150416135787964,2.1733181476593018,2.19353985786438,2.196157693862915,2.1936540603637695,2.1838269233703613,2.194380283355713,2.2114429473876953,2.2259268760681152,2.207014322280884,2.196653127670288,2.1934585571289062,2.2060956954956055,2.2138328552246094,2.228721857070923,2.212991237640381,2.2181060314178467,2.2088141441345215,2.196571111679077,2.18302583694458,2.1666319370269775,2.150251626968384,2.149284839630127,2.162191867828369,2.19722318649292,2.2144362926483154,2.2209277153015137,2.2160847187042236,2.204134225845337,2.1965367794036865,2.169386148452759,2.145163059234619,2.139037847518921,2.1354875564575195,2.1190454959869385,2.0794315338134766,2.050640344619751,2.0405147075653076,2.044403314590454,2.0471599102020264,2.069934129714966,2.0847432613372803,2.0850276947021484,2.0760087966918945,2.052149772644043,2.0450470447540283,2.056840658187866,2.075385093688965,2.0877392292022705,2.101658344268799,2.115619659423828,2.114016532897949,2.0987448692321777,2.094357490539551,2.100489377975464,2.115978479385376,2.12959361076355,2.1303980350494385,2.124372720718384,2.1158154010772705,2.117527484893799,2.134312391281128,2.152801036834717,2.160238265991211,2.1495089530944824,2.1424789428710938,2.149846076965332,2.160661458969116,2.1707310676574707,2.162397861480713,2.1580657958984375,2.147115707397461,2.142803192138672,2.1440324783325195,2.136227607727051,2.123208999633789,2.1060221195220947,2.1030871868133545,2.1283986568450928,2.160832405090332,2.1904892921447754,2.205202579498291,2.2050576210021973,2.2016494274139404,2.217285633087158,2.2101190090179443,2.200692653656006,2.1995534896850586,2.213334798812866,2.2405736446380615,2.2680583000183105,2.300265312194824,2.3104546070098877,2.297285795211792,2.286193609237671,2.285801410675049,2.2938835620880127,2.317631483078003,2.334401845932007,2.303349256515503,2.261904001235962,2.231952667236328,2.223862409591675,2.223499298095703,2.214831829071045,2.1945013999938965,2.17834210395813,2.180680990219116,2.183408498764038,2.1898515224456787,2.1986546516418457,2.216677188873291,2.249952554702759,2.2806150913238525,2.286184549331665,2.281712055206299,2.285829544067383,2.2989349365234375,2.313629150390625,2.328768014907837,2.330056667327881,2.3171350955963135,2.316178798675537,2.303429126739502,2.2948365211486816,2.2827279567718506,2.291982889175415,2.314671754837036,2.329968214035034,2.3380162715911865,2.350053548812866,2.355952024459839,2.3714253902435303,2.383571147918701,2.380376100540161,2.393974542617798,2.4058215618133545,2.41475248336792,2.4184510707855225,2.422947645187378,2.422424793243408,2.397380828857422,2.384376049041748,2.3848752975463867,2.3929176330566406,2.4033915996551514,2.4010026454925537,2.400012969970703,2.411579132080078,2.427307605743408,2.4512414932250977,2.466259479522705,2.4670538902282715,2.452967882156372,2.4246041774749756,2.4071044921875,2.394089937210083,2.3953065872192383,2.393561363220215,2.3772659301757812,2.3685250282287598,2.3791773319244385,2.389791965484619,2.3873066902160645,2.3530752658843994,2.337475061416626,2.331738233566284,2.3308396339416504,2.3425583839416504,2.3421380519866943,2.3166754245758057,2.2982869148254395,2.2805778980255127,2.2562010288238525,2.246617317199707,2.2534077167510986,2.276723623275757,2.274493455886841,2.2496261596679688,2.220947265625,2.2138683795928955,2.2290899753570557,2.2582290172576904,2.2533984184265137,2.2286345958709717,2.205821990966797,2.2050957679748535,2.2082231044769287,2.198303699493408,2.176546335220337,2.150561571121216,2.125687837600708,2.1068286895751953,2.084857702255249,2.0646555423736572,2.055332660675049,2.0481081008911133,2.046107053756714,2.050096273422241,2.0411884784698486,2.0370140075683594,2.0368025302886963,2.037081718444824,2.0419483184814453,2.0635814666748047,2.055095911026001,2.043121814727783,2.057509422302246,2.0816867351531982,2.1015756130218506,2.1024529933929443,2.1122472286224365,2.12286376953125,2.131908655166626,2.1457736492156982,2.1605424880981445,2.1609435081481934,2.15295672416687,2.163454055786133,2.167579412460327,2.152524471282959,2.134071111679077,2.104494094848633,2.1041951179504395,2.1078639030456543,2.129201889038086,2.143481731414795,2.148167371749878,2.140748977661133,2.141317844390869,2.16926646232605,2.201489210128784,2.215834856033325,2.2086710929870605,2.217715263366699,2.2275466918945312,2.258589029312134,2.2877275943756104,2.279693126678467,2.254322052001953,2.2348079681396484,2.2377572059631348,2.2485241889953613,2.210822582244873,2.163499116897583,2.1312243938446045,2.128408670425415,2.144439458847046,2.1673800945281982,2.1779162883758545,2.159003257751465,2.1411261558532715,2.142486095428467,2.1619200706481934,2.183058500289917,2.2032766342163086,2.2165112495422363,2.2183337211608887,2.214881658554077,2.2231006622314453,2.2264633178710938,2.233821153640747,2.2382712364196777,2.2590584754943848,2.2957425117492676,2.335190534591675,2.3664543628692627,2.3551511764526367,2.3382346630096436,2.341722249984741,2.355581760406494,2.359661102294922,2.357908248901367,2.349773645401001,2.3473455905914307,2.369518280029297,2.3686509132385254,2.3398547172546387,2.3102078437805176,2.2965333461761475,2.304461717605591,2.331745147705078,2.3616983890533447,2.3681864738464355,2.3455984592437744,2.338306188583374,2.3342814445495605,2.3502161502838135,2.3637659549713135,2.3594043254852295,2.3527586460113525,2.353170156478882,2.371093273162842,2.3969650268554688,2.405121326446533,2.38287353515625,2.329927444458008,2.284489154815674,2.2501680850982666,2.2321276664733887,2.222665548324585,2.204716205596924,2.1718900203704834,2.134336471557617,2.1089274883270264,2.0998852252960205,2.112204074859619,2.1180267333984375,2.1093854904174805,2.131175994873047,2.1438310146331787,2.1602392196655273,2.17812180519104,2.185330629348755,2.2030208110809326,2.2201788425445557,2.2421395778656006,2.2689130306243896,2.278935670852661,2.2887279987335205,2.2932519912719727,2.3097071647644043,2.3451414108276367,2.3590991497039795,2.35360050201416,2.3586740493774414,2.3733861446380615,2.4017398357391357,2.412647247314453,2.419792890548706,2.421696662902832,2.4277536869049072,2.421506404876709,2.4115536212921143,2.392317533493042,2.3784985542297363,2.3806281089782715,2.374262809753418,2.3681042194366455,2.3698618412017822,2.3676300048828125,2.366558074951172,2.3496925830841064,2.3095409870147705,2.2807281017303467,2.278658390045166,2.2869133949279785,2.2734949588775635,2.238755702972412,2.200840711593628,2.185148239135742,2.182922840118408,2.193296194076538,2.187086820602417,2.176644802093506,2.154677629470825,2.1417431831359863,2.140678644180298,2.1474339962005615,2.165719747543335,2.1724536418914795,2.161181688308716,2.167715549468994,2.1841557025909424,2.198146343231201,2.214500904083252,2.2020514011383057,2.188852548599243,2.20611834526062,2.2338287830352783,2.247454881668091,2.24124813079834,2.2231364250183105,2.221445322036743,2.231508255004883,2.254190683364868,2.266881227493286,2.285956382751465,2.2988545894622803,2.2970709800720215,2.2851943969726562,2.286215305328369,2.2743706703186035,2.276282548904419,2.2824604511260986,2.2832415103912354,2.278959274291992,2.2632198333740234,2.247087001800537,2.2417025566101074,2.255202054977417,2.278996706008911,2.2966997623443604,2.2881381511688232,2.266153573989868,2.259693145751953,2.2634193897247314,2.287755250930786,2.3241546154022217,2.3387744426727295,2.325058937072754,2.309178590774536,2.298427104949951,2.285705089569092,2.277723789215088,2.281637668609619,2.2923715114593506,2.2842907905578613,2.273071050643921,2.2731106281280518,2.279400587081909,2.292539358139038,2.299177646636963,2.299680471420288,2.282482385635376,2.2687835693359375,2.275144338607788,2.2882535457611084,2.2903501987457275,2.2777533531188965,2.2742722034454346,2.281801700592041,2.294755220413208,2.307466745376587,2.309577226638794,2.2949378490448,2.2769505977630615,2.2669551372528076,2.2674176692962646,2.2844343185424805,2.2899107933044434,2.268022060394287,2.226224422454834,2.1924631595611572,2.1765637397766113,2.1869308948516846,2.184755563735962,2.1791629791259766,2.1730077266693115,2.1583001613616943,2.145890951156616,2.147914409637451,2.1619527339935303,2.179987668991089,2.1822433471679688,2.185617685317993,2.2011168003082275,2.2013747692108154,2.1953165531158447,2.1672375202178955,2.159109115600586,2.166149139404297,2.180133581161499,2.187591075897217,2.175417900085449,2.1483399868011475,2.1357269287109375,2.1423652172088623,2.165940999984741,2.18149471282959,2.1715660095214844,2.153865337371826,2.1459641456604004,2.1630163192749023,2.182219982147217,2.1920270919799805,2.167156934738159,2.1462624073028564,2.1423099040985107,2.151646137237549,2.174474000930786,2.189938545227051,2.1756694316864014,2.158284902572632,2.141796350479126,2.1342549324035645,2.145742654800415,2.160630226135254,2.174437999725342,2.168958902359009,2.1681368350982666,2.1638541221618652,2.166374921798706,2.1728556156158447,2.180368423461914,2.183079242706299,2.1730620861053467,2.158902168273926,2.158372640609741,2.1648218631744385,2.1845061779022217,2.2060723304748535,2.1966404914855957,2.192671060562134,2.197469472885132,2.1937930583953857,2.190863847732544,2.1788835525512695,2.168168067932129,2.1632235050201416,2.15874981880188,2.155843496322632,2.149519681930542,2.141672134399414,2.126427173614502,2.113837242126465,2.1113028526306152,2.1168277263641357,2.122880697250366,2.1255998611450195,2.1202476024627686,2.1176488399505615,2.123570680618286,2.138310670852661,2.128573179244995,2.105769157409668,2.089311361312866,2.091064214706421,2.11255145072937,2.1350791454315186,2.135183095932007,2.1317801475524902,2.1321356296539307,2.136996269226074,2.1444122791290283,2.162525177001953,2.173022985458374,2.1807026863098145,2.1717448234558105,2.1713476181030273,2.1713056564331055,2.176931858062744,2.163728713989258,2.138374090194702,2.1304595470428467,2.133934497833252,2.1391489505767822,2.1344897747039795,2.1151134967803955,2.1012089252471924,2.0976688861846924,2.0931413173675537,2.099823474884033,2.0971105098724365,2.093128204345703,2.102224349975586,2.1161110401153564,2.1125898361206055,2.1143288612365723,2.1245648860931396,2.1346242427825928,2.149143695831299,2.168048143386841,2.184779167175293,2.1970152854919434,2.1834166049957275,2.1677258014678955,2.165313482284546,2.1782937049865723,2.197885274887085,2.206162452697754,2.206897735595703,2.1884515285491943,2.180440664291382,2.181313991546631,2.18947172164917,2.192244291305542,2.200685739517212,2.2105801105499268,2.2124128341674805,2.2100110054016113,2.20563006401062,2.2125558853149414,2.2260377407073975,2.232377767562866,2.231275796890259,2.2341432571411133,2.2142059803009033,2.189375877380371,2.185065507888794,2.197392225265503,2.217130184173584,2.227529287338257,2.2204084396362305,2.2088751792907715,2.204740524291992,2.2136008739471436,2.223907709121704,2.2393641471862793,2.2419018745422363,2.234323501586914,2.233131170272827,2.2446861267089844,2.241049289703369,2.2289085388183594,2.2290596961975098,2.2224791049957275,2.209346294403076,2.214545488357544,2.208409547805786,2.2014291286468506,2.181786060333252,2.170036554336548,2.169020175933838,2.18265700340271,2.1984755992889404,2.201759099960327,2.20477557182312,2.203582286834717,2.211655378341675,2.2233963012695312,2.256211757659912,2.262483596801758,2.22904634475708,2.1970489025115967,2.18991756439209,2.2085936069488525,2.220885992050171,2.2197132110595703,2.19646954536438,2.180607557296753,2.1763384342193604,2.180306911468506,2.1759121417999268,2.170722007751465,2.165630340576172,2.1471633911132812,2.136793375015259,2.143746852874756,2.14928936958313,2.1397881507873535,2.1256940364837646,2.126749038696289,2.13608717918396,2.1463723182678223,2.143035888671875,2.1403090953826904,2.1491050720214844,2.164937734603882,2.1746888160705566,2.177018642425537,2.181959390640259,2.1850178241729736,2.199411392211914,2.21344256401062,2.212690591812134,2.2044732570648193,2.206202507019043,2.2143898010253906,2.2117056846618652,2.216038703918457,2.2397844791412354,2.2622315883636475,2.278618812561035,2.277629852294922,2.273005485534668,2.2720062732696533,2.275484085083008,2.2796525955200195,2.3033053874969482,2.3318302631378174,2.3317837715148926,2.290011167526245,2.2550570964813232,2.23309588432312,2.2226626873016357,2.212502956390381,2.204979181289673,2.2152044773101807,2.221238374710083,2.2302727699279785,2.2339415550231934,2.233271360397339,2.232452630996704,2.2223167419433594,2.222743034362793,2.2340967655181885,2.2606847286224365,2.275236129760742,2.2671456336975098,2.248551368713379,2.256453514099121,2.263063669204712,2.2634565830230713,2.2633726596832275,2.2562527656555176,2.2395923137664795,2.2288661003112793,2.225048780441284,2.220470905303955,2.2226643562316895,2.2087881565093994,2.1952362060546875,2.1960716247558594,2.2110772132873535,2.222038507461548,2.202975273132324,2.182670831680298,2.175614833831787,2.1779935359954834,2.1953723430633545,2.204192638397217,2.185187339782715,2.1598124504089355,2.156048536300659,2.165816068649292,2.161247491836548,2.1588551998138428,2.1548912525177,2.147911310195923,2.147759437561035,2.147106409072876,2.145397901535034,2.1378257274627686,2.141829490661621,2.1461451053619385,2.155350685119629,2.16290545463562,2.1717100143432617,2.1621928215026855,2.1373791694641113,2.1353704929351807,2.145622491836548,2.1591808795928955,2.1658451557159424,2.16450572013855,2.1456079483032227,2.136181354522705,2.137139320373535,2.143399477005005,2.159438371658325,2.174232006072998,2.165994167327881,2.134618043899536,2.123500347137451,2.133603811264038,2.1473777294158936,2.1622886657714844,2.164764404296875,2.1652731895446777,2.1691365242004395,2.1763811111450195,2.1857776641845703,2.1732215881347656,2.1630735397338867,2.1577506065368652,2.163573741912842,2.182069778442383,2.193289279937744,2.2005326747894287,2.189924478530884,2.18959641456604,2.19880747795105,2.212263822555542,2.2178866863250732,2.217153310775757,2.208733558654785,2.212217330932617,2.2078473567962646,2.207771062850952,2.2192206382751465,2.2245941162109375,2.2207512855529785,2.2154037952423096,2.217139959335327,2.2264628410339355,2.2374918460845947,2.236156940460205,2.225611686706543,2.214599847793579,2.210984945297241,2.215371608734131,2.2025885581970215,2.1935551166534424,2.195492744445801,2.202139377593994,2.2081563472747803,2.2119946479797363,2.2068097591400146,2.183347463607788,2.1666171550750732,2.1611812114715576,2.1606831550598145,2.161083936691284,2.1475062370300293,2.141503095626831,2.1206796169281006,2.1126387119293213,2.103792190551758,2.093867063522339,2.0855367183685303,2.0804967880249023,2.076036214828491,2.0743303298950195,2.074472665786743,2.077625274658203,2.083019971847534,2.089055299758911,2.095585823059082,2.100710391998291,2.106290817260742,2.109602928161621,2.11055326461792,2.1093437671661377,2.106940746307373,2.100630521774292,2.097966194152832,2.097727060317993,2.099778652191162,2.1039867401123047,2.1116816997528076,2.121486186981201,2.132229804992676,2.1397106647491455,2.1449806690216064,2.147749185562134,2.14809513092041,2.146707773208618,2.145221710205078,2.1446175575256348,2.1429128646850586,2.140385150909424,2.140073776245117,2.139942169189453,2.1409363746643066,2.1455204486846924,2.1497299671173096,2.1542041301727295,2.153862714767456,2.1537845134735107,2.153351306915283,2.15378475189209,2.1517581939697266,2.1490249633789062,2.146916627883911,2.1447155475616455,2.141540765762329,2.1404869556427,2.141350746154785,2.140450954437256,2.1391713619232178,2.1404926776885986,2.141906499862671,2.142049551010132,2.1430044174194336,2.145285129547119,2.147202491760254,2.146962881088257,2.1481945514678955,2.149043083190918,2.1454362869262695,2.143946647644043,2.144204616546631,2.145057201385498,2.144577741622925,2.1450271606445312,2.143364906311035,2.141721487045288,2.1407065391540527,2.140378713607788,2.1415934562683105,2.145205020904541,2.1484832763671875,2.15085768699646,2.152222156524658,2.1515491008758545,2.149742364883423,2.1448354721069336,2.1412830352783203,2.1388590335845947,2.136855363845825,2.1355512142181396,2.1366758346557617,2.1397039890289307,2.143566131591797,2.147589683532715,2.150325298309326,2.1529693603515625,2.154052972793579,2.153043031692505,2.1492302417755127,2.145731210708618,2.143052577972412,2.1407597064971924,2.140381336212158,2.142258405685425,2.1456398963928223,2.14787220954895,2.151118755340576,2.150951623916626,2.151369094848633,2.1509552001953125,2.1486852169036865,2.1447012424468994,2.1436376571655273,2.1444180011749268,2.1473004817962646,2.150773763656616,2.1554393768310547,2.159747838973999,2.162733316421509,2.1630289554595947,2.1611931324005127,2.1576290130615234,2.1548736095428467,2.1535534858703613,2.154844284057617,2.158982038497925,2.161681890487671,2.165505886077881,2.170637369155884,2.176132917404175,2.179841995239258,2.181905508041382,2.1837291717529297,2.1835951805114746,2.182084083557129,2.1815104484558105,2.181370973587036,2.1812682151794434,2.181016445159912,2.1817502975463867,2.1806719303131104,2.177644729614258,2.174530267715454,2.172598361968994,2.17134428024292,2.171461343765259,2.172823190689087,2.175516366958618,2.1764326095581055,2.1779000759124756,2.17899227142334,2.180311441421509,2.1779961585998535,2.1747117042541504,2.1719400882720947,2.1705949306488037,2.1720473766326904,2.172936201095581,2.175349473953247,2.1795589923858643,2.1810805797576904,2.1832685470581055,2.183159112930298,2.182406187057495,2.1806583404541016,2.178319215774536,2.1778199672698975,2.179133415222168,2.181530475616455,2.183147430419922,2.187467336654663,2.1903750896453857,2.193521022796631,2.19451642036438,2.1950724124908447,2.1919679641723633,2.1877503395080566,2.1853787899017334,2.1848199367523193,2.1871893405914307,2.190439224243164,2.1969892978668213,2.201683282852173,2.2035887241363525,2.203258514404297,2.2001771926879883,2.196662425994873,2.193532943725586,2.189243793487549,2.1885182857513428,2.1892354488372803,2.1909353733062744,2.1899831295013428,2.190714120864868,2.1923160552978516,2.1916351318359375,2.1922762393951416,2.192729949951172,2.1931121349334717,2.1945130825042725,2.195392370223999,2.197664260864258,2.2002334594726562,2.203230857849121,2.2060601711273193,2.207677125930786,2.2064926624298096,2.2053449153900146,2.2032763957977295,2.200518846511841,2.1972594261169434,2.1946613788604736,2.1939315795898438,2.192837953567505,2.1929118633270264,2.1937687397003174,2.1971073150634766,2.2006070613861084,2.2031190395355225,2.2027575969696045,2.201395034790039,2.1965222358703613,2.1924643516540527,2.1922659873962402,2.1943047046661377,2.1979689598083496,2.203720808029175,2.209193468093872,2.213657855987549,2.2152111530303955,2.2142767906188965,2.2117919921875,2.207352876663208,2.2015223503112793,2.1965348720550537,2.1954293251037598,2.195078134536743,2.1978490352630615,2.201801061630249,2.2058186531066895,2.2077622413635254,2.2091004848480225,2.2082204818725586,2.2065179347991943,2.202630043029785,2.199801206588745,2.198561906814575,2.199248790740967,2.199089288711548,2.2018260955810547,2.2043044567108154,2.2069244384765625,2.207249641418457,2.2043957710266113,2.200706720352173,2.197317361831665,2.1961729526519775,2.1962196826934814,2.1971869468688965,2.197831153869629,2.196276903152466,2.1956236362457275,2.1968250274658203,2.1971640586853027,2.1999080181121826,2.2027361392974854,2.204632043838501,2.206321954727173,2.2077338695526123,2.2066099643707275,2.2059383392333984,2.203422784805298,2.201759099960327,2.2012035846710205,2.2030773162841797,2.205249547958374,2.208635091781616,2.2101588249206543,2.211059093475342,2.212282180786133,2.212038993835449,2.210449695587158,2.207335948944092,2.20566987991333,2.205273151397705,2.204939365386963,2.203296184539795,2.2055277824401855,2.208005666732788,2.211482048034668,2.2140519618988037,2.2166857719421387,2.217869281768799,2.2168097496032715,2.215763568878174,2.2150394916534424,2.2133729457855225,2.2127203941345215,2.2120418548583984,2.21232533454895,2.2140955924987793,2.2159645557403564,2.218618392944336,2.2204248905181885,2.2210536003112793,2.2202515602111816,2.2189369201660156,2.2181293964385986,2.216779947280884,2.2134785652160645,2.2134006023406982,2.2163116931915283,2.218863010406494,2.222141742706299,2.2262778282165527,2.229374408721924,2.2326133251190186,2.23451828956604,2.236475944519043,2.235332489013672,2.2330386638641357,2.2303147315979004,2.2298781871795654,2.2290525436401367,2.2276039123535156,2.2277486324310303,2.2259738445281982,2.227001905441284,2.226487874984741,2.222811222076416,2.220032215118408,2.2176289558410645,2.216047525405884,2.215890407562256,2.2176992893218994,2.2218170166015625,2.2251474857330322,2.2267820835113525,2.22835636138916,2.227652072906494,2.2250735759735107,2.224764585494995,2.225696325302124,2.224452495574951,2.223447322845459,2.2247018814086914,2.22575306892395,2.2252819538116455,2.2242720127105713,2.225118637084961,2.2220561504364014,2.215690851211548,2.214242935180664,2.2167999744415283,2.2199673652648926,2.2263286113739014,2.2315855026245117,2.235305070877075,2.239191770553589,2.239724636077881,2.236457347869873,2.2318265438079834,2.2265913486480713,2.2230474948883057,2.2182958126068115,2.215681314468384,2.2111568450927734,2.207721471786499,2.2048187255859375,2.2029573917388916,2.2029149532318115,2.203374147415161,2.2037761211395264,2.2043936252593994,2.204578399658203,2.2033305168151855,2.2018227577209473,2.2010433673858643,2.1979241371154785,2.194218873977661,2.1907267570495605,2.187936305999756,2.184321165084839,2.1842310428619385,2.184786081314087,2.1852829456329346,2.186216354370117,2.1861891746520996,2.186359405517578,2.186279058456421,2.1878910064697266,2.1904327869415283,2.193862199783325,2.195115566253662,2.1923537254333496,2.189783811569214,2.186525821685791,2.186293601989746,2.1868393421173096,2.1846930980682373,2.1835474967956543,2.1823976039886475,2.183138608932495,2.182741165161133,2.1842305660247803,2.188136577606201,2.1899845600128174,2.193167209625244,2.1952810287475586,2.1945419311523438,2.191030263900757,2.188251256942749,2.18780255317688,2.187255382537842,2.1880760192871094,2.1899871826171875,2.1922171115875244,2.19462513923645,2.1960017681121826,2.195354461669922,2.1937270164489746,2.1935534477233887,2.193923234939575,2.194904327392578,2.1942546367645264,2.194284200668335,2.194822311401367,2.195542573928833,2.193849802017212,2.192920684814453,2.1919331550598145,2.190246343612671,2.188520908355713,2.188481092453003,2.190627336502075,2.1938822269439697,2.1978566646575928,2.199808120727539,2.202075958251953,2.2022476196289062,2.203157424926758,2.201995849609375,2.1999027729034424,2.197291851043701,2.196967363357544,2.197176933288574,2.19936203956604,2.2032787799835205,2.209279775619507,2.2164716720581055,2.2197396755218506,2.216350793838501,2.212070941925049,2.207590341567993,2.203723430633545,2.2017982006073,2.201479911804199,2.2036614418029785,2.205724000930786,2.206317186355591,2.205516815185547,2.2022907733917236,2.197749614715576,2.19140887260437,2.1871702671051025,2.181680917739868,2.1786885261535645,2.1810131072998047,2.185408592224121,2.1915857791900635,2.1968812942504883,2.2017319202423096,2.2036805152893066,2.2042248249053955,2.1997053623199463,2.194260597229004,2.1916749477386475,2.18988299369812,2.1896936893463135,2.192225694656372,2.1961541175842285,2.1963162422180176,2.1928517818450928,2.1881091594696045,2.182710647583008,2.178419828414917,2.177938222885132,2.178156614303589,2.17985200881958,2.182300090789795,2.180872678756714,2.181037664413452,2.1796228885650635,2.1773760318756104,2.17507004737854,2.176138401031494,2.1783907413482666,2.1796071529388428,2.1777005195617676,2.178926467895508,2.1818578243255615,2.1842076778411865,2.186440944671631,2.189016103744507,2.1899285316467285,2.1883671283721924,2.1856374740600586,2.1844236850738525,2.184516668319702,2.18727970123291,2.1905477046966553,2.193922281265259,2.197702646255493,2.199613571166992,2.200099468231201,2.199885368347168,2.1968629360198975,2.1947731971740723,2.193079710006714,2.191591262817383,2.1909053325653076,2.189594030380249,2.189072370529175,2.192709445953369,2.1981821060180664,2.20367431640625,2.2069597244262695,2.2085962295532227,2.2087366580963135,2.2052886486053467,2.2006654739379883,2.197382688522339,2.1960413455963135,2.198889970779419,2.2006731033325195,2.20444393157959,2.2078871726989746,2.211231231689453,2.2124531269073486,2.2105865478515625,2.20666766166687,2.2044315338134766,2.2041032314300537,2.204144239425659,2.2054035663604736,2.2056503295898438,2.204341173171997,2.2035086154937744,2.202509880065918,2.2044618129730225,2.204033136367798,2.202714204788208,2.2011806964874268,2.1991069316864014,2.1988818645477295,2.1999871730804443,2.2018675804138184,2.2056281566619873,2.207735538482666,2.209157705307007,2.209644317626953,2.210806369781494,2.211951971054077,2.209942102432251,2.2069411277770996,2.2053592205047607,2.205878496170044,2.2067036628723145,2.2056071758270264,2.2062876224517822,2.2046380043029785,2.2045533657073975,2.206958055496216,2.209339141845703,2.2130255699157715,2.216136932373047,2.216291666030884,2.208778142929077,2.201211452484131,2.1955726146698,2.1913766860961914,2.18741512298584,2.186131715774536,2.187664747238159,2.1905388832092285,2.1969075202941895,2.20371150970459,2.2090578079223633,2.210603952407837,2.2102596759796143,2.207156181335449,2.2036499977111816,2.200093984603882,2.1964902877807617,2.1963815689086914,2.2005040645599365,2.205627679824829,2.210347890853882,2.2165045738220215,2.221403121948242,2.225461959838867,2.227754592895508,2.229022264480591,2.2300474643707275,2.2287728786468506,2.229268789291382,2.2306249141693115,2.232065439224243,2.2327582836151123,2.2336766719818115,2.2324678897857666,2.230449914932251,2.2296295166015625,2.229480266571045,2.2286007404327393,2.229344129562378,2.2299880981445312,2.230278253555298,2.2322981357574463,2.235719919204712,2.240022897720337,2.2406651973724365,2.2420480251312256,2.2433552742004395,2.242478609085083,2.244109630584717,2.2450432777404785,2.245558738708496,2.243048667907715,2.2417473793029785,2.2428042888641357,2.2423219680786133,2.24234676361084,2.2389321327209473,2.237053871154785,2.235386610031128,2.2355785369873047,2.2346110343933105,2.235538959503174,2.2350852489471436,2.2327542304992676,2.2295567989349365,2.225928544998169,2.2222275733947754,2.2190768718719482,2.2193362712860107,2.2212045192718506,2.223339796066284,2.225740432739258,2.2278480529785156,2.2308316230773926,2.234849691390991,2.238279342651367,2.2376837730407715,2.2352564334869385,2.232828378677368,2.2303385734558105,2.2281816005706787,2.2268221378326416,2.226942539215088,2.226248264312744,2.2285101413726807,2.230147361755371,2.231276750564575,2.2330586910247803,2.234186887741089,2.2316625118255615,2.227728843688965,2.2252466678619385,2.226992607116699,2.228365898132324,2.2279653549194336,2.2275607585906982,2.2286007404327393,2.228675603866577,2.233175039291382,2.237762212753296,2.2452809810638428,2.2482144832611084,2.2522525787353516,2.2520828247070312,2.2470104694366455,2.241018772125244,2.2358574867248535,2.2306671142578125,2.229292392730713,2.2316105365753174,2.2355797290802,2.23817777633667,2.23960542678833,2.238356113433838,2.2374091148376465,2.235891819000244,2.2357258796691895,2.235360622406006,2.2353763580322266,2.23614764213562,2.238614320755005,2.244152545928955,2.2521564960479736,2.257797956466675,2.2595319747924805,2.2542755603790283,2.245767831802368,2.2393031120300293,2.2342376708984375,2.2291178703308105,2.226374626159668,2.22664737701416,2.232255697250366,2.23874568939209,2.2438464164733887,2.2432138919830322,2.2388875484466553,2.2348403930664062,2.231112003326416,2.2306106090545654,2.2332351207733154,2.2379229068756104,2.243586301803589,2.2492501735687256,2.254396677017212,2.2577550411224365,2.2596113681793213,2.259021759033203,2.254232406616211,2.250993490219116,2.2489795684814453,2.250129222869873,2.2515623569488525,2.2543628215789795,2.2569379806518555,2.2602996826171875,2.2632720470428467,2.2634520530700684,2.2664453983306885,2.264151096343994,2.2586076259613037,2.255575656890869,2.253920793533325,2.2543041706085205,2.25566029548645,2.2594330310821533,2.2648348808288574,2.269556760787964,2.2675282955169678,2.263126850128174,2.2576518058776855,2.2554938793182373,2.25651216506958,2.261239767074585,2.266061305999756,2.269205331802368,2.270782470703125,2.2706334590911865,2.2707161903381348,2.2687034606933594,2.266136646270752,2.262495279312134,2.2590887546539307,2.2600839138031006,2.26157808303833,2.263615131378174,2.2646801471710205,2.2634541988372803,2.263418674468994,2.262488842010498,2.262906312942505,2.263531446456909,2.2651262283325195,2.268079996109009,2.269098997116089,2.2681314945220947,2.266458034515381,2.2648980617523193,2.265969753265381,2.2704918384552,2.274690628051758,2.275233745574951,2.2717912197113037,2.268244504928589,2.2664706707000732,2.2657346725463867,2.2643373012542725,2.2635014057159424,2.263356924057007,2.265383005142212,2.2694737911224365,2.272451162338257,2.271507501602173,2.268955707550049,2.2677836418151855,2.2684383392333984,2.2682881355285645,2.2686734199523926,2.26786732673645,2.2690842151641846,2.269864320755005,2.266664505004883,2.265310764312744,2.2663424015045166,2.2701010704040527,2.272928476333618,2.2739615440368652,2.277405023574829,2.27893328666687,2.279020071029663,2.2776947021484375,2.274083137512207,2.269268035888672,2.267007827758789,2.266453981399536,2.269012451171875,2.2731881141662598,2.2763009071350098,2.2823214530944824,2.2885096073150635,2.2920548915863037,2.293090581893921,2.290687322616577,2.288480758666992,2.2874205112457275,2.2878899574279785,2.2904791831970215,2.293649673461914,2.297584295272827,2.2993998527526855,2.298431873321533,2.2981772422790527,2.2976362705230713,2.2976551055908203,2.297417163848877,2.298790454864502,2.300922393798828,2.30143666267395,2.3014142513275146,2.29866099357605,2.2961478233337402,2.294687271118164,2.2935943603515625,2.2927258014678955,2.292001962661743,2.293975353240967,2.2941033840179443,2.2942047119140625,2.2950429916381836,2.2926998138427734,2.2902913093566895,2.2858569622039795,2.281773328781128,2.2776057720184326,2.274191379547119,2.2705039978027344,2.2655465602874756,2.2628841400146484,2.2629101276397705,2.264845371246338,2.2670159339904785,2.2708146572113037,2.273116111755371,2.274068593978882,2.274991512298584,2.275792121887207,2.2784860134124756,2.279480457305908,2.2786147594451904,2.2822484970092773,2.287896156311035,2.2927398681640625,2.2944655418395996,2.2949235439300537,2.294957160949707,2.2941408157348633,2.293881893157959,2.295121908187866,2.296718120574951,2.299870729446411,2.304692506790161,2.310049057006836,2.315016031265259,2.319105625152588,2.3210649490356445,2.322863817214966,2.3233771324157715,2.3238577842712402,2.323439598083496,2.3223068714141846,2.3276073932647705,2.333436965942383,2.3401594161987305,2.3475341796875,2.3552334308624268,2.3606226444244385,2.363881826400757,2.367389678955078,2.3698818683624268,2.370408773422241,2.373418092727661,2.3764946460723877,2.380958080291748,2.384178400039673,2.388763904571533,2.3924806118011475,2.3944225311279297,2.395740032196045,2.39469838142395,2.389665365219116,2.384251117706299,2.3820433616638184,2.3822076320648193,2.3868587017059326,2.3940229415893555,2.401479959487915,2.407273054122925,2.409823417663574,2.4086880683898926,2.4053988456726074,2.401344060897827,2.399251937866211,2.401041269302368,2.4004809856414795,2.400526762008667,2.401278495788574,2.4024829864501953,2.4019458293914795,2.3999805450439453,2.3984413146972656,2.3940889835357666,2.388917922973633,2.384526014328003,2.381333589553833,2.3807060718536377,2.3811566829681396,2.3826375007629395,2.384011745452881,2.385305166244507,2.385361671447754,2.3862111568450928,2.3856544494628906,2.3857574462890625,2.38552188873291,2.386667490005493,2.3889360427856445,2.3926615715026855,2.399256944656372,2.40700626373291,2.4120359420776367,2.411998748779297,2.405579090118408,2.397724151611328,2.389557361602783,2.3798093795776367,2.372297525405884,2.369084358215332,2.3695573806762695,2.3728106021881104,2.377426862716675,2.380854606628418,2.382291793823242,2.382826805114746,2.3810689449310303,2.378445863723755,2.374638319015503,2.3726694583892822,2.370382785797119,2.3709352016448975,2.373250722885132,2.3749847412109375,2.374934673309326,2.372265577316284,2.370065689086914,2.367642879486084,2.365894079208374,2.3648810386657715,2.3652803897857666,2.366398572921753,2.3639843463897705,2.3629281520843506,2.3633832931518555,2.364996910095215,2.366608142852783,2.3698577880859375,2.372737407684326,2.373975992202759,2.3746960163116455,2.3752408027648926,2.374864339828491,2.3704493045806885,2.3691792488098145,2.365356206893921,2.362147808074951,2.3605761528015137,2.360743522644043,2.363750457763672,2.3680615425109863,2.3730602264404297,2.3763203620910645,2.3768868446350098,2.375950813293457,2.3713061809539795,2.3663249015808105,2.358125686645508,2.353977680206299,2.355802536010742,2.360665798187256,2.36517071723938,2.36594820022583,2.364211082458496,2.359187602996826,2.3532915115356445,2.348593235015869,2.347423791885376,2.3481619358062744,2.350435972213745,2.354365587234497,2.357767105102539,2.3553037643432617,2.3527636528015137,2.352489471435547,2.351519823074341,2.350680112838745,2.348874807357788,2.348876953125,2.3521041870117188,2.3550610542297363,2.3564674854278564,2.354825258255005,2.3533709049224854,2.349076747894287,2.34375262260437,2.3396358489990234,2.3395984172821045,2.339641571044922,2.3432583808898926,2.348750591278076,2.3535337448120117,2.3544459342956543,2.355030059814453,2.3555662631988525,2.3586411476135254,2.362959861755371,2.3700897693634033,2.378180980682373,2.382359743118286,2.382753610610962,2.3817684650421143,2.380455255508423,2.379485607147217,2.379276990890503,2.3798017501831055,2.380737066268921,2.381519317626953,2.3836352825164795,2.385228157043457,2.386814594268799,2.383633613586426,2.3820085525512695,2.3827290534973145,2.383671283721924,2.3854777812957764,2.3855316638946533,2.383111000061035,2.379997491836548,2.3751351833343506,2.371669054031372,2.3710668087005615,2.370844602584839,2.375974655151367,2.3822760581970215,2.389039993286133,2.3956491947174072,2.3982112407684326,2.3965635299682617,2.3957338333129883,2.39260196685791,2.3887529373168945,2.386571168899536,2.38492488861084,2.3858184814453125,2.38529372215271,2.384753465652466,2.384864568710327,2.387402296066284,2.3892760276794434,2.391005754470825,2.393252372741699,2.3946313858032227,2.3953194618225098,2.3937785625457764,2.3914718627929688,2.3892295360565186,2.387113332748413,2.385791778564453,2.385227680206299,2.3842906951904297,2.382310628890991,2.382533311843872,2.3848302364349365,2.388547658920288,2.391415596008301,2.3945207595825195,2.3944504261016846,2.3888001441955566,2.3825132846832275,2.3785014152526855,2.3766887187957764,2.3768417835235596,2.379056215286255,2.3830718994140625,2.3876423835754395,2.390401601791382,2.3918843269348145,2.391843557357788,2.3921618461608887,2.3922119140625,2.389183282852173,2.3875157833099365,2.3857028484344482,2.385328769683838,2.3874080181121826,2.387286424636841,2.3889999389648438,2.390511989593506,2.3903181552886963,2.3877296447753906,2.3872714042663574,2.3885767459869385,2.3882555961608887,2.3876965045928955,2.387842893600464,2.390343189239502,2.3936829566955566,2.397071361541748,2.397352457046509,2.398458242416382,2.398406505584717,2.3969297409057617,2.3977928161621094,2.401311159133911,2.401834011077881,2.4002888202667236,2.396164655685425,2.38999605178833,2.384481191635132,2.377718687057495,2.373602867126465,2.3717527389526367,2.372252941131592,2.376342296600342,2.37996244430542,2.3846254348754883,2.388364553451538,2.3909640312194824,2.3910722732543945,2.3916003704071045,2.389902353286743,2.3845226764678955,2.3802385330200195,2.3788907527923584,2.378981828689575,2.3813273906707764,2.3840980529785156,2.383404493331909,2.382657766342163,2.3819758892059326,2.381526231765747,2.383021116256714,2.3857669830322266,2.388256788253784,2.3892157077789307,2.3863863945007324,2.3831963539123535,2.380589723587036,2.38181209564209,2.3803212642669678,2.3771514892578125,2.3745570182800293,2.373013973236084,2.3720171451568604,2.373115301132202,2.373933792114258,2.3766348361968994,2.3805246353149414,2.385910749435425,2.390671491622925,2.391840696334839,2.391232490539551,2.388850450515747,2.385068893432617,2.3790130615234375,2.373382568359375,2.3715574741363525,2.3696606159210205,2.371206283569336,2.374289035797119,2.3795173168182373,2.3846166133880615,2.3880581855773926,2.3855173587799072,2.3823025226593018,2.3784704208374023,2.372950315475464,2.3711800575256348,2.370961904525757,2.3723108768463135,2.3751866817474365,2.3779289722442627,2.3799121379852295,2.382370710372925,2.380596399307251,2.3787355422973633,2.37559175491333,2.3717739582061768,2.367772340774536,2.3667590618133545,2.367541551589966,2.369779109954834,2.3714358806610107,2.3732709884643555,2.3747339248657227,2.375042676925659,2.372850179672241,2.3713173866271973,2.3700079917907715,2.366734027862549,2.362342119216919,2.3608930110931396,2.3615431785583496,2.363802194595337,2.363262176513672,2.3611063957214355,2.3590354919433594,2.356341600418091,2.354966402053833,2.3553285598754883,2.3548989295959473,2.3540587425231934,2.3523621559143066,2.3527040481567383,2.351771593093872,2.3504626750946045,2.347989797592163,2.3489573001861572,2.3499605655670166,2.352832078933716,2.353355646133423,2.3553528785705566,2.3571248054504395,2.3585169315338135,2.358205795288086,2.3584234714508057,2.359433650970459,2.3594136238098145,2.3619039058685303,2.3651111125946045,2.3665642738342285,2.368962526321411,2.371617078781128,2.370659828186035,2.370570659637451,2.368662118911743,2.3658220767974854,2.364515542984009,2.363319158554077,2.3609418869018555,2.3621482849121094,2.362217664718628,2.362964630126953,2.3642213344573975,2.364265203475952,2.360846757888794,2.354727029800415,2.3458564281463623,2.338007926940918,2.3334853649139404,2.332005500793457,2.333300828933716,2.3381359577178955,2.343069076538086,2.3472254276275635,2.346477508544922,2.34263014793396,2.3347551822662354,2.3283462524414062,2.323702812194824,2.321647882461548,2.32206130027771,2.3221795558929443,2.323183298110962,2.3220438957214355,2.320704221725464,2.3202638626098633,2.3196356296539307,2.319993257522583,2.320491313934326,2.319999933242798,2.3200438022613525,2.320845365524292,2.322512149810791,2.322200059890747,2.3213701248168945,2.3191349506378174,2.316542863845825,2.316087484359741,2.3175551891326904,2.322161912918091,2.3261048793792725,2.3251755237579346,2.3238883018493652,2.3211047649383545,2.318488359451294,2.3205766677856445,2.323627233505249,2.3253960609436035,2.324640989303589,2.3248724937438965,2.3247251510620117,2.324244499206543,2.324523448944092,2.3237416744232178,2.3245768547058105,2.328709125518799,2.3321783542633057,2.335230827331543,2.3382668495178223,2.3409948348999023,2.34020733833313,2.3394596576690674,2.3396525382995605,2.3398947715759277,2.3393301963806152,2.3411343097686768,2.3446595668792725,2.350322961807251,2.3564655780792236,2.362894296646118,2.367338180541992,2.3690028190612793,2.368541717529297,2.3669958114624023,2.3639793395996094,2.3620336055755615,2.3601715564727783,2.3601491451263428,2.3619911670684814,2.365710735321045,2.3708348274230957,2.3742244243621826,2.375884771347046,2.3738651275634766,2.371134042739868,2.365726947784424,2.361722946166992,2.3578386306762695,2.3548543453216553,2.352003335952759,2.3529844284057617,2.354823589324951,2.354268789291382,2.35453724861145,2.3520727157592773,2.3496334552764893,2.3483357429504395,2.3447043895721436,2.343015670776367,2.3431670665740967,2.3447253704071045,2.3478543758392334,2.3515894412994385,2.3554532527923584,2.35895037651062,2.3600170612335205,2.3611161708831787,2.360200881958008,2.3593504428863525,2.3570966720581055,2.3553073406219482,2.3545730113983154,2.354508876800537,2.356435537338257,2.357823371887207,2.3611042499542236,2.3663978576660156,2.370936632156372,2.374523401260376,2.3757779598236084,2.376106023788452,2.3749821186065674,2.374772787094116,2.3785688877105713,2.3802266120910645,2.3806285858154297,2.380932569503784,2.379323959350586,2.3763813972473145,2.3700976371765137,2.366191864013672,2.3635008335113525,2.3644163608551025,2.367882251739502,2.3714041709899902,2.3760972023010254,2.3790576457977295,2.375168800354004,2.3696346282958984,2.3627419471740723,2.358200788497925,2.356779098510742,2.3564178943634033,2.358769655227661,2.3620800971984863,2.3667962551116943,2.3727924823760986,2.379096031188965,2.3810184001922607,2.3819921016693115,2.3818838596343994,2.381073474884033,2.380836009979248,2.3802831172943115,2.378828287124634,2.3785548210144043,2.37638783454895,2.3731508255004883,2.370551586151123,2.3680176734924316,2.367372989654541,2.36962628364563,2.375169277191162,2.3804266452789307,2.3836328983306885,2.3817033767700195,2.3758044242858887,2.368833303451538,2.364429473876953,2.3612077236175537,2.3604021072387695,2.3639910221099854,2.365614175796509,2.36427640914917,2.3608808517456055,2.358355760574341,2.3594911098480225,2.3592920303344727,2.361290454864502,2.363036632537842,2.366234064102173,2.369713544845581,2.3719000816345215,2.372617483139038,2.3718602657318115,2.3699464797973633,2.365217924118042,2.361582040786743,2.3591833114624023,2.3589484691619873,2.3620126247406006,2.3614659309387207,2.3622312545776367,2.3631865978240967,2.360631227493286,2.356355667114258,2.352630138397217,2.350857734680176,2.3500170707702637,2.3530845642089844,2.358781576156616,2.362027645111084,2.361375570297241,2.3582422733306885,2.3531270027160645,2.350856304168701,2.352872133255005,2.3566603660583496,2.3631112575531006,2.3691048622131348,2.3697423934936523,2.369689464569092,2.365466594696045,2.362539768218994,2.3597021102905273,2.3576908111572266,2.3575844764709473,2.361420154571533,2.364633321762085,2.3641486167907715,2.3623175621032715,2.3617935180664062,2.3598175048828125,2.35867977142334,2.3515326976776123,2.3485701084136963,2.3464479446411133,2.3473825454711914,2.351418972015381,2.355807304382324,2.3610076904296875,2.3624064922332764,2.362837553024292,2.3611392974853516,2.3575687408447266,2.35355281829834,2.350104331970215,2.347759962081909],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"rgba(35,128,132,0.8)\"},\"name\":\"Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499],\"y\":[0.07610423117876053,0.07967394590377808,0.07809514552354813,0.07532159239053726,0.07544823735952377,0.07805701345205307,0.07252535223960876,0.07948486506938934,0.07674805819988251,0.07470759004354477,0.07524962723255157,0.07501639425754547,0.07711230963468552,0.07304482907056808,0.08249560743570328,0.07069366425275803,0.07748397439718246,0.07301728427410126,0.07535562664270401,0.08138852566480637,0.07381117343902588,0.08172687888145447,0.07536465674638748,0.0731838271021843,0.0753689706325531,0.07304719090461731,0.07841378450393677,0.07313486933708191,0.07760108262300491,0.07313848286867142,0.07585187256336212,0.07771821320056915,0.07478557527065277,0.07594065368175507,0.07861091941595078,0.0795418918132782,0.07152335345745087,0.0785033330321312,0.07713694125413895,0.07805205881595612,0.0761791542172432,0.07315932959318161,0.0762573778629303,0.07494568079710007,0.07376742362976074,0.0744950994849205,0.07477393746376038,0.0754116103053093,0.07253440469503403,0.07434981316328049,0.07078368216753006,0.08454131335020065,0.07563173770904541,0.076069675385952,0.08056695759296417,0.07540586590766907,0.0821540355682373,0.07525487244129181,0.0809239074587822,0.08379126340150833,0.07220914214849472,0.07575555145740509,0.07508079707622528,0.08677925914525986,0.07412020117044449,0.07918986678123474,0.0752902552485466,0.07159194350242615,0.09033770859241486,0.0743837058544159,0.0753839910030365,0.07608342915773392,0.07244689017534256,0.07536087930202484,0.0793292224407196,0.07457786053419113,0.07433149963617325,0.07516980171203613,0.07452156394720078,0.07953422516584396,0.08008500188589096,0.07409550994634628,0.07931086421012878,0.08528348803520203,0.07792875915765762,0.07416252791881561,0.0733584389090538,0.07261260598897934,0.0710102841258049,0.0741298496723175,0.07845138758420944,0.07401017844676971,0.07588361203670502,0.07512769848108292,0.0772799700498581,0.07658601552248001,0.07418800890445709,0.07726123183965683,0.07785945385694504,0.07602536678314209,0.08068402856588364,0.08134596049785614,0.07355312258005142,0.07353249937295914,0.07441966235637665,0.07591422647237778,0.07529084384441376,0.07098514586687088,0.0789647251367569,0.07504923641681671,0.07342924177646637,0.07859079539775848,0.0768023133277893,0.08065646141767502,0.07308711856603622,0.0893431007862091,0.07579106837511063,0.08059931546449661,0.07974051684141159,0.0725662037730217,0.07292383164167404,0.07461453974246979,0.07429416477680206,0.07211693376302719,0.07466790825128555,0.0739780142903328,0.08521368354558945,0.0735725462436676,0.07042480260133743,0.07237989455461502,0.08203399926424026,0.07644835114479065,0.07464049011468887,0.07529659569263458,0.07383552938699722,0.07459571212530136,0.07858990132808685,0.07412397861480713,0.07530839741230011,0.07194668799638748,0.08875013142824173,0.07585539668798447,0.08333459496498108,0.0760427713394165,0.07174010574817657,0.07554614543914795,0.07359101623296738,0.07356807589530945,0.07942112535238266,0.07779128849506378,0.09330211579799652,0.07958018034696579,0.07671619951725006,0.07805430144071579,0.07369430363178253,0.08013266324996948,0.07078737020492554,0.0742851048707962,0.07483266294002533,0.0742482915520668,0.07709562033414841,0.0760309025645256,0.07314939796924591,0.0798092931509018,0.07465147972106934,0.08026552945375443,0.07712211459875107,0.07567458599805832,0.0738363265991211,0.07387930899858475,0.07944557815790176,0.07451983541250229,0.08330155164003372,0.07521902024745941,0.07362371683120728,0.08092309534549713,0.08122355490922928,0.07177165150642395,0.0780002772808075,0.07522182166576385,0.07827181369066238,0.07981512695550919,0.07665839791297913,0.07629949599504471,0.07285460829734802,0.07534587383270264,0.07417261600494385,0.07267986238002777,0.07409656792879105,0.07300074398517609,0.07409193366765976,0.07755548506975174,0.07328631728887558,0.08109074831008911,0.07731398940086365,0.09367984533309937,0.07597896456718445,0.07080519199371338,0.07356902956962585,0.0742456465959549,0.07653830945491791,0.07731109112501144,0.08123132586479187,0.07256260514259338,0.07348252087831497,0.0834847241640091,0.07538868486881256,0.07288356870412827,0.07707962393760681,0.07363981753587723,0.07855063676834106,0.08406481146812439,0.0730530321598053,0.07573489844799042,0.08100643754005432,0.07919269055128098,0.07312958687543869,0.07614792138338089,0.07628873735666275,0.07721016556024551,0.08105101436376572,0.07967587560415268,0.09001301974058151,0.07674204558134079,0.07574690133333206,0.08747972548007965,0.08090005069971085,0.07714295387268066,0.07775909453630447,0.07880552113056183,0.0756649598479271,0.07305751740932465,0.0807567685842514,0.08315533399581909,0.07817428559064865,0.07825340330600739,0.0799124538898468,0.08373849838972092,0.07684921473264694,0.07692639529705048,0.07518631219863892,0.07550127804279327,0.07301155477762222,0.08109994232654572,0.0752907395362854,0.07411447167396545,0.07452894002199173,0.07905984669923782,0.07653135061264038,0.0735565647482872,0.07807851582765579,0.08433511853218079,0.07447578758001328,0.07450908422470093,0.07593528926372528,0.07671229541301727,0.07593493908643723,0.07909359037876129,0.07724052667617798,0.07497461885213852,0.08778367936611176,0.07571457326412201,0.0779697522521019,0.07847860455513,0.07884841412305832,0.09232097864151001,0.0773388147354126,0.08078917115926743,0.07604791224002838,0.07520215213298798,0.07595691829919815,0.0768364667892456,0.07407118380069733,0.07412400096654892,0.07912222295999527,0.07059944421052933,0.0796162486076355,0.07657141238451004,0.07350543886423111,0.07096931338310242,0.07279559224843979,0.06920669972896576,0.07580271363258362,0.08811970055103302,0.08747334033250809,0.09002470970153809,0.0716431587934494,0.07256074249744415,0.08030317723751068,0.07988087832927704,0.08000322431325912,0.0776570662856102,0.07827792316675186,0.07517754286527634,0.07780645787715912,0.07550199329853058,0.0744137167930603,0.07207923382520676,0.07249660044908524,0.0748148038983345,0.07846507430076599,0.08265712112188339,0.09007436782121658,0.07003118842840195,0.07336526364088058,0.07835842669010162,0.08257375657558441,0.0766582265496254,0.07794506102800369,0.07442784309387207,0.08139732480049133,0.07421941310167313,0.07767362892627716,0.0740356296300888,0.0951831117272377,0.07723239064216614,0.07622873783111572,0.07619667798280716,0.0726146548986435,0.07862649112939835,0.07311738282442093,0.07073818147182465,0.07902199029922485,0.07781800627708435,0.07447638362646103,0.07730914652347565,0.08190510421991348,0.07792914658784866,0.07516356557607651,0.07634659111499786,0.07692698389291763,0.07352250814437866,0.07382918894290924,0.07794782519340515,0.07461731880903244,0.0716199055314064,0.07364615052938461,0.07999932020902634,0.07567967474460602,0.07614127546548843,0.08296288549900055,0.07857538759708405,0.08412031084299088,0.0984378457069397,0.07188520580530167,0.07407405227422714,0.07836707681417465,0.07625847309827805,0.08925478160381317,0.07648204267024994,0.07473500072956085,0.0756186917424202,0.08086761832237244,0.07426657527685165,0.07576428353786469,0.07625307142734528,0.07483182102441788,0.07743048667907715,0.07235699892044067,0.07846599072217941,0.07192380726337433,0.0785939022898674,0.07241471856832504,0.07407914847135544,0.0815352126955986,0.07735273987054825,0.07760373502969742,0.07288521528244019,0.0838967114686966,0.07351943850517273,0.07162278145551682,0.07335309684276581,0.0798424780368805,0.07642589509487152,0.0750504732131958,0.07515507191419601,0.080360546708107,0.0724465548992157,0.07369125634431839,0.07257995009422302,0.07573076337575912,0.07686961442232132,0.10256251692771912,0.07478497177362442,0.07312215119600296,0.07852679491043091,0.07272468507289886,0.07551909983158112,0.07514173537492752,0.08078233897686005,0.08192736655473709,0.0747053399682045,0.07737064361572266,0.07687363773584366,0.07673465460538864,0.07916151732206345,0.07296443730592728,0.07655739784240723,0.07447727024555206,0.07482042163610458,0.07393618673086166,0.0756705105304718,0.08614444732666016,0.0770515725016594,0.07441938668489456,0.07912284135818481,0.07228577882051468,0.07650619000196457,0.08022865653038025,0.07349453121423721,0.07518233358860016,0.0784415602684021,0.07708947360515594,0.07536835223436356,0.07680734992027283,0.07899132370948792,0.07696280628442764,0.07997773587703705,0.07505384832620621,0.07466822117567062,0.07498984038829803,0.07529408484697342,0.08289516717195511,0.07551348954439163,0.07345544546842575,0.07610200345516205,0.07283256947994232,0.07509933412075043,0.0768439918756485,0.07849516719579697,0.07473909854888916,0.0863807275891304,0.07489492744207382,0.07744959741830826,0.08083104342222214,0.07218056917190552,0.07416398823261261,0.07312346994876862,0.07693345099687576,0.07533439993858337,0.08722730726003647,0.07518170773983002,0.07903307676315308,0.0740470364689827,0.07648548483848572,0.07851333171129227,0.08505312353372574,0.07797332853078842,0.08209437131881714,0.08963653445243835,0.07482799142599106,0.07472235709428787,0.07410011440515518,0.07519631832838058,0.07961134612560272,0.07958118617534637,0.10448293387889862,0.07721786946058273,0.0741964802145958,0.07631849497556686,0.07391920685768127,0.07821707427501678,0.07236912101507187,0.07168231904506683,0.0716758519411087,0.07725781947374344,0.07451635599136353,0.07240355014801025,0.07327808439731598,0.07700523734092712,0.067436084151268,0.07686560600996017,0.07616964727640152,0.08376003801822662,0.08766263723373413,0.10135859251022339,0.0730668157339096,0.07640818506479263,0.07412901520729065,0.0836208388209343,0.08070414513349533,0.0727696418762207,0.07687519490718842,0.073197141289711,0.08177850395441055,0.0788942202925682,0.0789976641535759,0.07457921653985977,0.0900053083896637,0.07363419234752655,0.07179848104715347,0.07907041907310486,0.07744767516851425,0.07215885072946548,0.07525249570608139,0.06958193331956863,0.07794811576604843,0.07642356306314468,0.07864126563072205,0.08051153272390366,0.0871218740940094,0.0884971171617508,0.0723281279206276,0.0801258236169815,0.07988549023866653,0.0811241865158081,0.0773288905620575,0.07898572087287903,0.0696183443069458,0.07973130792379379,0.07455559074878693,0.07571997493505478,0.07360021024942398,0.08304475247859955,0.07340613007545471,0.07278747111558914,0.07853403687477112,0.07064617425203323,0.07204516232013702,0.07301130145788193,0.08143940567970276,0.07498013973236084,0.07978714257478714,0.08990833908319473,0.080019012093544,0.07528157532215118,0.07611820846796036,0.07721764594316483,0.08566392213106155,0.07427725195884705,0.07451414316892624,0.07494048029184341,0.07310424000024796,0.07553904503583908,0.07664062082767487,0.074550561606884,0.07662857323884964,0.07228920608758926,0.0722220242023468,0.07758170366287231,0.0837486982345581,0.07264824211597443,0.08711367845535278,0.07741189748048782,0.07501740753650665,0.0777008906006813,0.07714490592479706,0.07088945806026459,0.07397713512182236,0.0709921345114708,0.07941017299890518,0.0757002905011177,0.07469011098146439,0.07806123048067093,0.07250391691923141,0.07816198468208313,0.07499366998672485,0.09041259437799454,0.07866781949996948,0.07970061898231506,0.07800958305597305,0.07518257200717926,0.07862558960914612,0.08166545629501343,0.0731554850935936,0.07223300635814667,0.07365472614765167,0.07565438747406006,0.07276783138513565,0.0742594376206398,0.07196450978517532,0.07414517551660538,0.08757665753364563,0.07648959010839462,0.07350052148103714,0.07535415142774582,0.07925301790237427,0.07398758828639984,0.07661822438240051,0.07220171391963959,0.07322730123996735,0.07491261512041092,0.07491783797740936,0.08022191375494003,0.08650621771812439,0.07645627856254578,0.07165996730327606,0.07511735707521439,0.07884939014911652,0.07256381958723068,0.07220626622438431,0.07880105078220367,0.0755632147192955,0.0756097137928009,0.07444984465837479,0.07197905331850052,0.07645082473754883,0.0887468010187149,0.08003406971693039,0.07612606137990952,0.07642309367656708,0.07082130014896393,0.07825593650341034,0.06950020790100098,0.08531723916530609,0.07349061965942383,0.0717734768986702,0.07526582479476929,0.0750659629702568,0.07465766370296478,0.07266435772180557,0.07593809813261032,0.09581690281629562,0.0789918601512909,0.0813482478260994,0.07609519362449646,0.07925048470497131,0.07615449279546738,0.07745114713907242,0.07361150532960892,0.07958470284938812,0.07801880687475204,0.10087896138429642,0.07320871204137802,0.07513444125652313,0.07897152751684189,0.07693134993314743,0.07519122213125229,0.07557680457830429,0.06834506243467331,0.07800466567277908,0.0908844992518425,0.07235337793827057,0.07806094735860825,0.0760505422949791,0.07684686779975891,0.08143124729394913,0.0709710642695427,0.07375990599393845,0.08837388455867767,0.07904206216335297,0.08503245562314987,0.0737076997756958,0.07406803965568542,0.07375052571296692,0.07444316893815994,0.07622379809617996,0.07704394310712814,0.07417087256908417,0.07758336514234543,0.07779557257890701,0.07473894208669662,0.07277863472700119,0.07433944940567017,0.07832007855176926,0.07583493739366531,0.07743006944656372,0.08100835978984833,0.07480292022228241,0.07665799558162689,0.07696328312158585,0.07420046627521515,0.07379889488220215,0.07348795235157013,0.07351101189851761,0.07199003547430038,0.07255146652460098,0.07288208603858948,0.07414136081933975,0.08345755189657211,0.07442089170217514,0.08225144445896149,0.07760943472385406,0.07712028175592422,0.07398729026317596,0.07595330476760864,0.07182326912879944,0.08742032200098038,0.07333806157112122,0.07933496683835983,0.07727967202663422,0.07323072850704193,0.07032673805952072,0.06912123411893845,0.07553612440824509,0.07393394410610199,0.07444888353347778,0.07279263436794281,0.0748562142252922,0.07399503141641617,0.07745885848999023,0.08263980597257614,0.0821736752986908,0.07727319747209549,0.0719977393746376,0.08041292428970337,0.08190366625785828,0.07706671953201294,0.08611105382442474,0.076845683157444,0.07202751934528351,0.07276042550802231,0.0784708559513092,0.07518216222524643,0.0735364630818367,0.07901182770729065,0.07172852009534836,0.07328082621097565,0.07560566812753677,0.07995237410068512,0.07451334595680237,0.0750098004937172,0.07877510040998459,0.07856669276952744,0.07280793786048889,0.07502272725105286,0.07531767338514328,0.07338588684797287,0.08203720301389694,0.08188004046678543,0.07901498675346375,0.07414327561855316,0.07688647508621216,0.07407134026288986,0.07375502586364746,0.0814397782087326,0.07998707890510559,0.07240095734596252,0.07771334052085876,0.07504841685295105,0.07919184863567352,0.0738876685500145,0.07670421898365021,0.07542253285646439,0.07787655293941498,0.07251790910959244,0.09808748960494995,0.08245712518692017,0.07499214261770248,0.07702437788248062,0.07250191271305084,0.07378475368022919,0.08537010103464127,0.07407354563474655,0.08029790967702866,0.07191430032253265,0.07907826453447342,0.07530082017183304,0.07278592884540558,0.08104357868432999,0.0739303007721901,0.07265622168779373,0.07767334580421448,0.0936887115240097,0.07929529249668121,0.07507432252168655,0.07388388365507126,0.07378298044204712,0.07768961787223816,0.07736638188362122,0.07281358540058136,0.07658518105745316,0.07214496284723282,0.07369084656238556,0.07275097072124481,0.07674358040094376,0.07713669538497925,0.07476712763309479,0.08089329302310944,0.07548075914382935,0.07256250083446503,0.07773362100124359,0.07107450813055038,0.07403747737407684,0.0724487230181694,0.07703621685504913,0.08076581358909607,0.08080869913101196,0.0706319734454155,0.07809054106473923,0.07908347994089127,0.07250905781984329,0.07478488981723785,0.07543517649173737,0.07154452055692673,0.08173418045043945,0.07379154115915298,0.08053883910179138,0.073716901242733,0.07155780494213104,0.0721283033490181,0.07347369194030762,0.07145261764526367,0.07357107102870941,0.07564111053943634,0.09692741930484772,0.07775169610977173,0.0737324059009552,0.07918057590723038,0.07505097985267639,0.07737085968255997,0.07267201691865921,0.07836300134658813,0.0714196190237999,0.0805559828877449,0.07412376999855042,0.07523719221353531,0.08854904770851135,0.06838041543960571,0.07162551581859589,0.07359456270933151,0.07042980194091797,0.0749422237277031,0.07668130844831467,0.0740140900015831,0.07376455515623093,0.07416890561580658,0.07454764097929001,0.0755329579114914,0.07266559451818466,0.07166334986686707,0.07380320131778717,0.07191287726163864,0.0809277817606926,0.07586804777383804,0.07953694462776184,0.08880451321601868,0.07395140826702118,0.07474814355373383,0.07254628837108612,0.07554452866315842,0.07957319915294647,0.07536707818508148,0.07543665915727615,0.07503427565097809,0.07183349877595901,0.06927778571844101,0.072035051882267,0.07323385030031204,0.07349219918251038,0.07271012663841248,0.08772997558116913,0.07364261150360107,0.07144416123628616,0.08356058597564697,0.07561247795820236,0.07263211160898209,0.0728696808218956,0.08173701912164688,0.07927963882684708,0.06991910934448242,0.07810372114181519,0.07222927361726761,0.07118082791566849,0.08656422048807144,0.07238578796386719,0.07261396199464798,0.07489031553268433,0.07410711795091629,0.07614019513130188,0.07626060396432877,0.08069072663784027,0.07609134912490845,0.07410026341676712,0.07883550971746445,0.07391680777072906,0.07283288240432739,0.07508771121501923,0.0694480836391449,0.07653496414422989,0.0814749225974083,0.07345902174711227,0.07471313327550888,0.07545317709445953,0.07644437998533249,0.07301300019025803,0.07558337599039078,0.07620126008987427,0.07666044682264328,0.07309512794017792,0.07400219142436981,0.08085695654153824,0.07630155235528946,0.07602996379137039,0.0768246203660965,0.09260865300893784,0.07460734248161316,0.07590002566576004,0.07338791340589523,0.07388272136449814,0.06945450603961945,0.07249514013528824,0.07494901865720749,0.07162749022245407,0.07744213193655014,0.07332470268011093,0.07562050968408585,0.07542122155427933,0.08243800699710846,0.07353757321834564,0.07180459797382355,0.07636041939258575,0.07294395565986633,0.07690554857254028,0.07199767231941223,0.07605987787246704,0.07721991837024689,0.07562346756458282,0.07558667659759521,0.0788358673453331,0.0749533548951149,0.07869361340999603,0.07321960479021072,0.08868483453989029,0.0777212455868721,0.07339421659708023,0.07548002898693085,0.07097122818231583,0.07248684018850327,0.07323093712329865,0.08002636581659317,0.08356590569019318,0.07135897129774094,0.0745178610086441,0.0760812759399414,0.07383497059345245,0.07353444397449493,0.07503842562437057,0.07832594960927963,0.08307473361492157,0.06882594525814056,0.08251063525676727,0.07467769831418991,0.0759764015674591,0.07664388418197632,0.07647596299648285,0.09415622055530548,0.07697586715221405,0.07408761233091354,0.07513534277677536,0.0752965584397316,0.07225438952445984,0.07529635727405548,0.0754418820142746,0.0803760215640068,0.07462751120328903,0.07370764017105103,0.07658632844686508,0.07843778282403946,0.0767366886138916,0.07438001781702042,0.07308420538902283,0.07369066029787064,0.08610613644123077,0.07602334767580032,0.07504108548164368,0.07855252921581268,0.08950761705636978,0.07434173673391342,0.06898052990436554,0.07474040240049362,0.07717405259609222,0.07772766798734665,0.07499337196350098,0.07661017775535583,0.07228043675422668,0.07433918863534927,0.08032345771789551,0.07340119779109955,0.07607445120811462,0.07426496595144272,0.08103733509778976,0.1078772321343422,0.07570944726467133,0.07744759321212769,0.0700015276670456,0.07015345990657806,0.07214745879173279,0.07382304221391678,0.07729772478342056,0.0826515406370163,0.078700952231884,0.08211105316877365,0.08324377238750458,0.07438337802886963,0.07547619193792343,0.0773959755897522,0.07690104842185974,0.07098954170942307,0.07770994305610657,0.07268884032964706,0.07564467936754227,0.07252649962902069,0.07719076424837112,0.08918606489896774,0.0750744491815567,0.07139373570680618,0.07685563713312149,0.0735836997628212,0.07346886396408081,0.07361147552728653,0.07022188603878021,0.07503531873226166,0.0721791461110115,0.08246640115976334,0.08371202647686005,0.0789884552359581,0.07507196813821793,0.07520975172519684,0.08174492418766022,0.07346832007169724,0.08759775757789612,0.0772814154624939,0.07847583293914795,0.07671881467103958,0.0723252221941948,0.08616074174642563,0.0707758441567421,0.08321035653352737,0.07426087558269501,0.06959717720746994,0.07125730812549591,0.07632244378328323,0.07954536378383636,0.06976626068353653,0.0842600092291832,0.0732981264591217,0.07192131876945496,0.08007367700338364,0.08462870121002197,0.07302495092153549,0.07439080625772476,0.07155664265155792,0.07599420845508575,0.07322987914085388,0.07061287760734558,0.07079315185546875,0.07351277023553848,0.07437321543693542,0.08168834447860718,0.07475140690803528,0.08844860643148422,0.06971129029989243,0.07665320485830307,0.07789092510938644,0.07613077759742737,0.07874634116888046,0.07677401602268219,0.06961125880479813,0.08342783898115158,0.0709623470902443,0.08066505193710327,0.07916760444641113,0.06857028603553772,0.08917286992073059,0.07201430201530457,0.07063151150941849,0.07707924395799637,0.07362933456897736,0.07355185598134995,0.07397717982530594,0.07347364723682404,0.07429022341966629,0.06873498857021332,0.08958522230386734,0.07796794921159744,0.07053007185459137,0.0709078311920166,0.07184693217277527,0.08012112230062485,0.07797103375196457,0.06644949316978455,0.08420935273170471,0.07927972823381424,0.07367613911628723,0.07913034409284592,0.07388428598642349,0.07756601274013519,0.07868458330631256,0.07652163505554199,0.07479357719421387,0.08168016374111176,0.07893911749124527,0.07485584914684296,0.07842938601970673,0.0773828998208046,0.0726054236292839,0.06990369409322739,0.07355793565511703,0.07247968018054962,0.07401996850967407,0.07592082023620605,0.07405004650354385,0.07756607234477997,0.07524071633815765,0.07619687169790268,0.08110400289297104,0.07544313371181488,0.07477617263793945,0.07435373961925507,0.09269730001688004,0.08088910579681396,0.08539050817489624,0.07545430213212967,0.07449936121702194,0.08670374751091003,0.07390255481004715,0.07313775271177292,0.07794606685638428,0.0795341208577156,0.0724916160106659,0.07246417552232742,0.07790818065404892,0.07368353754281998,0.06972582638263702,0.07131537050008774,0.08234405517578125,0.07635075598955154,0.09535728394985199,0.07340670377016068,0.07593715935945511,0.07367578893899918,0.07430241256952286,0.07175590842962265,0.074795663356781,0.07603569328784943,0.07442719489336014,0.07730355858802795,0.07811776548624039,0.07350646704435349,0.07604414224624634,0.08227348327636719,0.07576528191566467,0.07465925067663193,0.07517171651124954,0.07494737207889557,0.07316683232784271,0.08404301851987839,0.07673949003219604,0.09462397545576096,0.08149942010641098,0.07376303523778915,0.07414054870605469,0.08343128859996796,0.07652576267719269,0.07424166798591614,0.07180684059858322,0.07634210586547852,0.10175628215074539,0.07684647291898727,0.06871460378170013,0.0725855827331543,0.07080162316560745,0.07447994500398636,0.07663757354021072,0.0714571550488472,0.07996076345443726,0.08228945732116699,0.07318932563066483,0.07578816264867783,0.07673372328281403,0.07037781924009323,0.0702308714389801,0.07580270618200302,0.07663196325302124,0.07793817669153214,0.07018666714429855,0.07813122123479843,0.07767687737941742,0.07557982206344604,0.072287917137146,0.07519560307264328,0.07123138010501862,0.07089651376008987,0.07187533378601074,0.07280018925666809,0.0716073215007782,0.07884658873081207,0.07368165999650955,0.08712808042764664,0.07701756805181503,0.07460525631904602,0.07425923645496368,0.07009388506412506,0.06933317333459854,0.07324935495853424,0.08011936396360397,0.06793301552534103,0.0706486850976944,0.07726481556892395,0.0733218863606453,0.07415886223316193,0.07202904671430588,0.07240427285432816,0.07463691383600235,0.09511879831552505,0.0708688348531723,0.07528682798147202,0.07346618175506592,0.078033946454525,0.07722777873277664,0.08367472141981125,0.07308532297611237,0.07156727463006973,0.08980921655893326,0.0741887092590332,0.07554738968610764,0.0768539234995842,0.07761745899915695,0.08057456463575363,0.07036139070987701,0.07200667262077332,0.0785004049539566,0.07189033180475235,0.07611813396215439,0.0739252120256424,0.07509376108646393,0.0778418630361557,0.08184798061847687,0.07692106068134308,0.07577462494373322,0.0757112130522728,0.07810258865356445,0.07251765578985214,0.07442115992307663,0.07674064487218857,0.07405214011669159,0.07717648893594742,0.07078384608030319,0.06790623068809509,0.07311952859163284,0.0766107439994812,0.06982254236936569,0.07740027457475662,0.07896889001131058,0.09383057057857513,0.07114744931459427,0.07483816146850586,0.07382947206497192,0.07077258825302124,0.07404686510562897,0.07403509318828583,0.07179675996303558,0.07526498287916183,0.07379596680402756,0.07391995191574097,0.07173017412424088,0.0769476592540741,0.08095476031303406,0.06863755732774734,0.07345245778560638,0.07379526644945145,0.0735664889216423,0.07451160997152328,0.07589638233184814,0.07291845977306366,0.08305241167545319,0.0769176110625267,0.07123516499996185,0.07761745899915695,0.0727209821343422,0.07068026065826416,0.07558148354291916,0.08085578680038452,0.09040253609418869,0.07654127478599548,0.07017643749713898,0.07911094278097153,0.07852587848901749,0.07184242457151413,0.07592630386352539,0.07888736575841904,0.07454004883766174,0.06969711929559708,0.07747607678174973,0.07446595281362534,0.1019318550825119,0.07520020753145218,0.08503498136997223,0.07242487370967865,0.07745338976383209,0.07915803045034409,0.0742693617939949,0.07687071710824966,0.07165352255105972,0.07430646568536758,0.07386067509651184,0.0724516212940216,0.0827447697520256,0.0758681371808052,0.07199180126190186,0.07886394113302231,0.0735074058175087,0.07493853569030762,0.0760781541466713,0.07343390583992004,0.07328975945711136,0.07314249128103256,0.07575318962335587,0.07588814198970795,0.07743296027183533,0.07405086606740952,0.07723213732242584,0.07291810214519501,0.07276895642280579,0.07806220650672913,0.07536875456571579,0.07139598578214645,0.0764426440000534,0.07480709254741669,0.07768741995096207,0.08711573481559753,0.07464605569839478,0.07681525498628616,0.07151749730110168,0.07745173573493958,0.07962334901094437,0.07554181665182114,0.07453689724206924,0.08147570490837097,0.07656456530094147,0.0778336450457573,0.07300521433353424,0.08428126573562622,0.07920677959918976,0.07822190225124359,0.07371153682470322,0.07975078374147415,0.0720985159277916,0.0753188505768776,0.07637804746627808,0.07652557641267776,0.07468602061271667,0.07202426344156265,0.07650038599967957,0.07582138478755951,0.08451781421899796,0.07707403600215912,0.0759277269244194,0.07358057051897049,0.07348968088626862,0.07036499679088593,0.07433033734560013,0.07632945477962494,0.07467714697122574,0.06948373466730118,0.0762777253985405,0.07389802485704422,0.07755963504314423,0.08168134838342667,0.07889898866415024,0.07202482968568802,0.07543820887804031,0.0728069394826889,0.07549963891506195,0.07726205140352249,0.0755726620554924,0.07225360721349716,0.07082369178533554,0.07648393511772156,0.07266879081726074,0.07223659753799438,0.084754578769207,0.07378189265727997,0.07496235519647598,0.07170408219099045,0.07541807740926743,0.08137337118387222,0.07497838139533997,0.076601542532444,0.07223224639892578,0.07092225551605225,0.0801524668931961,0.07272860407829285,0.07715209573507309,0.09243207424879074,0.0767873078584671,0.07579343020915985,0.0761648491024971,0.07194162905216217,0.08100681751966476,0.07780669629573822,0.07470664381980896,0.07555007189512253,0.07349397242069244,0.07466339319944382,0.08617933839559555,0.07173462212085724,0.07693943381309509,0.07380444556474686,0.071368508040905,0.07640170305967331,0.07150153070688248,0.08017262071371078,0.08242224156856537,0.07199547439813614,0.07762112468481064,0.07587374001741409,0.08367487043142319,0.08195090293884277,0.07614021003246307,0.07635730504989624,0.07937534153461456,0.07946745306253433,0.0716261938214302,0.08154211938381195,0.07234456390142441,0.07342808693647385,0.07301169633865356,0.07674788683652878,0.08055949956178665,0.07544992119073868,0.07256215065717697,0.07174422591924667,0.09347007423639297,0.0730876475572586,0.07370418310165405,0.07570719718933105,0.07442417740821838,0.07617206871509552,0.07562316954135895,0.07738012820482254,0.08364900201559067,0.08193019032478333,0.08295601606369019,0.07784517854452133,0.08167726546525955,0.07062795758247375,0.07209719717502594,0.07333134114742279,0.0702301561832428,0.07495913654565811,0.07432092726230621,0.0746106505393982,0.0732807144522667,0.0783938392996788,0.07564936578273773,0.07444624602794647,0.07537063956260681,0.07203477621078491,0.07100172340869904,0.07248251140117645,0.07154911011457443,0.07360407710075378,0.0736958384513855,0.08925441652536392,0.07353325188159943,0.07738903909921646,0.07802406698465347,0.06873571127653122,0.07577669620513916,0.07893083244562149,0.07352107763290405,0.07469000667333603,0.08111675828695297,0.0833560973405838,0.07459086924791336,0.07133286446332932,0.07762342691421509,0.0755935087800026,0.07060691714286804,0.07242314517498016,0.07324261218309402,0.07472369074821472,0.07056088000535965,0.07361192256212234,0.07717597484588623,0.07150520384311676,0.07335656136274338,0.07620639353990555,0.08000250905752182,0.07335495203733444,0.07484295964241028,0.0733163058757782,0.07508540153503418,0.07485389709472656,0.07849641889333725,0.07798422127962112,0.08714596182107925,0.07524498552083969,0.0702223852276802,0.07516459375619888,0.07553556561470032,0.07208755612373352,0.07455138117074966,0.07259763777256012,0.07475750148296356,0.07927849143743515,0.07682543992996216,0.07629159837961197,0.07159318029880524,0.08633963763713837,0.08885311335325241,0.0741078332066536,0.07224549353122711,0.07629342377185822,0.07437589019536972,0.08593948930501938,0.06994185596704483,0.0736408457159996,0.07537282258272171,0.07099630683660507,0.07564827799797058,0.07772213965654373,0.07274247705936432,0.07170765846967697,0.0770631805062294,0.07205402851104736,0.0699041336774826,0.07136804610490799,0.07799125462770462,0.07690397650003433,0.08202719688415527,0.07357773184776306,0.1092197448015213,0.0737210065126419,0.07392577081918716,0.07272310554981232,0.07636738568544388,0.07322186231613159,0.11252311617136002,0.08038850873708725,0.07967858761548996,0.07876777648925781,0.07407636940479279,0.07267093658447266,0.0786513090133667,0.07849500328302383,0.07668232172727585,0.0722164586186409,0.075348399579525,0.07441441714763641,0.07442431896924973,0.07638619095087051,0.08426479250192642,0.07754214107990265,0.07372019439935684,0.08283669501543045,0.07463369518518448,0.07034210860729218,0.07436005026102066,0.07365859299898148,0.07322407513856888,0.07959812134504318,0.07139476388692856,0.0872948095202446,0.0755562111735344,0.07906864583492279,0.079449363052845,0.07998667657375336,0.07509645074605942,0.07615578174591064,0.07455877959728241,0.08878953009843826,0.07300367951393127,0.07575517892837524,0.07433929294347763,0.08421166986227036,0.07032349705696106,0.0871012806892395,0.07491105049848557,0.07293525338172913,0.0781082957983017,0.07398402690887451,0.077547587454319,0.07559863477945328,0.07148022949695587,0.07207125425338745,0.0728592574596405,0.07154905050992966,0.0699661374092102,0.07535320520401001,0.0828276053071022,0.07521646469831467,0.07075199484825134,0.07459869235754013,0.07426871359348297,0.07302357256412506,0.07977130264043808,0.07091065496206284,0.0736631527543068,0.08624906092882156,0.08687535673379898,0.07468180358409882,0.07367291301488876,0.07270176708698273,0.07304494827985764,0.07352314889431,0.07307903468608856,0.07724957168102264,0.0710836723446846,0.07690813392400742,0.07461182773113251,0.07527261227369308,0.07686884701251984,0.0767018124461174,0.07456156611442566,0.07997332513332367,0.09066111594438553,0.07719790190458298,0.07337077707052231,0.07433543354272842,0.07337549328804016,0.08418495953083038,0.07103972136974335,0.07797308266162872,0.07590501755475998,0.07306910306215286,0.07574484497308731,0.07343431562185287,0.07685749232769012,0.07676701247692108,0.07338681817054749,0.07278565317392349,0.0739009752869606,0.07439681887626648,0.0719224214553833,0.07329950481653214,0.07631003856658936,0.07635898143053055,0.07659225165843964,0.07615049928426743,0.07435541599988937,0.07983208447694778,0.07338343560695648,0.07664280384778976,0.0769093930721283,0.07391056418418884,0.07098463922739029,0.07509256154298782,0.07628408819437027,0.09236284345388412,0.07110387086868286,0.07376820594072342,0.07621705532073975,0.07067401707172394,0.0719168484210968,0.07444871217012405,0.08365850150585175,0.0742187425494194,0.07271639257669449,0.07242263108491898,0.07498057186603546,0.08181385695934296,0.07495696097612381,0.06902284920215607,0.07409710437059402,0.08460699021816254,0.07604669779539108,0.07200998067855835,0.07232461124658585,0.0701499953866005,0.07278142869472504,0.0719415470957756,0.0775134339928627,0.07124427706003189,0.07308806478977203,0.07508530467748642,0.07862412929534912,0.07438741624355316,0.08126986771821976,0.07357420027256012,0.07677173614501953,0.07611090689897537,0.07328823208808899,0.08694720268249512,0.10332836955785751,0.07553805410861969,0.07237353175878525,0.07168197631835938,0.072876937687397,0.08245417475700378,0.07303951680660248,0.07323343306779861,0.07162933051586151,0.07300302386283875,0.07684837281703949,0.07216237485408783,0.0760723128914833,0.07891559600830078,0.08092007786035538,0.07272353023290634,0.07433249801397324,0.08606559038162231,0.07475743442773819,0.076036736369133,0.07812056690454483,0.07523256540298462,0.07738400995731354,0.07176873832941055,0.0707240104675293,0.07080762833356857,0.07523541152477264,0.07205229997634888,0.08739764243364334,0.0723349079489708,0.0752057358622551,0.07499454915523529,0.07580191642045975,0.07609473913908005,0.07369387149810791,0.07132043689489365,0.0813402384519577,0.07562126964330673,0.07141711562871933,0.08799796551465988,0.09107387810945511,0.0754341334104538,0.07958462834358215,0.07478827238082886,0.0755482167005539,0.08088391274213791,0.0695849135518074,0.086598701775074,0.07727020233869553,0.0747344046831131,0.07415428012609482,0.07772047817707062,0.0798000618815422,0.07023356854915619,0.07366958260536194,0.07219517976045609,0.09158821403980255,0.07392008602619171,0.07417825609445572,0.07939673960208893,0.08737090229988098,0.07540754228830338,0.07888256013393402,0.07129187136888504,0.07299352437257767,0.0765528604388237,0.07429414242506027,0.07297329604625702,0.08905492722988129,0.07740272581577301,0.07446212321519852,0.0749254822731018,0.07261163741350174,0.08084774017333984,0.07981321215629578,0.07338806986808777,0.07362066209316254,0.07367363572120667,0.0768994390964508,0.07042567431926727,0.0762634202837944,0.0756569653749466,0.0756804570555687,0.07758894562721252,0.06862801313400269,0.07427838444709778,0.07569330185651779,0.07363288104534149,0.07066752016544342,0.07418128848075867,0.07482381910085678,0.07936403900384903,0.07837028056383133,0.08430589735507965,0.07688060402870178,0.07446201145648956,0.07882837951183319,0.07513215392827988,0.07467931509017944,0.07317407429218292,0.09518061578273773,0.07792039960622787,0.07762039452791214,0.07458783686161041,0.09508009254932404,0.0817936360836029,0.09017236530780792,0.08082040399312973,0.0716787651181221,0.07394475489854813,0.07408533990383148,0.07732365280389786,0.08046312630176544,0.07987679541110992,0.07412038743495941,0.07285162061452866,0.07560238987207413,0.0736711248755455,0.07463537156581879,0.07614709436893463,0.07315405458211899,0.06985897570848465,0.08270557224750519,0.07152062654495239,0.07291227579116821,0.07707206159830093,0.07218354940414429,0.08461689949035645,0.07639146596193314,0.07310328632593155,0.08968973904848099,0.07393910735845566,0.0755540281534195,0.0764898955821991,0.06951802223920822,0.07467728108167648,0.07383152842521667,0.09508820623159409,0.07412346452474594,0.07245169579982758,0.0750216692686081,0.0909184068441391,0.07044482231140137,0.07393131405115128,0.08064445853233337,0.07715904712677002,0.07243235409259796,0.07240908592939377,0.08340763300657272,0.07706046104431152,0.07468922436237335,0.07233279943466187,0.0713367611169815,0.07490064203739166,0.07690560072660446,0.07084748148918152,0.07105416059494019,0.07507886737585068,0.07130353152751923,0.07576809823513031,0.07466968894004822,0.07689809054136276,0.07511752098798752,0.07683705538511276,0.07441279292106628,0.07643995434045792,0.07067728787660599,0.07251013815402985,0.07477769255638123,0.0740756019949913,0.08438143879175186,0.07333225011825562,0.07140816748142242,0.08737530559301376,0.07530349493026733,0.07226074486970901,0.07022376358509064,0.07430204749107361,0.07534616440534592,0.07100285589694977,0.07321032136678696,0.07010006159543991,0.07552795857191086,0.07352995127439499,0.07466213405132294,0.07892835140228271,0.07797221839427948,0.0722385123372078,0.0706152692437172,0.07963493466377258,0.07846787571907043,0.07556270807981491,0.07812368124723434,0.06921481341123581,0.07106301933526993,0.07596204429864883,0.085628941655159,0.07286327332258224,0.08369013667106628,0.07383725792169571,0.0776272788643837,0.07218983769416809,0.07341312617063522,0.07982198894023895,0.07530892640352249,0.08325590938329697,0.07598597556352615,0.08516892790794373,0.06959201395511627,0.07321392744779587,0.08090697228908539,0.07616397738456726,0.0748044103384018,0.07039092481136322,0.07442501187324524,0.08271293342113495,0.08583112061023712,0.07336156815290451,0.0730450227856636,0.07163411378860474,0.09039163589477539,0.07157835364341736,0.07261954993009567,0.07112462818622589,0.07118681073188782,0.09381707012653351,0.08335001766681671,0.07405833899974823,0.07312759757041931,0.07147268950939178,0.07015158236026764,0.07298967242240906,0.07441975176334381,0.07435940206050873,0.07219086587429047,0.07227712124586105,0.07330570369958878,0.07478051632642746,0.07450657337903976,0.07591433823108673,0.07079168409109116,0.07297860831022263,0.07334636151790619,0.07308650761842728,0.07329894602298737,0.07127406448125839,0.07541986554861069,0.074337899684906,0.07170330733060837,0.07090367376804352,0.07468069344758987,0.07418607920408249,0.0854920968413353,0.07683722674846649,0.07605411857366562,0.07095882296562195,0.07533437758684158,0.07417598366737366,0.07381613552570343,0.0682191401720047,0.07193749397993088,0.07614783942699432,0.08553890138864517,0.07569747418165207,0.07705686241388321,0.07288369536399841,0.07225944846868515,0.07704424858093262,0.0754624456167221,0.07616909593343735,0.07274607568979263,0.06881976872682571,0.07259334623813629,0.08849388360977173,0.07184265553951263,0.08804529905319214,0.07348793745040894,0.07435371726751328,0.07941702008247375,0.07585300505161285,0.08273452520370483,0.07326408475637436,0.07096413522958755,0.07455652207136154,0.07344531267881393,0.07119651883840561,0.07746492326259613,0.08942072838544846,0.07909362763166428,0.07578308880329132,0.07162458449602127,0.07425003498792648,0.07796091586351395,0.07339800894260406,0.07513559609651566,0.08203590661287308,0.07095615565776825,0.07838723808526993,0.07782461494207382,0.08099275082349777,0.0712774395942688,0.07786699384450912,0.08046475797891617,0.0737324059009552,0.0770253986120224,0.07569334656000137,0.08077947795391083,0.07787114381790161,0.07439052313566208,0.07746610790491104,0.07488095760345459,0.07348217815160751,0.08659721910953522,0.07637764513492584,0.07345600426197052,0.07042685151100159,0.07516061514616013,0.07336942851543427,0.07430321723222733,0.08036736398935318,0.07372646033763885,0.07261361181735992,0.07656865566968918,0.07380668818950653,0.07221654802560806,0.0751059278845787,0.07081179320812225,0.07335583865642548,0.0753585547208786,0.07436810433864594,0.07223302870988846,0.07525241374969482,0.07553637772798538,0.06948612630367279,0.07509654015302658,0.0722225084900856,0.07166825979948044,0.07825847715139389,0.07421312481164932,0.0729999840259552,0.07146815210580826,0.09009464830160141,0.07198663055896759,0.09466372430324554,0.08040609210729599,0.07468467950820923,0.07314293831586838,0.07312913984060287,0.08534665405750275,0.0725654661655426,0.08591345697641373,0.07322913408279419,0.07508482038974762,0.0744280219078064,0.07261575758457184,0.07635624706745148,0.07174932211637497,0.07969599217176437,0.07494771480560303,0.07049767673015594,0.06836608797311783,0.07314132153987885,0.07357491552829742,0.08048580586910248,0.07855253666639328,0.07569091022014618,0.07340873032808304,0.07389701157808304,0.07561731338500977,0.0766577422618866,0.07497431337833405,0.07359299808740616,0.09384534507989883,0.07398873567581177,0.0729779526591301,0.08394206315279007,0.08088558167219162,0.07549307495355606,0.07450132817029953,0.076377272605896,0.08020423352718353,0.07243651896715164,0.07184524834156036,0.07256191968917847,0.07278358936309814,0.07638468593358994,0.07268130779266357,0.07897510379552841,0.07172918319702148,0.07542078197002411,0.07699400931596756,0.0862402394413948,0.07265497744083405,0.07181652635335922,0.07850434631109238,0.07516960054636002,0.0813247412443161,0.07941150665283203,0.07217404991388321,0.0737360343337059,0.07383206486701965,0.07442349940538406,0.08014824241399765,0.08382441103458405,0.07267903536558151,0.07094607502222061,0.07316921651363373,0.07259609550237656,0.07313696295022964,0.07723376154899597,0.08041765540838242,0.0719340518116951,0.0786072164773941,0.07441665977239609,0.07180196791887283,0.07614781707525253,0.07753490656614304,0.09578318148851395,0.07666420191526413,0.07514865696430206,0.07879580557346344,0.07408448308706284,0.08207284659147263,0.0753587931394577,0.076616570353508,0.07737398147583008,0.07866859436035156,0.07492047548294067,0.06918583065271378,0.07524385303258896,0.07542473822832108,0.06818320602178574,0.07945088297128677,0.07770705968141556,0.07080211490392685,0.07137725502252579,0.07266343384981155,0.0778188705444336,0.09361163526773453,0.074679434299469,0.10115069150924683,0.075018510222435,0.08118224143981934,0.07890364527702332,0.07274289429187775,0.07618709653615952,0.07503622770309448,0.07496356219053268,0.07666026055812836,0.07439780235290527,0.07123544812202454,0.07286170870065689,0.07275283336639404,0.07262292504310608,0.08915621042251587,0.07138869166374207,0.07409251481294632,0.0722501277923584,0.07539834827184677,0.07251353561878204,0.0693712830543518,0.07325039058923721,0.0810774713754654,0.0690871998667717,0.07749661803245544,0.0744447410106659,0.08028645813465118,0.0756412073969841,0.09432297945022583,0.07420367002487183,0.0735178291797638,0.07682128995656967,0.0762799084186554,0.07529261708259583,0.07341824471950531,0.07327470183372498,0.07574646174907684,0.07652976363897324,0.06998833268880844,0.07816758006811142,0.07023640722036362,0.07646967470645905,0.07108830660581589,0.0743132159113884,0.07291395962238312,0.07388944923877716,0.07286559790372849,0.07738149911165237,0.0762772262096405,0.0774354636669159,0.07962306588888168,0.07401397079229355,0.07540955394506454,0.07051392644643784,0.07378099113702774,0.07404165714979172,0.07793351262807846,0.07786029577255249,0.07140500098466873,0.07645045220851898,0.07375261932611465,0.08346645534038544,0.08905559778213501,0.07274710386991501,0.07653689384460449,0.07240193337202072,0.07778799533843994,0.09221279621124268,0.07578299939632416,0.07158723473548889,0.07551814615726471,0.07787462323904037,0.07160752266645432,0.07441120594739914,0.07313522696495056,0.0712168887257576,0.07572288811206818,0.07446733117103577,0.07939957082271576,0.07093430310487747,0.08173985779285431,0.07860355079174042,0.07084880024194717,0.07599707692861557,0.07639265060424805,0.07448451966047287,0.07516181468963623,0.077431321144104,0.08051673322916031,0.07944559305906296,0.0756041631102562,0.07463885843753815,0.08120254427194595,0.07133800536394119,0.07387675344944,0.073189377784729,0.07488004863262177,0.07539812475442886,0.07357911020517349,0.07658890634775162,0.07257281243801117,0.07386583089828491,0.08077038079500198,0.0739009901881218,0.07630516588687897,0.0720478892326355,0.07282035797834396,0.08258844912052155,0.0740625336766243,0.10096185654401779,0.07887645810842514,0.07184344530105591,0.08875177055597305,0.07330235093832016,0.0729839876294136,0.08852285891771317,0.08008886128664017,0.07457820326089859,0.08275146037340164,0.07549235969781876,0.07306622713804245,0.0739574059844017,0.07487113028764725,0.07320478558540344,0.07590163499116898,0.08024943619966507,0.07259275764226913,0.08713241666555405,0.06968670338392258,0.07143950462341309,0.10047940164804459,0.07510605454444885,0.08166200667619705,0.07536308467388153,0.07697860896587372,0.07579188048839569,0.07051321864128113,0.0750679224729538,0.07200448215007782,0.07581140846014023,0.07329723984003067,0.07487712800502777,0.07291162759065628,0.07688821852207184,0.07802379131317139,0.07366449385881424,0.0758500024676323,0.07673542946577072,0.0780370831489563,0.07144352793693542,0.07555539906024933,0.07719222456216812,0.0754215270280838,0.07056132704019547,0.11794044077396393,0.07153253257274628,0.0738707184791565,0.0722937285900116,0.08171185106039047,0.08201174437999725,0.07276808470487595,0.07661284506320953,0.07660018652677536,0.0711606964468956,0.07126233726739883,0.07252467423677444,0.07334887981414795,0.07655751705169678,0.07025832682847977,0.07916799187660217,0.07145198434591293,0.06965668499469757,0.07784205675125122,0.0746675506234169,0.07002369314432144,0.0731201320886612,0.07892380654811859,0.07512186467647552,0.07361932098865509,0.07970359176397324,0.08338863402605057,0.08285029977560043,0.07334917038679123,0.07188744843006134,0.07543197274208069,0.08044002205133438,0.0785154476761818,0.07334217429161072,0.089918352663517,0.08218371868133545,0.0819619819521904,0.076800137758255,0.07453224807977676,0.09104182571172714,0.07472118735313416,0.07583807408809662,0.0759773850440979,0.08204081654548645,0.0752023383975029,0.08230479061603546,0.07771071791648865,0.0836031585931778,0.07473422586917877,0.0778605192899704,0.06911724805831909,0.07592005282640457,0.07223249971866608,0.07446175813674927,0.080196812748909,0.07062344253063202,0.08885122090578079,0.0790451318025589,0.07587181776762009,0.07399097084999084,0.07471982389688492,0.0726407840847969,0.07330898940563202,0.0767495408654213,0.08583996444940567,0.0741446241736412,0.07236682623624802,0.07793984562158585,0.07293083518743515,0.07188680022954941,0.07250871509313583,0.07572077214717865,0.0760706439614296,0.07904422283172607,0.07539823651313782,0.07405209541320801,0.07426441460847855,0.07458128035068512,0.07415132224559784,0.07661274075508118,0.10371184349060059,0.07961316406726837,0.07562092691659927,0.07213752716779709,0.08012354373931885,0.07421661913394928,0.07952568680047989,0.07627958804368973,0.08843334764242172,0.07040384411811829,0.08422359079122543,0.07330441474914551,0.0878162533044815,0.07041513174772263,0.07658276706933975,0.07603684812784195,0.07415401935577393,0.07397615909576416,0.08138298243284225,0.07538162171840668,0.07305973023176193,0.07438034564256668,0.07847755402326584,0.07412904500961304,0.07304766029119492,0.07619605958461761,0.07636985927820206,0.07437489181756973,0.07669825851917267,0.07558270543813705,0.06934706121683121,0.07383152842521667,0.07278601825237274,0.0772080272436142,0.06934888660907745,0.07563726603984833,0.0737626776099205,0.07337532937526703,0.06941179186105728,0.07842104882001877,0.0707247257232666,0.07737912982702255,0.06977398693561554,0.07071350514888763,0.07425478845834732,0.07298865169286728,0.07030453532934189,0.06922373175621033,0.07375741004943848,0.07595205307006836,0.08304149657487869,0.07415114343166351,0.0734153538942337,0.0732060894370079,0.07053525745868683,0.07097411900758743,0.07384225726127625,0.08051928132772446,0.07414130866527557,0.08155012130737305,0.07743403315544128,0.07448580116033554,0.0726739913225174,0.07370717823505402,0.07313363254070282,0.07372356951236725,0.07341272383928299,0.07304782420396805,0.07283250242471695,0.07620300352573395,0.08068452775478363,0.07514256238937378,0.07136771827936172,0.07340896129608154,0.07231676578521729,0.08509785681962967,0.06905725598335266,0.07200362533330917,0.08003122359514236,0.07377228885889053,0.07055459171533585,0.07054277509450912,0.07584940642118454,0.07514823973178864,0.07807593047618866,0.07614723592996597,0.07525518536567688,0.08388479053974152,0.07476100325584412,0.07727871090173721,0.08273564279079437,0.10812152922153473,0.07359755784273148,0.07282473146915436,0.07299984246492386,0.0708208754658699,0.07829352468252182,0.07973021268844604,0.07681970298290253,0.07069502025842667,0.07696229964494705,0.0748450830578804,0.08084756135940552,0.0753740593791008,0.07630467414855957,0.07523050904273987,0.07460664212703705,0.0764336958527565,0.07309862971305847,0.07418688386678696,0.07395068556070328,0.07180403172969818,0.0717337429523468,0.07061046361923218,0.08360261470079422,0.0788470134139061,0.0914452001452446,0.07270093262195587,0.07412439584732056,0.0733376294374466,0.082256980240345,0.07608067244291306,0.07481683790683746,0.07216254621744156,0.07358094304800034,0.07422806322574615,0.07653948664665222,0.07021646946668625,0.0786692202091217,0.07816008478403091,0.07325917482376099,0.06948859244585037,0.07559296488761902,0.07279777526855469,0.07428492605686188,0.08550657331943512,0.07563462108373642,0.07012518495321274,0.07825380563735962,0.07143722474575043,0.07771214842796326,0.07091474533081055,0.08501425385475159,0.07227567583322525,0.07430301606655121,0.07538509368896484,0.07201211899518967,0.08661101013422012,0.07148457318544388,0.08777055889368057,0.07752687484025955,0.06949407607316971,0.08173120766878128,0.08055291324853897,0.07835903018712997,0.0693337693810463,0.07130254060029984,0.07648742198944092,0.085023894906044,0.10367866605520248,0.07281029224395752,0.07490474730730057,0.07357681542634964,0.07037743926048279,0.07454092055559158,0.07923809438943863,0.07340020686388016,0.08208193629980087,0.07512238621711731,0.07173527032136917,0.07847823202610016,0.07533677667379379,0.07371009141206741,0.07353385537862778,0.07812294363975525,0.07046554982662201,0.07514757663011551,0.06895843893289566,0.07387016713619232,0.07359606772661209,0.07411197572946548,0.06983593106269836,0.07502288371324539,0.0809326320886612,0.07624910026788712,0.07198163121938705,0.07247768342494965,0.07451913505792618,0.07826540619134903,0.09635243564844131,0.07110542058944702,0.07369325309991837,0.07218538224697113,0.0716443732380867,0.07882309705018997,0.07245974987745285,0.07614941895008087,0.07879500836133957,0.0719558596611023,0.0747646912932396,0.07787588238716125,0.07581619173288345,0.07437705993652344,0.0737491026520729,0.07731787115335464,0.07851319760084152,0.07126140594482422,0.07111338526010513,0.0996301993727684,0.0879453644156456,0.07252359390258789,0.07193367183208466,0.07650134712457657,0.06864818185567856,0.07375447452068329,0.07644815742969513,0.07423564791679382,0.07150953263044357,0.0815696194767952,0.06956395506858826,0.07848767936229706,0.07724215090274811,0.07461003214120865,0.06931212544441223,0.07162091881036758,0.07446997612714767,0.08454456925392151,0.0938173234462738,0.08158396184444427,0.07096924632787704,0.07335411757230759,0.08184589445590973,0.07507234811782837,0.07196635007858276,0.06995952874422073,0.06941382586956024,0.07246564328670502,0.07381492853164673,0.07259111106395721,0.07511387020349503,0.07140085101127625,0.07863474637269974,0.07155226171016693,0.07283268123865128,0.07004868239164352,0.08296019583940506,0.07428890466690063,0.08320530503988266,0.0812392458319664,0.07291830331087112,0.07387496531009674,0.07608401775360107,0.07635525614023209,0.08213705569505692,0.0732073187828064,0.07718001306056976,0.0762861967086792,0.07973139733076096,0.07328271120786667,0.07243960350751877,0.07657895237207413,0.07273560017347336,0.0928284078836441,0.0732860118150711,0.0723377987742424,0.07216177135705948,0.06951592862606049,0.07034561038017273,0.07210445404052734,0.07263217121362686,0.07433944195508957,0.07308842241764069,0.07681459188461304,0.07382547110319138,0.07362000644207001,0.07902888208627701,0.07617849111557007,0.08237934857606888,0.07003062963485718,0.08364250510931015,0.07212921977043152,0.0755707249045372,0.07762140780687332,0.06890426576137543,0.07338385283946991,0.07857199758291245,0.08153475821018219,0.07027939707040787,0.07920382916927338,0.08398567885160446,0.07360129803419113,0.07144162058830261,0.07048872113227844,0.0802958682179451,0.07287035137414932,0.07592359930276871,0.08852430433034897,0.10756878554821014,0.07712077349424362,0.08012367784976959,0.07062055170536041,0.07037823647260666,0.07753843814134598,0.07179878652095795,0.08777623623609543,0.07506750524044037,0.07354352623224258,0.07064568996429443,0.07394107431173325,0.07354224473237991,0.07540344446897507,0.0779241994023323,0.07137039303779602,0.09521883726119995,0.07213106006383896,0.07587943971157074,0.07513027638196945,0.09013204276561737,0.07502094656229019,0.07588346302509308,0.08093997091054916,0.08610528707504272,0.07107367366552353,0.09495700150728226,0.07929777354001999,0.07667095959186554,0.09218966960906982,0.08433926105499268,0.07877646386623383,0.07354134321212769,0.08162233978509903,0.08226398378610611,0.07165113836526871,0.0752507820725441,0.07370053231716156,0.07208588719367981,0.07188422232866287,0.07596001774072647,0.07273036986589432,0.07473034411668777,0.0783667340874672,0.07335949689149857,0.07167082279920578,0.07500562816858292,0.07036662846803665,0.07072728127241135,0.0713200569152832,0.07958122342824936,0.07526277005672455,0.07413291931152344,0.0739121064543724,0.07347875088453293,0.08258666843175888,0.07709983736276627,0.06888538599014282,0.07415522634983063,0.07947780936956406,0.07558093965053558,0.07339822500944138,0.07304739207029343,0.07133644819259644,0.07816553860902786,0.06888490915298462,0.07254602015018463,0.07193990051746368,0.07694674283266068,0.0754299908876419,0.07353618741035461,0.07593579590320587,0.06930070370435715,0.07333284616470337,0.07189785689115524,0.0844947025179863,0.07809474319219589,0.07952465862035751,0.08074595779180527,0.07232342660427094,0.08063855022192001,0.06952916085720062,0.07879206538200378,0.07171519845724106,0.08018141239881516,0.07224905490875244,0.07951528578996658,0.07618772983551025,0.06946270167827606,0.07375407963991165,0.07150926440954208,0.06839185208082199,0.07736963033676147,0.07200266420841217,0.07432908564805984,0.08076513558626175,0.08017788082361221,0.0717122033238411,0.0763525441288948,0.0742485523223877,0.07399012893438339,0.07045359164476395,0.07130648195743561,0.07850488275289536,0.07519713789224625,0.07229691743850708,0.07402542233467102,0.07245103269815445,0.08193475008010864,0.07303304225206375,0.07543691992759705,0.07236991822719574,0.07851338386535645,0.0740593671798706,0.07285035401582718,0.0749153271317482,0.07323633879423141,0.07451901584863663,0.08195672184228897,0.07452747225761414,0.07373490929603577,0.07677771896123886,0.07468662410974503,0.07628370821475983,0.0731763169169426,0.07637617737054825,0.07582438737154007,0.07654280960559845,0.07252867519855499,0.07250075787305832,0.07599061727523804,0.06913881748914719,0.08039534091949463,0.0762283131480217,0.07410674542188644,0.07455045729875565,0.0729837417602539,0.07318863272666931,0.07828472554683685,0.07702169567346573,0.07456788420677185,0.094632089138031,0.09062807261943817,0.07453324645757675,0.07415060698986053,0.0750066414475441,0.07265122979879379,0.07555779069662094,0.07045472413301468,0.07786142826080322,0.07228071987628937,0.07824813574552536,0.08035966008901596,0.07140326499938965,0.07343839108943939,0.0717993825674057,0.0732523575425148,0.07260161638259888,0.0711476281285286,0.07224058359861374,0.06947320699691772,0.07026497274637222,0.08056860417127609,0.07851506024599075,0.07615399360656738,0.07746198028326035,0.06875347346067429,0.0773148462176323,0.07282191514968872,0.07143505662679672,0.07368161529302597,0.07729450613260269,0.07986163347959518,0.07330875843763351,0.07015920430421829,0.07317005842924118,0.07641608268022537,0.0763622596859932,0.07907545566558838,0.0776444524526596,0.07100516557693481,0.07459171116352081,0.07386783510446548,0.08024714887142181,0.08109301328659058,0.07126683741807938,0.07395365834236145,0.07248495519161224,0.07788221538066864,0.07082632184028625,0.07174886018037796,0.07369649410247803,0.08926843851804733,0.08120644092559814,0.075546994805336,0.07622624933719635,0.0784369558095932,0.07547713816165924,0.0740916058421135,0.07145281881093979,0.07465174794197083,0.07566115260124207,0.07746892422437668,0.07064812630414963,0.07966078817844391,0.07291725277900696,0.07543535530567169,0.07034572213888168,0.07695098221302032,0.07995400577783585,0.08537716418504715,0.07374460250139236,0.07237382233142853,0.07078418135643005,0.07436320185661316,0.07444389164447784,0.0717615932226181,0.07217000424861908,0.07291236519813538,0.07439032942056656,0.11776630580425262,0.08476770669221878,0.0735851302742958,0.07361634075641632,0.0822918564081192,0.07345174252986908,0.07448911666870117,0.07451841980218887,0.07029519975185394,0.08366593718528748,0.07306767255067825,0.0741230919957161,0.07366465777158737,0.07204575836658478,0.0814596489071846,0.07495033740997314,0.0782281681895256,0.07733898609876633,0.07351412624120712,0.07507991045713425,0.06798303872346878,0.07062744349241257,0.08422563970088959,0.07153448462486267,0.07153762131929398,0.07317028194665909,0.07892054319381714,0.07356558740139008,0.07376562803983688,0.07310488820075989,0.0692911222577095,0.07672598212957382,0.07356547564268112,0.07440903782844543,0.07831434160470963,0.07535316795110703,0.07076204568147659,0.07759100943803787,0.08917081356048584,0.0734369084239006,0.07498384267091751,0.07544761896133423,0.07189591228961945,0.0741514042019844,0.07724476605653763,0.07296670973300934,0.07214153558015823,0.07584436237812042,0.07247276604175568,0.07932240515947342,0.06970653682947159,0.07437101751565933,0.07499247044324875,0.07296112924814224,0.07184410840272903,0.07751122862100601,0.07475785166025162,0.07608408480882645,0.07026989758014679,0.07921981066465378,0.07498025894165039,0.07499247044324875,0.07492386549711227,0.07945620268583298,0.07441640645265579,0.07391559332609177,0.06824014335870743,0.07920049130916595,0.07243583351373672,0.08272978663444519,0.07209999859333038,0.07360994070768356,0.07488910108804703,0.0725366547703743,0.07413533329963684,0.07641555368900299,0.07165349274873734,0.0838690847158432,0.08545738458633423,0.07015539705753326,0.07232300937175751,0.0862448513507843,0.07183521240949631,0.07657817006111145,0.07598306238651276,0.07543022930622101,0.0763869509100914,0.07489898800849915,0.07332798838615417,0.09348540008068085,0.07158312201499939,0.07284438610076904,0.07335736602544785,0.06986846774816513,0.06829362362623215,0.07360897958278656,0.07276806980371475,0.07092589884996414,0.07222271710634232,0.07723952829837799,0.07271943241357803,0.07028514891862869,0.08026488125324249,0.08388814330101013,0.09650284051895142,0.07561302185058594,0.0762772485613823,0.0721922516822815,0.0821099653840065,0.07760439813137054,0.0832304134964943,0.07328696548938751,0.07286787033081055,0.08196612447500229,0.07291916012763977,0.07380451261997223,0.07085686922073364,0.0816325917840004,0.07741028815507889,0.07611856609582901,0.07575599104166031,0.07537251710891724,0.08064934611320496,0.08423741906881332,0.07775185257196426,0.07268443703651428,0.07314213365316391,0.08988887071609497,0.0732925608754158,0.07906682789325714,0.07378266751766205,0.07758769392967224,0.06928491592407227,0.07737243175506592,0.07408704608678818,0.07332538813352585,0.07441554218530655,0.07298920303583145,0.07026674598455429,0.06691748648881912,0.08200974017381668,0.08023207634687424,0.07278545945882797,0.07551878690719604,0.07574072480201721,0.07360506057739258,0.07571876794099808,0.08329663425683975,0.07783287763595581,0.07863489538431168,0.0865728035569191,0.078377366065979,0.07916616648435593,0.07536226511001587,0.08199711889028549,0.07519549131393433,0.07221204042434692,0.07366495579481125,0.07733238488435745,0.06752507388591766,0.09135174751281738,0.07606688141822815,0.07445421069860458,0.0820353627204895,0.08500416576862335,0.07912226021289825,0.07488595694303513,0.07007940858602524,0.07428112626075745,0.08296861499547958,0.07712893933057785,0.08047738671302795,0.08010156452655792,0.07526371628046036,0.07392096519470215,0.07581529021263123,0.07714284211397171,0.0974692776799202,0.07725772261619568,0.07760075479745865,0.07381127029657364,0.1009356677532196,0.08064128458499908,0.09211995452642441,0.07768075913190842,0.08836517482995987,0.08159078657627106,0.08032774180173874,0.0730777159333229,0.07433860003948212,0.07627025991678238,0.07779104262590408,0.07833310216665268,0.08649223297834396,0.08183372020721436,0.07141278684139252,0.07696772366762161,0.0750286728143692,0.07276638597249985,0.07754155993461609,0.07204288244247437,0.08527396619319916,0.07920417934656143,0.08324512839317322,0.0901288390159607,0.07914923131465912,0.09146593511104584,0.08582998067140579,0.08852536231279373,0.0863979309797287,0.08351225405931473,0.07755500078201294,0.07772687822580338,0.08134609460830688,0.08340597152709961,0.09004341810941696,0.08652152866125107,0.08487910032272339,0.07636191695928574,0.0764162465929985,0.07901585847139359,0.10607396066188812,0.07745737582445145,0.0807233676314354,0.08090229332447052,0.07640390843153,0.07675603032112122,0.08924227207899094,0.08258352428674698,0.07886717468500137,0.07997025549411774,0.07819066941738129,0.09418466687202454,0.08097749203443527,0.07523776590824127,0.0773550420999527,0.08156414330005646,0.07524151355028152,0.08494042605161667,0.08590870350599289,0.08242868632078171,0.07723776996135712,0.07715124636888504,0.09136345237493515,0.07951615005731583,0.07315502315759659,0.07969039678573608,0.07552388310432434,0.07684116810560226,0.07688339054584503,0.07526081055402756,0.07770492136478424,0.09176044166088104,0.08272132277488708,0.08437956869602203,0.07253682613372803,0.08308302611112595,0.0889456495642662,0.0767374187707901,0.08079823851585388,0.08158312737941742,0.08419942110776901,0.09245649725198746,0.07626301795244217,0.0797145739197731,0.07079055160284042,0.08656146377325058,0.08091586828231812,0.08689132332801819,0.07938031107187271,0.09952043741941452,0.07774493098258972,0.07529087364673615,0.07728461921215057,0.08403836190700531,0.08130449056625366,0.07937232404947281,0.07923021167516708,0.07731090486049652,0.10516621917486191,0.08957011252641678,0.07777576893568039,0.0786815881729126,0.08139015734195709,0.07878127694129944,0.0946577712893486,0.07892081886529922,0.07876738905906677,0.08131199330091476,0.08045243471860886,0.07874774932861328,0.08974315971136093,0.07842478156089783,0.10778366029262543,0.07405485212802887,0.08060503005981445,0.08492636680603027,0.07626981288194656,0.07096540182828903,0.09678374975919724,0.07864586263895035,0.09396045655012131,0.10496331751346588,0.07839876413345337,0.0872073620557785,0.06805877387523651,0.08486427366733551,0.07899586111307144,0.07691659033298492,0.08507456630468369,0.08190152794122696,0.07474306970834732,0.07796614617109299,0.08094905316829681,0.07330282777547836,0.07792840152978897,0.08130687475204468,0.08475451171398163,0.07911261171102524,0.07380414009094238,0.08722864836454391,0.07587949931621552,0.08294708281755447,0.08590613305568695,0.08107069879770279,0.082187220454216,0.07824523746967316,0.07327649742364883,0.09435231983661652,0.08107869327068329,0.07352893799543381,0.09970851987600327,0.07332873344421387,0.07552485167980194,0.08422381430864334,0.07525454461574554,0.09442181885242462,0.07703553140163422,0.08893999457359314,0.07300649583339691,0.08375605940818787,0.08179602771997452,0.074668288230896,0.0774301066994667,0.07860931754112244,0.08448517322540283,0.08136585354804993,0.09461227804422379,0.08029063791036606,0.0765313059091568,0.08071982115507126,0.08643636852502823,0.07850620895624161,0.08823204040527344,0.08086684346199036,0.0940532460808754,0.07974706590175629,0.08157345652580261,0.08335345983505249,0.07101941108703613,0.08918076753616333,0.07870172709226608,0.08114612847566605,0.09105745702981949,0.07779557257890701,0.08920667320489883,0.08864960074424744,0.0975051149725914,0.08297727257013321,0.07711184769868851,0.09349007904529572,0.0782313197851181,0.07964619994163513,0.08749115467071533,0.10882624983787537,0.09341567009687424,0.08793113380670547,0.07788261771202087,0.08351598680019379,0.0790676698088646,0.08522532135248184,0.08022990822792053,0.07563629746437073,0.076073057949543,0.08191639930009842,0.092856764793396,0.08608196675777435,0.08516351133584976,0.08228027820587158,0.07853905111551285,0.08234904706478119,0.08023588359355927,0.09116613119840622,0.07999943941831589,0.07482526451349258,0.09208177030086517,0.08320087939500809,0.07819797098636627,0.07640786468982697,0.08072935044765472,0.07046285271644592,0.08374442905187607,0.07438796013593674,0.08788830041885376,0.07606233656406403,0.08028143644332886,0.07567810267210007,0.08087030053138733,0.08095501363277435,0.08232741802930832,0.07935729622840881,0.08311492949724197,0.07773210108280182,0.07942487299442291,0.08219381421804428,0.09433092921972275,0.07994432002305984,0.07594233751296997,0.08099164813756943,0.07860352843999863,0.07708494365215302,0.07497803866863251,0.0805128961801529,0.07895021140575409,0.08024317771196365,0.08051805198192596,0.07953091710805893,0.07282575219869614,0.07836287468671799,0.07970727980136871,0.07822957634925842,0.0777408629655838,0.0748848021030426,0.09622164070606232,0.08463291078805923,0.07505832612514496,0.0909566655755043,0.07691216468811035,0.08334621042013168,0.0827288031578064,0.07921899855136871,0.07970912009477615,0.08830989897251129,0.08217861503362656,0.08135101199150085,0.08770891278982162,0.08552944660186768,0.07458902895450592,0.09447243064641953,0.09137658029794693,0.0734514519572258,0.08093050867319107,0.07990346848964691,0.07697483897209167,0.07381235808134079,0.07746618241071701,0.08461219072341919,0.07815053313970566,0.08028658479452133,0.090029276907444,0.07555530220270157,0.07655346393585205,0.10529869049787521,0.0780094787478447,0.10704435408115387,0.07377558201551437,0.08716022968292236,0.07637929171323776,0.07798563688993454,0.0822715312242508,0.07719200849533081,0.08745063841342926,0.07749497890472412,0.07765820622444153,0.07115845382213593,0.07139469683170319,0.07856789976358414,0.07247668504714966,0.08539300411939621,0.12032222002744675,0.09568405896425247,0.0753026008605957,0.07278750836849213,0.07798973470926285,0.07459373772144318,0.08625767379999161,0.08333086967468262,0.07652934640645981,0.10717126727104187,0.0768359825015068,0.08469881862401962,0.09681492298841476,0.07662007212638855,0.0733446553349495,0.08802319318056107,0.08103109151124954,0.07567913085222244,0.08960365504026413,0.08797328174114227,0.1011994257569313,0.07907572388648987,0.08161956816911697,0.07624147832393646,0.08442678302526474,0.07480285316705704,0.07992218434810638,0.07683578133583069,0.07727770507335663,0.07496508955955505,0.09427506476640701,0.07925385981798172,0.09235011786222458,0.08005891740322113,0.07510236650705338,0.0948905497789383,0.07488898932933807,0.07378640025854111,0.07787936925888062,0.09027531743049622,0.09508366882801056,0.07482468336820602,0.09598787128925323,0.07705593854188919,0.10354979336261749,0.07208476960659027,0.08287134766578674,0.07942747324705124,0.08002548664808273,0.07904957979917526,0.07753964513540268,0.0760374665260315,0.09640026837587357,0.0792326107621193,0.08146020770072937,0.09635261446237564,0.0776933953166008,0.07888121157884598,0.07727532833814621,0.07924146205186844,0.08496523648500443,0.08233577013015747,0.07729099690914154,0.09565898776054382,0.07289847731590271,0.07647907733917236,0.07878066599369049,0.08101789653301239,0.0872647687792778,0.091378353536129,0.07956737279891968,0.08238058537244797,0.07778897881507874,0.07793977111577988,0.08012079447507858,0.08246796578168869,0.07589314877986908,0.08200591057538986,0.0898330882191658,0.08886481821537018,0.08625271171331406,0.07409307360649109,0.07655210793018341,0.082163505256176,0.0980304554104805,0.08144740760326385,0.07797642052173615,0.07307712733745575,0.0768912136554718,0.07521089911460876,0.07280681282281876,0.08948313444852829,0.07394171506166458,0.07480956614017487,0.08402537554502487,0.0846286490559578,0.07854532450437546,0.09047684073448181,0.07507944852113724,0.07151292264461517,0.0822676420211792,0.07357937097549438,0.07778187841176987,0.07775704562664032,0.07900508493185043,0.0899144783616066,0.09638817608356476,0.0784129723906517,0.07811347395181656,0.07556230574846268,0.10589221864938736,0.0757211446762085,0.07770028710365295,0.08243193477392197,0.07779861241579056,0.07731090486049652,0.08857181668281555,0.0965711697936058,0.08452403545379639,0.07444903254508972,0.07641635090112686,0.12170761823654175,0.07705869525671005,0.09071341156959534,0.07960003614425659,0.07699702680110931,0.08406385034322739,0.07370589673519135,0.07935114949941635,0.08021312952041626,0.07505057007074356,0.07442978769540787,0.08601900190114975,0.0779467299580574,0.07412358373403549,0.07745567709207535,0.07747336477041245,0.08196333795785904,0.08113179355859756,0.07984720915555954,0.07466607540845871,0.08672615885734558,0.0854877308011055,0.07785879820585251,0.08108577877283096,0.0815531313419342,0.08229973912239075,0.08031242340803146,0.0810723602771759,0.08393262326717377,0.07637997716665268,0.07492617517709732,0.08746261149644852,0.07531972229480743,0.08457759022712708,0.08451848477125168,0.10240858048200607,0.0876753181219101,0.08381203562021255,0.08215099573135376,0.07904981821775436,0.08034694939851761,0.07513198256492615,0.08294561505317688,0.10999749600887299,0.07590796798467636,0.08452574163675308,0.07962749153375626,0.07356864213943481,0.07656018435955048,0.07721835374832153,0.07903377711772919,0.07613480091094971,0.10499993711709976,0.11954803764820099,0.07925537973642349,0.08189795911312103,0.08151747286319733,0.07132245600223541,0.08099275082349777,0.08250010758638382,0.09669995307922363,0.0813044086098671,0.0754501223564148,0.08351689577102661,0.08028509467840195,0.0974237248301506,0.09540460258722305,0.0813852921128273,0.08166446536779404,0.07452858239412308,0.07985789328813553,0.07621748745441437,0.07526906579732895,0.07845599949359894,0.0754631757736206,0.072838194668293,0.08127351850271225,0.08080732822418213,0.076851487159729,0.0727054551243782,0.07709599286317825,0.07529132068157196,0.07495778053998947,0.09344348311424255,0.07364199310541153,0.0982930064201355,0.08820840716362,0.08465592563152313,0.0772770494222641,0.07342840731143951,0.07876202464103699,0.07532583177089691,0.0743725374341011,0.08698723465204239,0.08192994445562363,0.07603999972343445,0.07425116747617722,0.07727812230587006,0.08512520790100098,0.08140040189027786,0.08667419850826263,0.08304189145565033,0.08012720197439194,0.07720968127250671,0.07561124116182327,0.0822032019495964,0.081731878221035,0.09471553564071655,0.07693617045879364,0.07584640383720398,0.07970857620239258,0.07338079810142517,0.08065798133611679,0.07756835967302322,0.08062340319156647,0.07960662990808487,0.07839474827051163,0.09193086624145508,0.07916919142007828,0.07811720669269562,0.10727164894342422,0.08347386121749878,0.07196696847677231,0.09827425330877304,0.08741651475429535,0.07487093657255173,0.07395753264427185,0.08918186277151108,0.07713339477777481,0.07973353564739227,0.07605814188718796,0.0782691016793251,0.07670950889587402,0.08493625372648239,0.0726277306675911,0.08334483951330185,0.07846886664628983,0.0947180911898613,0.07947201281785965,0.09500082582235336,0.07875824719667435,0.07845579832792282,0.07805266231298447,0.07450374215841293,0.07891495525836945,0.07325614988803864,0.07575525343418121,0.08442249149084091,0.07779660820960999,0.07568428665399551,0.07350403815507889,0.07708972692489624,0.10776152461767197,0.08081234246492386,0.07910700142383575,0.07494069635868073,0.08027160912752151,0.07769397646188736,0.07308845221996307,0.08691282570362091,0.07210673391819,0.10425370931625366,0.07377003878355026,0.07878021150827408,0.08121659606695175,0.07533387839794159,0.08531453460454941,0.08783042430877686,0.0765330046415329,0.0759400799870491,0.07461369782686234,0.08179347962141037,0.08006784319877625,0.08388121426105499,0.08083466440439224,0.08044048398733139,0.08163261413574219,0.07543880492448807,0.07464214414358139,0.0749574825167656,0.0775270015001297,0.07488636672496796,0.08036744594573975,0.09383604675531387,0.07743240147829056,0.08581070601940155,0.07267875969409943,0.07740426808595657,0.08279762417078018,0.07917662709951401,0.0844884067773819,0.08082639425992966,0.07391390949487686,0.09050072729587555,0.08072797954082489,0.07511921226978302,0.07464282959699631,0.07949849963188171,0.07756484299898148,0.07821378111839294,0.07673485577106476,0.078120656311512,0.07511238008737564,0.07614706456661224,0.07958614081144333,0.07935216277837753,0.08346571028232574,0.07381994277238846,0.07790394127368927,0.07380111515522003,0.08152438700199127,0.0827004685997963,0.07768876105546951,0.07575508952140808,0.07368997484445572,0.0742231011390686,0.08013071864843369,0.07821623235940933,0.08274126052856445,0.0858069509267807,0.08227616548538208,0.08371181041002274,0.07146358489990234,0.07979926466941833,0.07886883616447449,0.08543679118156433,0.07488963007926941,0.08166633546352386,0.07539328932762146,0.08353333175182343,0.08281625807285309,0.07805794477462769,0.08890091627836227,0.08151475340127945,0.08605611324310303,0.08573344349861145,0.08268352597951889,0.07286472618579865,0.07655178755521774,0.07925049960613251,0.0832521989941597,0.08146105706691742,0.07694461941719055,0.0738452896475792,0.07173744589090347,0.08094789832830429,0.07829566299915314,0.07617636770009995,0.08129989355802536,0.0777965560555458,0.07940126210451126,0.07372183352708817,0.08137624710798264,0.07750348001718521,0.08098234981298447,0.0752134770154953,0.0784984603524208,0.08142994344234467,0.07529505342245102,0.08781364560127258,0.08354508876800537,0.07213083654642105,0.07919764518737793,0.07275174558162689,0.0941174328327179,0.08165458589792252,0.07743500173091888,0.08051936328411102,0.08118458092212677,0.0891607403755188,0.07244791090488434,0.08328957855701447,0.08882193267345428,0.08290871977806091,0.08144287765026093,0.08029604703187943,0.09763532876968384,0.07922084629535675,0.0901879072189331,0.08273577690124512,0.07468394935131073,0.08561226725578308,0.07625708729028702,0.08068343997001648,0.08878550678491592,0.075620137155056,0.07619591057300568,0.07279244065284729,0.08281967043876648,0.08593405038118362,0.07859505712985992,0.08102784305810928,0.07759415358304977,0.08576637506484985,0.07988738268613815,0.08031991869211197,0.08313583582639694,0.07758790999650955,0.07865379005670547,0.07931763678789139,0.08207153528928757,0.0816114991903305,0.07943163067102432,0.08377349376678467,0.09406604617834091,0.07754917442798615,0.07745995372533798,0.08125714957714081,0.07551955431699753,0.08865997195243835,0.07276598364114761,0.07484463602304459,0.0789373368024826,0.0771997943520546,0.07536251097917557,0.08029162138700485,0.07500243932008743,0.08559797704219818,0.07331827282905579,0.08266274631023407,0.07412922382354736,0.07569358497858047,0.073714978992939,0.07884049415588379,0.08913072198629379,0.0815930962562561,0.07405038177967072,0.07915037125349045,0.06995876878499985,0.08997561782598495,0.0756746456027031,0.07901129126548767,0.07426518946886063,0.0710391253232956,0.0749167799949646,0.08668036013841629,0.07798196375370026,0.07907609641551971,0.09006296843290329,0.08049897849559784,0.0820118710398674,0.08052621781826019,0.07740063220262527,0.08016234636306763,0.08184468001127243,0.07638988643884659,0.07723714411258698,0.0842386931180954,0.07550671696662903,0.07907480001449585,0.08148664236068726,0.0847359374165535,0.07244296371936798,0.08018723875284195,0.08398076891899109,0.08679544180631638,0.09754912555217743,0.07891438901424408,0.07630936056375504,0.07495860010385513,0.07717851549386978,0.08517378568649292,0.07304023206233978,0.07716995477676392,0.0723673552274704,0.09451283514499664,0.07945132255554199,0.07828697562217712,0.07239498198032379,0.0836649090051651,0.08725162595510483,0.08239113539457321,0.07793272286653519,0.1079319566488266,0.08878516405820847,0.08113414794206619,0.07687361538410187,0.07443876564502716,0.07395362854003906,0.07284443080425262,0.07684473693370819,0.08133938163518906,0.07631811499595642,0.08172956854104996,0.0780596062541008,0.10876037925481796,0.07439017295837402,0.07787472009658813,0.07679613679647446,0.07445649802684784,0.08114554733037949,0.086977019906044,0.08297468721866608,0.07932098954916,0.07457654178142548,0.07387518137693405,0.08369557559490204,0.07465264946222305,0.07186689227819443,0.07592388242483139,0.07508069276809692,0.07757475227117538,0.08062329888343811,0.07422328740358353,0.0808868482708931,0.07170790433883667,0.07405396550893784,0.07642633467912674,0.08029108494520187,0.07259359955787659,0.078987255692482,0.08147098869085312,0.07842857390642166,0.07650429755449295,0.07535183429718018,0.07161601632833481,0.07390986382961273,0.0819626897573471,0.07873145490884781,0.1049954816699028,0.07713968306779861,0.07067461311817169,0.07126732915639877,0.0792449489235878,0.07250262051820755,0.08542240411043167,0.09077005088329315,0.075895294547081,0.09092186391353607,0.0870959609746933,0.09739872068166733,0.0765143558382988,0.08273246884346008,0.06987655907869339,0.075541652739048,0.07567544281482697,0.07908132672309875,0.08820438385009766,0.07994131743907928,0.08512930572032928,0.07728267461061478,0.08299673348665237,0.07313389331102371,0.07824280112981796,0.07328888028860092,0.07598450779914856,0.08370130509138107,0.08173035085201263,0.09909515082836151,0.0762547105550766,0.08485060930252075,0.0804535523056984,0.07995593547821045,0.08436161279678345,0.07576624304056168,0.07778539508581161,0.07781487703323364,0.07350270450115204,0.08347777277231216,0.09012134373188019,0.08124501258134842,0.07352588325738907,0.07547243684530258,0.10148998349905014,0.07728403061628342,0.08887172490358353,0.10306252539157867,0.08660592138767242,0.07964809983968735,0.07798001915216446,0.07608521729707718,0.07534769177436829,0.08873028308153152,0.07997346669435501,0.0767955482006073,0.07289944589138031,0.07793484628200531,0.08103838562965393,0.08240564912557602,0.07895765453577042,0.07327830046415329,0.07834239304065704,0.07860454171895981,0.08226258307695389,0.07563333213329315,0.07941224426031113,0.07748587429523468,0.08826334029436111,0.07582905888557434,0.07772725075483322,0.09834595024585724,0.07675929367542267,0.08523602783679962,0.08123435080051422,0.08450917899608612,0.07547535747289658,0.0771084874868393,0.08758658170700073,0.08195312321186066,0.07682892680168152,0.09002744406461716,0.07319480925798416,0.07361753284931183,0.08055061101913452,0.07551851123571396,0.07542555034160614,0.07159041613340378,0.08105798065662384,0.07436873018741608,0.07720156759023666,0.0821370780467987,0.07629521936178207,0.08052706718444824,0.07374395430088043,0.07190337777137756,0.07677747309207916,0.08645838499069214,0.0803104117512703,0.0746048167347908,0.0799643024802208,0.07889801263809204,0.07578548043966293,0.08712828904390335,0.07419832050800323,0.07348103076219559,0.08380768448114395,0.09014485031366348,0.07752916216850281,0.0784187987446785,0.07667658478021622,0.09290728718042374,0.07617464661598206,0.0744095891714096,0.0763591006398201,0.07496598362922668,0.07687123119831085,0.08248994499444962,0.08129467815160751,0.07466225326061249,0.08580846339464188,0.08179300278425217,0.1079416424036026,0.07480484992265701,0.07549481093883514,0.0784166157245636,0.07884531468153,0.07892530411481857,0.07722888141870499,0.07515115290880203,0.08673003315925598,0.08139627426862717,0.09385521709918976,0.08772097527980804,0.07762076705694199,0.0743170827627182,0.07493161410093307,0.07500728219747543,0.09153129905462265,0.07384341955184937,0.07130294293165207,0.07493896782398224,0.08183129131793976,0.08335848152637482,0.07645300030708313,0.07976829260587692,0.07627028971910477,0.08156616985797882,0.07491244375705719,0.07834693044424057,0.07363033294677734,0.07386656105518341,0.07303925603628159,0.07852216064929962,0.07081131637096405,0.07437147200107574,0.07373868674039841,0.0731377825140953,0.09506045281887054,0.07235810160636902,0.08170785009860992,0.07971418648958206,0.0897432342171669,0.08426358550786972,0.07582584023475647,0.07007410377264023,0.11900933086872101,0.0714176818728447,0.06946608424186707,0.07267157733440399,0.07888093590736389,0.07546543329954147,0.07547493278980255,0.06987445801496506,0.07427286356687546,0.0727570503950119,0.07194351404905319,0.0825599730014801,0.07441255450248718,0.06996136903762817,0.07314500957727432,0.07908005267381668,0.07593227177858353,0.07609876245260239,0.08472815155982971,0.07705242186784744,0.0728459507226944,0.0689866840839386,0.07905569672584534,0.07157769799232483,0.07471524178981781,0.07284793257713318,0.07461930811405182,0.07049146294593811,0.07518085092306137,0.08710404485464096,0.07430806010961533,0.07919080555438995,0.07222209870815277,0.07140889763832092,0.0719817578792572,0.07161279022693634,0.07233360409736633,0.08875063061714172,0.07856151461601257,0.0752653256058693,0.07846514135599136,0.06599070131778717,0.07444155216217041,0.08089736104011536,0.07070116698741913,0.07488274574279785,0.08390151709318161,0.07299845665693283,0.07628706842660904,0.07816343754529953,0.07550175487995148,0.07883130759000778,0.0752880722284317,0.0849999189376831,0.07805198431015015,0.07509538531303406,0.080099917948246,0.07537179440259933,0.07284965366125107,0.07703851163387299,0.07144717127084732,0.0729198306798935,0.06926339864730835,0.07762011885643005,0.09590363502502441,0.07606839388608932,0.07271690666675568,0.07348161935806274,0.07369033247232437,0.07585109025239944,0.07325545698404312,0.0763905942440033,0.07167263329029083,0.07009667158126831,0.07853379100561142,0.08200189471244812,0.06607946753501892,0.08268488943576813,0.07749424874782562,0.07467342168092728,0.0845242589712143,0.07449464499950409,0.06825434416532516,0.0754634216427803,0.07220378518104553,0.0728398934006691,0.07301542162895203,0.07336948812007904,0.07112711668014526,0.06984085589647293,0.07276484370231628,0.0801907330751419,0.07996084541082382,0.07469423860311508,0.08285201340913773,0.07765548676252365,0.07559563964605331,0.07027487456798553,0.0919334813952446,0.07808610796928406,0.07471645623445511,0.07374613732099533,0.08246121555566788,0.08383054286241531,0.0717628076672554,0.07760949432849884,0.07709871977567673,0.07551403343677521,0.09424576163291931,0.07294456660747528,0.07661758363246918,0.07526423037052155,0.07102115452289581,0.07517892867326736,0.0730130523443222,0.07082291692495346,0.07155333459377289,0.07199107110500336,0.07436522841453552,0.08520305901765823,0.07639693468809128,0.07314392179250717,0.07236927002668381,0.06906455755233765,0.0731600672006607,0.07333807647228241,0.07436995208263397,0.07601092755794525,0.08083435148000717,0.08512377738952637,0.07896288484334946,0.07480239123106003,0.07156386971473694,0.0756043791770935,0.07602639496326447,0.07582182437181473,0.07245638221502304,0.07511316984891891,0.07160016149282455,0.07402163743972778,0.0722423568367958,0.07765089720487595,0.07726124674081802,0.07200793921947479,0.0789041593670845,0.07338091731071472,0.08352327346801758,0.08011126518249512,0.07059842348098755,0.07722581923007965,0.07175401598215103,0.0773688480257988,0.07247919589281082,0.07947416603565216,0.07087531685829163,0.07139132171869278,0.07501958310604095,0.07476140558719635,0.07467616349458694,0.07317197322845459,0.07681027054786682,0.07622674852609634,0.07487653940916061,0.07327184826135635,0.07034531235694885,0.08112078905105591,0.0867808535695076,0.07948372513055801,0.07294217497110367,0.07310385257005692,0.07099992781877518,0.07144515961408615,0.07444208115339279,0.07424858957529068,0.07839400321245193,0.08267956227064133,0.07563597708940506,0.07160554826259613,0.0709124282002449,0.07485588639974594,0.07211168110370636,0.07605843991041183,0.07295239716768265,0.0730414018034935,0.07416372001171112,0.07666556537151337,0.10616336017847061,0.07249470800161362,0.07414595782756805,0.07117418944835663,0.0722353383898735,0.06970719993114471,0.07064802944660187,0.0714387372136116,0.0722615197300911,0.0710768774151802,0.07492424547672272,0.07389853149652481,0.0836310014128685,0.06983908265829086,0.07326699793338776,0.07486502826213837,0.0716933012008667,0.06827293336391449,0.07573741674423218,0.08709732443094254,0.07829922437667847,0.07531952857971191,0.06953451782464981,0.07645567506551743,0.07037735730409622,0.07009164243936539,0.07972833514213562,0.07589825987815857,0.06695495545864105,0.07794500887393951,0.08474786579608917,0.08039209246635437,0.07797139883041382,0.07242055237293243,0.073198102414608,0.0706767737865448,0.07102988660335541,0.08581966161727905,0.0700056180357933,0.0795096904039383,0.07141488790512085,0.07695622742176056,0.07416564226150513,0.07426943629980087,0.07568453252315521,0.07827865332365036,0.0738663598895073,0.06984680145978928,0.072414830327034,0.08553417772054672,0.07091142982244492,0.07265859842300415,0.07212243229150772,0.07150381058454514,0.07493460178375244,0.06980976462364197,0.07072474807500839,0.07425111532211304,0.07129444181919098,0.07019425183534622,0.07894821465015411,0.08083546161651611,0.07176679372787476,0.07240696251392365,0.0756983682513237,0.07456877827644348,0.08130016922950745,0.07613510638475418,0.07213813066482544,0.07556595653295517,0.0743018239736557,0.07119295746088028,0.07166725397109985,0.07823173701763153,0.0736401379108429,0.07028107345104218,0.07177526503801346,0.0756528377532959,0.0743107944726944,0.07142991572618484,0.07479960471391678,0.07715780287981033,0.08242791891098022,0.07383914291858673,0.071139395236969,0.0723375678062439,0.07048332691192627,0.07832295447587967,0.07278451323509216,0.07406814396381378,0.07692056894302368,0.0715612843632698,0.08675520867109299,0.07409168779850006,0.07769947499036789,0.06999695301055908,0.07517489045858383,0.08054567873477936,0.07332149147987366,0.06620517373085022,0.07425607740879059,0.07542441040277481,0.0765482485294342,0.07761684060096741,0.07234581559896469,0.07291319966316223,0.07862476259469986,0.08076528459787369,0.08596819639205933,0.07570672780275345,0.09448662400245667,0.09520633518695831,0.0725962296128273,0.07859110087156296,0.07117671519517899,0.07573392987251282,0.07087788730859756,0.07767045497894287,0.07243861258029938,0.07518938183784485,0.06822709739208221,0.08445726335048676,0.0750657469034195,0.07114659994840622,0.07329829782247543,0.06968908756971359,0.06941147148609161,0.06985881179571152,0.07768747210502625,0.08773405104875565,0.0760498121380806,0.07093069702386856,0.0801108106970787,0.07356645911931992,0.08708511292934418,0.06971433013677597,0.07281823456287384,0.06788545846939087,0.07619725912809372,0.07609400153160095,0.07224161177873611,0.07218937575817108,0.071403868496418,0.08950679004192352,0.06984979659318924,0.07489880919456482,0.07765907794237137,0.07081231474876404,0.08626382052898407,0.08055267482995987,0.07979073375463486,0.0736108049750328,0.07389497011899948,0.07316455990076065,0.06930860131978989,0.08135031163692474,0.08775598555803299,0.0781484991312027,0.08002162724733353,0.07748433202505112,0.0763309895992279,0.07585866004228592,0.07608865201473236,0.07347304373979568,0.06993123888969421,0.07435913383960724,0.07368055731058121,0.09132362902164459,0.07328887283802032,0.08759967237710953,0.07093831151723862,0.07039333134889603,0.07145145535469055,0.07103675603866577,0.07638689875602722,0.07355872541666031,0.08240113407373428,0.07933887094259262,0.07207486778497696,0.078595370054245,0.0697479248046875,0.07271607220172882,0.07180631905794144,0.07149851322174072,0.0732102319598198,0.06984895467758179,0.07293304055929184,0.07485640048980713,0.08197424560785294,0.07397900521755219,0.06953129917383194,0.06864546239376068,0.07899365574121475,0.07634031772613525,0.07351657003164291,0.07403415441513062,0.09541207551956177,0.07257400453090668,0.0750458613038063,0.07550645619630814,0.07932935655117035,0.07076328247785568,0.07285846769809723,0.0733148381114006,0.08896838873624802,0.0748964473605156,0.08270774781703949,0.08630982041358948,0.07296780496835709,0.07034959644079208,0.07807697355747223,0.07101189345121384,0.06566157937049866,0.0751643031835556,0.07087535411119461,0.07968967407941818,0.07119394093751907,0.06954628974199295,0.07675143331289291,0.09258702397346497,0.06973012536764145,0.06763987243175507,0.07676153630018234,0.07622344046831131,0.08114508539438248,0.07321114838123322,0.07118517905473709,0.07260162383317947,0.074969083070755,0.07153435796499252,0.07486166805028915,0.0930883064866066,0.07902351766824722,0.07613766193389893,0.07322356104850769,0.08017890900373459,0.07432550191879272,0.0730552077293396,0.07225340604782104,0.08204533904790878,0.08751717954874039,0.07595105469226837,0.08227293193340302,0.07164079695940018,0.06783894449472427,0.07423830032348633,0.0695774257183075,0.06923993676900864,0.07332360744476318,0.07811932265758514,0.08894870430231094,0.07190035283565521,0.07046858221292496,0.06800097972154617,0.06828564405441284,0.07076279073953629,0.07305063307285309,0.07817843556404114,0.07700269669294357,0.07172104716300964,0.07051727920770645,0.07269687950611115,0.07369425892829895,0.07374672591686249,0.08399643748998642,0.10253942757844925,0.10576116293668747,0.0749351978302002,0.06917029619216919,0.07394520193338394,0.0762529969215393,0.07509668171405792,0.07425893843173981,0.08821327239274979,0.06910338997840881,0.07400112599134445,0.0740751102566719,0.07000767439603806,0.0805491954088211,0.07179364562034607,0.06935841590166092,0.07998809963464737,0.07206892967224121,0.07438468933105469,0.08022527396678925,0.0757494568824768,0.07669684290885925,0.07596444338560104,0.07255680859088898,0.07632577419281006,0.07075925916433334,0.07093294709920883,0.07291312515735626,0.07508592307567596,0.07564638555049896,0.0780196338891983,0.07356594502925873,0.08595332503318787,0.07523220777511597,0.07328438758850098,0.07231780141592026,0.07044706493616104,0.08209431171417236,0.07516056299209595,0.0705825537443161,0.07619282603263855,0.08077467978000641,0.07879064977169037,0.07709769904613495,0.08206276595592499,0.07231693714857101,0.08070628345012665,0.07201720029115677,0.07475012540817261,0.08082924783229828,0.07516633719205856,0.06980513036251068,0.07175721973180771,0.07059502601623535,0.07463431358337402,0.07312507927417755,0.07565575838088989,0.07360046356916428,0.07800182700157166,0.0797012597322464,0.07759693264961243,0.08266209065914154,0.07559268176555634,0.07293153554201126,0.0733921155333519,0.0766175165772438,0.07121280580759048,0.07999226450920105,0.07805728912353516,0.07347333431243896,0.06643170118331909,0.07812348008155823,0.07338164001703262,0.09278738498687744,0.07755141705274582,0.06695304065942764,0.07308503240346909,0.07560878992080688,0.07116902619600296,0.07080381363630295,0.08548997342586517,0.06940973550081253,0.0783296599984169,0.08730007708072662,0.07253001630306244,0.07298189401626587,0.0743032917380333,0.0737982764840126,0.07353628426790237,0.07161525636911392,0.08928299695253372,0.07503880560398102,0.08411341905593872,0.07126452773809433,0.08042676746845245,0.0756969302892685,0.07820809632539749,0.07256415486335754,0.07078796625137329,0.08256369829177856,0.07565382868051529,0.07418406009674072,0.0743342712521553,0.07528743147850037,0.06811323016881943,0.07306763529777527,0.07945552468299866,0.08090601861476898,0.06927483528852463,0.06943359225988388,0.07060840725898743,0.07330215722322464,0.06978719681501389,0.07497777044773102,0.07148198038339615,0.07102648913860321,0.07288656383752823,0.07635141164064407,0.06962411850690842,0.07077226042747498,0.09189431369304657,0.07182464003562927,0.07554060220718384,0.06966548413038254,0.07156971842050552,0.07379171997308731,0.07126296311616898,0.07360853999853134,0.06965669244527817,0.07396110147237778,0.07447253912687302,0.0715484693646431,0.07117560505867004,0.07359156757593155,0.07472693175077438,0.08331623673439026,0.07758387923240662,0.07036443054676056,0.07124727219343185,0.06880567222833633,0.07817625254392624,0.0734744518995285,0.07099682837724686,0.07253071665763855,0.06858590245246887,0.07418538630008698,0.07582151889801025,0.07273222506046295,0.0680689662694931,0.07723982632160187,0.07193497568368912,0.0738481879234314,0.06932397931814194,0.06892622262239456,0.07126449793577194,0.07052141427993774,0.08394329994916916,0.08292102068662643,0.07348623871803284,0.07020718604326248,0.07767654210329056,0.0737006664276123,0.07210691273212433,0.08169287443161011,0.067501001060009,0.08169899880886078,0.06815358996391296,0.07053106278181076,0.07242868095636368,0.10957793891429901,0.07322274148464203,0.06986159831285477,0.07600070536136627,0.07353061437606812,0.07410257309675217,0.06923072040081024,0.07284727692604065,0.07136838883161545,0.06928780674934387,0.07617929577827454,0.06714475154876709,0.07219196110963821,0.06746786087751389,0.07626478374004364,0.07658088207244873,0.06885082274675369,0.0736963301897049,0.07660435140132904,0.06837788224220276,0.06541626155376434,0.07521330565214157,0.07243119180202484,0.0717291533946991,0.07563583552837372,0.07126256823539734,0.07832778990268707,0.06963879615068436,0.07186214625835419,0.07597192376852036,0.0757146030664444,0.07149633020162582,0.07591838389635086,0.0704197958111763,0.07203707844018936,0.07477830350399017,0.07678575068712234,0.07334412634372711,0.0706794261932373,0.07483275234699249,0.07802009582519531,0.07750433683395386,0.07878093421459198,0.07155877351760864,0.07037922739982605,0.07339378446340561,0.07876768708229065,0.08850172907114029,0.07251148670911789,0.0708388239145279,0.0708976462483406,0.07345899194478989,0.0716022253036499,0.07572316378355026,0.07072605192661285,0.06962177902460098,0.06988198310136795,0.07555841654539108,0.08028802275657654,0.07761314511299133,0.07675394415855408,0.07399560511112213,0.07605528831481934,0.07600302249193192,0.07054901123046875,0.0757606253027916,0.0750654935836792,0.07288704067468643,0.07132629305124283,0.07068052887916565,0.07589840143918991,0.08099627494812012,0.0706702396273613,0.07003449648618698,0.07152725011110306,0.0700201541185379,0.0692160427570343,0.07315735518932343,0.08011248707771301,0.0753897875547409,0.07494829595088959,0.07058726251125336,0.08575016260147095,0.0711812749505043,0.06948359310626984,0.08126039803028107,0.0749126672744751,0.0681041032075882,0.07007475942373276,0.06723940372467041,0.07206667214632034,0.07262122631072998,0.0709335058927536,0.07185043394565582,0.07010958343744278,0.07194595783948898,0.07334665954113007,0.07180832326412201,0.06730563938617706,0.06848631799221039,0.07025451213121414,0.0688449963927269,0.06761634349822998,0.07531678676605225,0.06537094712257385,0.07485609501600266,0.07979846745729446,0.08177956193685532,0.07683730870485306,0.06729782372713089,0.06729287654161453,0.07310006767511368,0.07427824288606644,0.07185211032629013,0.0849481001496315,0.07014995813369751,0.08491791039705276,0.07748913764953613,0.07117535918951035,0.07595046609640121,0.08860041201114655,0.07817589491605759,0.07807046175003052,0.07730372250080109,0.06838028132915497,0.07013746351003647,0.09622380137443542,0.09363995492458344,0.07191382348537445,0.0697534829378128,0.0716632604598999,0.06818732619285583,0.07579778879880905,0.06537771970033646,0.07055870443582535,0.07165303826332092,0.07617878168821335,0.07363209128379822,0.0671594887971878,0.07227349281311035,0.07250823080539703,0.07187658548355103,0.07207075506448746,0.07324562221765518,0.06993655115365982,0.0751735046505928,0.07197295874357224,0.0695309191942215,0.07206279039382935,0.07712407410144806,0.0727766826748848,0.07194805145263672,0.07853570580482483,0.07169676572084427,0.07266873121261597,0.07075141370296478,0.07294892519712448,0.07075486332178116,0.07534073293209076,0.06770144402980804,0.07496360689401627,0.08763864636421204,0.0673343688249588,0.07624765485525131,0.07034754008054733,0.07294400781393051,0.07205817848443985,0.07076362520456314,0.07833628356456757,0.07585863769054413,0.08263102173805237,0.07039172202348709,0.07568559050559998,0.07302114367485046,0.0710882693529129,0.07590208947658539,0.07364945113658905,0.0730992779135704,0.07341881096363068,0.07074937969446182,0.07548076659440994,0.0800924077630043,0.07229264825582504,0.07120148837566376,0.06936348974704742,0.0808984637260437,0.07345358282327652,0.08248389512300491,0.06806743144989014,0.07116664946079254,0.07225734740495682,0.07397346943616867,0.06980009377002716,0.0763208344578743,0.06610032171010971,0.07095210254192352,0.07335822284221649,0.07490824162960052,0.07746323943138123,0.07540811598300934,0.07490459084510803,0.07352462410926819,0.06980398297309875,0.06947168707847595,0.06873715668916702,0.07518570125102997,0.07154830545186996,0.07879135757684708,0.06977252662181854,0.0724075585603714,0.07295763492584229,0.0741635337471962,0.0687161386013031,0.08245844393968582,0.06863448768854141,0.06839688867330551,0.07196506857872009,0.07322844117879868,0.06955164670944214,0.07138712704181671,0.07412593066692352,0.07550030946731567,0.07030606269836426,0.0740058571100235,0.07078660279512405,0.0839683935046196,0.07631614059209824,0.07553722709417343,0.06654030829668045,0.07288353890180588,0.07137103378772736,0.06956145912408829,0.07403897494077682,0.06691641360521317,0.07055728882551193,0.07295939326286316,0.06800127029418945,0.06808216124773026,0.06930861622095108,0.07198913395404816,0.0814797431230545,0.07054679840803146,0.06750725954771042,0.07314524799585342,0.06902092695236206,0.0694921463727951,0.06755461543798447,0.07088259607553482,0.07993670552968979,0.07136712968349457,0.07893772423267365,0.0779397264122963,0.0749749019742012,0.07054086029529572,0.07928131520748138,0.08407511562108994,0.07096420228481293,0.07767853885889053,0.07035108655691147,0.08519063889980316,0.06650326400995255,0.06968889385461807,0.07282678037881851,0.07445747405290604,0.07451710850000381,0.0811147466301918,0.08360402286052704,0.07907693088054657,0.07514318078756332,0.07081009447574615,0.06872489303350449,0.07137298583984375,0.07969783991575241,0.06992536783218384,0.07505737245082855,0.07106518745422363,0.07008635252714157,0.07109856605529785,0.069129578769207,0.0774545818567276,0.07145541906356812,0.07351049780845642,0.07380952686071396,0.07320358604192734,0.07551580667495728,0.08295396715402603,0.07241892069578171,0.06991325318813324,0.07154814898967743,0.07684021443128586,0.07377992570400238,0.07071057707071304,0.07451704889535904,0.06839865446090698,0.07246021926403046,0.07463636249303818,0.08094199746847153,0.07196028530597687,0.07281263917684555,0.07137206196784973,0.06816510111093521,0.074807308614254,0.0677725151181221,0.07012667506933212,0.07519308477640152,0.07109590619802475,0.07195321470499039,0.06967628747224808,0.0685533955693245,0.06944409757852554,0.08698216825723648,0.06993479281663895,0.07024058699607849,0.07222124189138412,0.07276201248168945,0.07565692812204361,0.07164143770933151,0.07152322679758072,0.07813558727502823,0.07034769654273987,0.07189509272575378,0.07856923341751099,0.07063800096511841,0.07875695824623108,0.06816702336072922,0.07476617395877838,0.07055094093084335,0.07526468485593796,0.0749640092253685,0.0710199773311615,0.08079973608255386,0.08069558441638947,0.06929121166467667,0.07001722604036331,0.07566147297620773,0.07788677513599396,0.06926286220550537,0.07259387522935867,0.07399486750364304,0.08196993172168732,0.07446323335170746,0.07128489017486572,0.0702858716249466,0.07119601964950562,0.06946777552366257,0.06914635002613068,0.0749519094824791,0.09005185216665268,0.07208865135908127,0.08039579540491104,0.07319924980401993,0.07900701463222504,0.06879732012748718,0.07580529153347015,0.07410717755556107,0.0707831010222435,0.07134175300598145,0.07160889357328415,0.06886742264032364,0.07617703825235367,0.07193532586097717,0.07116203010082245,0.08044767379760742,0.06882018595933914,0.07339510321617126,0.07385893911123276,0.06866931915283203,0.07670114189386368,0.0698658898472786,0.07301828265190125,0.0841781347990036,0.07336826622486115,0.0787348598241806,0.06948541849851608,0.06832324713468552,0.06863214075565338,0.0779721587896347,0.07322601974010468,0.07211843878030777,0.07054164260625839,0.07434982061386108,0.07786939293146133,0.07419343292713165,0.07171763479709625,0.07679782062768936,0.06886813789606094,0.07298299670219421,0.0726456269621849,0.07840097695589066,0.07532574981451035,0.06892582029104233,0.07106605917215347,0.07849864661693573,0.06790212541818619,0.08722100406885147,0.06929811090230942,0.07311464101076126,0.07900181412696838,0.06663262099027634,0.07273659110069275,0.0702807754278183,0.0684233009815216,0.0696106031537056,0.06901051849126816,0.07592257857322693,0.0730002373456955,0.07457611709833145,0.06751919537782669,0.08491376042366028,0.07394901663064957,0.0701698288321495,0.0687980204820633,0.0805114135146141,0.07503017038106918,0.07099539786577225,0.06996262073516846,0.07500478625297546,0.07188241928815842,0.07690969109535217,0.06897252798080444,0.0702972561120987,0.09587977826595306,0.07413491606712341,0.06728437542915344,0.07405629009008408,0.07676485925912857,0.071328304708004,0.07189866155385971,0.07065315544605255,0.07245820015668869,0.0720488429069519,0.06917843222618103,0.06890756636857986,0.08011958003044128,0.07215937972068787,0.08495202660560608,0.08438393473625183,0.0705304965376854,0.07274362444877625,0.07915659993886948,0.0686882734298706,0.07095437496900558,0.0733906701207161,0.07143744081258774,0.07289072871208191,0.07343947142362595,0.0668332651257515,0.07180866599082947,0.06798223406076431,0.07881970703601837,0.06839164346456528,0.07429488003253937,0.08110996335744858,0.08496953547000885,0.07368019968271255,0.08860080689191818,0.06906461715698242,0.07378829270601273,0.08969184756278992,0.06866872310638428,0.08029878884553909,0.07139036059379578,0.07324490696191788,0.0672789216041565,0.07400013506412506,0.06822007894515991,0.07378962635993958,0.07070992887020111,0.07414635270833969,0.07730234414339066,0.07811915129423141,0.06830743700265884,0.06780258566141129,0.09857863187789917,0.07307411730289459,0.07298579812049866,0.08655858784914017,0.06944305449724197,0.07697434723377228,0.0737624242901802,0.07176042348146439,0.07272003591060638,0.06980223208665848,0.0721067562699318,0.07243000715970993,0.0678156390786171,0.07187916338443756,0.0695517361164093,0.06451446563005447,0.06995198130607605,0.08595085144042969,0.07234235107898712,0.07046172022819519,0.06830766052007675,0.07160963118076324,0.06968569755554199,0.0746515691280365,0.06973356753587723,0.06675783544778824,0.07151201367378235,0.07130876183509827,0.07362593710422516,0.0700610801577568,0.07274089008569717,0.07398760318756104,0.06643620133399963,0.07119715213775635,0.0708552747964859,0.07330064475536346,0.0720091238617897,0.07009708881378174,0.07077839225530624,0.06852719187736511,0.07841332256793976,0.08256039768457413,0.07592375576496124,0.07065531611442566,0.07122667133808136,0.06798450648784637,0.07286831736564636,0.07342182099819183,0.08497220277786255,0.07016777247190475,0.06997527927160263,0.07914324849843979,0.06756682693958282,0.09766852855682373,0.08096982538700104,0.07975000143051147,0.07298070192337036,0.07092463225126266,0.07995005697011948,0.1103072389960289,0.06740567088127136,0.07440987974405289,0.07327671349048615,0.06899334490299225,0.07446040958166122,0.07627607882022858,0.0752735584974289,0.07180175185203552,0.10346011072397232,0.07643544673919678,0.08043505251407623,0.07045771926641464,0.08567299693822861,0.07679788023233414,0.07165350764989853,0.07284988462924957,0.07056303322315216,0.06921729445457458,0.07690934836864471,0.06754296272993088,0.07256406545639038,0.07493741065263748,0.094480000436306,0.06635848432779312,0.0696607157588005,0.07324843853712082,0.07003741711378098,0.06589966267347336,0.07054640352725983,0.07169973105192184,0.07479828596115112,0.06765468418598175,0.0658596009016037,0.07180195301771164,0.06894541531801224,0.07004010677337646,0.0832158550620079,0.07289200276136398,0.07013748586177826,0.06956660747528076,0.0757276639342308,0.06933371722698212,0.07860355824232101,0.07052864134311676,0.07143215835094452,0.0740419402718544,0.07365965098142624,0.07035398483276367,0.06989049166440964,0.07715685665607452,0.0729682594537735,0.08857874572277069,0.06793059408664703,0.06934966146945953,0.07769127190113068,0.0716877430677414,0.06982026994228363,0.07114479690790176,0.07416686415672302,0.08050264418125153,0.0715722143650055,0.07246069610118866,0.08277127891778946,0.06716737151145935,0.07981747388839722,0.07153605669736862,0.06572657823562622,0.07439784705638885,0.07135521620512009,0.07274886965751648,0.06637236475944519,0.07363545149564743,0.07518625259399414,0.07321406900882721,0.07000625133514404,0.07456997036933899,0.07213518023490906,0.07283526659011841,0.07276258617639542,0.07379261404275894,0.0686044842004776,0.06947335600852966,0.08156172186136246,0.0736045092344284,0.06948143243789673,0.07060309499502182,0.06830644607543945,0.06591857224702835,0.07112903892993927,0.07020757347345352,0.07012642920017242,0.07918953895568848,0.07248545438051224,0.0825515016913414,0.07552662491798401,0.07189842313528061,0.09426216781139374,0.07500965893268585,0.07268060743808746,0.06676166504621506,0.07298935949802399,0.07166718691587448,0.07311995327472687,0.07925257086753845,0.07643202692270279,0.07429986447095871,0.0740751326084137,0.08720358461141586,0.07771054655313492,0.08814128488302231,0.07292574644088745,0.07015618681907654,0.08865773677825928,0.06719238311052322,0.06909306347370148,0.07847704738378525,0.07034910470247269,0.07352975755929947,0.06965093314647675,0.06809055805206299,0.07517184317111969,0.07285481691360474,0.06919384747743607,0.07448499649763107,0.07104341685771942,0.06674448400735855,0.082868792116642,0.0745377317070961,0.07202582061290741,0.07057656347751617,0.07147347927093506,0.07100020349025726,0.07670045644044876,0.072209432721138,0.068352609872818,0.07312794774770737,0.07205970585346222,0.07928241789340973,0.07556602358818054,0.07665606588125229,0.07065991312265396,0.0831606462597847,0.07869888097047806,0.0682046040892601,0.08695917576551437,0.0680699348449707,0.0673503577709198,0.06722808629274368,0.07186602056026459,0.06979093700647354,0.08814425766468048,0.07278714329004288,0.07056078314781189,0.07407831400632858,0.07176481187343597,0.07585557550191879,0.07058782875537872,0.06941568851470947,0.07596329599618912,0.07001791894435883,0.07869290560483932,0.0743098109960556,0.07283574342727661,0.06845900416374207,0.07610059529542923,0.07273870706558228,0.06997150182723999,0.07193691283464432,0.07041040062904358,0.07363823801279068,0.06919880956411362,0.0716698169708252,0.073158860206604,0.07384113222360611,0.07044591009616852,0.07197052985429764,0.07031077146530151,0.07341860234737396,0.0731661394238472,0.07737474143505096,0.07845218479633331,0.07152356207370758,0.07120422273874283,0.06995917111635208,0.07239047437906265,0.07048043608665466,0.07108095288276672,0.0676039382815361,0.08197035640478134,0.06902337819337845,0.07104437798261642,0.07412862777709961,0.08079694211483002,0.07402416318655014,0.08038836717605591,0.073895163834095,0.07028865814208984,0.07280479371547699,0.07776141166687012,0.06674760580062866,0.06898019462823868,0.07235017418861389,0.07504072040319443,0.0708477795124054,0.06996222585439682,0.07479941099882126,0.07136117666959763,0.0692652240395546,0.0769873857498169,0.07252860069274902,0.07052527368068695,0.07405176013708115,0.07038723677396774,0.06605953723192215,0.07019935548305511,0.0703827440738678,0.07189194113016129,0.06922435760498047,0.06655354797840118,0.0668983981013298,0.0735144391655922,0.08161801844835281,0.07038575410842896,0.07484700530767441,0.0718349814414978,0.07283100485801697,0.08695565909147263,0.06942564249038696,0.06783029437065125,0.06950022280216217,0.06758111715316772,0.06833023577928543,0.08152063190937042,0.07169470936059952,0.08256284147500992,0.07198767364025116,0.07192500680685043,0.06983163207769394,0.06964630633592606,0.07158193737268448,0.07513459771871567,0.07142674177885056,0.0766778513789177,0.06799083948135376,0.07865419238805771,0.07888766378164291,0.07366914302110672,0.06512492895126343,0.06879895180463791,0.0735408291220665,0.07336191833019257,0.07460375875234604,0.07481937110424042,0.08215612918138504,0.07412779331207275,0.06991929560899734,0.06780073046684265,0.07907790690660477,0.07192341983318329,0.06789342314004898,0.073208287358284,0.07621493190526962,0.07619913667440414,0.07537072151899338,0.07168935984373093,0.06927352398633957,0.07071889936923981,0.09909471124410629,0.07345544546842575,0.07853513956069946,0.06964971870183945,0.08393486589193344,0.07132486253976822,0.06589886546134949,0.07528221607208252,0.0672096461057663,0.07164651155471802,0.07160700112581253,0.07495380938053131,0.09375092387199402,0.07426679134368896,0.07205348461866379,0.07214496284723282,0.08449127525091171,0.07713024318218231,0.07041501253843307,0.07265447080135345,0.07346199452877045,0.07026172429323196,0.07136385142803192,0.07086825370788574,0.07890082895755768,0.070710189640522,0.07156909257173538,0.07239726185798645,0.07032138109207153,0.07386144995689392,0.06461690366268158,0.07123221457004547,0.06753618270158768,0.07872387766838074,0.07058214396238327,0.07271286845207214,0.07380113750696182,0.07100383937358856],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.94]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"<b>Loss Val_Loss</b>\"}},\"yaxis2\":{\"anchor\":\"x\",\"overlaying\":\"y\",\"side\":\"right\"},\"title\":{\"text\":\"Loss Val_Loss\"},\"autosize\":false,\"width\":1200,\"height\":600,\"paper_bgcolor\":\"rgb(200,200,200)\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8fa36b59-2405-4072-a0c6-059379748714');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPua1LbQis668JqRpFQg2pA",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}