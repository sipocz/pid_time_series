{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/pid_NN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "7a526ed7-8baa-4d70-894f-07b94b600189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint1\"\n",
        "data_source=\"5\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_error_part1.csv\"\n",
        "fname_bad = \"415_SC_error_part2.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "3c57867d-1367-4d21-d578-39fd80aa3c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part1.csv\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part2.csv\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vUcMjZAGKvtt"
      },
      "outputs": [],
      "source": [
        "df_good = pd.read_csv(path_good,usecols=None)\n",
        "df_bad = pd.read_csv(path_bad,usecols=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYuDXKraLOt4",
        "outputId": "43f2f7a7-a9dd-470a-f408-69b655227305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(df_good.isnull().values.any())\n",
        "print(df_bad.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "3b1c1340-2626-422c-964d-316c363fb45d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0          1          2          3          4        5         6  \\\n",
              "0 -54.810024 -80.342186 -60.770203 -41.081482 -21.779583 -3.82353 -0.806820   \n",
              "1 -80.342186 -60.770203 -41.081482 -21.779583  -3.823530 -0.80682  0.220875   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-152345e2-c1c8-413f-9fe5-36e27304b2aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-54.810024</td>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.82353</td>\n",
              "      <td>-0.806820</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.823530</td>\n",
              "      <td>-0.80682</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-152345e2-c1c8-413f-9fe5-36e27304b2aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-152345e2-c1c8-413f-9fe5-36e27304b2aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-152345e2-c1c8-413f-9fe5-36e27304b2aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_good.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "df_ = df_good\n",
        "\n",
        "# You must normalize the data before applying the fit method\n",
        "df_good_normalized=(df_ - df_.mean()) / df_.std()\n",
        "\n",
        "# Normalize bad data with the good data parameters\n",
        "df_bad_normalized=(df_bad - df_.mean()) / df_.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": [
        "df_good_normalized[\"state\"]=0\n",
        "df_bad_normalized[\"state\"]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3W5mi70VM6hL",
        "outputId": "94a2ec46-4548-48f0-f1da-4b1911273e6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0          1          2          3         4         5  \\\n",
              "0    -10.681306 -16.586266 -14.612051 -11.087981 -6.293341 -1.192618   \n",
              "1    -15.654548 -12.549683  -9.889987  -5.905180 -1.164099 -0.314249   \n",
              "2    -11.842250  -8.489023  -5.260696  -1.083756 -0.302359 -0.015017   \n",
              "3     -8.007214  -4.508142  -0.954188  -0.273732 -0.008793 -0.015017   \n",
              "4     -4.247524  -0.804833  -0.230672   0.002217 -0.008793 -0.015017   \n",
              "...         ...        ...        ...        ...       ...       ...   \n",
              "1053   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1054   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1055   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1056   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1057   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "\n",
              "             6         7         8         9  ...        11        12  \\\n",
              "0    -0.315574 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "2    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "3    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "4    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...  ...       ...       ...   \n",
              "1053 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1054 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1055 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1056 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1057 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "\n",
              "            13        14        15        16        17        18        19  \\\n",
              "0    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "2    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "3    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "4    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1053 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1054 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1055 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1056 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1057 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "\n",
              "      state  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  \n",
              "...     ...  \n",
              "1053      0  \n",
              "1054      0  \n",
              "1055      0  \n",
              "1056      0  \n",
              "1057      0  \n",
              "\n",
              "[1058 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01892a1c-25fe-4e35-ae92-c0764d2836b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10.681306</td>\n",
              "      <td>-16.586266</td>\n",
              "      <td>-14.612051</td>\n",
              "      <td>-11.087981</td>\n",
              "      <td>-6.293341</td>\n",
              "      <td>-1.192618</td>\n",
              "      <td>-0.315574</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-15.654548</td>\n",
              "      <td>-12.549683</td>\n",
              "      <td>-9.889987</td>\n",
              "      <td>-5.905180</td>\n",
              "      <td>-1.164099</td>\n",
              "      <td>-0.314249</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-11.842250</td>\n",
              "      <td>-8.489023</td>\n",
              "      <td>-5.260696</td>\n",
              "      <td>-1.083756</td>\n",
              "      <td>-0.302359</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-8.007214</td>\n",
              "      <td>-4.508142</td>\n",
              "      <td>-0.954188</td>\n",
              "      <td>-0.273732</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.247524</td>\n",
              "      <td>-0.804833</td>\n",
              "      <td>-0.230672</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1058 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01892a1c-25fe-4e35-ae92-c0764d2836b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01892a1c-25fe-4e35-ae92-c0764d2836b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01892a1c-25fe-4e35-ae92-c0764d2836b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df_good_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_normalized=pd.concat([df_good_normalized,df_bad_normalized],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "80753e64-f7ae-45a1-81aa-b14659d6dfd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "1263  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1264  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1265  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1266  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1267  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "1263 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1264 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1265 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1266 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1267 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "1263 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1264 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1265 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1266 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1267 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-125aa257-04ef-4fe1-b679-7e6aec52a655\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1263</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1265</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-125aa257-04ef-4fe1-b679-7e6aec52a655')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-125aa257-04ef-4fe1-b679-7e6aec52a655 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-125aa257-04ef-4fe1-b679-7e6aec52a655');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df_all_normalized.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=20\n",
        "drop_frac0=0.05  \n",
        "drop_frac1=0.0  \n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "l1_out=Dense(135,activation=\"swish\",kernel_initializer='glorot_uniform',)(input1) # kernel_initializer='lecun_normal'\n",
        "l2_out=Dropout(drop_frac0)(l1_out)\n",
        "\n",
        "\n",
        "l3_out=Dense(15,activation=\"swish\",kernel_initializer='glorot_uniform',)(l2_out) #kernel_initializer='lecun_normal',\n",
        "l4_out=Dropout(drop_frac1)(l3_out)\n",
        "\n",
        "pred=Dense(1, activation=\"sigmoid\",)(l4_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=0.001,) #\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# 35 5 1 relu relu sigmoid SGD 0.01 loss: 0.1402 - accuracy: 0.9435 - val_loss: 0.7302 - val_accuracy: 0.8548\n",
        "# 35 12 1 relu relu sigmoid SGD 0.01 loss 0.1162 94.6% test : 85%\n",
        "# 17 5 1 relu relu sigmoid SGD 0.01  loss: 0.1714 - accuracy: 0.9300 - val_loss: 0.9535 - val_accuracy: 0.8503\n",
        "# 35 5 1 relu relu sigmoid Adam 0.01 loss: 0.1238 - accuracy: 0.9467 - val_loss: 5.7545 - val_accuracy: 0.8653\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.01 loss: 0.1184 - accuracy: 0.9525 - val_loss: 3.5327 - val_accuracy: 0.8428\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1185 - accuracy: 0.9525 - val_loss: 2.3218 - val_accuracy: 0.8593\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1041 - accuracy: 0.9576 - val_loss: 5.1465 - val_accuracy: 0.8353  +1300 epoch \n",
        "# 135 15 1 swish swish sigmoid Adamax 0.001 batch size:1 epoch 100 loss: 0.1707 - accuracy: 0.9352 - val_loss: 0.8066 - val_accuracy: 0.8892   **** egész jó\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\"]\n",
        "X_NN=df_all_normalized[prediktorok][:-100]  # \n",
        "y_NN=df_all_normalized[\"state\"][:-100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget $model_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2aTLUet22yv",
        "outputId": "f39b5bb5-48af-4fec-9a86-1218bb905be9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-15 22:28:54--  https://github.com/sipocz/pid_time_series/raw/main/model/model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/pid_time_series/main/model/model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5 [following]\n",
            "--2022-12-15 22:28:54--  https://raw.githubusercontent.com/sipocz/pid_time_series/main/model/model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93340 (91K) [application/octet-stream]\n",
            "Saving to: ‘model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5’\n",
            "\n",
            "model_PID__54_loss_ 100%[===================>]  91.15K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2022-12-15 22:28:54 (18.5 MB/s) - ‘model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5’ saved [93340/93340]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "zhXTFzW73ECt"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=0.7,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname=\"./model_PID_\"\n",
        "callbacks = [#callback_LR,\n",
        "       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{accuracy:.3f}_vacc_{val_accuracy:.3f}.hdf5\", monitor='loss',\n",
        "                        verbose=2, save_best_only=True, mode='min')]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ol0mW6WRlkS",
        "outputId": "54d42331-b8fb-46a9-aa4c-921e11853527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "503/520 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9483\n",
            "Epoch 1: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1187 - accuracy: 0.9487 - val_loss: 0.1092 - val_accuracy: 0.9581\n",
            "Epoch 2/100\n",
            "517/520 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9516\n",
            "Epoch 2: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1321 - accuracy: 0.9506 - val_loss: 0.1084 - val_accuracy: 0.9581\n",
            "Epoch 3/100\n",
            "501/520 [===========================>..] - ETA: 0s - loss: 0.1262 - accuracy: 0.9488\n",
            "Epoch 3: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9493 - val_loss: 0.1091 - val_accuracy: 0.9566\n",
            "Epoch 4/100\n",
            "506/520 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9453\n",
            "Epoch 4: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1300 - accuracy: 0.9461 - val_loss: 0.1102 - val_accuracy: 0.9581\n",
            "Epoch 5/100\n",
            "501/520 [===========================>..] - ETA: 0s - loss: 0.1193 - accuracy: 0.9501\n",
            "Epoch 5: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1191 - accuracy: 0.9506 - val_loss: 0.1102 - val_accuracy: 0.9581\n",
            "Epoch 6/100\n",
            "511/520 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9459\n",
            "Epoch 6: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1365 - accuracy: 0.9461 - val_loss: 0.1110 - val_accuracy: 0.9581\n",
            "Epoch 7/100\n",
            "519/520 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9493\n",
            "Epoch 7: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1196 - accuracy: 0.9493 - val_loss: 0.1103 - val_accuracy: 0.9566\n",
            "Epoch 8/100\n",
            "488/520 [===========================>..] - ETA: 0s - loss: 0.1154 - accuracy: 0.9536\n",
            "Epoch 8: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1181 - accuracy: 0.9506 - val_loss: 0.1118 - val_accuracy: 0.9566\n",
            "Epoch 9/100\n",
            "508/520 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9495\n",
            "Epoch 9: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1203 - accuracy: 0.9499 - val_loss: 0.1103 - val_accuracy: 0.9566\n",
            "Epoch 10/100\n",
            "515/520 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9495\n",
            "Epoch 10: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1332 - accuracy: 0.9499 - val_loss: 0.1107 - val_accuracy: 0.9566\n",
            "Epoch 11/100\n",
            "490/520 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9497\n",
            "Epoch 11: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1188 - accuracy: 0.9512 - val_loss: 0.1110 - val_accuracy: 0.9566\n",
            "Epoch 12/100\n",
            "504/520 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9471\n",
            "Epoch 12: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1574 - accuracy: 0.9480 - val_loss: 0.1109 - val_accuracy: 0.9566\n",
            "Epoch 13/100\n",
            "512/520 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9525\n",
            "Epoch 13: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1277 - accuracy: 0.9525 - val_loss: 0.1109 - val_accuracy: 0.9566\n",
            "Epoch 14/100\n",
            "503/520 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9543\n",
            "Epoch 14: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1183 - accuracy: 0.9531 - val_loss: 0.1109 - val_accuracy: 0.9566\n",
            "Epoch 15/100\n",
            "502/520 [===========================>..] - ETA: 0s - loss: 0.1203 - accuracy: 0.9535\n",
            "Epoch 15: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1193 - accuracy: 0.9538 - val_loss: 0.1108 - val_accuracy: 0.9566\n",
            "Epoch 16/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9508\n",
            "Epoch 16: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1204 - accuracy: 0.9519 - val_loss: 0.1110 - val_accuracy: 0.9566\n",
            "Epoch 17/100\n",
            "500/520 [===========================>..] - ETA: 0s - loss: 0.1227 - accuracy: 0.9520\n",
            "Epoch 17: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1235 - accuracy: 0.9519 - val_loss: 0.1121 - val_accuracy: 0.9566\n",
            "Epoch 18/100\n",
            "506/520 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9480\n",
            "Epoch 18: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1260 - accuracy: 0.9493 - val_loss: 0.1111 - val_accuracy: 0.9566\n",
            "Epoch 19/100\n",
            "508/520 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9488\n",
            "Epoch 19: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1250 - accuracy: 0.9499 - val_loss: 0.1097 - val_accuracy: 0.9566\n",
            "Epoch 20/100\n",
            "496/520 [===========================>..] - ETA: 0s - loss: 0.1214 - accuracy: 0.9503\n",
            "Epoch 20: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9506 - val_loss: 0.1101 - val_accuracy: 0.9566\n",
            "Epoch 21/100\n",
            "516/520 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9541\n",
            "Epoch 21: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1197 - accuracy: 0.9531 - val_loss: 0.1107 - val_accuracy: 0.9566\n",
            "Epoch 22/100\n",
            "503/520 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9496\n",
            "Epoch 22: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9506 - val_loss: 0.1114 - val_accuracy: 0.9551\n",
            "Epoch 23/100\n",
            "506/520 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.9486\n",
            "Epoch 23: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1470 - accuracy: 0.9480 - val_loss: 0.1107 - val_accuracy: 0.9581\n",
            "Epoch 24/100\n",
            "518/520 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9498\n",
            "Epoch 24: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1355 - accuracy: 0.9499 - val_loss: 0.1121 - val_accuracy: 0.9566\n",
            "Epoch 25/100\n",
            "484/520 [==========================>...] - ETA: 0s - loss: 0.1181 - accuracy: 0.9511\n",
            "Epoch 25: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1186 - accuracy: 0.9506 - val_loss: 0.1120 - val_accuracy: 0.9581\n",
            "Epoch 26/100\n",
            "500/520 [===========================>..] - ETA: 0s - loss: 0.1342 - accuracy: 0.9507\n",
            "Epoch 26: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1331 - accuracy: 0.9519 - val_loss: 0.1130 - val_accuracy: 0.9566\n",
            "Epoch 27/100\n",
            "518/520 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9517\n",
            "Epoch 27: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9519 - val_loss: 0.1126 - val_accuracy: 0.9566\n",
            "Epoch 28/100\n",
            "515/520 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.9515\n",
            "Epoch 28: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1311 - accuracy: 0.9519 - val_loss: 0.1118 - val_accuracy: 0.9566\n",
            "Epoch 29/100\n",
            "509/520 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9509\n",
            "Epoch 29: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1283 - accuracy: 0.9506 - val_loss: 0.1132 - val_accuracy: 0.9566\n",
            "Epoch 30/100\n",
            "501/520 [===========================>..] - ETA: 0s - loss: 0.1389 - accuracy: 0.9468\n",
            "Epoch 30: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1360 - accuracy: 0.9487 - val_loss: 0.1128 - val_accuracy: 0.9566\n",
            "Epoch 31/100\n",
            "508/520 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9508\n",
            "Epoch 31: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.9499 - val_loss: 0.1134 - val_accuracy: 0.9566\n",
            "Epoch 32/100\n",
            "491/520 [===========================>..] - ETA: 0s - loss: 0.1315 - accuracy: 0.9511\n",
            "Epoch 32: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1312 - accuracy: 0.9506 - val_loss: 0.1131 - val_accuracy: 0.9566\n",
            "Epoch 33/100\n",
            "485/520 [==========================>...] - ETA: 0s - loss: 0.1169 - accuracy: 0.9519\n",
            "Epoch 33: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9506 - val_loss: 0.1137 - val_accuracy: 0.9566\n",
            "Epoch 34/100\n",
            "512/520 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9512\n",
            "Epoch 34: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9499 - val_loss: 0.1133 - val_accuracy: 0.9581\n",
            "Epoch 35/100\n",
            "519/520 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9550\n",
            "Epoch 35: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1241 - accuracy: 0.9551 - val_loss: 0.1127 - val_accuracy: 0.9566\n",
            "Epoch 36/100\n",
            "488/520 [===========================>..] - ETA: 0s - loss: 0.1160 - accuracy: 0.9515\n",
            "Epoch 36: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1176 - accuracy: 0.9499 - val_loss: 0.1123 - val_accuracy: 0.9566\n",
            "Epoch 37/100\n",
            "515/520 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9495\n",
            "Epoch 37: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9499 - val_loss: 0.1133 - val_accuracy: 0.9566\n",
            "Epoch 38/100\n",
            "490/520 [===========================>..] - ETA: 0s - loss: 0.1279 - accuracy: 0.9476\n",
            "Epoch 38: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1245 - accuracy: 0.9499 - val_loss: 0.1130 - val_accuracy: 0.9581\n",
            "Epoch 39/100\n",
            "511/520 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9524\n",
            "Epoch 39: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1178 - accuracy: 0.9525 - val_loss: 0.1126 - val_accuracy: 0.9581\n",
            "Epoch 40/100\n",
            "484/520 [==========================>...] - ETA: 0s - loss: 0.1166 - accuracy: 0.9504\n",
            "Epoch 40: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1195 - accuracy: 0.9499 - val_loss: 0.1134 - val_accuracy: 0.9566\n",
            "Epoch 41/100\n",
            "490/520 [===========================>..] - ETA: 0s - loss: 0.1334 - accuracy: 0.9469\n",
            "Epoch 41: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1310 - accuracy: 0.9487 - val_loss: 0.1130 - val_accuracy: 0.9566\n",
            "Epoch 42/100\n",
            "493/520 [===========================>..] - ETA: 0s - loss: 0.1265 - accuracy: 0.9520\n",
            "Epoch 42: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1253 - accuracy: 0.9519 - val_loss: 0.1128 - val_accuracy: 0.9566\n",
            "Epoch 43/100\n",
            "514/520 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9527\n",
            "Epoch 43: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1194 - accuracy: 0.9525 - val_loss: 0.1143 - val_accuracy: 0.9566\n",
            "Epoch 44/100\n",
            "488/520 [===========================>..] - ETA: 0s - loss: 0.1169 - accuracy: 0.9522\n",
            "Epoch 44: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1285 - accuracy: 0.9499 - val_loss: 0.1138 - val_accuracy: 0.9566\n",
            "Epoch 45/100\n",
            "490/520 [===========================>..] - ETA: 0s - loss: 0.1189 - accuracy: 0.9531\n",
            "Epoch 45: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1181 - accuracy: 0.9531 - val_loss: 0.1137 - val_accuracy: 0.9566\n",
            "Epoch 46/100\n",
            "510/520 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9542\n",
            "Epoch 46: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1290 - accuracy: 0.9525 - val_loss: 0.1144 - val_accuracy: 0.9566\n",
            "Epoch 47/100\n",
            "498/520 [===========================>..] - ETA: 0s - loss: 0.1252 - accuracy: 0.9572\n",
            "Epoch 47: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1294 - accuracy: 0.9538 - val_loss: 0.1141 - val_accuracy: 0.9551\n",
            "Epoch 48/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1127 - accuracy: 0.9522\n",
            "Epoch 48: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1172 - accuracy: 0.9506 - val_loss: 0.1124 - val_accuracy: 0.9551\n",
            "Epoch 49/100\n",
            "494/520 [===========================>..] - ETA: 0s - loss: 0.1168 - accuracy: 0.9494\n",
            "Epoch 49: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1185 - accuracy: 0.9487 - val_loss: 0.1134 - val_accuracy: 0.9581\n",
            "Epoch 50/100\n",
            "510/520 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9503\n",
            "Epoch 50: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9499 - val_loss: 0.1137 - val_accuracy: 0.9581\n",
            "Epoch 51/100\n",
            "512/520 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9505\n",
            "Epoch 51: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1293 - accuracy: 0.9512 - val_loss: 0.1140 - val_accuracy: 0.9581\n",
            "Epoch 52/100\n",
            "491/520 [===========================>..] - ETA: 0s - loss: 0.1280 - accuracy: 0.9498\n",
            "Epoch 52: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9499 - val_loss: 0.1127 - val_accuracy: 0.9581\n",
            "Epoch 53/100\n",
            "493/520 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9540\n",
            "Epoch 53: loss did not improve from 0.11664\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1194 - accuracy: 0.9525 - val_loss: 0.1128 - val_accuracy: 0.9581\n",
            "Epoch 54/100\n",
            "485/520 [==========================>...] - ETA: 0s - loss: 0.1164 - accuracy: 0.9526\n",
            "Epoch 54: loss improved from 0.11664 to 0.11583, saving model to ./model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1158 - accuracy: 0.9525 - val_loss: 0.1146 - val_accuracy: 0.9581\n",
            "Epoch 55/100\n",
            "493/520 [===========================>..] - ETA: 0s - loss: 0.1204 - accuracy: 0.9500\n",
            "Epoch 55: loss did not improve from 0.11583\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1189 - accuracy: 0.9506 - val_loss: 0.1142 - val_accuracy: 0.9566\n",
            "Epoch 56/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9502\n",
            "Epoch 56: loss improved from 0.11583 to 0.11559, saving model to ./model_PID__56_loss_0.116_vloss_0.113_acc_0.951_vacc_0.955.hdf5\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1156 - accuracy: 0.9506 - val_loss: 0.1135 - val_accuracy: 0.9551\n",
            "Epoch 57/100\n",
            "509/520 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.9496\n",
            "Epoch 57: loss did not improve from 0.11559\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1313 - accuracy: 0.9506 - val_loss: 0.1149 - val_accuracy: 0.9551\n",
            "Epoch 58/100\n",
            "506/520 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9499\n",
            "Epoch 58: loss did not improve from 0.11559\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1181 - accuracy: 0.9499 - val_loss: 0.1138 - val_accuracy: 0.9551\n",
            "Epoch 59/100\n",
            "503/520 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9490\n",
            "Epoch 59: loss did not improve from 0.11559\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1206 - accuracy: 0.9506 - val_loss: 0.1140 - val_accuracy: 0.9551\n",
            "Epoch 60/100\n",
            "487/520 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9555\n",
            "Epoch 60: loss did not improve from 0.11559\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9538 - val_loss: 0.1159 - val_accuracy: 0.9551\n",
            "Epoch 61/100\n",
            "504/520 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9517\n",
            "Epoch 61: loss did not improve from 0.11559\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1185 - accuracy: 0.9519 - val_loss: 0.1150 - val_accuracy: 0.9551\n",
            "Epoch 62/100\n",
            "511/520 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9537\n",
            "Epoch 62: loss improved from 0.11559 to 0.11430, saving model to ./model_PID__62_loss_0.114_vloss_0.115_acc_0.954_vacc_0.957.hdf5\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1143 - accuracy: 0.9544 - val_loss: 0.1150 - val_accuracy: 0.9566\n",
            "Epoch 63/100\n",
            "487/520 [===========================>..] - ETA: 0s - loss: 0.1183 - accuracy: 0.9487\n",
            "Epoch 63: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1179 - accuracy: 0.9487 - val_loss: 0.1171 - val_accuracy: 0.9566\n",
            "Epoch 64/100\n",
            "510/520 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9536\n",
            "Epoch 64: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1156 - accuracy: 0.9538 - val_loss: 0.1183 - val_accuracy: 0.9566\n",
            "Epoch 65/100\n",
            "489/520 [===========================>..] - ETA: 0s - loss: 0.1222 - accuracy: 0.9489\n",
            "Epoch 65: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1181 - accuracy: 0.9506 - val_loss: 0.1186 - val_accuracy: 0.9551\n",
            "Epoch 66/100\n",
            "504/520 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9517\n",
            "Epoch 66: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1249 - accuracy: 0.9512 - val_loss: 0.1162 - val_accuracy: 0.9581\n",
            "Epoch 67/100\n",
            "500/520 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9473\n",
            "Epoch 67: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1193 - accuracy: 0.9487 - val_loss: 0.1165 - val_accuracy: 0.9566\n",
            "Epoch 68/100\n",
            "496/520 [===========================>..] - ETA: 0s - loss: 0.1232 - accuracy: 0.9516\n",
            "Epoch 68: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9525 - val_loss: 0.1178 - val_accuracy: 0.9566\n",
            "Epoch 69/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1256 - accuracy: 0.9522\n",
            "Epoch 69: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1278 - accuracy: 0.9499 - val_loss: 0.1174 - val_accuracy: 0.9566\n",
            "Epoch 70/100\n",
            "497/520 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9517\n",
            "Epoch 70: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1272 - accuracy: 0.9525 - val_loss: 0.1191 - val_accuracy: 0.9581\n",
            "Epoch 71/100\n",
            "519/520 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9486\n",
            "Epoch 71: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1207 - accuracy: 0.9487 - val_loss: 0.1198 - val_accuracy: 0.9566\n",
            "Epoch 72/100\n",
            "504/520 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9530\n",
            "Epoch 72: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1161 - accuracy: 0.9531 - val_loss: 0.1205 - val_accuracy: 0.9551\n",
            "Epoch 73/100\n",
            "507/520 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9513\n",
            "Epoch 73: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1310 - accuracy: 0.9519 - val_loss: 0.1196 - val_accuracy: 0.9566\n",
            "Epoch 74/100\n",
            "518/520 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9511\n",
            "Epoch 74: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1175 - accuracy: 0.9512 - val_loss: 0.1194 - val_accuracy: 0.9566\n",
            "Epoch 75/100\n",
            "483/520 [==========================>...] - ETA: 0s - loss: 0.1153 - accuracy: 0.9524\n",
            "Epoch 75: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1162 - accuracy: 0.9519 - val_loss: 0.1189 - val_accuracy: 0.9581\n",
            "Epoch 76/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1146 - accuracy: 0.9508\n",
            "Epoch 76: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1163 - accuracy: 0.9499 - val_loss: 0.1196 - val_accuracy: 0.9551\n",
            "Epoch 77/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1795 - accuracy: 0.9468\n",
            "Epoch 77: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1748 - accuracy: 0.9480 - val_loss: 0.1184 - val_accuracy: 0.9551\n",
            "Epoch 78/100\n",
            "519/520 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9525\n",
            "Epoch 78: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1269 - accuracy: 0.9525 - val_loss: 0.1176 - val_accuracy: 0.9581\n",
            "Epoch 79/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1351 - accuracy: 0.9481\n",
            "Epoch 79: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1326 - accuracy: 0.9487 - val_loss: 0.1172 - val_accuracy: 0.9566\n",
            "Epoch 80/100\n",
            "495/520 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.9508\n",
            "Epoch 80: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1366 - accuracy: 0.9506 - val_loss: 0.1183 - val_accuracy: 0.9566\n",
            "Epoch 81/100\n",
            "487/520 [===========================>..] - ETA: 0s - loss: 0.1138 - accuracy: 0.9528\n",
            "Epoch 81: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1191 - accuracy: 0.9499 - val_loss: 0.1176 - val_accuracy: 0.9566\n",
            "Epoch 82/100\n",
            "496/520 [===========================>..] - ETA: 0s - loss: 0.1238 - accuracy: 0.9516\n",
            "Epoch 82: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.9512 - val_loss: 0.1183 - val_accuracy: 0.9536\n",
            "Epoch 83/100\n",
            "518/520 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9492\n",
            "Epoch 83: loss did not improve from 0.11430\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1195 - accuracy: 0.9493 - val_loss: 0.1180 - val_accuracy: 0.9551\n",
            "Epoch 84/100\n",
            "506/520 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9526\n",
            "Epoch 84: loss improved from 0.11430 to 0.11364, saving model to ./model_PID__84_loss_0.114_vloss_0.119_acc_0.953_vacc_0.954.hdf5\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1136 - accuracy: 0.9525 - val_loss: 0.1185 - val_accuracy: 0.9536\n",
            "Epoch 85/100\n",
            "514/520 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9507\n",
            "Epoch 85: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1296 - accuracy: 0.9512 - val_loss: 0.1185 - val_accuracy: 0.9536\n",
            "Epoch 86/100\n",
            "508/520 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9534\n",
            "Epoch 86: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1306 - accuracy: 0.9512 - val_loss: 0.1195 - val_accuracy: 0.9536\n",
            "Epoch 87/100\n",
            "509/520 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.9489\n",
            "Epoch 87: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.9499 - val_loss: 0.1181 - val_accuracy: 0.9551\n",
            "Epoch 88/100\n",
            "514/520 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9494\n",
            "Epoch 88: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9499 - val_loss: 0.1182 - val_accuracy: 0.9536\n",
            "Epoch 89/100\n",
            "501/520 [===========================>..] - ETA: 0s - loss: 0.1179 - accuracy: 0.9534\n",
            "Epoch 89: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1167 - accuracy: 0.9538 - val_loss: 0.1176 - val_accuracy: 0.9551\n",
            "Epoch 90/100\n",
            "501/520 [===========================>..] - ETA: 0s - loss: 0.1176 - accuracy: 0.9508\n",
            "Epoch 90: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1162 - accuracy: 0.9512 - val_loss: 0.1193 - val_accuracy: 0.9536\n",
            "Epoch 91/100\n",
            "509/520 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9528\n",
            "Epoch 91: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1265 - accuracy: 0.9525 - val_loss: 0.1181 - val_accuracy: 0.9536\n",
            "Epoch 92/100\n",
            "485/520 [==========================>...] - ETA: 0s - loss: 0.1171 - accuracy: 0.9546\n",
            "Epoch 92: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1172 - accuracy: 0.9531 - val_loss: 0.1194 - val_accuracy: 0.9521\n",
            "Epoch 93/100\n",
            "488/520 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9529\n",
            "Epoch 93: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1178 - accuracy: 0.9512 - val_loss: 0.1186 - val_accuracy: 0.9536\n",
            "Epoch 94/100\n",
            "488/520 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9501\n",
            "Epoch 94: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1268 - accuracy: 0.9506 - val_loss: 0.1195 - val_accuracy: 0.9521\n",
            "Epoch 95/100\n",
            "507/520 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9540\n",
            "Epoch 95: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1141 - accuracy: 0.9551 - val_loss: 0.1200 - val_accuracy: 0.9521\n",
            "Epoch 96/100\n",
            "492/520 [===========================>..] - ETA: 0s - loss: 0.1293 - accuracy: 0.9519\n",
            "Epoch 96: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1313 - accuracy: 0.9499 - val_loss: 0.1188 - val_accuracy: 0.9521\n",
            "Epoch 97/100\n",
            "513/520 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9513\n",
            "Epoch 97: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1191 - accuracy: 0.9519 - val_loss: 0.1187 - val_accuracy: 0.9521\n",
            "Epoch 98/100\n",
            "520/520 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9474\n",
            "Epoch 98: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1466 - accuracy: 0.9474 - val_loss: 0.1190 - val_accuracy: 0.9536\n",
            "Epoch 99/100\n",
            "490/520 [===========================>..] - ETA: 0s - loss: 0.1234 - accuracy: 0.9524\n",
            "Epoch 99: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9538 - val_loss: 0.1202 - val_accuracy: 0.9521\n",
            "Epoch 100/100\n",
            "504/520 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9524\n",
            "Epoch 100: loss did not improve from 0.11364\n",
            "520/520 [==============================] - 1s 2ms/step - loss: 0.1204 - accuracy: 0.9525 - val_loss: 0.1185 - val_accuracy: 0.9521\n"
          ]
        }
      ],
      "source": [
        " history = model.fit(X_train, y_train, epochs=100, batch_size=3, validation_data=(X_test, y_test),\n",
        "                    \n",
        "                   verbose=1,callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgzklVywoNmk"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwcWQ94IpDFu",
        "outputId": "1c526885-5dee-4efc-87f7-066e639b3a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 995us/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bina_transformer=Binarizer(threshold=0.5)\n",
        "y_pred_transform=bina_transformer.fit_transform(y_pred)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_test,y_pred_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Z69kCq3T-pMo",
        "outputId": "31f0c737-2c9b-465c-c2b4-c62d8e38cb90"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEGCAYAAADxD4m3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaKElEQVR4nO3debRU5Znv8e+PwyCjyCAiYhDFAbUlNuKU2KhxzICm00bT3TFqX5K0xhjN7ZiY1aY19vUmxiEdtRuHiJ2o0VavOARUokuNJgoqRkAiIhEQRQaRmTM894+9jxRwTp2qQ9Wpqn1+n7X2ovZbe+/3OYfFwzvtvRURmJllUZdKB2BmVi5OcGaWWU5wZpZZTnBmlllOcGaWWV0rHUCuvgO6xuBhPSodhhVh+ev++6olG1nH5tikHbnGScf2jhUrGws6duZrm6ZFxMk7Ut+OqKoEN3hYD378wIGVDsOK8Mv9PlHpEKwIf4zpO3yNFSsbeXHangUdWzf0zUE7XOEOqKoEZ2bVL4AmmiodRkGc4MysKEFQH4V1USvNCc7MiuYWnJllUhA01sgtnk5wZla0JpzgzCyDAmh0gjOzrHILzswyKYB6j8GZWRYF4S6qmWVUQGNt5DcnODMrTnInQ21wgjOzIolGduh+/Q7jBGdmRUkmGZzgzCyDknVwTnBmllFNbsGZWRa5BWdmmRWIxhp524ETnJkVzV1UM8ukQGyOukqHURAnODMrSrLQ111UM8soTzKYWSZFiMZwC87MMqqpRlpwtZGGzaxqJJMMXQva8pG0k6QXJc2SNFvSv6Xle0n6o6T5kn4jqXta3iPdn59+P6KtWJ3gzKwozZMMhWxt2AQcFxGHAGOAkyUdAfxf4LqI2AdYBZyXHn8esCotvy49Li8nODMrWmOooC2fSKxNd7ulWwDHAf+Tlk8GTks/T0j3Sb8/XlLeSpzgzKwozXcyFLIBgyTNyNkm5l5LUp2kV4FlwBPAW8CHEdGQHrIYGJZ+HgYsAki/Xw0MzBerJxnMrGhNhc+iLo+Isa19GRGNwBhJ/YEHgf1LEN7HnODMrCjJzfal7fxFxIeSngKOBPpL6pq20vYAlqSHLQGGA4sldQV2Blbku667qGZWlEDUR11BWz6SBqctNyT1BE4A5gJPAV9KDzsbeCj9PCXdJ/3+dxH5X+/lFpyZFSWCUi30HQpMllRH0ti6NyIekTQHuEfSj4FXgNvS428D/lvSfGAlcGZbFTjBmVmRVJKFvhHxGvDJFsoXAONaKN8I/F0xdTjBmVlRgpK14MrOCc7MiuYHXppZJgXyAy/NLJuS1wbWRuqojSjNrIr4xc9mllFBUXcyVJQTnJkVzS04M8ukCLkFZ2bZlEwy+K1aZpZJfieDmWVUMsngMTgzyyjfyWBmmeQ7Gcws0/xmezPLpAiob3KCM7MMSrqoTnBmllG+k6GTWLu0jmf/ZRAbVnRBgn3PWMuBZ69h5RvdeP7ygdSvF32HNXDMNcvp3id4a0pvXr+t38fnr5zXjS88uJSBB9RX8KewZr37NfKdaxYxYv+NRMC1Fw9n7szelQ6rqniZSErSycANQB1wa0RcXc76KqFLHRx26SoGHbiZ+rViyt8OZdjRG/n9ZQM57Hur2G3cJv78P715/dZ+HHrRavb+wjr2/sI6IEluvzt/sJNbFfnmFUuY8XRffjxxBF27NdGjZ953mnRStdNFLVuU6YskbgROAUYDZ0kaXa76KqXXro0MOnAzAN36BDuPrGfd+3WsXtiNIYdtAmD3ozey8PFe25379qO92euz6zs0Xmtdr76NHHzEOqbeNQCAhvourPuoNm5J6mhN6XsZ2toqrZxpeBwwPyIWRMRm4B5gQhnrq7g1i+tYObc7gw/ZRP9Rm3lnek8AFk7txbql2zeW336sFyM/u66jw7RW7LbnZlavqOOS6xZx4+PzuOiaRfTo2VjpsKpOMotaV9BWaeVMcMOARTn7i9OyrUiaKGmGpBlrVjaUMZzyql8nnrpwMON+sJLufYJPXbWCN+7qy5Qv7kb9ui7Udd+6q/PBrO7U9Qx22dfd02pRVxfsc/AGHrlzIOefuB8b13fhyxcsq3RYVad5oW8hW6VVfJIhIiYBkwBGHty7Jgc8murhdxcOZuTn1zHixA0A9N+7gZNuT/5xrH67K4uf7rnVOQse7e3WW5VZvrQbHyztxrxXkkmF5x7ZmTOc4FpUDd3PQpSzBbcEGJ6zv0dalikR8NxlA+k/sp6DzlnzcfmGFcmvNppg1s07s9+ZW76LJlj4216M9PhbVVn1QTeWv9udPfbeCMCYT6/lnTd3qnBU1ad5FnVHW3CShkt6StIcSbMlfTst/5GkJZJeTbdTc875vqT5kuZJOqmtWMvZgnsJGCVpL5LEdibwlTLWVxHLZvbgrYf6sMu+m3lowlAADr14FR8t7MYbd/UF4BMnrGfU325prb33Ug96D22k7/Da7ZJn1Y0/HMb3fvEOXbsF773TnZ99Z3jbJ3VCJZpFbQAuiYiXJfUFZkp6Iv3uuoi4JvfgdJLyTOBAYHfgSUn7RkSrA6VlS3AR0SDpAmAayTKR2yNidrnqq5QhYzdxzry/bP/F32zkwLPXbF8ODD18E5+7970yR2btsWB2T751yr6VDqOqRYiGEiS4iFgKLE0/r5E0lxbG6XNMAO6JiE3A25Lmk0xmvtDaCWVdzBIRj0XEvhGxd0RcVc66zKzjFNFFHdQ8iZhuE1u6nqQRwCeBP6ZFF0h6TdLtknZJywqauMxV8UkGM6stRd7JsDwixuY7QFIf4H7gooj4SNLNwJVpVVcCPwPObU+sTnBmVrRSLQGR1I0kuf06Ih4AiIj3c76/BXgk3S164rI27rcws6pRqnVwkgTcBsyNiGtzyofmHHY68Hr6eQpwpqQe6eTlKODFfHW4BWdmRSvROrijgX8E/iTp1bTsByS3dY4h6aIuBL4OEBGzJd0LzCGZgT0/3wwqOMGZWZEioKEED7yMiOegxUz5WJ5zrgIKnrB0gjOzolXDbViFcIIzs6L4pTNmlmnhBGdmWVUrN9s7wZlZUSI8BmdmmSUa/dpAM8sqj8GZWSb5rVpmll2RjMPVAic4MyuaZ1HNLJPCkwxmlmXuoppZZnkW1cwyKcIJzswyzMtEzCyzPAZnZpkUiCbPoppZVtVIA84JzsyK5EkGM8u0GmnCOcGZWdFqvgUn6T/Ik6cj4sKyRGRmVS2ApqYaT3DAjA6LwsxqRwC13oKLiMm5+5J6RcT68odkZtWuFOvgJA0H7gSGkKTNSRFxg6QBwG+AESQvfj4jIlZJEnADcCqwHvhaRLycr442F7NIOlLSHOCNdP8QSTe1+6cys9oXBW75NQCXRMRo4AjgfEmjgUuB6RExCpie7gOcAoxKt4nAzW1VUMhqveuBk4AVABExCzimgPPMLJNERGFbPhGxtLkFFhFrgLnAMGAC0NyDnAycln6eANwZiT8A/SUNzVdHQcuRI2LRNkWNhZxnZhlVmhbcxySNAD4J/BEYEhFL06/eI+nCQpL8cnPR4rSsVYUsE1kk6SggJHUDvk2Sac2sMwqIwmdRB0nKnbCcFBGTcg+Q1Ae4H7goIj5KhtrSqiJCUrtH/ApJcN8gGdgbBrwLTAPOb2+FZpYFBSe45RExttWrJI2m+4FfR8QDafH7koZGxNK0C7osLV8CDM85fY+0rFVtdlEjYnlE/H1EDImIwRHxDxGxoq3zzCzDStBFTWdFbwPmRsS1OV9NAc5OP58NPJRT/lUljgBW53RlW1TILOpISQ9L+kDSMkkPSRrZ1nlmlmGlGYM7GvhH4DhJr6bbqcDVwAmS3gQ+k+4DPAYsAOYDtwD/3FYFhXRR7wJuBE5P988E7gYOL+BcM8uaEi30jYjnaL2ve3wLxwdFDo8VMovaKyL+OyIa0u1XwE7FVGJm2RJR2FZp+e5FHZB+/K2kS4F7SHL3l0maimbWWWXgXtSZJAmt+Sf5es53AXy/XEGZWXVr/8KNjpXvXtS9OjIQM6sRRS7iraSCngcn6SBgNDljbxFxZ7mCMrNqptp/mkgzSZcD40kS3GMkN7w+R/IUADPrjGqkBVfILOqXSKZs34uIc4BDgJ3LGpWZVbemArcKK6SLuiEimiQ1SOpHctvE8LZOMrOMysIDL3PMkNSfZOXwTGAt8EJZozKzqlbzs6jNIqL5doj/lDQV6BcRr5U3LDOrarWe4CQdmu+7th4VbGZWaflacD/L810Ax5U4FlbM7cWdR4wp9WWtjKa9+1SlQ7AijDupNK9VqfkuakQc25GBmFmNCDJxq5aZWctqvQVnZtaamu+impm1qkYSXCFP9JWkf5D0r+n+npLGlT80M6taJX6rVrkUcqvWTcCRwFnp/hqSJ/yaWSekKHyrtEK6qIdHxKGSXgGIiFWSupc5LjOrZhmaRa2XVEfa4JQ0mKq4jdbMKqUaWmeFKKSL+nPgQWBXSVeRPCrp38salZlVtxoZgyvkXtRfS5pJ8sgkAadFhN9sb9ZZVcn4WiEKeeDlnsB64OHcsoh4p5yBmVkVy0qCAx5ly8tndgL2AuYBB5YxLjOrYqqRUfg2x+Ai4uCI+Kv0z1HAOPw8ODMrAUm3S1om6fWcsh9JWrLN2+6bv/u+pPmS5kk6qa3rFzLJsJX0MUl+q71ZZ1a6SYY7gJNbKL8uIsak22MAkkYDZ5L0Hk8GbkpXeLSqkDG4i3N2uwCHAu8WFLqZZU8JJxki4hlJIwo8fAJwT0RsAt6WNJ82epSFtOD65mw9SMbkJhQYkJllUeEtuEGSZuRsEwus4QJJr6Vd2F3SsmHAopxjFqdlrcrbgkubf30j4rsFBmVmnUHhLbjlETG2yKvfDFyZ1nIlycN3zy3yGkD+R5Z3jYgGSUe358Jmlk2ivLOoEfH+x3VJtwCPpLtL2PqNfnukZa3K14J7kWS87VVJU4D7gHU5QTxQXNhmlgllXugraWhELE13TweaZ1inAHdJuhbYHRhFkqdaVcg6uJ2AFSTvYGheDxeAE5xZZ1WiBCfpbmA8yVjdYuByYLykMWktC4GvA0TEbEn3AnOABuD8iGjMd/18CW7XdAb1dbYktmY1so7ZzMqidLOoZ7VQfFue468Crir0+vkSXB3Qh60T28f1FFqBmWVPFu5FXRoRV3RYJGZWOzKQ4GrjiXZm1rGidu5FzZfgju+wKMysttR6Cy4iVnZkIGZWO7IwBmdm1jInODPLpCp5HHkhnODMrCjCXVQzyzAnODPLLic4M8ssJzgzy6QsvTbQzGw7TnBmllVZuFXLzKxF7qKaWTZ5oa+ZZZoTnJllke9kMLNMU1NtZDgnODMrjsfgzCzL3EU1s+xygjOzrKqVFlyXSgdgZjUoCtzaIOl2ScskvZ5TNkDSE5LeTP/cJS2XpJ9Lmi/pNUmHtnV9JzgzK076Vq1CtgLcAZy8TdmlwPSIGAVMT/cBTgFGpdtE4Oa2Lu4EZ2ZFaV4HV8jWloh4Btj2BVcTgMnp58nAaTnld0biD0B/SUPzXd9jcGZWvCh4EG6QpBk5+5MiYlIb5wyJiKXp5/eAIennYcCinOMWp2VLaYUTnJkVrYhJhuURMba99URESO2f0nCCK6FBu23kkv/zBrsM3EwETL1vdx761R6ce8lbHD5+OQ31XVi6qCfX/XA/1q3pVulwO63NG8UlX9yH+s1daGyAT392NV/93+99/P1NPxzGtHsG8ND8PwHwyJ0DefiOQXTpAj17N/Ltny7iE/tuqlT4lVf+hb7vSxoaEUvTLuiytHwJMDznuD3SslaVLcFJuh34HLAsIg4qVz3VpLFB3PqTvXlrbl969mrg5/fN5OUXduGVF3bhjuv3oqmxC+dc/BZn/K93+OW1e1c63E6rW4/gJ/e9Rc/eTTTUw8WnjeKw4z7igL9ez59n9WTt6rqtjj/29FV87qsrAHhhWj/+60fD+Pe7FlQi9KpR5ufBTQHOBq5O/3wop/wCSfcAhwOrc7qyLSrnJMMdbD87kmmrlvfgrbl9AdiwvivvLOjFoF038crzA2hqTH7Vb8zqx6Ahnfh//yogQc/eyb/QhnrRWC8kaGyEW67cnfN++O5Wx/fuu+Vf88b1XZA6NNyqVKpZVEl3Ay8A+0laLOk8ksR2gqQ3gc+k+wCPAQuA+cAtwD+3df2yteAi4hlJI8p1/Wq36+4b2PuAtbzxWr+tyk/84ns889vBFYrKmjU2wgUn7ce7C7vz+a8tZ/9D1/PgrYM48sSPGDikYbvjp/xyEA9MGkz9ZvGT++ZXIOIqEhQzyZD/UhFntfLV8S0cG8D5xVy/4stEJE2UNEPSjM2xsdLhlMROvRq47PrZTLp6Hzas2/J/yJcn/oXGBvHUI0PynG0doa4Obn5yHr+eOYd5r/biT3/ozbMP92fCuR+0ePwXzlnOHS/M5bzL3uWuG3br4GirT6mWiZRbxRNcREyKiLERMba7dqp0ODusrmsTl10/m6cfHcLzT25pqX3mtKWM+5sV/PR7B5CsJLJq0GfnRg45ai2zft+Hdxf24JyjRvPVcaPZtKELXzvqgO2OH3/ahzw/decKRFplSnQnQ7l5FrWkgouumMeiBb14cPKWyZ6//tQKvnTuIv7l7DFs2liX53zrCB+uqKNr1yS5bdogXn6mL2ecv4x7Zs3++JgJ+xzMHc/PBWDJgu4MG7kZgBef7MewvTr3GKofeNlJjT50NcdPeJ+35/XmP+5/CYDJ14/kGz94k27dgqtunQXAvFn9+MUV+1Uy1E5t5fvduObbe9LUJJqa4JjPf8gRJ3zU6vFTfjmYl5/tkyTF/g1894Z3OjDaKhThB16msyPjSVYyLwYuj4jbylVfNZjzcn9OPXD8duX/dMrAjg/GWjVy9EZueuLPeY9pXgMH8M0r8y616pxqI7+VdRa1tdkRM6tx7qKaWTYF0Nm7qGaWYbWR35zgzKx47qKaWWZ1+llUM8uoKlnEWwgnODMrSrLQtzYynBOcmRWvvI9LKhknODMrmltwZpZNHoMzs+zyvahmlmXuoppZJkXZ38lQMk5wZlY8t+DMLLNqI785wZlZ8dRUG31UJzgzK07ghb5mlk0ivNDXzDKsRAlO0kJgDdAINETEWEkDgN8AI4CFwBkRsao916/4awPNrAZFFLYV5tiIGBMRY9P9S4HpETEKmJ7ut4sTnJkVp3kMrpCtfSYAk9PPk4HT2nshJzgzK5qamgraSN6qNyNnm7jNpQJ4XNLMnO+GRMTS9PN7wJD2xukxODMrUlHdz+U5Xc+WfCoilkjaFXhC0htb1RQRUvsfkO4WnJkVJyjZGFxELEn/XAY8CIwD3pc0FCD9c1l7Q3WCM7PilWAMTlJvSX2bPwMnAq8DU4Cz08POBh5qb5juoppZ0Uq0Dm4I8KAkSHLRXRExVdJLwL2SzgP+ApzR3gqc4MyseCVIcBGxADikhfIVwPE7XAFOcGZWrAhorI17tZzgzKx4vlXLzDLLCc7MMikAv5PBzLIpIDwGZ2ZZFHiSwcwyzGNwZpZZTnBmlk1F3WxfUU5wZlacAPzSGTPLLLfgzCybfKuWmWVVQHgdnJlllu9kMLPM8hicmWVShGdRzSzD3IIzs2wKorGx0kEUxAnOzIrjxyWZWaZ5mYiZZVEA4RacmWVS+IGXZpZhtTLJoKii6V5JH5C86DVrBgHLKx2EFSWrf2efiIjBO3IBSVNJfj+FWB4RJ+9IfTuiqhJcVkmaERFjKx2HFc5/Z9nQpdIBmJmVixOcmWWWE1zHmFTpAKxo/jvLAI/BmVlmuQVnZpnlBGdmmeUEV0aSTpY0T9J8SZdWOh5rm6TbJS2T9HqlY7Ed5wRXJpLqgBuBU4DRwFmSRlc2KivAHUDFFqZaaTnBlc84YH5ELIiIzcA9wIQKx2RtiIhngJWVjsNKwwmufIYBi3L2F6dlZtZBnODMLLOc4MpnCTA8Z3+PtMzMOogTXPm8BIyStJek7sCZwJQKx2TWqTjBlUlENAAXANOAucC9ETG7slFZWyTdDbwA7CdpsaTzKh2TtZ9v1TKzzHILzswyywnOzDLLCc7MMssJzswyywnOzDLLCa6GSGqU9Kqk1yXdJ6nXDlzrDklfSj/fmu9BAJLGSzqqHXUslLTd25daK9/mmLVF1vUjSd8tNkbLNie42rIhIsZExEHAZuAbuV9Katd7biPinyJiTp5DxgNFJzizSnOCq13PAvukratnJU0B5kiqk/RTSS9Jek3S1wGU+EX6fLongV2bLyTpaUlj088nS3pZ0ixJ0yWNIEmk30lbj5+WNFjS/WkdL0k6Oj13oKTHJc2WdCugtn4ISf9P0sz0nInbfHddWj5d0uC0bG9JU9NznpW0fyl+mZZNfrN9DUpbaqcAU9OiQ4GDIuLtNEmsjojDJPUAfi/pceCTwH4kz6YbAswBbt/muoOBW4Bj0msNiIiVkv4TWBsR16TH3QVcFxHPSdqT5G6NA4DLgeci4gpJnwUKuQvg3LSOnsBLku6PiBVAb2BGRHxH0r+m176A5GUw34iINyUdDtwEHNeOX6N1Ak5wtaWnpFfTz88Ct5F0HV+MiLfT8hOBv2oeXwN2BkYBxwB3R0Qj8K6k37Vw/SOAZ5qvFRGtPRftM8Bo6eMGWj9JfdI6vpie+6ikVQX8TBdKOj39PDyNdQXQBPwmLf8V8EBax1HAfTl19yigDuuknOBqy4aIGJNbkP5DX5dbBHwrIqZtc9ypJYyjC3BERGxsIZaCSRpPkiyPjIj1kp4Gdmrl8Ejr/XDb34FZazwGlz3TgG9K6gYgaV9JvYFngC+nY3RDgWNbOPcPwDGS9krPHZCWrwH65hz3OPCt5h1JzQnnGeAradkpwC5txLozsCpNbvuTtCCbdQGaW6FfIen6fgS8Lenv0jok6ZA26rBOzAkue24lGV97OX1xyn+RtNQfBN5Mv7uT5IkZW4mID4CJJN3BWWzpIj4MnN48yQBcCIxNJzHmsGU2999IEuRskq7qO23EOhXoKmkucDVJgm22DhiX/gzHAVek5X8PnJfGNxs/Bt7y8NNEzCyz3IIzs8xygjOzzHKCM7PMcoIzs8xygjOzzHKCM7PMcoIzs8z6/+0GHMPAsOrfAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nZ0rmkNsBGnl",
        "outputId": "4e0e585b-dd51-4136-8961-754f68ae28c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fpr, tpr, _ = roc_curve( y_pred_transform,y_test,pos_label=1)\n",
        "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "auc = roc_auc_score(y_test, y_pred_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdmx5qIp9qsu",
        "outputId": "3b6dfef2-251a-402d-99a1-fa11cc4f7b06"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMRd5eGU9ASA",
        "outputId": "956a3c8e-8fe7-4f51-b107-a890bbe99fd2"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9599620235996202"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzlQR00e8Miq",
        "outputId": "120b209a-376a-47ed-84e8-b4044d02978a"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.06896552, 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(fpr,tpr)\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZRlw4piW7_uW",
        "outputId": "9878ddd7-3ee1-443c-896a-b073b9c0ebee"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df=pd.DataFrame({\"epoch\":history.epoch, \"loss\":history.history[\"loss\"],\"val_loss\":history.history[\"val_loss\"]})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "4ENvDCA-0U1g",
        "outputId": "20bd44c6-dc81-4bc9-c4fe-dc7050421a13"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Val_Loss\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"ec849a01-2b67-4718-b04e-cef7297b3277\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ec849a01-2b67-4718-b04e-cef7297b3277\")) {                    Plotly.newPlot(                        \"ec849a01-2b67-4718-b04e-cef7297b3277\",                        [{\"line\":{\"color\":\"rgba(193,99,99,0.8)\"},\"name\":\"Val_Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[0.27011582255363464,0.26504355669021606,0.27861326932907104,0.3256341516971588,0.2518749535083771,0.23726177215576172,0.2693060338497162,0.27373799681663513,0.2819504737854004,0.2960783839225769,0.30028754472732544,0.29665347933769226,0.21086841821670532,0.21068625152111053,0.22288021445274353,0.22752460837364197,0.2718503773212433,0.2735441029071808,0.2650343179702759,0.254242479801178],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"rgba(35,128,132,0.8)\"},\"name\":\"Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[0.1116427332162857,0.13237470388412476,0.113796167075634,0.11896958202123642,0.11896812915802002,0.12250975519418716,0.11482390761375427,0.13023237884044647,0.1121036559343338,0.12631264328956604,0.13757596909999847,0.10984506458044052,0.1389913260936737,0.13128413259983063,0.12573237717151642,0.12021035701036453,0.11861036717891693,0.11624758690595627,0.11918379366397858,0.11984959989786148],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.94]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"<b>Loss Val_Loss</b>\"}},\"yaxis2\":{\"anchor\":\"x\",\"overlaying\":\"y\",\"side\":\"right\"},\"title\":{\"text\":\"Loss Val_Loss\"},\"autosize\":false,\"width\":1200,\"height\":600,\"paper_bgcolor\":\"rgb(200,200,200)\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ec849a01-2b67-4718-b04e-cef7297b3277');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3ChPtEr57bQ41bVTQUMTl",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}