{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/model_3D/pid_NN_3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tNYnFR-6Xh",
        "outputId": "5b89c4db-38e8-4cff-e362-e71f3f6feb19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=103b35a6f820318815dc30a212c9d25b0f15215738a695acb833e103d3845494\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "c206709a-66d0-4c58-a797-66d1db35040a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint3\"\n",
        "data_source=\"SINT3_415/3D_transposed\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_3D_part\"\n",
        "fname_bad = \"415_SC_3D_part\"\n",
        "fname_good_ext=[\"1.csv\",\"3.csv\",\"5.csv\",\"7.csv\"]\n",
        "fname_bad_ext=[\"2.csv\",\"4.csv\",\"6.csv\",\"8.csv\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good \n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "ca5ed1ef-a2f3-4c56-eb04-97c9eba3c1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/SINT3_415/3D_transposed/415_SC_3D_part\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/SINT3_415/3D_transposed/415_SC_3D_part\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols=[str(i) for i in range(60)]"
      ],
      "metadata": {
        "id": "SpsF7dVwl7bH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "vUcMjZAGKvtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5333c159-33df-44c9-97f6-7f8795eff0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 60 columns]\n"
          ]
        }
      ],
      "source": [
        "df_good=pd.DataFrame(columns=cols)\n",
        "df_bad=pd.DataFrame(columns=cols)\n",
        "print(df_good.head())\n",
        "for i,goods in enumerate(fname_good_ext):\n",
        "    df_good_tmp = pd.read_csv(path_good+goods,usecols=None )\n",
        "    df_good_tmp.columns=cols\n",
        "    df_good=pd.concat([df_good,df_good_tmp],axis=0,)\n",
        "    df_bad_tmp = pd.read_csv(path_bad+fname_bad_ext[i],usecols=None)\n",
        "    df_bad_tmp.columns=cols\n",
        "    df_bad=pd.concat([df_bad,df_bad_tmp],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYuDXKraLOt4",
        "outputId": "c66c69e2-9d50-4472-fb1e-bc915d1f5633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(df_good.isnull().values.any())\n",
        "print(df_bad.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "e762f234-3514-41ff-e464-b2622c2261d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0     1     2     3     4     5     6     7     8     9  ...  \\\n",
              "569  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "570  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "571  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "572  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "573  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  45.0  ...   \n",
              "\n",
              "           50        51        52        53        54        55        56  \\\n",
              "569  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "570  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "571  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "572  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "573  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297  0.216297   \n",
              "\n",
              "           57        58        59  \n",
              "569  0.216297  0.216297  0.216297  \n",
              "570  0.216297  0.216297  0.216297  \n",
              "571  0.216297  0.216297  0.216297  \n",
              "572  0.216297  0.216297  0.163654  \n",
              "573  0.216297  0.163654  0.216297  \n",
              "\n",
              "[5 rows x 60 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ecbbaf9-ab20-46a6-9490-f35a6cbbd0f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.163654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.216297</td>\n",
              "      <td>0.163654</td>\n",
              "      <td>0.216297</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 60 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ecbbaf9-ab20-46a6-9490-f35a6cbbd0f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ecbbaf9-ab20-46a6-9490-f35a6cbbd0f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ecbbaf9-ab20-46a6-9490-f35a6cbbd0f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "df_good.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "df_ = df_good\n",
        "\n",
        "# You must normalize the data before applying the fit method\n",
        "df_good_normalized=(df_ - df_.mean()) / df_.std()\n",
        "\n",
        "# Normalize bad data with the good data parameters\n",
        "df_bad_normalized=(df_bad - df_.mean()) / df_.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": [
        "df_good_normalized[\"state\"]=0\n",
        "df_bad_normalized[\"state\"]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "3W5mi70VM6hL"
      },
      "outputs": [],
      "source": [
        "df_good_normalized=df_good_normalized.reindex()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_good_normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Vg9QgProrxU_",
        "outputId": "65ebdbf8-1472-480b-eb23-53bb2a34f110"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "1   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "2   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "3   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "4   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "569 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "570 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "571 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "572 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "573 -0.691638 -0.692917 -0.694198 -0.695481 -0.696765 -0.698051 -0.699339   \n",
              "\n",
              "            7         8         9  ...        51        52        53  \\\n",
              "0   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "1   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "2   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "3   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "4   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "569 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "570 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "571 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "572 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "573 -0.700628 -0.701918 -0.703211  ... -0.025480 -0.025480 -0.025480   \n",
              "\n",
              "           54        55        56        57        58        59  state  \n",
              "0   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "1   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "2   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "3   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "4   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "..        ...       ...       ...       ...       ...       ...    ...  \n",
              "569 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.025578      0  \n",
              "570 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.025578      0  \n",
              "571 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.025578      0  \n",
              "572 -0.025480 -0.025480 -0.025479 -0.025479 -0.025473 -0.041168      0  \n",
              "573 -0.025480 -0.025480 -0.025479 -0.025479 -0.041063 -0.025578      0  \n",
              "\n",
              "[2646 rows x 61 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bdf3eba6-8a93-43ed-8b14-c79f3204e73f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>-0.041168</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>-0.691638</td>\n",
              "      <td>-0.692917</td>\n",
              "      <td>-0.694198</td>\n",
              "      <td>-0.695481</td>\n",
              "      <td>-0.696765</td>\n",
              "      <td>-0.698051</td>\n",
              "      <td>-0.699339</td>\n",
              "      <td>-0.700628</td>\n",
              "      <td>-0.701918</td>\n",
              "      <td>-0.703211</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025480</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.025479</td>\n",
              "      <td>-0.041063</td>\n",
              "      <td>-0.025578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2646 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bdf3eba6-8a93-43ed-8b14-c79f3204e73f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bdf3eba6-8a93-43ed-8b14-c79f3204e73f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bdf3eba6-8a93-43ed-8b14-c79f3204e73f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_normalized=pd.concat([df_good_normalized,df_bad_normalized],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "f753461c-7cd3-4b81-be52-bf6108aed156"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "1   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "2   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "3   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "4   -1.453455 -1.454954 -1.456455 -1.457960 -1.459467 -1.460977 -1.462489   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "515 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "516 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "517 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "518 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "519 -0.310730 -0.311899 -0.313070 -0.314242 -0.315414 -0.316588 -0.317763   \n",
              "\n",
              "            7         8         9  ...        51        52        53  \\\n",
              "0   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "1   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "2   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "3   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "4   -1.464005 -1.465523 -1.467043  ... -0.024124 -0.024124 -0.024124   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "515 -0.318939 -0.320116 -0.321295  ... -0.370495  2.344881  3.525656   \n",
              "516 -0.318939 -0.320116 -0.321295  ...  2.344881  3.525656  1.690100   \n",
              "517 -0.318939 -0.320116 -0.321295  ...  3.525656  1.690100 -2.078618   \n",
              "518 -0.318939 -0.320116 -0.321295  ...  1.690100 -2.078619 -2.669684   \n",
              "519 -0.318939 -0.320116 -0.321295  ... -2.078619 -2.669684  0.108052   \n",
              "\n",
              "           54        55        56        57        58        59  state  \n",
              "0   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "1   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "2   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "3   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "4   -0.024124 -0.024124 -0.024124 -0.024124 -0.024118 -0.024223      0  \n",
              "..        ...       ...       ...       ...       ...       ...    ...  \n",
              "515  1.690100 -2.078618 -2.669683  0.108053  2.199832  2.951400      1  \n",
              "516 -2.078618 -2.669683  0.108053  2.199826  2.951541  3.221172      1  \n",
              "517 -2.669683  0.108053  2.199826  2.951536  3.221317  1.193817      1  \n",
              "518  0.108052  2.199826  2.951536  3.221312  1.193936 -2.382354      1  \n",
              "519  2.199826  2.951536  3.221312  1.193931 -2.382277  0.507865      1  \n",
              "\n",
              "[5658 rows x 61 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24cafcbb-5452-4df0-9bc1-555ce2d2740e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.453455</td>\n",
              "      <td>-1.454954</td>\n",
              "      <td>-1.456455</td>\n",
              "      <td>-1.457960</td>\n",
              "      <td>-1.459467</td>\n",
              "      <td>-1.460977</td>\n",
              "      <td>-1.462489</td>\n",
              "      <td>-1.464005</td>\n",
              "      <td>-1.465523</td>\n",
              "      <td>-1.467043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024124</td>\n",
              "      <td>-0.024118</td>\n",
              "      <td>-0.024223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.370495</td>\n",
              "      <td>2.344881</td>\n",
              "      <td>3.525656</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078618</td>\n",
              "      <td>-2.669683</td>\n",
              "      <td>0.108053</td>\n",
              "      <td>2.199832</td>\n",
              "      <td>2.951400</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>2.344881</td>\n",
              "      <td>3.525656</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078618</td>\n",
              "      <td>-2.669683</td>\n",
              "      <td>0.108053</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951541</td>\n",
              "      <td>3.221172</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>3.525656</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078618</td>\n",
              "      <td>-2.669683</td>\n",
              "      <td>0.108053</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951536</td>\n",
              "      <td>3.221317</td>\n",
              "      <td>1.193817</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>1.690100</td>\n",
              "      <td>-2.078619</td>\n",
              "      <td>-2.669684</td>\n",
              "      <td>0.108052</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951536</td>\n",
              "      <td>3.221312</td>\n",
              "      <td>1.193936</td>\n",
              "      <td>-2.382354</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>-0.310730</td>\n",
              "      <td>-0.311899</td>\n",
              "      <td>-0.313070</td>\n",
              "      <td>-0.314242</td>\n",
              "      <td>-0.315414</td>\n",
              "      <td>-0.316588</td>\n",
              "      <td>-0.317763</td>\n",
              "      <td>-0.318939</td>\n",
              "      <td>-0.320116</td>\n",
              "      <td>-0.321295</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.078619</td>\n",
              "      <td>-2.669684</td>\n",
              "      <td>0.108052</td>\n",
              "      <td>2.199826</td>\n",
              "      <td>2.951536</td>\n",
              "      <td>3.221312</td>\n",
              "      <td>1.193931</td>\n",
              "      <td>-2.382277</td>\n",
              "      <td>0.507865</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5658 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24cafcbb-5452-4df0-9bc1-555ce2d2740e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24cafcbb-5452-4df0-9bc1-555ce2d2740e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24cafcbb-5452-4df0-9bc1-555ce2d2740e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "df_all_normalized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "nVvhP84S_F1y"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_N1_=700\n",
        "_N2_=120\n",
        "_lr_=0.001\n",
        "_batch_size_=3\n",
        "_drop1_=0.1\n",
        "_drop2_=0.1\n",
        "_epochs_=5500\n"
      ],
      "metadata": {
        "id": "XC5_bGE0iyi4"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"lr\": _lr_, \"batch_size\": _batch_size_,\"architecture\": \"NN\", \n",
        "          \"depth\": 2,\n",
        "          \"layer1\":_N1_,  \"layer2\":_N2_, \n",
        "          \"drop1\":_drop1_,\"drop2\":_drop2_,\n",
        "          \"epochs\":_epochs_\n",
        "          \n",
        "          \n",
        "          }\n",
        "\n",
        "wandb.init(project=\"pid_3d\", entity=\"sipoczlaszlo\",config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "nOtKllcviuoj",
        "outputId": "3c396aab-e165-412f-fa80-1aa99b6fcaa3"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:kljviwhh) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇█▅▆▇▇▇▇▇▇▇██████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▁▂▂▂▂▂▃▁▁</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂█████▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>epoch/loss</td><td>█▆▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/lr</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂█████▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>epoch/val_accuracy</td><td>▁▂▂▃▄▅▆▆▆▆▆▇▇▇▇▇▇▄▇▆▇▇▇█▇▇▇█████████████</td></tr><tr><td>epoch/val_loss</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.99512</td></tr><tr><td>epoch/epoch</td><td>310</td></tr><tr><td>epoch/learning_rate</td><td>0.005</td></tr><tr><td>epoch/loss</td><td>0.01645</td></tr><tr><td>epoch/lr</td><td>0.005</td></tr><tr><td>epoch/val_accuracy</td><td>0.97182</td></tr><tr><td>epoch/val_loss</td><td>1.44293</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wise-wind-2</strong> at: <a href='https://wandb.ai/sipoczlaszlo/pid_3d/runs/kljviwhh' target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_3d/runs/kljviwhh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230316_150143-kljviwhh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:kljviwhh). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230316_154014-rw09nkvp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sipoczlaszlo/pid_3d/runs/rw09nkvp' target=\"_blank\">rose-firefly-3</a></strong> to <a href='https://wandb.ai/sipoczlaszlo/pid_3d' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sipoczlaszlo/pid_3d' target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_3d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sipoczlaszlo/pid_3d/runs/rw09nkvp' target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_3d/runs/rw09nkvp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sipoczlaszlo/pid_3d/runs/rw09nkvp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f6d926ba190>"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=60\n",
        "\n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "l1_out=Dense(_N1_,activation=\"swish\",kernel_initializer='glorot_uniform',)(input1) # kernel_initializer='lecun_normal'\n",
        "l2_out=Dropout(_drop1_)(l1_out)\n",
        "\n",
        "\n",
        "l3_out=Dense(_N2_,activation=\"swish\",kernel_initializer='glorot_uniform',)(l2_out) #kernel_initializer='lecun_normal',\n",
        "l4_out=Dropout(_drop2_)(l3_out)\n",
        "\n",
        "pred=Dense(1, activation=\"sigmoid\",)(l4_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=_lr_,) #\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# 35 5 1 relu relu sigmoid SGD 0.01 loss: 0.1402 - accuracy: 0.9435 - val_loss: 0.7302 - val_accuracy: 0.8548\n",
        "# 35 12 1 relu relu sigmoid SGD 0.01 loss 0.1162 94.6% test : 85%\n",
        "# 17 5 1 relu relu sigmoid SGD 0.01  loss: 0.1714 - accuracy: 0.9300 - val_loss: 0.9535 - val_accuracy: 0.8503\n",
        "# 35 5 1 relu relu sigmoid Adam 0.01 loss: 0.1238 - accuracy: 0.9467 - val_loss: 5.7545 - val_accuracy: 0.8653\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.01 loss: 0.1184 - accuracy: 0.9525 - val_loss: 3.5327 - val_accuracy: 0.8428\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1185 - accuracy: 0.9525 - val_loss: 2.3218 - val_accuracy: 0.8593\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1041 - accuracy: 0.9576 - val_loss: 5.1465 - val_accuracy: 0.8353  +1300 epoch \n",
        "# 135 15 1 swish swish sigmoid Adamax 0.001 batch size:1 epoch 100 loss: 0.1707 - accuracy: 0.9352 - val_loss: 0.8066 - val_accuracy: 0.8892   **** egész jó\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=cols\n",
        "X_NN=df_all_normalized[prediktorok][:-100]  # \n",
        "y_NN=df_all_normalized[\"state\"][:-100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\"\n",
        "#model_file=\"model_PID__94_loss_0.116_vloss_0.115_acc_0.950_vacc_0.966.hdf5\"\n",
        "model_file=\"model_PID__4491_loss_0.115_vloss_0.679_acc_0.954_vacc_0.880.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False"
      ],
      "metadata": {
        "id": "UIxI3AS6Yw3S"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=0.7,shuffle=True,random_state=33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "jJfOOTfGfDXi"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate_corrector(epoch,lr):\n",
        "    if epoch > 4000:\n",
        "        lr = 0.01\n",
        "        return lr\n",
        "    if epoch > 3000:\n",
        "        lr = 0.005\n",
        "        return lr\n",
        "    if epoch > 2000:\n",
        "        lr = 0.01\n",
        "        return lr\n",
        "    \n",
        "    if epoch > 500:\n",
        "        lr = 0.005\n",
        "        return lr\n",
        "    return lr\n",
        "    "
      ],
      "metadata": {
        "id": "A-Kv8ORiEfub"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.keras import WandbMetricsLogger\n",
        "fname=\"./model_PID_3D\"\n",
        "callbacks = [\n",
        "        LearningRateScheduler(learning_rate_corrector,verbose=1),\n",
        "        WandbMetricsLogger(),       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch:04.0f}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{accuracy:.3f}_vacc_{val_accuracy:.3f}.hdf5\", monitor='loss',\n",
        "                        verbose=2, save_best_only=True, mode='min')]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__learning__=True"
      ],
      "metadata": {
        "id": "O6ofy0moderd"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z3Z4q14D7eC"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "9Ol0mW6WRlkS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe3f4c1a-f091-4506-b0fb-9230d9d3f734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA streamkimeneten csak az utolsó 5000 sor látható.\u001b[0m\n",
            "Epoch 4581: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4581/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9892\n",
            "Epoch 4581: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0341 - accuracy: 0.9892 - val_loss: 1.1044 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4582: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4582/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9895\n",
            "Epoch 4582: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0297 - accuracy: 0.9895 - val_loss: 1.1073 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "\n",
            "Epoch 4583: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4583/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9833\n",
            "Epoch 4583: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.0432 - accuracy: 0.9833 - val_loss: 1.1232 - val_accuracy: 0.9538 - lr: 0.0100\n",
            "\n",
            "Epoch 4584: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4584/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9807\n",
            "Epoch 4584: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0654 - accuracy: 0.9807 - val_loss: 1.1552 - val_accuracy: 0.9359 - lr: 0.0100\n",
            "\n",
            "Epoch 4585: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4585/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9751\n",
            "Epoch 4585: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.0745 - accuracy: 0.9751 - val_loss: 1.0864 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4586: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4586/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9851\n",
            "Epoch 4586: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0379 - accuracy: 0.9851 - val_loss: 1.0704 - val_accuracy: 0.9514 - lr: 0.0100\n",
            "\n",
            "Epoch 4587: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4587/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9802\n",
            "Epoch 4587: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.0491 - accuracy: 0.9802 - val_loss: 1.0588 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4588: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4588/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9884\n",
            "Epoch 4588: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0294 - accuracy: 0.9884 - val_loss: 1.0719 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4589: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4589/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9859\n",
            "Epoch 4589: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0399 - accuracy: 0.9859 - val_loss: 1.0764 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4590: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4590/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9869\n",
            "Epoch 4590: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0431 - accuracy: 0.9869 - val_loss: 1.0724 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4591: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4591/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9869\n",
            "Epoch 4591: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0334 - accuracy: 0.9869 - val_loss: 1.0801 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 4592: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4592/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9833\n",
            "Epoch 4592: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0441 - accuracy: 0.9833 - val_loss: 1.0727 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4593: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4593/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9871\n",
            "Epoch 4593: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0451 - accuracy: 0.9871 - val_loss: 1.0674 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4594: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4594/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9892\n",
            "Epoch 4594: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0338 - accuracy: 0.9892 - val_loss: 1.0854 - val_accuracy: 0.9508 - lr: 0.0100\n",
            "\n",
            "Epoch 4595: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4595/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9853\n",
            "Epoch 4595: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0431 - accuracy: 0.9853 - val_loss: 1.0644 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4596: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4596/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9871\n",
            "Epoch 4596: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0323 - accuracy: 0.9871 - val_loss: 1.0551 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4597: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4597/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9892\n",
            "Epoch 4597: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0314 - accuracy: 0.9892 - val_loss: 1.0544 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4598: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4598/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9882\n",
            "Epoch 4598: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0339 - accuracy: 0.9882 - val_loss: 1.0504 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4599: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4599/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9887\n",
            "Epoch 4599: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0278 - accuracy: 0.9887 - val_loss: 1.0565 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4600: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4600/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9877\n",
            "Epoch 4600: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0315 - accuracy: 0.9877 - val_loss: 1.0693 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4601: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4601/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9905\n",
            "Epoch 4601: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0265 - accuracy: 0.9905 - val_loss: 1.0745 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4602: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4602/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9915\n",
            "Epoch 4602: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.0265 - accuracy: 0.9915 - val_loss: 1.0642 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4603: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4603/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9913\n",
            "Epoch 4603: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0221 - accuracy: 0.9913 - val_loss: 1.0627 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4604: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4604/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9920\n",
            "Epoch 4604: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0233 - accuracy: 0.9920 - val_loss: 1.0609 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4605: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4605/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9923\n",
            "Epoch 4605: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 1.0616 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4606: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4606/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9936\n",
            "Epoch 4606: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 1.0692 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4607: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4607/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9900\n",
            "Epoch 4607: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0254 - accuracy: 0.9900 - val_loss: 1.0585 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4608: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4608/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9913\n",
            "Epoch 4608: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.0236 - accuracy: 0.9913 - val_loss: 1.0484 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4609: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4609/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9905\n",
            "Epoch 4609: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.0226 - accuracy: 0.9905 - val_loss: 1.0481 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4610: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4610/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9892\n",
            "Epoch 4610: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0254 - accuracy: 0.9892 - val_loss: 1.0610 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4611: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4611/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9931\n",
            "Epoch 4611: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0242 - accuracy: 0.9931 - val_loss: 1.0612 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4612: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4612/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9902\n",
            "Epoch 4612: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0257 - accuracy: 0.9902 - val_loss: 1.0501 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4613: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4613/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9915\n",
            "Epoch 4613: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0252 - accuracy: 0.9915 - val_loss: 1.0434 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4614: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4614/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9920\n",
            "Epoch 4614: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 1.0274 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4615: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4615/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9920\n",
            "Epoch 4615: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 1.0217 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4616: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4616/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9907\n",
            "Epoch 4616: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 1.0196 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4617: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4617/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9892\n",
            "Epoch 4617: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 0.0273 - accuracy: 0.9892 - val_loss: 1.0067 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4618: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4618/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9910\n",
            "Epoch 4618: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 385ms/step - loss: 0.0242 - accuracy: 0.9910 - val_loss: 1.0010 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4619: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4619/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9895\n",
            "Epoch 4619: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0279 - accuracy: 0.9895 - val_loss: 0.9959 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4620: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4620/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9918\n",
            "Epoch 4620: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0225 - accuracy: 0.9918 - val_loss: 1.0009 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4621: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4621/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9933\n",
            "Epoch 4621: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0209 - accuracy: 0.9933 - val_loss: 1.0023 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4622: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4622/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9928\n",
            "Epoch 4622: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.0237 - accuracy: 0.9928 - val_loss: 1.0103 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4623: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4623/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9951\n",
            "Epoch 4623: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.0193 - accuracy: 0.9951 - val_loss: 1.0256 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4624: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4624/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9943\n",
            "Epoch 4624: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.0205 - accuracy: 0.9943 - val_loss: 1.0296 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4625: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4625/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9925\n",
            "Epoch 4625: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 1.0319 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4626: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4626/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9910\n",
            "Epoch 4626: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0244 - accuracy: 0.9910 - val_loss: 1.0281 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4627: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4627/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9951\n",
            "Epoch 4627: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.0196 - accuracy: 0.9951 - val_loss: 1.0162 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4628: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4628/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9954\n",
            "Epoch 4628: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 1.0041 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4629: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4629/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9931\n",
            "Epoch 4629: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.0201 - accuracy: 0.9931 - val_loss: 1.0003 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4630: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4630/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9938\n",
            "Epoch 4630: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.0204 - accuracy: 0.9938 - val_loss: 1.0005 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4631: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4631/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9931\n",
            "Epoch 4631: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 1.0049 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4632: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4632/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9897\n",
            "Epoch 4632: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.0303 - accuracy: 0.9897 - val_loss: 1.0053 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4633: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4633/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9913\n",
            "Epoch 4633: loss did not improve from 0.01696\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.0236 - accuracy: 0.9913 - val_loss: 0.9964 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4634: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4634/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9954\n",
            "Epoch 4634: loss improved from 0.01696 to 0.01586, saving model to ./model_PID_3D_4634_loss_0.016_vloss_0.998_acc_0.995_vacc_0.961.hdf5\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.9981 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4635: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4635/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9925\n",
            "Epoch 4635: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 0.0212 - accuracy: 0.9925 - val_loss: 1.0084 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4636: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4636/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9918\n",
            "Epoch 4636: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 1.0197 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4637: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4637/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9941\n",
            "Epoch 4637: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 388ms/step - loss: 0.0196 - accuracy: 0.9941 - val_loss: 1.0259 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4638: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4638/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9923\n",
            "Epoch 4638: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0207 - accuracy: 0.9923 - val_loss: 1.0208 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4639: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4639/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9902\n",
            "Epoch 4639: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0252 - accuracy: 0.9902 - val_loss: 1.0244 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4640: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4640/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9925\n",
            "Epoch 4640: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.0235 - accuracy: 0.9925 - val_loss: 1.0118 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4641: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4641/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9933\n",
            "Epoch 4641: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.0189 - accuracy: 0.9933 - val_loss: 1.0103 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 4642: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4642/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9861\n",
            "Epoch 4642: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.0300 - accuracy: 0.9861 - val_loss: 1.0113 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4643: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4643/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9928\n",
            "Epoch 4643: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0233 - accuracy: 0.9928 - val_loss: 1.0141 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4644: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4644/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9897\n",
            "Epoch 4644: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0282 - accuracy: 0.9897 - val_loss: 1.0156 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4645: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4645/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9838\n",
            "Epoch 4645: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0340 - accuracy: 0.9838 - val_loss: 1.0180 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4646: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4646/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9907\n",
            "Epoch 4646: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0268 - accuracy: 0.9907 - val_loss: 1.0171 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4647: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4647/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9918\n",
            "Epoch 4647: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 1.0148 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4648: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4648/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9905\n",
            "Epoch 4648: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0246 - accuracy: 0.9905 - val_loss: 1.0139 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4649: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4649/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9918\n",
            "Epoch 4649: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0209 - accuracy: 0.9918 - val_loss: 1.0250 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4650: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4650/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9892\n",
            "Epoch 4650: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0307 - accuracy: 0.9892 - val_loss: 1.0346 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4651: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4651/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9905\n",
            "Epoch 4651: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0261 - accuracy: 0.9905 - val_loss: 1.0389 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4652: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4652/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9933\n",
            "Epoch 4652: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.0227 - accuracy: 0.9933 - val_loss: 1.0393 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4653: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4653/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9884\n",
            "Epoch 4653: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0305 - accuracy: 0.9884 - val_loss: 1.0206 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4654: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4654/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9905\n",
            "Epoch 4654: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0227 - accuracy: 0.9905 - val_loss: 1.0168 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 4655: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4655/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9892\n",
            "Epoch 4655: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.0252 - accuracy: 0.9892 - val_loss: 1.0285 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4656: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4656/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9900\n",
            "Epoch 4656: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0297 - accuracy: 0.9900 - val_loss: 1.0175 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4657: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4657/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9931\n",
            "Epoch 4657: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0226 - accuracy: 0.9931 - val_loss: 1.0430 - val_accuracy: 0.9442 - lr: 0.0100\n",
            "\n",
            "Epoch 4658: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4658/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9823\n",
            "Epoch 4658: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0462 - accuracy: 0.9823 - val_loss: 1.0276 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4659: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4659/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9882\n",
            "Epoch 4659: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0339 - accuracy: 0.9882 - val_loss: 1.0194 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4660: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4660/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9869\n",
            "Epoch 4660: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.0338 - accuracy: 0.9869 - val_loss: 1.0162 - val_accuracy: 0.9502 - lr: 0.0100\n",
            "\n",
            "Epoch 4661: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4661/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9887\n",
            "Epoch 4661: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0337 - accuracy: 0.9887 - val_loss: 1.0023 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4662: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4662/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9895\n",
            "Epoch 4662: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0276 - accuracy: 0.9895 - val_loss: 1.0154 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 4663: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4663/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9851\n",
            "Epoch 4663: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0368 - accuracy: 0.9851 - val_loss: 1.0126 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4664: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4664/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9938\n",
            "Epoch 4664: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0204 - accuracy: 0.9938 - val_loss: 1.0381 - val_accuracy: 0.9484 - lr: 0.0100\n",
            "\n",
            "Epoch 4665: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4665/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9871\n",
            "Epoch 4665: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0365 - accuracy: 0.9871 - val_loss: 1.0503 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4666: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4666/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9853\n",
            "Epoch 4666: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0436 - accuracy: 0.9853 - val_loss: 1.0306 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4667: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4667/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9884\n",
            "Epoch 4667: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.0303 - accuracy: 0.9884 - val_loss: 1.0265 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4668: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4668/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9897\n",
            "Epoch 4668: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0284 - accuracy: 0.9897 - val_loss: 1.0200 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4669: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4669/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9889\n",
            "Epoch 4669: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0264 - accuracy: 0.9889 - val_loss: 1.0205 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4670: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4670/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9841\n",
            "Epoch 4670: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0380 - accuracy: 0.9841 - val_loss: 1.0225 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4671: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4671/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9905\n",
            "Epoch 4671: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0264 - accuracy: 0.9905 - val_loss: 1.0401 - val_accuracy: 0.9508 - lr: 0.0100\n",
            "\n",
            "Epoch 4672: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4672/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9900\n",
            "Epoch 4672: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0313 - accuracy: 0.9900 - val_loss: 1.0471 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4673: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4673/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9913\n",
            "Epoch 4673: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.0276 - accuracy: 0.9913 - val_loss: 1.0589 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4674: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4674/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9892\n",
            "Epoch 4674: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0296 - accuracy: 0.9892 - val_loss: 1.0771 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4675: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4675/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9871\n",
            "Epoch 4675: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.0319 - accuracy: 0.9871 - val_loss: 1.0842 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4676: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4676/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9915\n",
            "Epoch 4676: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0282 - accuracy: 0.9915 - val_loss: 1.0895 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4677: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4677/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9892\n",
            "Epoch 4677: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0296 - accuracy: 0.9892 - val_loss: 1.0867 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4678: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4678/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9871\n",
            "Epoch 4678: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0316 - accuracy: 0.9871 - val_loss: 1.0825 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4679: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4679/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9907\n",
            "Epoch 4679: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0245 - accuracy: 0.9907 - val_loss: 1.0985 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4680: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4680/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9905\n",
            "Epoch 4680: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0332 - accuracy: 0.9905 - val_loss: 1.0962 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4681: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4681/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9923\n",
            "Epoch 4681: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0216 - accuracy: 0.9923 - val_loss: 1.1159 - val_accuracy: 0.9538 - lr: 0.0100\n",
            "\n",
            "Epoch 4682: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4682/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9928\n",
            "Epoch 4682: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.0241 - accuracy: 0.9928 - val_loss: 1.1061 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4683: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4683/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9915\n",
            "Epoch 4683: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0277 - accuracy: 0.9915 - val_loss: 1.0949 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4684: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4684/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9900\n",
            "Epoch 4684: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0259 - accuracy: 0.9900 - val_loss: 1.1035 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4685: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4685/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9874\n",
            "Epoch 4685: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0311 - accuracy: 0.9874 - val_loss: 1.0971 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4686: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4686/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9907\n",
            "Epoch 4686: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0210 - accuracy: 0.9907 - val_loss: 1.1003 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4687: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4687/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9905\n",
            "Epoch 4687: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.0273 - accuracy: 0.9905 - val_loss: 1.0975 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4688: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4688/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9902\n",
            "Epoch 4688: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 395ms/step - loss: 0.0272 - accuracy: 0.9902 - val_loss: 1.0893 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4689: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4689/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9910\n",
            "Epoch 4689: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.0291 - accuracy: 0.9910 - val_loss: 1.0935 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4690: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4690/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9851\n",
            "Epoch 4690: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.0342 - accuracy: 0.9851 - val_loss: 1.0800 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4691: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4691/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9910\n",
            "Epoch 4691: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 0.0240 - accuracy: 0.9910 - val_loss: 1.0954 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4692: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4692/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9887\n",
            "Epoch 4692: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 376ms/step - loss: 0.0273 - accuracy: 0.9887 - val_loss: 1.1008 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4693: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4693/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9892\n",
            "Epoch 4693: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 1.0887 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4694: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4694/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9918\n",
            "Epoch 4694: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 1.0736 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4695: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4695/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9895\n",
            "Epoch 4695: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0249 - accuracy: 0.9895 - val_loss: 1.0595 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4696: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4696/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9928\n",
            "Epoch 4696: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0232 - accuracy: 0.9928 - val_loss: 1.0577 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4697: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4697/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9933\n",
            "Epoch 4697: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.0247 - accuracy: 0.9933 - val_loss: 1.0591 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4698: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4698/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9933\n",
            "Epoch 4698: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 1.0632 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4699: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4699/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9931\n",
            "Epoch 4699: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0228 - accuracy: 0.9931 - val_loss: 1.0636 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4700: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4700/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9910\n",
            "Epoch 4700: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.0234 - accuracy: 0.9910 - val_loss: 1.0629 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4701: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4701/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9941\n",
            "Epoch 4701: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0196 - accuracy: 0.9941 - val_loss: 1.0656 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4702: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4702/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9946\n",
            "Epoch 4702: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 1.0734 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4703: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4703/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9954\n",
            "Epoch 4703: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 1.0731 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4704: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4704/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9938\n",
            "Epoch 4704: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0204 - accuracy: 0.9938 - val_loss: 1.0706 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4705: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4705/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9913\n",
            "Epoch 4705: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0224 - accuracy: 0.9913 - val_loss: 1.0776 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4706: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4706/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9933\n",
            "Epoch 4706: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.0178 - accuracy: 0.9933 - val_loss: 1.0781 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4707: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4707/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9933\n",
            "Epoch 4707: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.0193 - accuracy: 0.9933 - val_loss: 1.0714 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4708: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4708/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9918\n",
            "Epoch 4708: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.0208 - accuracy: 0.9918 - val_loss: 1.0765 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4709: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4709/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9938\n",
            "Epoch 4709: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0213 - accuracy: 0.9938 - val_loss: 1.0734 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4710: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4710/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9928\n",
            "Epoch 4710: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.0214 - accuracy: 0.9928 - val_loss: 1.0808 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4711: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4711/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9915\n",
            "Epoch 4711: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.0205 - accuracy: 0.9915 - val_loss: 1.0904 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4712: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4712/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9936\n",
            "Epoch 4712: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.0212 - accuracy: 0.9936 - val_loss: 1.0903 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4713: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4713/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9933\n",
            "Epoch 4713: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 1.0927 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4714: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4714/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9902\n",
            "Epoch 4714: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0237 - accuracy: 0.9902 - val_loss: 1.1037 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4715: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4715/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9895\n",
            "Epoch 4715: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0284 - accuracy: 0.9895 - val_loss: 1.0840 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4716: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4716/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9936\n",
            "Epoch 4716: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0194 - accuracy: 0.9936 - val_loss: 1.0820 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4717: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4717/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928\n",
            "Epoch 4717: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 1.0841 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4718: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4718/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9931\n",
            "Epoch 4718: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 1.0803 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4719: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4719/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9946\n",
            "Epoch 4719: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.0187 - accuracy: 0.9946 - val_loss: 1.0817 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4720: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4720/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9954\n",
            "Epoch 4720: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 1.0909 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4721: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4721/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9931\n",
            "Epoch 4721: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.0234 - accuracy: 0.9931 - val_loss: 1.1006 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4722: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4722/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9943\n",
            "Epoch 4722: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0202 - accuracy: 0.9943 - val_loss: 1.0983 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4723: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4723/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9902\n",
            "Epoch 4723: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.0217 - accuracy: 0.9902 - val_loss: 1.1022 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4724: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4724/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9918\n",
            "Epoch 4724: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.0268 - accuracy: 0.9918 - val_loss: 1.1010 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4725: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4725/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9923\n",
            "Epoch 4725: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.0204 - accuracy: 0.9923 - val_loss: 1.1135 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4726: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4726/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9946\n",
            "Epoch 4726: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.0176 - accuracy: 0.9946 - val_loss: 1.1218 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4727: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4727/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9905\n",
            "Epoch 4727: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0236 - accuracy: 0.9905 - val_loss: 1.0991 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4728: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4728/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9925\n",
            "Epoch 4728: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 1.0975 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4729: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4729/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9938\n",
            "Epoch 4729: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0209 - accuracy: 0.9938 - val_loss: 1.1163 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4730: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4730/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9910\n",
            "Epoch 4730: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0256 - accuracy: 0.9910 - val_loss: 1.1199 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4731: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4731/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9874\n",
            "Epoch 4731: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0292 - accuracy: 0.9874 - val_loss: 1.1114 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4732: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4732/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9907\n",
            "Epoch 4732: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0209 - accuracy: 0.9907 - val_loss: 1.1137 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4733: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4733/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9938\n",
            "Epoch 4733: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.0214 - accuracy: 0.9938 - val_loss: 1.1180 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4734: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4734/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9933\n",
            "Epoch 4734: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0214 - accuracy: 0.9933 - val_loss: 1.1213 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4735: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4735/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9915\n",
            "Epoch 4735: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0228 - accuracy: 0.9915 - val_loss: 1.1267 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4736: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4736/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9931\n",
            "Epoch 4736: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0210 - accuracy: 0.9931 - val_loss: 1.1161 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4737: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4737/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9941\n",
            "Epoch 4737: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.0183 - accuracy: 0.9941 - val_loss: 1.1114 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4738: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4738/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9913\n",
            "Epoch 4738: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0217 - accuracy: 0.9913 - val_loss: 1.1102 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4739: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4739/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9933\n",
            "Epoch 4739: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0187 - accuracy: 0.9933 - val_loss: 1.1138 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4740: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4740/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9951\n",
            "Epoch 4740: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 1.1196 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4741: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4741/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9949\n",
            "Epoch 4741: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0185 - accuracy: 0.9949 - val_loss: 1.1230 - val_accuracy: 0.9688 - lr: 0.0100\n",
            "\n",
            "Epoch 4742: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4742/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9933\n",
            "Epoch 4742: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0206 - accuracy: 0.9933 - val_loss: 1.1199 - val_accuracy: 0.9688 - lr: 0.0100\n",
            "\n",
            "Epoch 4743: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4743/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9941\n",
            "Epoch 4743: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0184 - accuracy: 0.9941 - val_loss: 1.1185 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4744: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4744/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9910\n",
            "Epoch 4744: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0239 - accuracy: 0.9910 - val_loss: 1.1197 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4745: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4745/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9889\n",
            "Epoch 4745: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0299 - accuracy: 0.9889 - val_loss: 1.1342 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4746: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4746/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9923\n",
            "Epoch 4746: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 1.1270 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4747: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4747/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9959\n",
            "Epoch 4747: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0178 - accuracy: 0.9959 - val_loss: 1.1354 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4748: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4748/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9866\n",
            "Epoch 4748: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0356 - accuracy: 0.9866 - val_loss: 1.1583 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4749: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4749/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9820\n",
            "Epoch 4749: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0519 - accuracy: 0.9820 - val_loss: 1.1338 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4750: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4750/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9895\n",
            "Epoch 4750: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0253 - accuracy: 0.9895 - val_loss: 1.1229 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4751: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4751/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9879\n",
            "Epoch 4751: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.0313 - accuracy: 0.9879 - val_loss: 1.1218 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4752: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4752/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9859\n",
            "Epoch 4752: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 0.0416 - accuracy: 0.9859 - val_loss: 1.1150 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4753: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4753/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9900\n",
            "Epoch 4753: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0282 - accuracy: 0.9900 - val_loss: 1.1122 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4754: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4754/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9918\n",
            "Epoch 4754: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 1.1195 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 4755: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4755/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9877\n",
            "Epoch 4755: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0326 - accuracy: 0.9877 - val_loss: 1.1023 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4756: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4756/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9913\n",
            "Epoch 4756: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.0260 - accuracy: 0.9913 - val_loss: 1.0939 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4757: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4757/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9902\n",
            "Epoch 4757: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.0265 - accuracy: 0.9902 - val_loss: 1.0867 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4758: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4758/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9933\n",
            "Epoch 4758: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0198 - accuracy: 0.9933 - val_loss: 1.0949 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4759: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4759/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9895\n",
            "Epoch 4759: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0281 - accuracy: 0.9895 - val_loss: 1.1028 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4760: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4760/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9910\n",
            "Epoch 4760: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.0241 - accuracy: 0.9910 - val_loss: 1.1135 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4761: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4761/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9910\n",
            "Epoch 4761: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0281 - accuracy: 0.9910 - val_loss: 1.0864 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4762: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4762/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9864\n",
            "Epoch 4762: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.0259 - accuracy: 0.9864 - val_loss: 1.0722 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4763: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4763/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9907\n",
            "Epoch 4763: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.0236 - accuracy: 0.9907 - val_loss: 1.0600 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4764: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4764/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9925\n",
            "Epoch 4764: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 1.0657 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4765: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4765/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9874\n",
            "Epoch 4765: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0291 - accuracy: 0.9874 - val_loss: 1.0634 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4766: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4766/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9943\n",
            "Epoch 4766: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 0.0236 - accuracy: 0.9943 - val_loss: 1.0688 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4767: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4767/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9946\n",
            "Epoch 4767: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0215 - accuracy: 0.9946 - val_loss: 1.0825 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4768: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4768/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9889\n",
            "Epoch 4768: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0265 - accuracy: 0.9889 - val_loss: 1.0833 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4769: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4769/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9910\n",
            "Epoch 4769: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0234 - accuracy: 0.9910 - val_loss: 1.0823 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4770: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4770/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9925\n",
            "Epoch 4770: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0242 - accuracy: 0.9925 - val_loss: 1.0700 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4771: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4771/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9949\n",
            "Epoch 4771: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.0212 - accuracy: 0.9949 - val_loss: 1.0649 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4772: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4772/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9943\n",
            "Epoch 4772: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0214 - accuracy: 0.9943 - val_loss: 1.0553 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4773: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4773/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9943\n",
            "Epoch 4773: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0189 - accuracy: 0.9943 - val_loss: 1.0512 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4774: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4774/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9920\n",
            "Epoch 4774: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0198 - accuracy: 0.9920 - val_loss: 1.0589 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4775: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4775/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9913\n",
            "Epoch 4775: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.0258 - accuracy: 0.9913 - val_loss: 1.0531 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4776: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4776/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9900\n",
            "Epoch 4776: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.0205 - accuracy: 0.9900 - val_loss: 1.0528 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4777: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4777/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928\n",
            "Epoch 4777: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 1.0533 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4778: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4778/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9936\n",
            "Epoch 4778: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.0210 - accuracy: 0.9936 - val_loss: 1.0583 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4779: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4779/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9866\n",
            "Epoch 4779: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.0318 - accuracy: 0.9866 - val_loss: 1.1104 - val_accuracy: 0.9568 - lr: 0.0100\n",
            "\n",
            "Epoch 4780: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4780/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9812\n",
            "Epoch 4780: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.0647 - accuracy: 0.9812 - val_loss: 1.0749 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4781: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4781/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9835\n",
            "Epoch 4781: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0407 - accuracy: 0.9835 - val_loss: 1.0716 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4782: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4782/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9830\n",
            "Epoch 4782: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0428 - accuracy: 0.9830 - val_loss: 1.0582 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4783: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4783/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9853\n",
            "Epoch 4783: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0459 - accuracy: 0.9853 - val_loss: 1.0447 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4784: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4784/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9907\n",
            "Epoch 4784: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0253 - accuracy: 0.9907 - val_loss: 1.0707 - val_accuracy: 0.9460 - lr: 0.0100\n",
            "\n",
            "Epoch 4785: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4785/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9830\n",
            "Epoch 4785: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 1.0786 - val_accuracy: 0.9538 - lr: 0.0100\n",
            "\n",
            "Epoch 4786: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4786/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9666\n",
            "Epoch 4786: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0751 - accuracy: 0.9666 - val_loss: 1.0535 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4787: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4787/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9902\n",
            "Epoch 4787: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0307 - accuracy: 0.9902 - val_loss: 1.0737 - val_accuracy: 0.9436 - lr: 0.0100\n",
            "\n",
            "Epoch 4788: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4788/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9802\n",
            "Epoch 4788: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0473 - accuracy: 0.9802 - val_loss: 1.0588 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "\n",
            "Epoch 4789: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4789/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9807\n",
            "Epoch 4789: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0573 - accuracy: 0.9807 - val_loss: 1.0408 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4790: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4790/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9859\n",
            "Epoch 4790: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0312 - accuracy: 0.9859 - val_loss: 1.0663 - val_accuracy: 0.9460 - lr: 0.0100\n",
            "\n",
            "Epoch 4791: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4791/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9815\n",
            "Epoch 4791: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0488 - accuracy: 0.9815 - val_loss: 1.0435 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4792: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4792/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9851\n",
            "Epoch 4792: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0396 - accuracy: 0.9851 - val_loss: 1.0396 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4793: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4793/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9871\n",
            "Epoch 4793: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.0310 - accuracy: 0.9871 - val_loss: 1.0854 - val_accuracy: 0.9394 - lr: 0.0100\n",
            "\n",
            "Epoch 4794: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4794/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9704\n",
            "Epoch 4794: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0729 - accuracy: 0.9704 - val_loss: 1.0655 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4795: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4795/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9851\n",
            "Epoch 4795: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.0463 - accuracy: 0.9851 - val_loss: 1.0668 - val_accuracy: 0.9568 - lr: 0.0100\n",
            "\n",
            "Epoch 4796: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4796/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9841\n",
            "Epoch 4796: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.0499 - accuracy: 0.9841 - val_loss: 1.0693 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4797: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4797/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9828\n",
            "Epoch 4797: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0505 - accuracy: 0.9828 - val_loss: 1.0822 - val_accuracy: 0.9478 - lr: 0.0100\n",
            "\n",
            "Epoch 4798: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4798/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9763\n",
            "Epoch 4798: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0598 - accuracy: 0.9763 - val_loss: 1.0414 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4799: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4799/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9841\n",
            "Epoch 4799: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.0440 - accuracy: 0.9841 - val_loss: 1.0342 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4800: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4800/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9815\n",
            "Epoch 4800: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.0490 - accuracy: 0.9815 - val_loss: 1.0645 - val_accuracy: 0.9478 - lr: 0.0100\n",
            "\n",
            "Epoch 4801: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4801/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9815\n",
            "Epoch 4801: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.0521 - accuracy: 0.9815 - val_loss: 1.0331 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4802: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4802/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9871\n",
            "Epoch 4802: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0375 - accuracy: 0.9871 - val_loss: 1.0325 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4803: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4803/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9817\n",
            "Epoch 4803: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.0473 - accuracy: 0.9817 - val_loss: 1.0236 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4804: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4804/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9879\n",
            "Epoch 4804: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0372 - accuracy: 0.9879 - val_loss: 1.0217 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4805: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4805/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9869\n",
            "Epoch 4805: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0405 - accuracy: 0.9869 - val_loss: 1.0062 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4806: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4806/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9905\n",
            "Epoch 4806: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0316 - accuracy: 0.9905 - val_loss: 1.0052 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4807: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4807/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9887\n",
            "Epoch 4807: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0350 - accuracy: 0.9887 - val_loss: 1.0094 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4808: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4808/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9902\n",
            "Epoch 4808: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0290 - accuracy: 0.9902 - val_loss: 1.0201 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4809: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4809/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9902\n",
            "Epoch 4809: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.0297 - accuracy: 0.9902 - val_loss: 1.0128 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4810: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4810/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9895\n",
            "Epoch 4810: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0275 - accuracy: 0.9895 - val_loss: 1.0017 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4811: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4811/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9900\n",
            "Epoch 4811: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0272 - accuracy: 0.9900 - val_loss: 1.0008 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4812: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4812/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9902\n",
            "Epoch 4812: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0268 - accuracy: 0.9902 - val_loss: 1.0102 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4813: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4813/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9918\n",
            "Epoch 4813: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 1.0255 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4814: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4814/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9920\n",
            "Epoch 4814: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0255 - accuracy: 0.9920 - val_loss: 1.0174 - val_accuracy: 0.9694 - lr: 0.0100\n",
            "\n",
            "Epoch 4815: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4815/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9946\n",
            "Epoch 4815: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0198 - accuracy: 0.9946 - val_loss: 1.0194 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4816: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4816/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9900\n",
            "Epoch 4816: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0272 - accuracy: 0.9900 - val_loss: 1.0310 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4817: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4817/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9910\n",
            "Epoch 4817: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.0233 - accuracy: 0.9910 - val_loss: 1.0429 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4818: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4818/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9889\n",
            "Epoch 4818: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.0276 - accuracy: 0.9889 - val_loss: 1.0470 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4819: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4819/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9938\n",
            "Epoch 4819: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.0195 - accuracy: 0.9938 - val_loss: 1.0392 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4820: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4820/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9913\n",
            "Epoch 4820: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.0241 - accuracy: 0.9913 - val_loss: 1.0334 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4821: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4821/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9931\n",
            "Epoch 4821: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 1.0305 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4822: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4822/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9941\n",
            "Epoch 4822: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0186 - accuracy: 0.9941 - val_loss: 1.0272 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4823: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4823/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9923\n",
            "Epoch 4823: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0227 - accuracy: 0.9923 - val_loss: 1.0248 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4824: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4824/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9907\n",
            "Epoch 4824: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.0243 - accuracy: 0.9907 - val_loss: 1.0195 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4825: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4825/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9920\n",
            "Epoch 4825: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 1.0037 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4826: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4826/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9913\n",
            "Epoch 4826: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 0.0252 - accuracy: 0.9913 - val_loss: 1.0080 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4827: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4827/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9900\n",
            "Epoch 4827: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.0242 - accuracy: 0.9900 - val_loss: 1.0193 - val_accuracy: 0.9694 - lr: 0.0100\n",
            "\n",
            "Epoch 4828: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4828/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9938\n",
            "Epoch 4828: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.0207 - accuracy: 0.9938 - val_loss: 1.0226 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4829: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4829/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9941\n",
            "Epoch 4829: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 0.0179 - accuracy: 0.9941 - val_loss: 1.0280 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4830: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4830/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9892\n",
            "Epoch 4830: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0267 - accuracy: 0.9892 - val_loss: 1.0342 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4831: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4831/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9923\n",
            "Epoch 4831: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 1.0454 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4832: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4832/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9961\n",
            "Epoch 4832: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0168 - accuracy: 0.9961 - val_loss: 1.0453 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4833: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4833/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9928\n",
            "Epoch 4833: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.0211 - accuracy: 0.9928 - val_loss: 1.0409 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4834: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4834/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9933\n",
            "Epoch 4834: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0208 - accuracy: 0.9933 - val_loss: 1.0387 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4835: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4835/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9925\n",
            "Epoch 4835: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.0197 - accuracy: 0.9925 - val_loss: 1.0384 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4836: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4836/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9943\n",
            "Epoch 4836: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0185 - accuracy: 0.9943 - val_loss: 1.0347 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4837: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4837/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9928\n",
            "Epoch 4837: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 1.0290 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4838: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4838/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9946\n",
            "Epoch 4838: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.0188 - accuracy: 0.9946 - val_loss: 1.0276 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4839: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4839/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9928\n",
            "Epoch 4839: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.0199 - accuracy: 0.9928 - val_loss: 1.0323 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4840: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4840/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9928\n",
            "Epoch 4840: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 1.0366 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4841: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4841/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9938\n",
            "Epoch 4841: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0194 - accuracy: 0.9938 - val_loss: 1.0414 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4842: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4842/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9936\n",
            "Epoch 4842: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0208 - accuracy: 0.9936 - val_loss: 1.0388 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4843: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4843/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9943\n",
            "Epoch 4843: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 380ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 1.0344 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4844: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4844/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9946\n",
            "Epoch 4844: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.0176 - accuracy: 0.9946 - val_loss: 1.0326 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4845: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4845/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9949\n",
            "Epoch 4845: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 1.0333 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4846: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4846/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9941\n",
            "Epoch 4846: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 1.0324 - val_accuracy: 0.9682 - lr: 0.0100\n",
            "\n",
            "Epoch 4847: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4847/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9954\n",
            "Epoch 4847: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 1.0288 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4848: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4848/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9951\n",
            "Epoch 4848: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0177 - accuracy: 0.9951 - val_loss: 1.0233 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4849: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4849/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9949\n",
            "Epoch 4849: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0187 - accuracy: 0.9949 - val_loss: 1.0224 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4850: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4850/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9938\n",
            "Epoch 4850: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0203 - accuracy: 0.9938 - val_loss: 1.0212 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4851: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4851/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9918\n",
            "Epoch 4851: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0234 - accuracy: 0.9918 - val_loss: 1.0335 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4852: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4852/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9920\n",
            "Epoch 4852: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.0219 - accuracy: 0.9920 - val_loss: 1.0402 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4853: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4853/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9949\n",
            "Epoch 4853: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 1.0432 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4854: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4854/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9956\n",
            "Epoch 4854: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0167 - accuracy: 0.9956 - val_loss: 1.0335 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4855: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4855/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9936\n",
            "Epoch 4855: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.0183 - accuracy: 0.9936 - val_loss: 1.0278 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4856: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4856/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9918\n",
            "Epoch 4856: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0243 - accuracy: 0.9918 - val_loss: 1.0247 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4857: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4857/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9956\n",
            "Epoch 4857: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 1.0225 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4858: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4858/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9923\n",
            "Epoch 4858: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 1.0177 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4859: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4859/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9902\n",
            "Epoch 4859: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0230 - accuracy: 0.9902 - val_loss: 1.0135 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4860: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4860/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9938\n",
            "Epoch 4860: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0161 - accuracy: 0.9938 - val_loss: 1.0264 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4861: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4861/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9928\n",
            "Epoch 4861: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.0247 - accuracy: 0.9928 - val_loss: 1.0300 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4862: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4862/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9925\n",
            "Epoch 4862: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.0253 - accuracy: 0.9925 - val_loss: 1.0396 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4863: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4863/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9918\n",
            "Epoch 4863: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0230 - accuracy: 0.9918 - val_loss: 1.0516 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4864: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4864/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9931\n",
            "Epoch 4864: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0256 - accuracy: 0.9931 - val_loss: 1.0442 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4865: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4865/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9933\n",
            "Epoch 4865: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.0194 - accuracy: 0.9933 - val_loss: 1.0502 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4866: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4866/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9892\n",
            "Epoch 4866: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0245 - accuracy: 0.9892 - val_loss: 1.0614 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4867: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4867/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9925\n",
            "Epoch 4867: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0236 - accuracy: 0.9925 - val_loss: 1.0682 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4868: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4868/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9943\n",
            "Epoch 4868: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0176 - accuracy: 0.9943 - val_loss: 1.0821 - val_accuracy: 0.9490 - lr: 0.0100\n",
            "\n",
            "Epoch 4869: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4869/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9910\n",
            "Epoch 4869: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 1.0707 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4870: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4870/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9941\n",
            "Epoch 4870: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0183 - accuracy: 0.9941 - val_loss: 1.0671 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4871: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4871/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9905\n",
            "Epoch 4871: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 1.0597 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4872: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4872/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9882\n",
            "Epoch 4872: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.0264 - accuracy: 0.9882 - val_loss: 1.0582 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4873: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4873/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9923\n",
            "Epoch 4873: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.0183 - accuracy: 0.9923 - val_loss: 1.0622 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4874: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4874/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9897\n",
            "Epoch 4874: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0260 - accuracy: 0.9897 - val_loss: 1.0564 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4875: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4875/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9931\n",
            "Epoch 4875: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 1.0506 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4876: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4876/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9925\n",
            "Epoch 4876: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.0219 - accuracy: 0.9925 - val_loss: 1.0453 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4877: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4877/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9936\n",
            "Epoch 4877: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0245 - accuracy: 0.9936 - val_loss: 1.0341 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4878: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4878/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9943\n",
            "Epoch 4878: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0178 - accuracy: 0.9943 - val_loss: 1.0334 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4879: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4879/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9923\n",
            "Epoch 4879: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 1.0338 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4880: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4880/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9925\n",
            "Epoch 4880: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0200 - accuracy: 0.9925 - val_loss: 1.0395 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 4881: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4881/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9907\n",
            "Epoch 4881: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0263 - accuracy: 0.9907 - val_loss: 1.0400 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4882: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4882/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9936\n",
            "Epoch 4882: loss did not improve from 0.01586\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 1.0393 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4883: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4883/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9961\n",
            "Epoch 4883: loss improved from 0.01586 to 0.01399, saving model to ./model_PID_3D_4883_loss_0.014_vloss_1.042_acc_0.996_vacc_0.965.hdf5\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 1.0417 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4884: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4884/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9941\n",
            "Epoch 4884: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0172 - accuracy: 0.9941 - val_loss: 1.0387 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4885: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4885/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9938\n",
            "Epoch 4885: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 1.0371 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4886: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4886/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9946\n",
            "Epoch 4886: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 0.0180 - accuracy: 0.9946 - val_loss: 1.0414 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4887: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4887/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9954\n",
            "Epoch 4887: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 1.0488 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4888: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4888/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9933\n",
            "Epoch 4888: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 385ms/step - loss: 0.0193 - accuracy: 0.9933 - val_loss: 1.0481 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4889: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4889/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9941\n",
            "Epoch 4889: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0203 - accuracy: 0.9941 - val_loss: 1.0359 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4890: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4890/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9928\n",
            "Epoch 4890: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 383ms/step - loss: 0.0188 - accuracy: 0.9928 - val_loss: 1.0358 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4891: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4891/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9925\n",
            "Epoch 4891: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.0209 - accuracy: 0.9925 - val_loss: 1.0478 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4892: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4892/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9941\n",
            "Epoch 4892: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 1.0489 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4893: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4893/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9938\n",
            "Epoch 4893: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0195 - accuracy: 0.9938 - val_loss: 1.0421 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4894: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4894/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9946\n",
            "Epoch 4894: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 1.0395 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4895: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4895/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9931\n",
            "Epoch 4895: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 1.0425 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4896: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4896/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9959\n",
            "Epoch 4896: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.0145 - accuracy: 0.9959 - val_loss: 1.0495 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4897: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4897/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9949\n",
            "Epoch 4897: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0167 - accuracy: 0.9949 - val_loss: 1.0560 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4898: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4898/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9943\n",
            "Epoch 4898: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 1.0610 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4899: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4899/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9946\n",
            "Epoch 4899: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 1.0572 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4900: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4900/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9954\n",
            "Epoch 4900: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 1.0508 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4901: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4901/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9943\n",
            "Epoch 4901: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 0.0190 - accuracy: 0.9943 - val_loss: 1.0473 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4902: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4902/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9951\n",
            "Epoch 4902: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 380ms/step - loss: 0.0209 - accuracy: 0.9951 - val_loss: 1.0486 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4903: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4903/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9943\n",
            "Epoch 4903: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 432ms/step - loss: 0.0187 - accuracy: 0.9943 - val_loss: 1.0532 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4904: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4904/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9923\n",
            "Epoch 4904: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.0177 - accuracy: 0.9923 - val_loss: 1.0632 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4905: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4905/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9943\n",
            "Epoch 4905: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.0158 - accuracy: 0.9943 - val_loss: 1.0703 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4906: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4906/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9928\n",
            "Epoch 4906: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0199 - accuracy: 0.9928 - val_loss: 1.0626 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4907: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4907/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9941\n",
            "Epoch 4907: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.0186 - accuracy: 0.9941 - val_loss: 1.0537 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4908: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4908/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9923\n",
            "Epoch 4908: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.0188 - accuracy: 0.9923 - val_loss: 1.0607 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4909: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4909/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9897\n",
            "Epoch 4909: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0260 - accuracy: 0.9897 - val_loss: 1.0602 - val_accuracy: 0.9562 - lr: 0.0100\n",
            "\n",
            "Epoch 4910: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4910/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9877\n",
            "Epoch 4910: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0264 - accuracy: 0.9877 - val_loss: 1.0626 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4911: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4911/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9956\n",
            "Epoch 4911: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0189 - accuracy: 0.9956 - val_loss: 1.0572 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4912: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4912/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9918\n",
            "Epoch 4912: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0230 - accuracy: 0.9918 - val_loss: 1.0452 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4913: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4913/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9905\n",
            "Epoch 4913: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0211 - accuracy: 0.9905 - val_loss: 1.0404 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4914: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4914/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9918\n",
            "Epoch 4914: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0200 - accuracy: 0.9918 - val_loss: 1.0502 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4915: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4915/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9905\n",
            "Epoch 4915: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0266 - accuracy: 0.9905 - val_loss: 1.0559 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4916: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4916/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9933\n",
            "Epoch 4916: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 1.0449 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4917: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4917/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9949\n",
            "Epoch 4917: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.0179 - accuracy: 0.9949 - val_loss: 1.0383 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4918: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4918/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9961\n",
            "Epoch 4918: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0236 - accuracy: 0.9961 - val_loss: 1.0330 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4919: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4919/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9933\n",
            "Epoch 4919: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0183 - accuracy: 0.9933 - val_loss: 1.0312 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4920: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4920/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9928\n",
            "Epoch 4920: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.0214 - accuracy: 0.9928 - val_loss: 1.0338 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4921: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4921/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9951\n",
            "Epoch 4921: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0156 - accuracy: 0.9951 - val_loss: 1.0376 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4922: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4922/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9925\n",
            "Epoch 4922: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 1.0401 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4923: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4923/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9949\n",
            "Epoch 4923: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 1.0363 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4924: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4924/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9946\n",
            "Epoch 4924: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0162 - accuracy: 0.9946 - val_loss: 1.0353 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4925: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4925/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9933\n",
            "Epoch 4925: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0209 - accuracy: 0.9933 - val_loss: 1.0419 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4926: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4926/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9925\n",
            "Epoch 4926: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0209 - accuracy: 0.9925 - val_loss: 1.0525 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4927: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4927/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9931\n",
            "Epoch 4927: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.0182 - accuracy: 0.9931 - val_loss: 1.0695 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4928: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4928/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9920\n",
            "Epoch 4928: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0223 - accuracy: 0.9920 - val_loss: 1.0677 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4929: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4929/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9943\n",
            "Epoch 4929: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0192 - accuracy: 0.9943 - val_loss: 1.0533 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4930: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4930/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9902\n",
            "Epoch 4930: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.0258 - accuracy: 0.9902 - val_loss: 1.0592 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4931: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4931/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9897\n",
            "Epoch 4931: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0250 - accuracy: 0.9897 - val_loss: 1.0790 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4932: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4932/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9925\n",
            "Epoch 4932: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0239 - accuracy: 0.9925 - val_loss: 1.0849 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4933: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4933/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9920\n",
            "Epoch 4933: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0255 - accuracy: 0.9920 - val_loss: 1.0710 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4934: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4934/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9923\n",
            "Epoch 4934: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 1.0627 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4935: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4935/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9905\n",
            "Epoch 4935: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0253 - accuracy: 0.9905 - val_loss: 1.0565 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4936: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4936/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9920\n",
            "Epoch 4936: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 1.0501 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4937: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4937/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9946\n",
            "Epoch 4937: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0209 - accuracy: 0.9946 - val_loss: 1.0414 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4938: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4938/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9951\n",
            "Epoch 4938: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 1.0328 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4939: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4939/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9933\n",
            "Epoch 4939: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.0222 - accuracy: 0.9933 - val_loss: 1.0227 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4940: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4940/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9946\n",
            "Epoch 4940: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0187 - accuracy: 0.9946 - val_loss: 1.0212 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4941: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4941/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9913\n",
            "Epoch 4941: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.0219 - accuracy: 0.9913 - val_loss: 1.0233 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4942: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4942/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9946\n",
            "Epoch 4942: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 1.0354 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4943: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4943/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9920\n",
            "Epoch 4943: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0223 - accuracy: 0.9920 - val_loss: 1.0430 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4944: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4944/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9920\n",
            "Epoch 4944: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0211 - accuracy: 0.9920 - val_loss: 1.0297 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4945: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4945/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9943\n",
            "Epoch 4945: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0186 - accuracy: 0.9943 - val_loss: 1.0241 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4946: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4946/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9936\n",
            "Epoch 4946: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.0239 - accuracy: 0.9936 - val_loss: 1.0287 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "\n",
            "Epoch 4947: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4947/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9874\n",
            "Epoch 4947: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.0313 - accuracy: 0.9874 - val_loss: 1.0376 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4948: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4948/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9918\n",
            "Epoch 4948: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 1.0455 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4949: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4949/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9900\n",
            "Epoch 4949: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.0285 - accuracy: 0.9900 - val_loss: 1.0460 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4950: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4950/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9895\n",
            "Epoch 4950: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.0248 - accuracy: 0.9895 - val_loss: 1.0361 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4951: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4951/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9931\n",
            "Epoch 4951: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0194 - accuracy: 0.9931 - val_loss: 1.0463 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 4952: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4952/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9877\n",
            "Epoch 4952: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 0.0323 - accuracy: 0.9877 - val_loss: 1.0651 - val_accuracy: 0.9562 - lr: 0.0100\n",
            "\n",
            "Epoch 4953: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4953/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9851\n",
            "Epoch 4953: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0331 - accuracy: 0.9851 - val_loss: 1.0704 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 4954: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4954/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9902\n",
            "Epoch 4954: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0255 - accuracy: 0.9902 - val_loss: 1.0807 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4955: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4955/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9866\n",
            "Epoch 4955: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0301 - accuracy: 0.9866 - val_loss: 1.0758 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 4956: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4956/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9892\n",
            "Epoch 4956: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0261 - accuracy: 0.9892 - val_loss: 1.0610 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4957: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4957/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9938\n",
            "Epoch 4957: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0180 - accuracy: 0.9938 - val_loss: 1.0535 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4958: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4958/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9915\n",
            "Epoch 4958: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0238 - accuracy: 0.9915 - val_loss: 1.0544 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4959: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4959/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9923\n",
            "Epoch 4959: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 1.0637 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4960: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4960/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9931\n",
            "Epoch 4960: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 1.0628 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4961: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4961/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9936\n",
            "Epoch 4961: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 380ms/step - loss: 0.0234 - accuracy: 0.9936 - val_loss: 1.0498 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4962: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4962/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9884\n",
            "Epoch 4962: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0270 - accuracy: 0.9884 - val_loss: 1.0423 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4963: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4963/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9910\n",
            "Epoch 4963: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.0216 - accuracy: 0.9910 - val_loss: 1.0433 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4964: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4964/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9933\n",
            "Epoch 4964: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 1.0536 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4965: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4965/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9938\n",
            "Epoch 4965: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0235 - accuracy: 0.9938 - val_loss: 1.0564 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4966: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4966/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9907\n",
            "Epoch 4966: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0230 - accuracy: 0.9907 - val_loss: 1.0566 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4967: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4967/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9925\n",
            "Epoch 4967: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 1.0634 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 4968: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4968/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9897\n",
            "Epoch 4968: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 0.0230 - accuracy: 0.9897 - val_loss: 1.0621 - val_accuracy: 0.9568 - lr: 0.0100\n",
            "\n",
            "Epoch 4969: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4969/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9902\n",
            "Epoch 4969: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0198 - accuracy: 0.9902 - val_loss: 1.0576 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4970: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4970/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9923\n",
            "Epoch 4970: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0217 - accuracy: 0.9923 - val_loss: 1.0489 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4971: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4971/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9949\n",
            "Epoch 4971: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0176 - accuracy: 0.9949 - val_loss: 1.0442 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4972: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4972/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9941\n",
            "Epoch 4972: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.0203 - accuracy: 0.9941 - val_loss: 1.0434 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4973: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4973/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9936\n",
            "Epoch 4973: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.0189 - accuracy: 0.9936 - val_loss: 1.0457 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4974: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4974/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9938\n",
            "Epoch 4974: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0177 - accuracy: 0.9938 - val_loss: 1.0457 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4975: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4975/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9941\n",
            "Epoch 4975: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 1.0402 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4976: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4976/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9943\n",
            "Epoch 4976: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0157 - accuracy: 0.9943 - val_loss: 1.0348 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4977: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4977/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9946\n",
            "Epoch 4977: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.0192 - accuracy: 0.9946 - val_loss: 1.0372 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4978: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4978/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9936\n",
            "Epoch 4978: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0194 - accuracy: 0.9936 - val_loss: 1.0393 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4979: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4979/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9954\n",
            "Epoch 4979: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 1.0367 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4980: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4980/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9910\n",
            "Epoch 4980: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.0221 - accuracy: 0.9910 - val_loss: 1.0297 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4981: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4981/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9897\n",
            "Epoch 4981: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0241 - accuracy: 0.9897 - val_loss: 1.0319 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4982: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4982/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9925\n",
            "Epoch 4982: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.0218 - accuracy: 0.9925 - val_loss: 1.0367 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 4983: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4983/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9954\n",
            "Epoch 4983: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 1.0463 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4984: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4984/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9928\n",
            "Epoch 4984: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0233 - accuracy: 0.9928 - val_loss: 1.0445 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4985: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4985/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9918\n",
            "Epoch 4985: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0237 - accuracy: 0.9918 - val_loss: 1.0380 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 4986: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4986/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9941\n",
            "Epoch 4986: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0168 - accuracy: 0.9941 - val_loss: 1.0413 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 4987: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4987/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9902\n",
            "Epoch 4987: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0207 - accuracy: 0.9902 - val_loss: 1.0351 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 4988: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4988/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9951\n",
            "Epoch 4988: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0180 - accuracy: 0.9951 - val_loss: 1.0286 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 4989: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4989/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9923\n",
            "Epoch 4989: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 1.0302 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 4990: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4990/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9923\n",
            "Epoch 4990: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 1.0333 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4991: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4991/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9913\n",
            "Epoch 4991: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0262 - accuracy: 0.9913 - val_loss: 1.0273 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 4992: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4992/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9943\n",
            "Epoch 4992: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 1.0324 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 4993: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4993/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9923\n",
            "Epoch 4993: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 1.0366 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 4994: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4994/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9913\n",
            "Epoch 4994: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0244 - accuracy: 0.9913 - val_loss: 1.0222 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 4995: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4995/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9931\n",
            "Epoch 4995: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.0181 - accuracy: 0.9931 - val_loss: 1.0316 - val_accuracy: 0.9574 - lr: 0.0100\n",
            "\n",
            "Epoch 4996: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4996/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9877\n",
            "Epoch 4996: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.0293 - accuracy: 0.9877 - val_loss: 1.0337 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4997: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4997/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9897\n",
            "Epoch 4997: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0287 - accuracy: 0.9897 - val_loss: 1.0213 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 4998: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4998/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9918\n",
            "Epoch 4998: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0225 - accuracy: 0.9918 - val_loss: 1.0151 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 4999: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 4999/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9907\n",
            "Epoch 4999: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.0226 - accuracy: 0.9907 - val_loss: 1.0048 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5000: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5000/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9918\n",
            "Epoch 5000: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.0206 - accuracy: 0.9918 - val_loss: 1.0076 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5001: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5001/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9967\n",
            "Epoch 5001: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0176 - accuracy: 0.9967 - val_loss: 1.0157 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5002: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5002/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9920\n",
            "Epoch 5002: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0248 - accuracy: 0.9920 - val_loss: 1.0164 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5003: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5003/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9915\n",
            "Epoch 5003: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0269 - accuracy: 0.9915 - val_loss: 1.0116 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5004: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5004/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9915\n",
            "Epoch 5004: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0215 - accuracy: 0.9915 - val_loss: 1.0077 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5005: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5005/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9923\n",
            "Epoch 5005: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 1.0026 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5006: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5006/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9938\n",
            "Epoch 5006: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.9964 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5007: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5007/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9956\n",
            "Epoch 5007: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0151 - accuracy: 0.9956 - val_loss: 0.9963 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5008: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5008/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9941\n",
            "Epoch 5008: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 1.0005 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5009: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5009/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9928\n",
            "Epoch 5009: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0233 - accuracy: 0.9928 - val_loss: 0.9968 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5010: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5010/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9900\n",
            "Epoch 5010: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0239 - accuracy: 0.9900 - val_loss: 1.0041 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5011: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5011/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9949\n",
            "Epoch 5011: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0166 - accuracy: 0.9949 - val_loss: 1.0150 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5012: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5012/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9918\n",
            "Epoch 5012: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 1.0105 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5013: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5013/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9913\n",
            "Epoch 5013: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0232 - accuracy: 0.9913 - val_loss: 0.9967 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5014: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5014/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9902\n",
            "Epoch 5014: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.0236 - accuracy: 0.9902 - val_loss: 0.9812 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5015: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5015/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9887\n",
            "Epoch 5015: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 376ms/step - loss: 0.0270 - accuracy: 0.9887 - val_loss: 0.9878 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5016: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5016/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9933\n",
            "Epoch 5016: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 0.0192 - accuracy: 0.9933 - val_loss: 0.9772 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5017: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5017/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9923\n",
            "Epoch 5017: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0218 - accuracy: 0.9923 - val_loss: 0.9601 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5018: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5018/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9925\n",
            "Epoch 5018: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0200 - accuracy: 0.9925 - val_loss: 0.9578 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5019: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5019/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9933\n",
            "Epoch 5019: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.9580 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5020: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5020/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9938\n",
            "Epoch 5020: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.0238 - accuracy: 0.9938 - val_loss: 0.9842 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5021: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5021/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9938\n",
            "Epoch 5021: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 389ms/step - loss: 0.0216 - accuracy: 0.9938 - val_loss: 0.9790 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5022: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5022/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9956\n",
            "Epoch 5022: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 0.9773 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5023: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5023/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9918\n",
            "Epoch 5023: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.0202 - accuracy: 0.9918 - val_loss: 0.9783 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5024: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5024/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9913\n",
            "Epoch 5024: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 0.0217 - accuracy: 0.9913 - val_loss: 0.9824 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5025: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5025/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9931\n",
            "Epoch 5025: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.0165 - accuracy: 0.9931 - val_loss: 0.9855 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5026: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5026/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9949\n",
            "Epoch 5026: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0179 - accuracy: 0.9949 - val_loss: 0.9902 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5027: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5027/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9951\n",
            "Epoch 5027: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 0.9925 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5028: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5028/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9969\n",
            "Epoch 5028: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 0.0156 - accuracy: 0.9969 - val_loss: 1.0005 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5029: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5029/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9954\n",
            "Epoch 5029: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 1.0074 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5030: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5030/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9933\n",
            "Epoch 5030: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0183 - accuracy: 0.9933 - val_loss: 1.0206 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5031: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5031/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9933\n",
            "Epoch 5031: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0206 - accuracy: 0.9933 - val_loss: 1.0320 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5032: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5032/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9943\n",
            "Epoch 5032: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.0214 - accuracy: 0.9943 - val_loss: 1.0313 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5033: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5033/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9954\n",
            "Epoch 5033: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 1.0274 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5034: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5034/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9931\n",
            "Epoch 5034: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0190 - accuracy: 0.9931 - val_loss: 1.0322 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5035: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5035/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9941\n",
            "Epoch 5035: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 1.0309 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5036: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5036/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9936\n",
            "Epoch 5036: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0166 - accuracy: 0.9936 - val_loss: 1.0450 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5037: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5037/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9879\n",
            "Epoch 5037: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0279 - accuracy: 0.9879 - val_loss: 1.0517 - val_accuracy: 0.9574 - lr: 0.0100\n",
            "\n",
            "Epoch 5038: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5038/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9841\n",
            "Epoch 5038: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.0426 - accuracy: 0.9841 - val_loss: 1.0688 - val_accuracy: 0.9442 - lr: 0.0100\n",
            "\n",
            "Epoch 5039: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5039/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9841\n",
            "Epoch 5039: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.0376 - accuracy: 0.9841 - val_loss: 1.0589 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5040: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5040/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9936\n",
            "Epoch 5040: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 1.0592 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5041: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5041/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9915\n",
            "Epoch 5041: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0279 - accuracy: 0.9915 - val_loss: 1.0702 - val_accuracy: 0.9514 - lr: 0.0100\n",
            "\n",
            "Epoch 5042: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5042/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9781\n",
            "Epoch 5042: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0522 - accuracy: 0.9781 - val_loss: 1.0872 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "\n",
            "Epoch 5043: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5043/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9792\n",
            "Epoch 5043: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0647 - accuracy: 0.9792 - val_loss: 1.0443 - val_accuracy: 0.9568 - lr: 0.0100\n",
            "\n",
            "Epoch 5044: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5044/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9820\n",
            "Epoch 5044: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.0447 - accuracy: 0.9820 - val_loss: 1.0475 - val_accuracy: 0.9460 - lr: 0.0100\n",
            "\n",
            "Epoch 5045: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5045/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9766\n",
            "Epoch 5045: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0582 - accuracy: 0.9766 - val_loss: 1.0202 - val_accuracy: 0.9544 - lr: 0.0100\n",
            "\n",
            "Epoch 5046: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5046/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9758\n",
            "Epoch 5046: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 222ms/step - loss: 0.0663 - accuracy: 0.9758 - val_loss: 1.0097 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5047: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5047/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9866\n",
            "Epoch 5047: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 0.0359 - accuracy: 0.9866 - val_loss: 1.0336 - val_accuracy: 0.9460 - lr: 0.0100\n",
            "\n",
            "Epoch 5048: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5048/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9823\n",
            "Epoch 5048: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0452 - accuracy: 0.9823 - val_loss: 1.0037 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5049: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5049/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9830\n",
            "Epoch 5049: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0467 - accuracy: 0.9830 - val_loss: 0.9980 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5050: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5050/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9897\n",
            "Epoch 5050: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0335 - accuracy: 0.9897 - val_loss: 1.0128 - val_accuracy: 0.9544 - lr: 0.0100\n",
            "\n",
            "Epoch 5051: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5051/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9851\n",
            "Epoch 5051: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0394 - accuracy: 0.9851 - val_loss: 1.0014 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5052: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5052/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9869\n",
            "Epoch 5052: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0309 - accuracy: 0.9869 - val_loss: 1.0104 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5053: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5053/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9879\n",
            "Epoch 5053: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0310 - accuracy: 0.9879 - val_loss: 1.0530 - val_accuracy: 0.9436 - lr: 0.0100\n",
            "\n",
            "Epoch 5054: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5054/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9787\n",
            "Epoch 5054: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 0.0539 - accuracy: 0.9787 - val_loss: 1.0161 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5055: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5055/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9879\n",
            "Epoch 5055: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.0313 - accuracy: 0.9879 - val_loss: 1.0098 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5056: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5056/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9869\n",
            "Epoch 5056: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0335 - accuracy: 0.9869 - val_loss: 1.0134 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5057: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5057/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9846\n",
            "Epoch 5057: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0367 - accuracy: 0.9846 - val_loss: 0.9888 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5058: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5058/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9918\n",
            "Epoch 5058: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 0.9952 - val_accuracy: 0.9562 - lr: 0.0100\n",
            "\n",
            "Epoch 5059: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5059/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9871\n",
            "Epoch 5059: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0365 - accuracy: 0.9871 - val_loss: 1.0080 - val_accuracy: 0.9478 - lr: 0.0100\n",
            "\n",
            "Epoch 5060: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5060/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9882\n",
            "Epoch 5060: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0335 - accuracy: 0.9882 - val_loss: 0.9994 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5061: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5061/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9938\n",
            "Epoch 5061: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 1.0109 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5062: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5062/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9902\n",
            "Epoch 5062: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0274 - accuracy: 0.9902 - val_loss: 1.0165 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5063: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5063/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9905\n",
            "Epoch 5063: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.0232 - accuracy: 0.9905 - val_loss: 1.0182 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5064: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5064/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9910\n",
            "Epoch 5064: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 1.0176 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5065: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5065/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9910\n",
            "Epoch 5065: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0294 - accuracy: 0.9910 - val_loss: 1.0092 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5066: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5066/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9920\n",
            "Epoch 5066: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0230 - accuracy: 0.9920 - val_loss: 1.0151 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5067: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5067/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9874\n",
            "Epoch 5067: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0276 - accuracy: 0.9874 - val_loss: 1.0130 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5068: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5068/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9920\n",
            "Epoch 5068: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.0224 - accuracy: 0.9920 - val_loss: 1.0174 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5069: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5069/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9928\n",
            "Epoch 5069: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 1.0160 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 5070: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5070/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9918\n",
            "Epoch 5070: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0225 - accuracy: 0.9918 - val_loss: 1.0116 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5071: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5071/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9938\n",
            "Epoch 5071: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0177 - accuracy: 0.9938 - val_loss: 1.0145 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5072: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5072/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9943\n",
            "Epoch 5072: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0228 - accuracy: 0.9943 - val_loss: 1.0012 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5073: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5073/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9915\n",
            "Epoch 5073: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0208 - accuracy: 0.9915 - val_loss: 0.9959 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5074: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5074/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9936\n",
            "Epoch 5074: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0183 - accuracy: 0.9936 - val_loss: 1.0045 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5075: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5075/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9923\n",
            "Epoch 5075: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 1.0089 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 5076: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5076/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9925\n",
            "Epoch 5076: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0191 - accuracy: 0.9925 - val_loss: 0.9949 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 5077: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5077/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9933\n",
            "Epoch 5077: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0179 - accuracy: 0.9933 - val_loss: 0.9890 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5078: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5078/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9951\n",
            "Epoch 5078: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 0.9852 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5079: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5079/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9936\n",
            "Epoch 5079: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0203 - accuracy: 0.9936 - val_loss: 0.9949 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5080: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5080/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9913\n",
            "Epoch 5080: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 376ms/step - loss: 0.0209 - accuracy: 0.9913 - val_loss: 1.0005 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5081: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5081/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9964\n",
            "Epoch 5081: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 353ms/step - loss: 0.0158 - accuracy: 0.9964 - val_loss: 0.9998 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5082: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5082/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9951\n",
            "Epoch 5082: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 0.0164 - accuracy: 0.9951 - val_loss: 0.9966 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5083: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5083/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9925\n",
            "Epoch 5083: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 340ms/step - loss: 0.0195 - accuracy: 0.9925 - val_loss: 0.9991 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5084: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5084/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9928\n",
            "Epoch 5084: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 385ms/step - loss: 0.0187 - accuracy: 0.9928 - val_loss: 1.0062 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5085: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5085/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9951\n",
            "Epoch 5085: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 1.0068 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5086: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5086/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9946\n",
            "Epoch 5086: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.0158 - accuracy: 0.9946 - val_loss: 1.0005 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5087: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5087/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9943\n",
            "Epoch 5087: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 376ms/step - loss: 0.0187 - accuracy: 0.9943 - val_loss: 0.9991 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5088: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5088/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9925\n",
            "Epoch 5088: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.9970 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5089: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5089/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9938\n",
            "Epoch 5089: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.0181 - accuracy: 0.9938 - val_loss: 1.0116 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5090: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5090/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9920\n",
            "Epoch 5090: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 0.0204 - accuracy: 0.9920 - val_loss: 1.0083 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5091: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5091/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9941\n",
            "Epoch 5091: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0213 - accuracy: 0.9941 - val_loss: 0.9970 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5092: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5092/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9941\n",
            "Epoch 5092: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0205 - accuracy: 0.9941 - val_loss: 0.9926 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 5093: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5093/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9882\n",
            "Epoch 5093: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.0247 - accuracy: 0.9882 - val_loss: 0.9890 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5094: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5094/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9928\n",
            "Epoch 5094: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.0209 - accuracy: 0.9928 - val_loss: 0.9931 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5095: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5095/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9943\n",
            "Epoch 5095: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 383ms/step - loss: 0.0157 - accuracy: 0.9943 - val_loss: 1.0025 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 5096: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5096/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9931\n",
            "Epoch 5096: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0196 - accuracy: 0.9931 - val_loss: 0.9941 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5097: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5097/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9959\n",
            "Epoch 5097: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.9962 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5098: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5098/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9946\n",
            "Epoch 5098: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.9993 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5099: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5099/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9897\n",
            "Epoch 5099: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0222 - accuracy: 0.9897 - val_loss: 1.0146 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5100: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5100/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9951\n",
            "Epoch 5100: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 0.0169 - accuracy: 0.9951 - val_loss: 1.0130 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5101: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5101/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9949\n",
            "Epoch 5101: loss did not improve from 0.01399\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.0186 - accuracy: 0.9949 - val_loss: 1.0081 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5102: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5102/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9943\n",
            "Epoch 5102: loss improved from 0.01399 to 0.01397, saving model to ./model_PID_3D_5102_loss_0.014_vloss_1.005_acc_0.994_vacc_0.965.hdf5\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.0140 - accuracy: 0.9943 - val_loss: 1.0048 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5103: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5103/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9941\n",
            "Epoch 5103: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.0187 - accuracy: 0.9941 - val_loss: 1.0100 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5104: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5104/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9946\n",
            "Epoch 5104: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.0142 - accuracy: 0.9946 - val_loss: 1.0182 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5105: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5105/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9954\n",
            "Epoch 5105: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 1.0350 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5106: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5106/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9913\n",
            "Epoch 5106: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.0231 - accuracy: 0.9913 - val_loss: 1.0430 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5107: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5107/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9943\n",
            "Epoch 5107: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0198 - accuracy: 0.9943 - val_loss: 1.0448 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5108: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5108/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9946\n",
            "Epoch 5108: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 1.0571 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5109: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5109/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9925\n",
            "Epoch 5109: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0206 - accuracy: 0.9925 - val_loss: 1.0682 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5110: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5110/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9959\n",
            "Epoch 5110: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0156 - accuracy: 0.9959 - val_loss: 1.0799 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5111: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5111/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9938\n",
            "Epoch 5111: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 1.0838 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5112: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5112/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9946\n",
            "Epoch 5112: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0152 - accuracy: 0.9946 - val_loss: 1.0903 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5113: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5113/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9946\n",
            "Epoch 5113: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0176 - accuracy: 0.9946 - val_loss: 1.1038 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5114: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5114/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9946\n",
            "Epoch 5114: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.0177 - accuracy: 0.9946 - val_loss: 1.1083 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5115: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5115/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9951\n",
            "Epoch 5115: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 1.1140 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5116: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5116/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9946\n",
            "Epoch 5116: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 0.0181 - accuracy: 0.9946 - val_loss: 1.1174 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5117: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5117/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9923\n",
            "Epoch 5117: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 1.1134 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5118: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5118/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9949\n",
            "Epoch 5118: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 1.1137 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5119: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5119/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9938\n",
            "Epoch 5119: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0165 - accuracy: 0.9938 - val_loss: 1.1153 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5120: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5120/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9961\n",
            "Epoch 5120: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 1.1131 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5121: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5121/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9941\n",
            "Epoch 5121: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 1.1144 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5122: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5122/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9954\n",
            "Epoch 5122: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 1.1187 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5123: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5123/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9941\n",
            "Epoch 5123: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0159 - accuracy: 0.9941 - val_loss: 1.1173 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5124: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5124/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9951\n",
            "Epoch 5124: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0163 - accuracy: 0.9951 - val_loss: 1.1102 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5125: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5125/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9956\n",
            "Epoch 5125: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0171 - accuracy: 0.9956 - val_loss: 1.1073 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5126: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5126/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9949\n",
            "Epoch 5126: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0190 - accuracy: 0.9949 - val_loss: 1.1129 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5127: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5127/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9946\n",
            "Epoch 5127: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0154 - accuracy: 0.9946 - val_loss: 1.1181 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5128: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5128/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9946\n",
            "Epoch 5128: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0150 - accuracy: 0.9946 - val_loss: 1.1233 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5129: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5129/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9936\n",
            "Epoch 5129: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0171 - accuracy: 0.9936 - val_loss: 1.1245 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5130: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5130/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9946\n",
            "Epoch 5130: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 1.1238 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5131: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5131/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9951\n",
            "Epoch 5131: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 1.1184 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5132: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5132/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9951\n",
            "Epoch 5132: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 1.1118 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5133: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5133/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9964\n",
            "Epoch 5133: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0151 - accuracy: 0.9964 - val_loss: 1.1068 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5134: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5134/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9941\n",
            "Epoch 5134: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0162 - accuracy: 0.9941 - val_loss: 1.1032 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5135: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5135/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9923\n",
            "Epoch 5135: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0192 - accuracy: 0.9923 - val_loss: 1.1032 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5136: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5136/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9915\n",
            "Epoch 5136: loss did not improve from 0.01397\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 0.0221 - accuracy: 0.9915 - val_loss: 1.0927 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5137: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5137/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9961\n",
            "Epoch 5137: loss improved from 0.01397 to 0.01384, saving model to ./model_PID_3D_5137_loss_0.014_vloss_1.091_acc_0.996_vacc_0.966.hdf5\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.0138 - accuracy: 0.9961 - val_loss: 1.0909 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5138: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5138/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9954\n",
            "Epoch 5138: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 1.0962 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5139: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5139/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9954\n",
            "Epoch 5139: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 1.0913 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5140: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5140/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9923\n",
            "Epoch 5140: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0187 - accuracy: 0.9923 - val_loss: 1.0827 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5141: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5141/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9928\n",
            "Epoch 5141: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0198 - accuracy: 0.9928 - val_loss: 1.0765 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5142: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5142/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9943\n",
            "Epoch 5142: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0156 - accuracy: 0.9943 - val_loss: 1.0642 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5143: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5143/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9954\n",
            "Epoch 5143: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 1.0598 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5144: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5144/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9918\n",
            "Epoch 5144: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0217 - accuracy: 0.9918 - val_loss: 1.0676 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5145: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5145/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9931\n",
            "Epoch 5145: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0202 - accuracy: 0.9931 - val_loss: 1.0630 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5146: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5146/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9949\n",
            "Epoch 5146: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 1.0670 - val_accuracy: 0.9538 - lr: 0.0100\n",
            "\n",
            "Epoch 5147: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5147/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9910\n",
            "Epoch 5147: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.0231 - accuracy: 0.9910 - val_loss: 1.0557 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5148: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5148/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9943\n",
            "Epoch 5148: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.0169 - accuracy: 0.9943 - val_loss: 1.0477 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5149: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5149/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9951\n",
            "Epoch 5149: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 1.0419 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5150: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5150/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9907\n",
            "Epoch 5150: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.0231 - accuracy: 0.9907 - val_loss: 1.0518 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5151: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5151/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9920\n",
            "Epoch 5151: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0205 - accuracy: 0.9920 - val_loss: 1.0510 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5152: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5152/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9915\n",
            "Epoch 5152: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0229 - accuracy: 0.9915 - val_loss: 1.0549 - val_accuracy: 0.9490 - lr: 0.0100\n",
            "\n",
            "Epoch 5153: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5153/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9882\n",
            "Epoch 5153: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0314 - accuracy: 0.9882 - val_loss: 1.0438 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5154: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5154/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9925\n",
            "Epoch 5154: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.0247 - accuracy: 0.9925 - val_loss: 1.0366 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5155: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5155/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9951\n",
            "Epoch 5155: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 1.0478 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5156: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5156/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9928\n",
            "Epoch 5156: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.0204 - accuracy: 0.9928 - val_loss: 1.0508 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5157: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5157/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9913\n",
            "Epoch 5157: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0249 - accuracy: 0.9913 - val_loss: 1.0377 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5158: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5158/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9954\n",
            "Epoch 5158: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 1.0371 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5159: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5159/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9905\n",
            "Epoch 5159: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 0.0253 - accuracy: 0.9905 - val_loss: 1.0322 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5160: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5160/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9951\n",
            "Epoch 5160: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 355ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 1.0383 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5161: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5161/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9900\n",
            "Epoch 5161: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 0.0261 - accuracy: 0.9900 - val_loss: 1.0837 - val_accuracy: 0.9412 - lr: 0.0100\n",
            "\n",
            "Epoch 5162: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5162/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9835\n",
            "Epoch 5162: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 378ms/step - loss: 0.0477 - accuracy: 0.9835 - val_loss: 1.0528 - val_accuracy: 0.9562 - lr: 0.0100\n",
            "\n",
            "Epoch 5163: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5163/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9887\n",
            "Epoch 5163: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0333 - accuracy: 0.9887 - val_loss: 1.0392 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5164: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5164/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9902\n",
            "Epoch 5164: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0243 - accuracy: 0.9902 - val_loss: 1.0334 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5165: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5165/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9905\n",
            "Epoch 5165: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0273 - accuracy: 0.9905 - val_loss: 1.0190 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5166: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5166/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9905\n",
            "Epoch 5166: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 0.0248 - accuracy: 0.9905 - val_loss: 1.0250 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5167: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5167/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9913\n",
            "Epoch 5167: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 0.0239 - accuracy: 0.9913 - val_loss: 1.0426 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5168: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5168/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9941\n",
            "Epoch 5168: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 0.0203 - accuracy: 0.9941 - val_loss: 1.0500 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5169: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5169/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9936\n",
            "Epoch 5169: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.0208 - accuracy: 0.9936 - val_loss: 1.0462 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5170: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5170/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9902\n",
            "Epoch 5170: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.0271 - accuracy: 0.9902 - val_loss: 1.0360 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5171: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5171/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9902\n",
            "Epoch 5171: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.0213 - accuracy: 0.9902 - val_loss: 1.0384 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5172: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5172/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9902\n",
            "Epoch 5172: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0232 - accuracy: 0.9902 - val_loss: 1.0421 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5173: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5173/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9928\n",
            "Epoch 5173: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0217 - accuracy: 0.9928 - val_loss: 1.0554 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5174: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5174/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9913\n",
            "Epoch 5174: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0283 - accuracy: 0.9913 - val_loss: 1.0531 - val_accuracy: 0.9532 - lr: 0.0100\n",
            "\n",
            "Epoch 5175: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5175/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9884\n",
            "Epoch 5175: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0271 - accuracy: 0.9884 - val_loss: 1.0398 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5176: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5176/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9910\n",
            "Epoch 5176: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0251 - accuracy: 0.9910 - val_loss: 1.0433 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 5177: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5177/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9920\n",
            "Epoch 5177: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0205 - accuracy: 0.9920 - val_loss: 1.0480 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5178: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5178/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9902\n",
            "Epoch 5178: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0249 - accuracy: 0.9902 - val_loss: 1.0370 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5179: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5179/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9910\n",
            "Epoch 5179: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 224ms/step - loss: 0.0235 - accuracy: 0.9910 - val_loss: 1.0362 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5180: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5180/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9946\n",
            "Epoch 5180: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 1.0361 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5181: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5181/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928\n",
            "Epoch 5181: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 1.0345 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5182: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5182/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9941\n",
            "Epoch 5182: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0195 - accuracy: 0.9941 - val_loss: 1.0263 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5183: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5183/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9910\n",
            "Epoch 5183: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0219 - accuracy: 0.9910 - val_loss: 1.0249 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 5184: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5184/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9900\n",
            "Epoch 5184: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0244 - accuracy: 0.9900 - val_loss: 1.0244 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5185: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5185/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9928\n",
            "Epoch 5185: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0210 - accuracy: 0.9928 - val_loss: 1.0362 - val_accuracy: 0.9574 - lr: 0.0100\n",
            "\n",
            "Epoch 5186: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5186/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9864\n",
            "Epoch 5186: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.0315 - accuracy: 0.9864 - val_loss: 1.0382 - val_accuracy: 0.9400 - lr: 0.0100\n",
            "\n",
            "Epoch 5187: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5187/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9812\n",
            "Epoch 5187: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0389 - accuracy: 0.9812 - val_loss: 1.0280 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5188: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5188/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9900\n",
            "Epoch 5188: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 0.0275 - accuracy: 0.9900 - val_loss: 1.0380 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5189: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5189/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9874\n",
            "Epoch 5189: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0421 - accuracy: 0.9874 - val_loss: 1.0955 - val_accuracy: 0.9388 - lr: 0.0100\n",
            "\n",
            "Epoch 5190: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5190/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9774\n",
            "Epoch 5190: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0607 - accuracy: 0.9774 - val_loss: 1.0490 - val_accuracy: 0.9580 - lr: 0.0100\n",
            "\n",
            "Epoch 5191: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5191/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9828\n",
            "Epoch 5191: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0510 - accuracy: 0.9828 - val_loss: 1.0309 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5192: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5192/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9925\n",
            "Epoch 5192: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0245 - accuracy: 0.9925 - val_loss: 1.0502 - val_accuracy: 0.9472 - lr: 0.0100\n",
            "\n",
            "Epoch 5193: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5193/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9851\n",
            "Epoch 5193: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0442 - accuracy: 0.9851 - val_loss: 1.0389 - val_accuracy: 0.9514 - lr: 0.0100\n",
            "\n",
            "Epoch 5194: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5194/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9799\n",
            "Epoch 5194: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 0.0514 - accuracy: 0.9799 - val_loss: 1.0412 - val_accuracy: 0.9478 - lr: 0.0100\n",
            "\n",
            "Epoch 5195: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5195/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9761\n",
            "Epoch 5195: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0512 - accuracy: 0.9761 - val_loss: 1.0305 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5196: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5196/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9920\n",
            "Epoch 5196: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0220 - accuracy: 0.9920 - val_loss: 1.0504 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "\n",
            "Epoch 5197: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5197/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9841\n",
            "Epoch 5197: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.0491 - accuracy: 0.9841 - val_loss: 1.0435 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "\n",
            "Epoch 5198: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5198/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9825\n",
            "Epoch 5198: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0443 - accuracy: 0.9825 - val_loss: 1.0332 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 5199: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5199/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9848\n",
            "Epoch 5199: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.0371 - accuracy: 0.9848 - val_loss: 1.0247 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5200: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5200/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9846\n",
            "Epoch 5200: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0495 - accuracy: 0.9846 - val_loss: 1.0271 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 5201: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5201/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9851\n",
            "Epoch 5201: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0417 - accuracy: 0.9851 - val_loss: 1.0428 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 5202: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5202/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9859\n",
            "Epoch 5202: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0366 - accuracy: 0.9859 - val_loss: 1.0359 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5203: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5203/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9907\n",
            "Epoch 5203: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0268 - accuracy: 0.9907 - val_loss: 1.0396 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5204: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5204/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9902\n",
            "Epoch 5204: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.0263 - accuracy: 0.9902 - val_loss: 1.0369 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5205: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5205/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9910\n",
            "Epoch 5205: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 1.0465 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5206: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5206/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9864\n",
            "Epoch 5206: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0332 - accuracy: 0.9864 - val_loss: 1.0526 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5207: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5207/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9887\n",
            "Epoch 5207: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0298 - accuracy: 0.9887 - val_loss: 1.0483 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5208: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5208/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9920\n",
            "Epoch 5208: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0202 - accuracy: 0.9920 - val_loss: 1.0506 - val_accuracy: 0.9592 - lr: 0.0100\n",
            "\n",
            "Epoch 5209: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5209/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9900\n",
            "Epoch 5209: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0277 - accuracy: 0.9900 - val_loss: 1.0399 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5210: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5210/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9938\n",
            "Epoch 5210: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 0.0212 - accuracy: 0.9938 - val_loss: 1.0360 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5211: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5211/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9915\n",
            "Epoch 5211: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.0204 - accuracy: 0.9915 - val_loss: 1.0457 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5212: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5212/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9913\n",
            "Epoch 5212: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 0.0266 - accuracy: 0.9913 - val_loss: 1.0548 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5213: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5213/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9928\n",
            "Epoch 5213: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 1.0693 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5214: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5214/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9918\n",
            "Epoch 5214: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0244 - accuracy: 0.9918 - val_loss: 1.0764 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 5215: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5215/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9915\n",
            "Epoch 5215: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 1.0708 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5216: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5216/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9941\n",
            "Epoch 5216: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0190 - accuracy: 0.9941 - val_loss: 1.0749 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5217: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5217/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9925\n",
            "Epoch 5217: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0272 - accuracy: 0.9925 - val_loss: 1.0830 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5218: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5218/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9897\n",
            "Epoch 5218: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.0289 - accuracy: 0.9897 - val_loss: 1.0889 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5219: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5219/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9864\n",
            "Epoch 5219: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.0354 - accuracy: 0.9864 - val_loss: 1.0850 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5220: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5220/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9864\n",
            "Epoch 5220: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0330 - accuracy: 0.9864 - val_loss: 1.0571 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5221: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5221/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9913\n",
            "Epoch 5221: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 380ms/step - loss: 0.0205 - accuracy: 0.9913 - val_loss: 1.0455 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5222: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5222/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9882\n",
            "Epoch 5222: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.0320 - accuracy: 0.9882 - val_loss: 1.0546 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5223: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5223/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9920\n",
            "Epoch 5223: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 1.0783 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5224: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5224/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9923\n",
            "Epoch 5224: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0217 - accuracy: 0.9923 - val_loss: 1.0991 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5225: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5225/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9925\n",
            "Epoch 5225: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.0235 - accuracy: 0.9925 - val_loss: 1.1013 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5226: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5226/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9928\n",
            "Epoch 5226: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0202 - accuracy: 0.9928 - val_loss: 1.1020 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5227: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5227/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9923\n",
            "Epoch 5227: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 1.1022 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5228: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5228/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9915\n",
            "Epoch 5228: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.0231 - accuracy: 0.9915 - val_loss: 1.1012 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5229: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5229/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9943\n",
            "Epoch 5229: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 365ms/step - loss: 0.0177 - accuracy: 0.9943 - val_loss: 1.1054 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5230: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5230/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9961\n",
            "Epoch 5230: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0189 - accuracy: 0.9961 - val_loss: 1.1162 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5231: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5231/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9920\n",
            "Epoch 5231: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0211 - accuracy: 0.9920 - val_loss: 1.1151 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5232: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5232/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9956\n",
            "Epoch 5232: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.0167 - accuracy: 0.9956 - val_loss: 1.1155 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5233: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5233/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9931\n",
            "Epoch 5233: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0213 - accuracy: 0.9931 - val_loss: 1.1122 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5234: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5234/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9918\n",
            "Epoch 5234: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 0.0220 - accuracy: 0.9918 - val_loss: 1.1130 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5235: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5235/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9920\n",
            "Epoch 5235: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.0198 - accuracy: 0.9920 - val_loss: 1.1220 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5236: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5236/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9943\n",
            "Epoch 5236: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 1.1280 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5237: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5237/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9954\n",
            "Epoch 5237: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 1.1230 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5238: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5238/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9941\n",
            "Epoch 5238: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0180 - accuracy: 0.9941 - val_loss: 1.1247 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5239: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5239/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9943\n",
            "Epoch 5239: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.0195 - accuracy: 0.9943 - val_loss: 1.1066 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5240: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5240/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9928\n",
            "Epoch 5240: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0212 - accuracy: 0.9928 - val_loss: 1.0960 - val_accuracy: 0.9574 - lr: 0.0100\n",
            "\n",
            "Epoch 5241: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5241/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9895\n",
            "Epoch 5241: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0238 - accuracy: 0.9895 - val_loss: 1.0893 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5242: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5242/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9938\n",
            "Epoch 5242: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0203 - accuracy: 0.9938 - val_loss: 1.0775 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5243: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5243/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9933\n",
            "Epoch 5243: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 1.0674 - val_accuracy: 0.9574 - lr: 0.0100\n",
            "\n",
            "Epoch 5244: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5244/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9915\n",
            "Epoch 5244: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0218 - accuracy: 0.9915 - val_loss: 1.0636 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5245: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5245/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9933\n",
            "Epoch 5245: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.0168 - accuracy: 0.9933 - val_loss: 1.0714 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5246: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5246/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9931\n",
            "Epoch 5246: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0175 - accuracy: 0.9931 - val_loss: 1.0792 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5247: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5247/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9938\n",
            "Epoch 5247: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0182 - accuracy: 0.9938 - val_loss: 1.0936 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5248: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5248/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9931\n",
            "Epoch 5248: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 1.1122 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5249: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5249/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9941\n",
            "Epoch 5249: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0201 - accuracy: 0.9941 - val_loss: 1.1257 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5250: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5250/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9943\n",
            "Epoch 5250: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 1.1306 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5251: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5251/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9933\n",
            "Epoch 5251: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.0174 - accuracy: 0.9933 - val_loss: 1.1391 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5252: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5252/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9956\n",
            "Epoch 5252: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0159 - accuracy: 0.9956 - val_loss: 1.1475 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5253: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5253/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9941\n",
            "Epoch 5253: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 1.1546 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5254: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5254/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9943\n",
            "Epoch 5254: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0188 - accuracy: 0.9943 - val_loss: 1.1598 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5255: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5255/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9946\n",
            "Epoch 5255: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.0190 - accuracy: 0.9946 - val_loss: 1.1592 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5256: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5256/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9956\n",
            "Epoch 5256: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0160 - accuracy: 0.9956 - val_loss: 1.1519 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5257: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5257/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9936\n",
            "Epoch 5257: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0173 - accuracy: 0.9936 - val_loss: 1.1435 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5258: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5258/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9941\n",
            "Epoch 5258: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0153 - accuracy: 0.9941 - val_loss: 1.1369 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5259: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5259/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9943\n",
            "Epoch 5259: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0153 - accuracy: 0.9943 - val_loss: 1.1367 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5260: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5260/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9949\n",
            "Epoch 5260: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0185 - accuracy: 0.9949 - val_loss: 1.1261 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5261: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5261/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9938\n",
            "Epoch 5261: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0168 - accuracy: 0.9938 - val_loss: 1.1169 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5262: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5262/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9936\n",
            "Epoch 5262: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0171 - accuracy: 0.9936 - val_loss: 1.1151 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5263: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5263/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9949\n",
            "Epoch 5263: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 1.1115 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5264: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5264/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9941\n",
            "Epoch 5264: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 1.1031 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5265: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5265/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9931\n",
            "Epoch 5265: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0175 - accuracy: 0.9931 - val_loss: 1.1001 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5266: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5266/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9943\n",
            "Epoch 5266: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.0177 - accuracy: 0.9943 - val_loss: 1.0919 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5267: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5267/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9946\n",
            "Epoch 5267: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0181 - accuracy: 0.9946 - val_loss: 1.0875 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5268: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5268/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9931\n",
            "Epoch 5268: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 1.0899 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5269: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5269/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9954\n",
            "Epoch 5269: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 1.0959 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5270: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5270/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9949\n",
            "Epoch 5270: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 1.0964 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5271: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5271/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9951\n",
            "Epoch 5271: loss did not improve from 0.01384\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 1.0922 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5272: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5272/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9969\n",
            "Epoch 5272: loss improved from 0.01384 to 0.01218, saving model to ./model_PID_3D_5272_loss_0.012_vloss_1.093_acc_0.997_vacc_0.966.hdf5\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 1.0929 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5273: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5273/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9956\n",
            "Epoch 5273: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 1.0893 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5274: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5274/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9964\n",
            "Epoch 5274: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0126 - accuracy: 0.9964 - val_loss: 1.0945 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5275: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5275/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9936\n",
            "Epoch 5275: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.0171 - accuracy: 0.9936 - val_loss: 1.1031 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5276: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5276/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9951\n",
            "Epoch 5276: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0164 - accuracy: 0.9951 - val_loss: 1.1061 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5277: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5277/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9949\n",
            "Epoch 5277: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 1.1002 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5278: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5278/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9954\n",
            "Epoch 5278: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 1.0964 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5279: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5279/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9918\n",
            "Epoch 5279: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 1.1040 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5280: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5280/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9936\n",
            "Epoch 5280: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 363ms/step - loss: 0.0193 - accuracy: 0.9936 - val_loss: 1.0974 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5281: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5281/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9936\n",
            "Epoch 5281: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 1.0934 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5282: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5282/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9907\n",
            "Epoch 5282: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.0239 - accuracy: 0.9907 - val_loss: 1.0846 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5283: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5283/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9897\n",
            "Epoch 5283: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 388ms/step - loss: 0.0287 - accuracy: 0.9897 - val_loss: 1.0704 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5284: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5284/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9943\n",
            "Epoch 5284: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0186 - accuracy: 0.9943 - val_loss: 1.0731 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5285: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5285/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9946\n",
            "Epoch 5285: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0184 - accuracy: 0.9946 - val_loss: 1.0910 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5286: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5286/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9887\n",
            "Epoch 5286: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.0274 - accuracy: 0.9887 - val_loss: 1.0782 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5287: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5287/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9936\n",
            "Epoch 5287: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 1.0754 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5288: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5288/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9918\n",
            "Epoch 5288: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0182 - accuracy: 0.9918 - val_loss: 1.0713 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5289: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5289/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9936\n",
            "Epoch 5289: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.0223 - accuracy: 0.9936 - val_loss: 1.0648 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5290: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5290/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9941\n",
            "Epoch 5290: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.0173 - accuracy: 0.9941 - val_loss: 1.0703 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5291: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5291/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928\n",
            "Epoch 5291: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 383ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 1.0742 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5292: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5292/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9931\n",
            "Epoch 5292: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 385ms/step - loss: 0.0234 - accuracy: 0.9931 - val_loss: 1.0697 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5293: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5293/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9946\n",
            "Epoch 5293: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.0199 - accuracy: 0.9946 - val_loss: 1.0600 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5294: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5294/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9943\n",
            "Epoch 5294: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 1.0578 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5295: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5295/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9938\n",
            "Epoch 5295: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0163 - accuracy: 0.9938 - val_loss: 1.0618 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5296: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5296/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9951\n",
            "Epoch 5296: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 1.0726 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5297: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5297/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9956\n",
            "Epoch 5297: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 341ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 1.0796 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5298: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5298/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9954\n",
            "Epoch 5298: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 1.0821 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5299: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5299/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9946\n",
            "Epoch 5299: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 364ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 1.0771 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5300: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5300/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9933\n",
            "Epoch 5300: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 0.0182 - accuracy: 0.9933 - val_loss: 1.0884 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5301: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5301/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9956\n",
            "Epoch 5301: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.0166 - accuracy: 0.9956 - val_loss: 1.0881 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5302: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5302/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9933\n",
            "Epoch 5302: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 0.0197 - accuracy: 0.9933 - val_loss: 1.0846 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5303: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5303/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9946\n",
            "Epoch 5303: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 1.0833 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5304: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5304/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9943\n",
            "Epoch 5304: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.0175 - accuracy: 0.9943 - val_loss: 1.0888 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5305: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5305/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9949\n",
            "Epoch 5305: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.0164 - accuracy: 0.9949 - val_loss: 1.0965 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5306: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5306/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9959\n",
            "Epoch 5306: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0157 - accuracy: 0.9959 - val_loss: 1.0948 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5307: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5307/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9946\n",
            "Epoch 5307: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 1.0886 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5308: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5308/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9941\n",
            "Epoch 5308: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0167 - accuracy: 0.9941 - val_loss: 1.0806 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5309: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5309/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9951\n",
            "Epoch 5309: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 1.0716 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5310: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5310/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9938\n",
            "Epoch 5310: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0176 - accuracy: 0.9938 - val_loss: 1.0680 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5311: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5311/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9946\n",
            "Epoch 5311: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 1.0751 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5312: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5312/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9949\n",
            "Epoch 5312: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0171 - accuracy: 0.9949 - val_loss: 1.0692 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5313: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5313/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9954\n",
            "Epoch 5313: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 1.0649 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5314: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5314/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9949\n",
            "Epoch 5314: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0155 - accuracy: 0.9949 - val_loss: 1.0608 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5315: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5315/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9946\n",
            "Epoch 5315: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 1.0574 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5316: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5316/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9951\n",
            "Epoch 5316: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 1.0656 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5317: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5317/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9949\n",
            "Epoch 5317: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 1.0666 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5318: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5318/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9941\n",
            "Epoch 5318: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0186 - accuracy: 0.9941 - val_loss: 1.0552 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5319: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5319/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9954\n",
            "Epoch 5319: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 1.0489 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5320: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5320/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9915\n",
            "Epoch 5320: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.0192 - accuracy: 0.9915 - val_loss: 1.0461 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5321: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5321/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9931\n",
            "Epoch 5321: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 1.0383 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5322: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5322/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9946\n",
            "Epoch 5322: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0170 - accuracy: 0.9946 - val_loss: 1.0377 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5323: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5323/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9943\n",
            "Epoch 5323: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0156 - accuracy: 0.9943 - val_loss: 1.0338 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5324: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5324/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9946\n",
            "Epoch 5324: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 1.0277 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5325: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5325/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9941\n",
            "Epoch 5325: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0184 - accuracy: 0.9941 - val_loss: 1.0354 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5326: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5326/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9933\n",
            "Epoch 5326: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 0.0162 - accuracy: 0.9933 - val_loss: 1.0557 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5327: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5327/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9943\n",
            "Epoch 5327: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 1.0585 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5328: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5328/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9941\n",
            "Epoch 5328: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0163 - accuracy: 0.9941 - val_loss: 1.0506 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5329: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5329/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9925\n",
            "Epoch 5329: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0179 - accuracy: 0.9925 - val_loss: 1.0462 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5330: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5330/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9951\n",
            "Epoch 5330: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 1.0490 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5331: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5331/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9943\n",
            "Epoch 5331: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0184 - accuracy: 0.9943 - val_loss: 1.0566 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5332: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5332/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9949\n",
            "Epoch 5332: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0180 - accuracy: 0.9949 - val_loss: 1.0606 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5333: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5333/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9956\n",
            "Epoch 5333: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0147 - accuracy: 0.9956 - val_loss: 1.0669 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5334: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5334/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9933\n",
            "Epoch 5334: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 0.0172 - accuracy: 0.9933 - val_loss: 1.0568 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5335: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5335/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9931\n",
            "Epoch 5335: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0185 - accuracy: 0.9931 - val_loss: 1.0592 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5336: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5336/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9943\n",
            "Epoch 5336: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0160 - accuracy: 0.9943 - val_loss: 1.0576 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5337: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5337/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9946\n",
            "Epoch 5337: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 1.0580 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5338: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5338/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9956\n",
            "Epoch 5338: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0157 - accuracy: 0.9956 - val_loss: 1.0572 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5339: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5339/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9949\n",
            "Epoch 5339: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 1.0594 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5340: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5340/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9946\n",
            "Epoch 5340: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 1.0638 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5341: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5341/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9951\n",
            "Epoch 5341: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 1.0653 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5342: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5342/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9925\n",
            "Epoch 5342: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 0.0191 - accuracy: 0.9925 - val_loss: 1.0626 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5343: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5343/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9959\n",
            "Epoch 5343: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0151 - accuracy: 0.9959 - val_loss: 1.0609 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5344: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5344/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9951\n",
            "Epoch 5344: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.0171 - accuracy: 0.9951 - val_loss: 1.0593 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5345: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5345/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9938\n",
            "Epoch 5345: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0181 - accuracy: 0.9938 - val_loss: 1.0725 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5346: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5346/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9959\n",
            "Epoch 5346: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 1.0853 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5347: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5347/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9936\n",
            "Epoch 5347: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0213 - accuracy: 0.9936 - val_loss: 1.0856 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5348: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5348/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9956\n",
            "Epoch 5348: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0157 - accuracy: 0.9956 - val_loss: 1.0740 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5349: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5349/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9936\n",
            "Epoch 5349: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0160 - accuracy: 0.9936 - val_loss: 1.0718 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5350: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5350/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9946\n",
            "Epoch 5350: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 1.0740 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5351: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5351/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9974\n",
            "Epoch 5351: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0123 - accuracy: 0.9974 - val_loss: 1.0712 - val_accuracy: 0.9646 - lr: 0.0100\n",
            "\n",
            "Epoch 5352: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5352/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9951\n",
            "Epoch 5352: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 1.0620 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5353: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5353/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9964\n",
            "Epoch 5353: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0125 - accuracy: 0.9964 - val_loss: 1.0565 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5354: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5354/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9961\n",
            "Epoch 5354: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 1.0536 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5355: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5355/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9972\n",
            "Epoch 5355: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.0141 - accuracy: 0.9972 - val_loss: 1.0576 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5356: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5356/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9951\n",
            "Epoch 5356: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 1.0662 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 5357: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5357/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9961\n",
            "Epoch 5357: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 1.0594 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5358: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5358/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9943\n",
            "Epoch 5358: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 1.0622 - val_accuracy: 0.9688 - lr: 0.0100\n",
            "\n",
            "Epoch 5359: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5359/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9956\n",
            "Epoch 5359: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 0.0138 - accuracy: 0.9956 - val_loss: 1.0663 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 5360: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5360/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9967\n",
            "Epoch 5360: loss did not improve from 0.01218\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 1.0714 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5361: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5361/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9972\n",
            "Epoch 5361: loss improved from 0.01218 to 0.01034, saving model to ./model_PID_3D_5361_loss_0.010_vloss_1.073_acc_0.997_vacc_0.967.hdf5\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 0.0103 - accuracy: 0.9972 - val_loss: 1.0725 - val_accuracy: 0.9670 - lr: 0.0100\n",
            "\n",
            "Epoch 5362: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5362/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9967\n",
            "Epoch 5362: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0133 - accuracy: 0.9967 - val_loss: 1.0702 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 5363: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5363/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9964\n",
            "Epoch 5363: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0159 - accuracy: 0.9964 - val_loss: 1.0653 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5364: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5364/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9946\n",
            "Epoch 5364: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 1.0705 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5365: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5365/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9954\n",
            "Epoch 5365: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 1.0815 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5366: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5366/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9946\n",
            "Epoch 5366: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0176 - accuracy: 0.9946 - val_loss: 1.0863 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5367: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5367/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9956\n",
            "Epoch 5367: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 1.0834 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5368: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5368/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9964\n",
            "Epoch 5368: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.0134 - accuracy: 0.9964 - val_loss: 1.0830 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5369: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5369/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9920\n",
            "Epoch 5369: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 0.0190 - accuracy: 0.9920 - val_loss: 1.0971 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5370: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5370/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9931\n",
            "Epoch 5370: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 0.0242 - accuracy: 0.9931 - val_loss: 1.0921 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5371: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5371/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9879\n",
            "Epoch 5371: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 0.0271 - accuracy: 0.9879 - val_loss: 1.0900 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5372: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5372/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9941\n",
            "Epoch 5372: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0197 - accuracy: 0.9941 - val_loss: 1.0891 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5373: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5373/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9951\n",
            "Epoch 5373: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 1.0915 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5374: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5374/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9920\n",
            "Epoch 5374: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0213 - accuracy: 0.9920 - val_loss: 1.1087 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5375: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5375/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928\n",
            "Epoch 5375: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 1.0912 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5376: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5376/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9918\n",
            "Epoch 5376: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0186 - accuracy: 0.9918 - val_loss: 1.0805 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5377: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5377/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9933\n",
            "Epoch 5377: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0178 - accuracy: 0.9933 - val_loss: 1.0792 - val_accuracy: 0.9640 - lr: 0.0100\n",
            "\n",
            "Epoch 5378: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5378/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9923\n",
            "Epoch 5378: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 1.0780 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5379: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5379/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9936\n",
            "Epoch 5379: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0187 - accuracy: 0.9936 - val_loss: 1.0841 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5380: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5380/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9951\n",
            "Epoch 5380: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 1.0810 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5381: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5381/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9938\n",
            "Epoch 5381: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0183 - accuracy: 0.9938 - val_loss: 1.0668 - val_accuracy: 0.9634 - lr: 0.0100\n",
            "\n",
            "Epoch 5382: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5382/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9938\n",
            "Epoch 5382: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.0175 - accuracy: 0.9938 - val_loss: 1.0641 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5383: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5383/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9913\n",
            "Epoch 5383: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0218 - accuracy: 0.9913 - val_loss: 1.0739 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5384: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5384/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9936\n",
            "Epoch 5384: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.0181 - accuracy: 0.9936 - val_loss: 1.0840 - val_accuracy: 0.9664 - lr: 0.0100\n",
            "\n",
            "Epoch 5385: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5385/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9910\n",
            "Epoch 5385: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.0247 - accuracy: 0.9910 - val_loss: 1.0703 - val_accuracy: 0.9658 - lr: 0.0100\n",
            "\n",
            "Epoch 5386: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5386/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9943\n",
            "Epoch 5386: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 1.0641 - val_accuracy: 0.9652 - lr: 0.0100\n",
            "\n",
            "Epoch 5387: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5387/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9959\n",
            "Epoch 5387: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 0.0169 - accuracy: 0.9959 - val_loss: 1.0814 - val_accuracy: 0.9616 - lr: 0.0100\n",
            "\n",
            "Epoch 5388: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5388/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9895\n",
            "Epoch 5388: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 1.0991 - val_accuracy: 0.9460 - lr: 0.0100\n",
            "\n",
            "Epoch 5389: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5389/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9861\n",
            "Epoch 5389: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.0366 - accuracy: 0.9861 - val_loss: 1.0902 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5390: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5390/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9879\n",
            "Epoch 5390: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 0.0311 - accuracy: 0.9879 - val_loss: 1.0751 - val_accuracy: 0.9610 - lr: 0.0100\n",
            "\n",
            "Epoch 5391: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5391/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9931\n",
            "Epoch 5391: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 1.0963 - val_accuracy: 0.9448 - lr: 0.0100\n",
            "\n",
            "Epoch 5392: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5392/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9830\n",
            "Epoch 5392: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 0.0410 - accuracy: 0.9830 - val_loss: 1.0946 - val_accuracy: 0.9448 - lr: 0.0100\n",
            "\n",
            "Epoch 5393: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5393/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9763\n",
            "Epoch 5393: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 0.0581 - accuracy: 0.9763 - val_loss: 1.1009 - val_accuracy: 0.9442 - lr: 0.0100\n",
            "\n",
            "Epoch 5394: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5394/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9799\n",
            "Epoch 5394: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.0479 - accuracy: 0.9799 - val_loss: 1.0777 - val_accuracy: 0.9676 - lr: 0.0100\n",
            "\n",
            "Epoch 5395: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5395/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9915\n",
            "Epoch 5395: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 0.0233 - accuracy: 0.9915 - val_loss: 1.0869 - val_accuracy: 0.9604 - lr: 0.0100\n",
            "\n",
            "Epoch 5396: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5396/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9856\n",
            "Epoch 5396: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.0476 - accuracy: 0.9856 - val_loss: 1.0621 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5397: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5397/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9897\n",
            "Epoch 5397: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 0.0249 - accuracy: 0.9897 - val_loss: 1.0817 - val_accuracy: 0.9436 - lr: 0.0100\n",
            "\n",
            "Epoch 5398: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5398/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9817\n",
            "Epoch 5398: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 0.0468 - accuracy: 0.9817 - val_loss: 1.0463 - val_accuracy: 0.9628 - lr: 0.0100\n",
            "\n",
            "Epoch 5399: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5399/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9913\n",
            "Epoch 5399: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0307 - accuracy: 0.9913 - val_loss: 1.0499 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5400: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5400/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9866\n",
            "Epoch 5400: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 228ms/step - loss: 0.0370 - accuracy: 0.9866 - val_loss: 1.1284 - val_accuracy: 0.9388 - lr: 0.0100\n",
            "\n",
            "Epoch 5401: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5401/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9748\n",
            "Epoch 5401: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0676 - accuracy: 0.9748 - val_loss: 1.0732 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5402: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5402/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9828\n",
            "Epoch 5402: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 0.0471 - accuracy: 0.9828 - val_loss: 1.0753 - val_accuracy: 0.9598 - lr: 0.0100\n",
            "\n",
            "Epoch 5403: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5403/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9848\n",
            "Epoch 5403: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.0587 - accuracy: 0.9848 - val_loss: 1.0703 - val_accuracy: 0.9562 - lr: 0.0100\n",
            "\n",
            "Epoch 5404: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5404/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9794\n",
            "Epoch 5404: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 0.0567 - accuracy: 0.9794 - val_loss: 1.0548 - val_accuracy: 0.9574 - lr: 0.0100\n",
            "\n",
            "Epoch 5405: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5405/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9797\n",
            "Epoch 5405: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 0.0530 - accuracy: 0.9797 - val_loss: 1.0358 - val_accuracy: 0.9586 - lr: 0.0100\n",
            "\n",
            "Epoch 5406: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5406/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9833\n",
            "Epoch 5406: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0442 - accuracy: 0.9833 - val_loss: 1.0462 - val_accuracy: 0.9538 - lr: 0.0100\n",
            "\n",
            "Epoch 5407: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5407/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9828\n",
            "Epoch 5407: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.0518 - accuracy: 0.9828 - val_loss: 1.0529 - val_accuracy: 0.9562 - lr: 0.0100\n",
            "\n",
            "Epoch 5408: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5408/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9825\n",
            "Epoch 5408: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 0.0473 - accuracy: 0.9825 - val_loss: 1.0567 - val_accuracy: 0.9574 - lr: 0.0100\n",
            "\n",
            "Epoch 5409: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5409/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9843\n",
            "Epoch 5409: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 0.0457 - accuracy: 0.9843 - val_loss: 1.0542 - val_accuracy: 0.9550 - lr: 0.0100\n",
            "\n",
            "Epoch 5410: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5410/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9841\n",
            "Epoch 5410: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.0450 - accuracy: 0.9841 - val_loss: 1.0506 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "\n",
            "Epoch 5411: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5411/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9846\n",
            "Epoch 5411: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 0.0456 - accuracy: 0.9846 - val_loss: 1.0459 - val_accuracy: 0.9538 - lr: 0.0100\n",
            "\n",
            "Epoch 5412: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5412/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9843\n",
            "Epoch 5412: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.0429 - accuracy: 0.9843 - val_loss: 1.0362 - val_accuracy: 0.9550 - lr: 0.0100\n",
            "\n",
            "Epoch 5413: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5413/5500\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9859\n",
            "Epoch 5413: loss did not improve from 0.01034\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 0.0348 - accuracy: 0.9859 - val_loss: 1.0406 - val_accuracy: 0.9622 - lr: 0.0100\n",
            "\n",
            "Epoch 5414: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 5414/5500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-4cb890618aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__learning__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_epochs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_epochs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __learning__: \n",
        "    history = model.fit(X_train, y_train, epochs=_epochs_, batch_size=_epochs_, validation_data=(X_test, y_test),verbose=1,callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=True\n",
        "model_file=\"model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5\"\n",
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model3/\"+model_file"
      ],
      "metadata": {
        "id": "EGg1PjCJDTKF"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "JgzklVywoNmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "11235797-ab49-4490-a3cf-87a0e6bdd332"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-16 15:30:13--  https://github.com/sipocz/pid_time_series/raw/main/model3/model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/pid_time_series/main/model3/model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5 [following]\n",
            "--2023-03-16 15:30:13--  https://raw.githubusercontent.com/sipocz/pid_time_series/main/model3/model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 207192 (202K) [application/octet-stream]\n",
            "Saving to: ‘model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5’\n",
            "\n",
            "model_PID__0634_los 100%[===================>] 202.34K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-16 15:30:13 (8.36 MB/s) - ‘model_PID__0634_loss_0.086_vloss_1.253_acc_0.961_vacc_0.886.hdf5’ saved [207192/207192]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-3299eb440fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' rm *.hdf5 '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' wget $model_url'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   4300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4301\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4302\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4303\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot assign value to variable ' dense/kernel:0': Shape mismatch.The variable shape (60, 700), and the assigned value shape (20, 234) are incompatible."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwcWQ94IpDFu"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bina_transformer=Binarizer(threshold=0.5)\n",
        "y_pred_transform=bina_transformer.fit_transform(y_pred)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_test,y_pred_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "id": "Z69kCq3T-pMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ0rmkNsBGnl"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve( y_pred_transform,y_test,pos_label=1)\n",
        "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "auc = roc_auc_score(y_test, y_pred_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_normalized"
      ],
      "metadata": {
        "id": "sNIc1l6vF6Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_changer(arr):\n",
        "    o=[\"r\" if i>0.5 else \"g\" for i in arr]\n",
        "    return o"
      ],
      "metadata": {
        "id": "YFJoZO8TG1ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotgraf(df_in, predicted):\n",
        "    xkoordinata=[i for i in range(len(df_in[\"0\"]))]\n",
        "    plot.figure(figsize=(12,6))\n",
        "    col_ch=color_changer(predicted)\n",
        "    plot.scatter(xkoordinata,df_in[\"0\"],c=col_ch,marker=\".\",alpha=0.3)\n",
        "    plot.ylabel('értékek')\n",
        "    plot.xlabel('index')\n",
        "    plot\n",
        "    plot.show()"
      ],
      "metadata": {
        "id": "YMHy-wbZGeqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_all_predict"
      ],
      "metadata": {
        "id": "HuL1OutGHHEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_all_predict=model.predict(df_all_normalized[prediktorok])"
      ],
      "metadata": {
        "id": "4aXpzheKE7bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_predict=df_y_all_predict.round()"
      ],
      "metadata": {
        "id": "70M96KLSdCHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_predict"
      ],
      "metadata": {
        "id": "e72-bHKzdZve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotgraf(df_all_normalized[400:600],df_y_predict[400:600])"
      ],
      "metadata": {
        "id": "BO3xdrHVGXhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "id": "WMRd5eGU9ASA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_df=pd.DataFrame({\"epoch\":history.epoch, \"loss\":history.history[\"loss\"],\"val_loss\":history.history[\"val_loss\"]})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "id": "4ENvDCA-0U1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLYd3b9INdKjhvV5MkveG3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}