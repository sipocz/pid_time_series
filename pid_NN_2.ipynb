{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/pid_time_series/blob/main/pid_NN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tNYnFR-6Xh",
        "outputId": "e71993bd-21b5-4ed2-e7da-b379a29148e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.7-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 19.8 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.12.0-py2.py3-none-any.whl (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 24.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 11.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 75.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 64.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 55.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 12.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 32.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 68.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 68.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 61.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 52.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 67.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=df0db4f9803f3c69e724e0fa9dcc5d52f409d148cc4b8e1b14d87db49261ec2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OWFIUUUGKGdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ag6zIuPmKTux"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqJiGk7KW_4",
        "outputId": "8d45361c-f8ca-4819-f396-9093dc6bb01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-_usNw7yKZDt"
      },
      "outputs": [],
      "source": [
        "#user = \"Anna\"\n",
        "user = \"SL\"\n",
        "uzem = \"Szint1\"\n",
        "data_source=\"5\"\n",
        "#fname=\"72C03_TC_error_toNN.csv\"\n",
        "fname_good = \"415_SC_error_part1.csv\"\n",
        "fname_bad = \"415_SC_error_part2.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OkO7F6NaKbxi"
      },
      "outputs": [],
      "source": [
        "# Elérési út a 415_SC_error-hoz\n",
        "if user==\"Anna\":\n",
        "    path_good = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/Egyetem_MSc/Diplomamunka/2022Anna/Datapipeline/plots/\"\n",
        "else:\n",
        "    path_good = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_good\n",
        "    path_bad = \"/content/drive/MyDrive/2022Anna/Datapipeline/\" + data_source + \"/\" + fname_bad\n",
        "    path_fig = \"/content/drive/MyDrive/2022Anna/Datapipeline/plots/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5ZDDiY9KfAQ",
        "outputId": "18a3f2ec-bc53-44fe-c358-1c7e34ee5481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part1.csv\n",
            "/content/drive/MyDrive/2022Anna/Datapipeline/5/415_SC_error_part2.csv\n"
          ]
        }
      ],
      "source": [
        "print(path_good)\n",
        "print(path_bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vUcMjZAGKvtt"
      },
      "outputs": [],
      "source": [
        "df_good = pd.read_csv(path_good,usecols=None)\n",
        "df_bad = pd.read_csv(path_bad,usecols=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYuDXKraLOt4",
        "outputId": "a7c98eb0-50f4-49dc-f042-444f5584bed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(df_good.isnull().values.any())\n",
        "print(df_bad.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "vzl5zIO1LUoq",
        "outputId": "b2fbd11f-df15-418f-cb23-fc241f4a6be3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0          1          2          3          4        5         6  \\\n",
              "0 -54.810024 -80.342186 -60.770203 -41.081482 -21.779583 -3.82353 -0.806820   \n",
              "1 -80.342186 -60.770203 -41.081482 -21.779583  -3.823530 -0.80682  0.220875   \n",
              "\n",
              "          7         8         9        10        11        12        13  \\\n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875   \n",
              "\n",
              "         14        15        16        17        18        19  \n",
              "0  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  \n",
              "1  0.220875  0.220875  0.220875  0.220875  0.220875  0.220875  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0458bad4-6086-4e22-96c5-ad907183b6e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-54.810024</td>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.82353</td>\n",
              "      <td>-0.806820</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-80.342186</td>\n",
              "      <td>-60.770203</td>\n",
              "      <td>-41.081482</td>\n",
              "      <td>-21.779583</td>\n",
              "      <td>-3.823530</td>\n",
              "      <td>-0.80682</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "      <td>0.220875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0458bad4-6086-4e22-96c5-ad907183b6e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0458bad4-6086-4e22-96c5-ad907183b6e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0458bad4-6086-4e22-96c5-ad907183b6e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df_good.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f0xJfadFMOfA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hIMQw2sULmj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "df_ = df_good\n",
        "\n",
        "# You must normalize the data before applying the fit method\n",
        "df_good_normalized=(df_ - df_.mean()) / df_.std()\n",
        "\n",
        "# Normalize bad data with the good data parameters\n",
        "df_bad_normalized=(df_bad - df_.mean()) / df_.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wknFhIRBNQ7k"
      },
      "outputs": [],
      "source": [
        "df_good_normalized[\"state\"]=0\n",
        "df_bad_normalized[\"state\"]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3W5mi70VM6hL",
        "outputId": "415f0c63-b499-4e56-a427-03e61a02c046"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0          1          2          3         4         5  \\\n",
              "0    -10.681306 -16.586266 -14.612051 -11.087981 -6.293341 -1.192618   \n",
              "1    -15.654548 -12.549683  -9.889987  -5.905180 -1.164099 -0.314249   \n",
              "2    -11.842250  -8.489023  -5.260696  -1.083756 -0.302359 -0.015017   \n",
              "3     -8.007214  -4.508142  -0.954188  -0.273732 -0.008793 -0.015017   \n",
              "4     -4.247524  -0.804833  -0.230672   0.002217 -0.008793 -0.015017   \n",
              "...         ...        ...        ...        ...       ...       ...   \n",
              "1053   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1054   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1055   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1056   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "1057   0.037801   0.029297   0.015806   0.002217 -0.008793 -0.015017   \n",
              "\n",
              "             6         7         8         9  ...        11        12  \\\n",
              "0    -0.315574 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "2    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "3    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "4    -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...  ...       ...       ...   \n",
              "1053 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1054 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1055 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1056 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "1057 -0.016141 -0.016425 -0.016425 -0.016425  ... -0.016425 -0.016425   \n",
              "\n",
              "            13        14        15        16        17        18        19  \\\n",
              "0    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "2    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "3    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "4    -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1053 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1054 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1055 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1056 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "1057 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425 -0.016425   \n",
              "\n",
              "      state  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  \n",
              "...     ...  \n",
              "1053      0  \n",
              "1054      0  \n",
              "1055      0  \n",
              "1056      0  \n",
              "1057      0  \n",
              "\n",
              "[1058 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-432e72de-6e05-4a12-bc1a-c073a567792e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10.681306</td>\n",
              "      <td>-16.586266</td>\n",
              "      <td>-14.612051</td>\n",
              "      <td>-11.087981</td>\n",
              "      <td>-6.293341</td>\n",
              "      <td>-1.192618</td>\n",
              "      <td>-0.315574</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-15.654548</td>\n",
              "      <td>-12.549683</td>\n",
              "      <td>-9.889987</td>\n",
              "      <td>-5.905180</td>\n",
              "      <td>-1.164099</td>\n",
              "      <td>-0.314249</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-11.842250</td>\n",
              "      <td>-8.489023</td>\n",
              "      <td>-5.260696</td>\n",
              "      <td>-1.083756</td>\n",
              "      <td>-0.302359</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-8.007214</td>\n",
              "      <td>-4.508142</td>\n",
              "      <td>-0.954188</td>\n",
              "      <td>-0.273732</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.247524</td>\n",
              "      <td>-0.804833</td>\n",
              "      <td>-0.230672</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>0.037801</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>-0.008793</td>\n",
              "      <td>-0.015017</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>-0.016425</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1058 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-432e72de-6e05-4a12-bc1a-c073a567792e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-432e72de-6e05-4a12-bc1a-c073a567792e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-432e72de-6e05-4a12-bc1a-c073a567792e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df_good_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9nY0OMtYPT8J"
      },
      "outputs": [],
      "source": [
        "df_all_normalized=pd.concat([df_good_normalized,df_bad_normalized],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ClfUnwBRPwgK",
        "outputId": "b93b8ad3-94a6-4e57-cfea-e341711f7e04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "1263  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1264  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1265  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1266  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "1267  0.029553  0.020564  0.005651 -0.009153 -0.020888 -0.027346 -0.028478   \n",
              "\n",
              "             7         8         9  ...        11        12        13  \\\n",
              "1263 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1264 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1265 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1266 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "1267 -0.028763 -0.028763 -0.028763  ... -0.028763 -0.028763 -0.028763   \n",
              "\n",
              "            14        15        16        17        18        19  state  \n",
              "1263 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1264 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1265 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1266 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "1267 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763 -0.028763      1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38fc9f15-9f3e-4210-95fd-e0ff6ea63f10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1263</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1265</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.020564</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>-0.009153</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.027346</td>\n",
              "      <td>-0.028478</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>-0.028763</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38fc9f15-9f3e-4210-95fd-e0ff6ea63f10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38fc9f15-9f3e-4210-95fd-e0ff6ea63f10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38fc9f15-9f3e-4210-95fd-e0ff6ea63f10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_all_normalized.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "nVvhP84S_F1y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_N1_=134\n",
        "_N2_=15\n",
        "_lr_=0.001\n",
        "_batch_size_=1\n",
        "_drop1_=0.5\n",
        "_drop2_=0.1\n"
      ],
      "metadata": {
        "id": "XC5_bGE0iyi4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"lr\": _lr_, \"batch_size\": _batch_size_,\"architecture\": \"NN\", \n",
        "          \"depth\": 2,\n",
        "          \"layer1\":_N1_,  \"layer2\":_N2_, \n",
        "          \"drop1\":_drop1_,\"drop2\":_drop1_\n",
        "          \n",
        "          \n",
        "          }\n",
        "\n",
        "wandb.init(project=\"pid_1\", entity=\"sipoczlaszlo\",config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482,
          "referenced_widgets": [
            "18f72c05c3994f66a54bbaf3091308be",
            "2d1973648af446f38f74919bb5c41cef",
            "7a61f7954c4c4c94a1b7eee21032e259",
            "7e689a6d32bc4ee2be6ef277d9df2927",
            "59d4e8bc6b154935bfb8a9ea4a0fd573",
            "0e604228deab4c0b8b7e861b1e3a26ac",
            "cd3e37ae408541e4adb2c670bfcf8564",
            "11d9a50a2c264df09b9140aeeb8bc5c0"
          ]
        },
        "id": "nOtKllcviuoj",
        "outputId": "89ec7d1a-6839-41db-d0bd-8614cf4860cc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:1kwcjis9) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18f72c05c3994f66a54bbaf3091308be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇▇██████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▂▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▅▅▄▄▅▅▅▆▆▆▆▆▆▆▇▆▇▇▆▇▇▇▇▇█▇▇▇██▇████▇██▇</td></tr><tr><td>epoch/val_loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.83184</td></tr><tr><td>epoch/epoch</td><td>367</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.40587</td></tr><tr><td>epoch/val_accuracy</td><td>0.79042</td></tr><tr><td>epoch/val_loss</td><td>0.48214</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">youthful-plasma-16</strong>: <a href=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/1kwcjis9\" target=\"_blank\">https://wandb.ai/sipoczlaszlo/pid_1/runs/1kwcjis9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221217_221802-1kwcjis9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:1kwcjis9). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221217_225257-1dhm4644</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/1dhm4644\" target=\"_blank\">rose-smoke-17</a></strong> to <a href=\"https://wandb.ai/sipoczlaszlo/pid_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sipoczlaszlo/pid_1/runs/1dhm4644?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f01b6e70d60>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rcPrX4lWP2R_"
      },
      "outputs": [],
      "source": [
        "from keras.engine.base_layer import regularizers\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import SGD,Adam,Adamax,Nadam,Ftrl,Adadelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "clear_session()\n",
        "\n",
        "kernel_reg_1=tf.keras.regularizers.L2(0.1)\n",
        "\n",
        "input_size=20\n",
        "\n",
        "\n",
        "input1=Input(shape=(input_size,))\n",
        "l1_out=Dense(_N1_,activation=\"swish\",kernel_initializer='glorot_uniform',)(input1) # kernel_initializer='lecun_normal'\n",
        "l2_out=Dropout(_drop1_)(l1_out)\n",
        "\n",
        "\n",
        "l3_out=Dense(_N2_,activation=\"swish\",kernel_initializer='glorot_uniform',)(l2_out) #kernel_initializer='lecun_normal',\n",
        "l4_out=Dropout(_drop2_)(l3_out)\n",
        "\n",
        "pred=Dense(1, activation=\"sigmoid\",)(l4_out)\n",
        "\n",
        "model = Model(inputs=input1, outputs=pred)\n",
        "optimizer=Adamax(learning_rate=_lr_,) #\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "yLzRRMnbIk9X"
      },
      "outputs": [],
      "source": [
        "# 35 5 1 relu relu sigmoid SGD 0.01 loss: 0.1402 - accuracy: 0.9435 - val_loss: 0.7302 - val_accuracy: 0.8548\n",
        "# 35 12 1 relu relu sigmoid SGD 0.01 loss 0.1162 94.6% test : 85%\n",
        "# 17 5 1 relu relu sigmoid SGD 0.01  loss: 0.1714 - accuracy: 0.9300 - val_loss: 0.9535 - val_accuracy: 0.8503\n",
        "# 35 5 1 relu relu sigmoid Adam 0.01 loss: 0.1238 - accuracy: 0.9467 - val_loss: 5.7545 - val_accuracy: 0.8653\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.01 loss: 0.1184 - accuracy: 0.9525 - val_loss: 3.5327 - val_accuracy: 0.8428\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1185 - accuracy: 0.9525 - val_loss: 2.3218 - val_accuracy: 0.8593\n",
        "# 35 5 1 relu relu sigmoid Adamax 0.001 loss: 0.1041 - accuracy: 0.9576 - val_loss: 5.1465 - val_accuracy: 0.8353  +1300 epoch \n",
        "# 135 15 1 swish swish sigmoid Adamax 0.001 batch size:1 epoch 100 loss: 0.1707 - accuracy: 0.9352 - val_loss: 0.8066 - val_accuracy: 0.8892   **** egész jó\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RGIztQ3tQ3ni"
      },
      "outputs": [],
      "source": [
        "prediktorok=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\"]\n",
        "X_NN=df_all_normalized[prediktorok][:-100]  # \n",
        "y_NN=df_all_normalized[\"state\"][:-100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_file=\"model_PID__54_loss_0.116_vloss_0.115_acc_0.953_vacc_0.958.hdf5\"\n",
        "#model_file=\"model_PID__94_loss_0.116_vloss_0.115_acc_0.950_vacc_0.966.hdf5\""
      ],
      "metadata": {
        "id": "DgjVCU185nNO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url=\"https://github.com/sipocz/pid_time_series/raw/main/model/\"+model_file"
      ],
      "metadata": {
        "id": "iUhe0_4L5ufk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__load_file__=False"
      ],
      "metadata": {
        "id": "UIxI3AS6Yw3S"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __load_file__:\n",
        "    ! rm *.hdf5 \n",
        "    ! wget $model_url\n",
        "    model.load_weights(model_file)"
      ],
      "metadata": {
        "id": "ZNjx5XGesZPO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rdH49nLKRVoh"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_NN,y_NN,train_size=0.7,shuffle=True,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5 "
      ],
      "metadata": {
        "id": "jJfOOTfGfDXi"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wandb.keras import WandbMetricsLogger\n",
        "fname=\"./model_PID_\"\n",
        "callbacks = [\n",
        "        WandbMetricsLogger(),       \n",
        "        ModelCheckpoint(filepath=fname+\"_{epoch:03.0f}\"+\"_loss_{loss:.3f}_vloss_{val_loss:.3f}_acc_{accuracy:.3f}_vacc_{val_accuracy:.3f}.hdf5\", monitor='loss',\n",
        "                        verbose=2, save_best_only=True, mode='min')]\n"
      ],
      "metadata": {
        "id": "RNfi--Kfo4HM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__learning__=True"
      ],
      "metadata": {
        "id": "O6ofy0moderd"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9Ol0mW6WRlkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dd42f8-b403-4b0c-8043-696c2d69a074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.6357 - accuracy: 0.7020\n",
            "Epoch 1: loss improved from inf to 0.63447, saving model to ./model_PID__001_loss_0.634_vloss_0.605_acc_0.702_vacc_0.725.hdf5\n",
            "1558/1558 [==============================] - 9s 5ms/step - loss: 0.6345 - accuracy: 0.7015 - val_loss: 0.6046 - val_accuracy: 0.7246\n",
            "Epoch 2/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.7449\n",
            "Epoch 2: loss improved from 0.63447 to 0.58765, saving model to ./model_PID__002_loss_0.588_vloss_0.591_acc_0.745_vacc_0.744.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5877 - accuracy: 0.7452 - val_loss: 0.5915 - val_accuracy: 0.7440\n",
            "Epoch 3/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.7503\n",
            "Epoch 3: loss improved from 0.58765 to 0.56672, saving model to ./model_PID__003_loss_0.567_vloss_0.588_acc_0.750_vacc_0.751.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5667 - accuracy: 0.7503 - val_loss: 0.5882 - val_accuracy: 0.7515\n",
            "Epoch 4/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.5360 - accuracy: 0.7666\n",
            "Epoch 4: loss improved from 0.56672 to 0.53946, saving model to ./model_PID__004_loss_0.539_vloss_0.588_acc_0.766_vacc_0.759.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5395 - accuracy: 0.7657 - val_loss: 0.5878 - val_accuracy: 0.7590\n",
            "Epoch 5/500\n",
            "1536/1558 [============================>.] - ETA: 0s - loss: 0.5509 - accuracy: 0.7721\n",
            "Epoch 5: loss did not improve from 0.53946\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5494 - accuracy: 0.7721 - val_loss: 0.5862 - val_accuracy: 0.7605\n",
            "Epoch 6/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.5166 - accuracy: 0.7808\n",
            "Epoch 6: loss improved from 0.53946 to 0.51680, saving model to ./model_PID__006_loss_0.517_vloss_0.587_acc_0.780_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5168 - accuracy: 0.7805 - val_loss: 0.5866 - val_accuracy: 0.7665\n",
            "Epoch 7/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7802\n",
            "Epoch 7: loss did not improve from 0.51680\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5226 - accuracy: 0.7805 - val_loss: 0.5870 - val_accuracy: 0.7635\n",
            "Epoch 8/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.5018 - accuracy: 0.7801\n",
            "Epoch 8: loss improved from 0.51680 to 0.50032, saving model to ./model_PID__008_loss_0.500_vloss_0.588_acc_0.782_vacc_0.763.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5003 - accuracy: 0.7818 - val_loss: 0.5879 - val_accuracy: 0.7635\n",
            "Epoch 9/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.5105 - accuracy: 0.7826\n",
            "Epoch 9: loss did not improve from 0.50032\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5106 - accuracy: 0.7818 - val_loss: 0.5888 - val_accuracy: 0.7635\n",
            "Epoch 10/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.5003 - accuracy: 0.7930\n",
            "Epoch 10: loss improved from 0.50032 to 0.49922, saving model to ./model_PID__010_loss_0.499_vloss_0.587_acc_0.794_vacc_0.765.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4992 - accuracy: 0.7940 - val_loss: 0.5871 - val_accuracy: 0.7650\n",
            "Epoch 11/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.5035 - accuracy: 0.7836\n",
            "Epoch 11: loss did not improve from 0.49922\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5069 - accuracy: 0.7837 - val_loss: 0.5821 - val_accuracy: 0.7665\n",
            "Epoch 12/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.4943 - accuracy: 0.7904\n",
            "Epoch 12: loss improved from 0.49922 to 0.49383, saving model to ./model_PID__012_loss_0.494_vloss_0.580_acc_0.791_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4938 - accuracy: 0.7908 - val_loss: 0.5800 - val_accuracy: 0.7665\n",
            "Epoch 13/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.4841 - accuracy: 0.7982\n",
            "Epoch 13: loss improved from 0.49383 to 0.48304, saving model to ./model_PID__013_loss_0.483_vloss_0.580_acc_0.798_vacc_0.765.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4830 - accuracy: 0.7985 - val_loss: 0.5804 - val_accuracy: 0.7650\n",
            "Epoch 14/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.5026 - accuracy: 0.7892\n",
            "Epoch 14: loss did not improve from 0.48304\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.5024 - accuracy: 0.7895 - val_loss: 0.5766 - val_accuracy: 0.7665\n",
            "Epoch 15/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.4787 - accuracy: 0.8025\n",
            "Epoch 15: loss improved from 0.48304 to 0.47782, saving model to ./model_PID__015_loss_0.478_vloss_0.576_acc_0.804_vacc_0.765.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4778 - accuracy: 0.8036 - val_loss: 0.5762 - val_accuracy: 0.7650\n",
            "Epoch 16/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.4892 - accuracy: 0.7918\n",
            "Epoch 16: loss did not improve from 0.47782\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.4895 - accuracy: 0.7914 - val_loss: 0.5744 - val_accuracy: 0.7695\n",
            "Epoch 17/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.4778 - accuracy: 0.7925\n",
            "Epoch 17: loss did not improve from 0.47782\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.4783 - accuracy: 0.7927 - val_loss: 0.5736 - val_accuracy: 0.7680\n",
            "Epoch 18/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.4698 - accuracy: 0.8088\n",
            "Epoch 18: loss improved from 0.47782 to 0.47145, saving model to ./model_PID__018_loss_0.471_vloss_0.576_acc_0.807_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4715 - accuracy: 0.8074 - val_loss: 0.5757 - val_accuracy: 0.7665\n",
            "Epoch 19/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4956 - accuracy: 0.7987\n",
            "Epoch 19: loss did not improve from 0.47145\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4930 - accuracy: 0.8004 - val_loss: 0.5721 - val_accuracy: 0.7650\n",
            "Epoch 20/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.4734 - accuracy: 0.7906\n",
            "Epoch 20: loss did not improve from 0.47145\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4726 - accuracy: 0.7914 - val_loss: 0.5709 - val_accuracy: 0.7680\n",
            "Epoch 21/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.4737 - accuracy: 0.8034\n",
            "Epoch 21: loss did not improve from 0.47145\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4733 - accuracy: 0.8036 - val_loss: 0.5695 - val_accuracy: 0.7680\n",
            "Epoch 22/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.4604 - accuracy: 0.8053\n",
            "Epoch 22: loss improved from 0.47145 to 0.45996, saving model to ./model_PID__022_loss_0.460_vloss_0.566_acc_0.806_vacc_0.768.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4600 - accuracy: 0.8055 - val_loss: 0.5665 - val_accuracy: 0.7680\n",
            "Epoch 23/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.4655 - accuracy: 0.8068\n",
            "Epoch 23: loss did not improve from 0.45996\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4651 - accuracy: 0.8074 - val_loss: 0.5662 - val_accuracy: 0.7680\n",
            "Epoch 24/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4660 - accuracy: 0.8123\n",
            "Epoch 24: loss did not improve from 0.45996\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4652 - accuracy: 0.8132 - val_loss: 0.5641 - val_accuracy: 0.7635\n",
            "Epoch 25/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.4703 - accuracy: 0.8036\n",
            "Epoch 25: loss did not improve from 0.45996\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4693 - accuracy: 0.8042 - val_loss: 0.5614 - val_accuracy: 0.7650\n",
            "Epoch 26/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.8004\n",
            "Epoch 26: loss did not improve from 0.45996\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4638 - accuracy: 0.8004 - val_loss: 0.5579 - val_accuracy: 0.7680\n",
            "Epoch 27/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.4700 - accuracy: 0.8009\n",
            "Epoch 27: loss did not improve from 0.45996\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4690 - accuracy: 0.8017 - val_loss: 0.5576 - val_accuracy: 0.7635\n",
            "Epoch 28/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.4690 - accuracy: 0.8077\n",
            "Epoch 28: loss did not improve from 0.45996\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4664 - accuracy: 0.8094 - val_loss: 0.5594 - val_accuracy: 0.7635\n",
            "Epoch 29/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.4674 - accuracy: 0.8057\n",
            "Epoch 29: loss did not improve from 0.45996\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4667 - accuracy: 0.8062 - val_loss: 0.5567 - val_accuracy: 0.7650\n",
            "Epoch 30/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.8164\n",
            "Epoch 30: loss improved from 0.45996 to 0.44911, saving model to ./model_PID__030_loss_0.449_vloss_0.557_acc_0.816_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4491 - accuracy: 0.8164 - val_loss: 0.5574 - val_accuracy: 0.7665\n",
            "Epoch 31/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.4614 - accuracy: 0.8041\n",
            "Epoch 31: loss did not improve from 0.44911\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4601 - accuracy: 0.8049 - val_loss: 0.5543 - val_accuracy: 0.7665\n",
            "Epoch 32/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.4485 - accuracy: 0.8107\n",
            "Epoch 32: loss improved from 0.44911 to 0.44743, saving model to ./model_PID__032_loss_0.447_vloss_0.553_acc_0.812_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4474 - accuracy: 0.8119 - val_loss: 0.5532 - val_accuracy: 0.7665\n",
            "Epoch 33/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4458 - accuracy: 0.8054\n",
            "Epoch 33: loss improved from 0.44743 to 0.44549, saving model to ./model_PID__033_loss_0.445_vloss_0.555_acc_0.806_vacc_0.763.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4455 - accuracy: 0.8055 - val_loss: 0.5550 - val_accuracy: 0.7635\n",
            "Epoch 34/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.4428 - accuracy: 0.8066\n",
            "Epoch 34: loss improved from 0.44549 to 0.44232, saving model to ./model_PID__034_loss_0.442_vloss_0.557_acc_0.807_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4423 - accuracy: 0.8068 - val_loss: 0.5573 - val_accuracy: 0.7665\n",
            "Epoch 35/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.4415 - accuracy: 0.8117\n",
            "Epoch 35: loss improved from 0.44232 to 0.44206, saving model to ./model_PID__035_loss_0.442_vloss_0.556_acc_0.811_vacc_0.763.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4421 - accuracy: 0.8107 - val_loss: 0.5561 - val_accuracy: 0.7635\n",
            "Epoch 36/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4534 - accuracy: 0.8051\n",
            "Epoch 36: loss did not improve from 0.44206\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4534 - accuracy: 0.8049 - val_loss: 0.5526 - val_accuracy: 0.7665\n",
            "Epoch 37/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4419 - accuracy: 0.8157\n",
            "Epoch 37: loss did not improve from 0.44206\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4426 - accuracy: 0.8151 - val_loss: 0.5542 - val_accuracy: 0.7665\n",
            "Epoch 38/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.8200\n",
            "Epoch 38: loss improved from 0.44206 to 0.43296, saving model to ./model_PID__038_loss_0.433_vloss_0.555_acc_0.820_vacc_0.765.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4330 - accuracy: 0.8196 - val_loss: 0.5553 - val_accuracy: 0.7650\n",
            "Epoch 39/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4376 - accuracy: 0.8089\n",
            "Epoch 39: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4377 - accuracy: 0.8087 - val_loss: 0.5526 - val_accuracy: 0.7710\n",
            "Epoch 40/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4455 - accuracy: 0.8109\n",
            "Epoch 40: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4466 - accuracy: 0.8094 - val_loss: 0.5498 - val_accuracy: 0.7710\n",
            "Epoch 41/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.4552 - accuracy: 0.8048\n",
            "Epoch 41: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4577 - accuracy: 0.8030 - val_loss: 0.5449 - val_accuracy: 0.7710\n",
            "Epoch 42/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.4400 - accuracy: 0.8091\n",
            "Epoch 42: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4418 - accuracy: 0.8094 - val_loss: 0.5441 - val_accuracy: 0.7710\n",
            "Epoch 43/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.4484 - accuracy: 0.8094\n",
            "Epoch 43: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4465 - accuracy: 0.8107 - val_loss: 0.5413 - val_accuracy: 0.7710\n",
            "Epoch 44/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.4405 - accuracy: 0.8087\n",
            "Epoch 44: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.4405 - accuracy: 0.8087 - val_loss: 0.5386 - val_accuracy: 0.7725\n",
            "Epoch 45/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.8073\n",
            "Epoch 45: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4434 - accuracy: 0.8074 - val_loss: 0.5387 - val_accuracy: 0.7710\n",
            "Epoch 46/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.4398 - accuracy: 0.8058\n",
            "Epoch 46: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4395 - accuracy: 0.8062 - val_loss: 0.5415 - val_accuracy: 0.7710\n",
            "Epoch 47/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.4398 - accuracy: 0.8070\n",
            "Epoch 47: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4384 - accuracy: 0.8081 - val_loss: 0.5377 - val_accuracy: 0.7695\n",
            "Epoch 48/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.4330 - accuracy: 0.8179\n",
            "Epoch 48: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4330 - accuracy: 0.8177 - val_loss: 0.5393 - val_accuracy: 0.7710\n",
            "Epoch 49/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.4367 - accuracy: 0.8147\n",
            "Epoch 49: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4366 - accuracy: 0.8145 - val_loss: 0.5400 - val_accuracy: 0.7710\n",
            "Epoch 50/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.4368 - accuracy: 0.8098\n",
            "Epoch 50: loss did not improve from 0.43296\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4367 - accuracy: 0.8100 - val_loss: 0.5382 - val_accuracy: 0.7710\n",
            "Epoch 51/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.4282 - accuracy: 0.8221\n",
            "Epoch 51: loss improved from 0.43296 to 0.42866, saving model to ./model_PID__051_loss_0.429_vloss_0.536_acc_0.822_vacc_0.771.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4287 - accuracy: 0.8216 - val_loss: 0.5361 - val_accuracy: 0.7710\n",
            "Epoch 52/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4389 - accuracy: 0.8182\n",
            "Epoch 52: loss did not improve from 0.42866\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4396 - accuracy: 0.8177 - val_loss: 0.5353 - val_accuracy: 0.7725\n",
            "Epoch 53/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.4315 - accuracy: 0.8119\n",
            "Epoch 53: loss did not improve from 0.42866\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4310 - accuracy: 0.8119 - val_loss: 0.5362 - val_accuracy: 0.7725\n",
            "Epoch 54/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.8124\n",
            "Epoch 54: loss did not improve from 0.42866\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4297 - accuracy: 0.8126 - val_loss: 0.5368 - val_accuracy: 0.7725\n",
            "Epoch 55/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.4379 - accuracy: 0.8178\n",
            "Epoch 55: loss did not improve from 0.42866\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4379 - accuracy: 0.8177 - val_loss: 0.5375 - val_accuracy: 0.7725\n",
            "Epoch 56/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.4257 - accuracy: 0.8162\n",
            "Epoch 56: loss improved from 0.42866 to 0.42550, saving model to ./model_PID__056_loss_0.426_vloss_0.538_acc_0.816_vacc_0.772.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4255 - accuracy: 0.8164 - val_loss: 0.5383 - val_accuracy: 0.7725\n",
            "Epoch 57/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.4365 - accuracy: 0.8157\n",
            "Epoch 57: loss did not improve from 0.42550\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4366 - accuracy: 0.8151 - val_loss: 0.5368 - val_accuracy: 0.7710\n",
            "Epoch 58/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8220\n",
            "Epoch 58: loss improved from 0.42550 to 0.42207, saving model to ./model_PID__058_loss_0.422_vloss_0.536_acc_0.823_vacc_0.769.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4221 - accuracy: 0.8228 - val_loss: 0.5360 - val_accuracy: 0.7695\n",
            "Epoch 59/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.4236 - accuracy: 0.8171\n",
            "Epoch 59: loss did not improve from 0.42207\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4228 - accuracy: 0.8177 - val_loss: 0.5368 - val_accuracy: 0.7710\n",
            "Epoch 60/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.4278 - accuracy: 0.8207\n",
            "Epoch 60: loss did not improve from 0.42207\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4275 - accuracy: 0.8209 - val_loss: 0.5363 - val_accuracy: 0.7725\n",
            "Epoch 61/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8222\n",
            "Epoch 61: loss did not improve from 0.42207\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4232 - accuracy: 0.8216 - val_loss: 0.5336 - val_accuracy: 0.7695\n",
            "Epoch 62/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4500 - accuracy: 0.8129\n",
            "Epoch 62: loss did not improve from 0.42207\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4500 - accuracy: 0.8132 - val_loss: 0.5286 - val_accuracy: 0.7725\n",
            "Epoch 63/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4170 - accuracy: 0.8162\n",
            "Epoch 63: loss improved from 0.42207 to 0.41901, saving model to ./model_PID__063_loss_0.419_vloss_0.529_acc_0.815_vacc_0.771.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4190 - accuracy: 0.8145 - val_loss: 0.5289 - val_accuracy: 0.7710\n",
            "Epoch 64/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.8191\n",
            "Epoch 64: loss did not improve from 0.41901\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4263 - accuracy: 0.8190 - val_loss: 0.5325 - val_accuracy: 0.7665\n",
            "Epoch 65/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8141\n",
            "Epoch 65: loss did not improve from 0.41901\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4209 - accuracy: 0.8132 - val_loss: 0.5327 - val_accuracy: 0.7725\n",
            "Epoch 66/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8191\n",
            "Epoch 66: loss did not improve from 0.41901\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4205 - accuracy: 0.8190 - val_loss: 0.5342 - val_accuracy: 0.7725\n",
            "Epoch 67/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4199 - accuracy: 0.8239\n",
            "Epoch 67: loss did not improve from 0.41901\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4225 - accuracy: 0.8235 - val_loss: 0.5326 - val_accuracy: 0.7665\n",
            "Epoch 68/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8243\n",
            "Epoch 68: loss did not improve from 0.41901\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4209 - accuracy: 0.8228 - val_loss: 0.5314 - val_accuracy: 0.7680\n",
            "Epoch 69/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4228 - accuracy: 0.8247\n",
            "Epoch 69: loss did not improve from 0.41901\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4235 - accuracy: 0.8241 - val_loss: 0.5285 - val_accuracy: 0.7710\n",
            "Epoch 70/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4153 - accuracy: 0.8245\n",
            "Epoch 70: loss improved from 0.41901 to 0.41528, saving model to ./model_PID__070_loss_0.415_vloss_0.530_acc_0.825_vacc_0.768.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.4153 - accuracy: 0.8248 - val_loss: 0.5302 - val_accuracy: 0.7680\n",
            "Epoch 71/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.4106 - accuracy: 0.8188\n",
            "Epoch 71: loss improved from 0.41528 to 0.41272, saving model to ./model_PID__071_loss_0.413_vloss_0.531_acc_0.819_vacc_0.768.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4127 - accuracy: 0.8190 - val_loss: 0.5313 - val_accuracy: 0.7680\n",
            "Epoch 72/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8205\n",
            "Epoch 72: loss improved from 0.41272 to 0.41028, saving model to ./model_PID__072_loss_0.410_vloss_0.533_acc_0.820_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4103 - accuracy: 0.8203 - val_loss: 0.5331 - val_accuracy: 0.7665\n",
            "Epoch 73/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.4078 - accuracy: 0.8271\n",
            "Epoch 73: loss improved from 0.41028 to 0.40830, saving model to ./model_PID__073_loss_0.408_vloss_0.532_acc_0.827_vacc_0.766.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4083 - accuracy: 0.8267 - val_loss: 0.5316 - val_accuracy: 0.7665\n",
            "Epoch 74/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.4128 - accuracy: 0.8249\n",
            "Epoch 74: loss did not improve from 0.40830\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4124 - accuracy: 0.8248 - val_loss: 0.5346 - val_accuracy: 0.7620\n",
            "Epoch 75/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8260\n",
            "Epoch 75: loss improved from 0.40830 to 0.40274, saving model to ./model_PID__075_loss_0.403_vloss_0.540_acc_0.825_vacc_0.765.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4027 - accuracy: 0.8248 - val_loss: 0.5397 - val_accuracy: 0.7650\n",
            "Epoch 76/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.4077 - accuracy: 0.8299\n",
            "Epoch 76: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4072 - accuracy: 0.8306 - val_loss: 0.5369 - val_accuracy: 0.7680\n",
            "Epoch 77/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.4167 - accuracy: 0.8219\n",
            "Epoch 77: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4161 - accuracy: 0.8222 - val_loss: 0.5329 - val_accuracy: 0.7680\n",
            "Epoch 78/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.4167 - accuracy: 0.8280\n",
            "Epoch 78: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4167 - accuracy: 0.8280 - val_loss: 0.5305 - val_accuracy: 0.7695\n",
            "Epoch 79/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8156\n",
            "Epoch 79: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4204 - accuracy: 0.8158 - val_loss: 0.5301 - val_accuracy: 0.7710\n",
            "Epoch 80/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8223\n",
            "Epoch 80: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4084 - accuracy: 0.8235 - val_loss: 0.5320 - val_accuracy: 0.7695\n",
            "Epoch 81/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8224\n",
            "Epoch 81: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4101 - accuracy: 0.8222 - val_loss: 0.5312 - val_accuracy: 0.7680\n",
            "Epoch 82/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.4025 - accuracy: 0.8294\n",
            "Epoch 82: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4041 - accuracy: 0.8286 - val_loss: 0.5320 - val_accuracy: 0.7710\n",
            "Epoch 83/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.4079 - accuracy: 0.8306\n",
            "Epoch 83: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4098 - accuracy: 0.8299 - val_loss: 0.5297 - val_accuracy: 0.7710\n",
            "Epoch 84/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4087 - accuracy: 0.8292\n",
            "Epoch 84: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4084 - accuracy: 0.8293 - val_loss: 0.5331 - val_accuracy: 0.7710\n",
            "Epoch 85/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4063 - accuracy: 0.8195\n",
            "Epoch 85: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4060 - accuracy: 0.8196 - val_loss: 0.5309 - val_accuracy: 0.7725\n",
            "Epoch 86/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.4045 - accuracy: 0.8284\n",
            "Epoch 86: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4042 - accuracy: 0.8286 - val_loss: 0.5336 - val_accuracy: 0.7725\n",
            "Epoch 87/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.4050 - accuracy: 0.8260\n",
            "Epoch 87: loss did not improve from 0.40274\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4059 - accuracy: 0.8261 - val_loss: 0.5331 - val_accuracy: 0.7740\n",
            "Epoch 88/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3995 - accuracy: 0.8371\n",
            "Epoch 88: loss improved from 0.40274 to 0.39934, saving model to ./model_PID__088_loss_0.399_vloss_0.533_acc_0.837_vacc_0.772.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3993 - accuracy: 0.8370 - val_loss: 0.5334 - val_accuracy: 0.7725\n",
            "Epoch 89/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3959 - accuracy: 0.8305\n",
            "Epoch 89: loss improved from 0.39934 to 0.39609, saving model to ./model_PID__089_loss_0.396_vloss_0.536_acc_0.830_vacc_0.769.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3961 - accuracy: 0.8299 - val_loss: 0.5356 - val_accuracy: 0.7695\n",
            "Epoch 90/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.4170 - accuracy: 0.8193\n",
            "Epoch 90: loss did not improve from 0.39609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4161 - accuracy: 0.8196 - val_loss: 0.5351 - val_accuracy: 0.7710\n",
            "Epoch 91/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.4098 - accuracy: 0.8293\n",
            "Epoch 91: loss did not improve from 0.39609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4097 - accuracy: 0.8286 - val_loss: 0.5321 - val_accuracy: 0.7710\n",
            "Epoch 92/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.4003 - accuracy: 0.8235\n",
            "Epoch 92: loss did not improve from 0.39609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4004 - accuracy: 0.8235 - val_loss: 0.5347 - val_accuracy: 0.7740\n",
            "Epoch 93/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.8244\n",
            "Epoch 93: loss did not improve from 0.39609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4072 - accuracy: 0.8241 - val_loss: 0.5360 - val_accuracy: 0.7725\n",
            "Epoch 94/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.4043 - accuracy: 0.8258\n",
            "Epoch 94: loss did not improve from 0.39609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4034 - accuracy: 0.8261 - val_loss: 0.5384 - val_accuracy: 0.7695\n",
            "Epoch 95/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.4042 - accuracy: 0.8346\n",
            "Epoch 95: loss did not improve from 0.39609\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.4039 - accuracy: 0.8344 - val_loss: 0.5368 - val_accuracy: 0.7740\n",
            "Epoch 96/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3965 - accuracy: 0.8315\n",
            "Epoch 96: loss improved from 0.39609 to 0.39578, saving model to ./model_PID__096_loss_0.396_vloss_0.534_acc_0.832_vacc_0.774.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3958 - accuracy: 0.8318 - val_loss: 0.5342 - val_accuracy: 0.7740\n",
            "Epoch 97/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.8298\n",
            "Epoch 97: loss improved from 0.39578 to 0.38609, saving model to ./model_PID__097_loss_0.386_vloss_0.538_acc_0.830_vacc_0.774.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3861 - accuracy: 0.8299 - val_loss: 0.5379 - val_accuracy: 0.7740\n",
            "Epoch 98/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3961 - accuracy: 0.8319\n",
            "Epoch 98: loss did not improve from 0.38609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3965 - accuracy: 0.8318 - val_loss: 0.5345 - val_accuracy: 0.7725\n",
            "Epoch 99/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.8357\n",
            "Epoch 99: loss did not improve from 0.38609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3961 - accuracy: 0.8357 - val_loss: 0.5323 - val_accuracy: 0.7725\n",
            "Epoch 100/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.4021 - accuracy: 0.8247\n",
            "Epoch 100: loss did not improve from 0.38609\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4020 - accuracy: 0.8248 - val_loss: 0.5319 - val_accuracy: 0.7725\n",
            "Epoch 101/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.8314\n",
            "Epoch 101: loss improved from 0.38609 to 0.38288, saving model to ./model_PID__101_loss_0.383_vloss_0.535_acc_0.831_vacc_0.772.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3829 - accuracy: 0.8312 - val_loss: 0.5349 - val_accuracy: 0.7725\n",
            "Epoch 102/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8270\n",
            "Epoch 102: loss did not improve from 0.38288\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3981 - accuracy: 0.8273 - val_loss: 0.5295 - val_accuracy: 0.7695\n",
            "Epoch 103/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3915 - accuracy: 0.8396\n",
            "Epoch 103: loss did not improve from 0.38288\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3915 - accuracy: 0.8395 - val_loss: 0.5348 - val_accuracy: 0.7725\n",
            "Epoch 104/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.8375\n",
            "Epoch 104: loss improved from 0.38288 to 0.38166, saving model to ./model_PID__104_loss_0.382_vloss_0.538_acc_0.838_vacc_0.772.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3817 - accuracy: 0.8383 - val_loss: 0.5376 - val_accuracy: 0.7725\n",
            "Epoch 105/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3881 - accuracy: 0.8321\n",
            "Epoch 105: loss did not improve from 0.38166\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3882 - accuracy: 0.8325 - val_loss: 0.5406 - val_accuracy: 0.7710\n",
            "Epoch 106/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8341\n",
            "Epoch 106: loss did not improve from 0.38166\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4023 - accuracy: 0.8318 - val_loss: 0.5404 - val_accuracy: 0.7710\n",
            "Epoch 107/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3957 - accuracy: 0.8329\n",
            "Epoch 107: loss did not improve from 0.38166\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3957 - accuracy: 0.8331 - val_loss: 0.5387 - val_accuracy: 0.7710\n",
            "Epoch 108/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3952 - accuracy: 0.8310\n",
            "Epoch 108: loss did not improve from 0.38166\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3958 - accuracy: 0.8306 - val_loss: 0.5372 - val_accuracy: 0.7710\n",
            "Epoch 109/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3806 - accuracy: 0.8363\n",
            "Epoch 109: loss improved from 0.38166 to 0.38008, saving model to ./model_PID__109_loss_0.380_vloss_0.533_acc_0.837_vacc_0.775.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3801 - accuracy: 0.8370 - val_loss: 0.5330 - val_accuracy: 0.7754\n",
            "Epoch 110/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.4026 - accuracy: 0.8315\n",
            "Epoch 110: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4022 - accuracy: 0.8318 - val_loss: 0.5341 - val_accuracy: 0.7725\n",
            "Epoch 111/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3840 - accuracy: 0.8359\n",
            "Epoch 111: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3828 - accuracy: 0.8363 - val_loss: 0.5356 - val_accuracy: 0.7710\n",
            "Epoch 112/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.8346\n",
            "Epoch 112: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3926 - accuracy: 0.8344 - val_loss: 0.5356 - val_accuracy: 0.7725\n",
            "Epoch 113/500\n",
            "1537/1558 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8289\n",
            "Epoch 113: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3969 - accuracy: 0.8273 - val_loss: 0.5318 - val_accuracy: 0.7740\n",
            "Epoch 114/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3827 - accuracy: 0.8389\n",
            "Epoch 114: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3832 - accuracy: 0.8395 - val_loss: 0.5326 - val_accuracy: 0.7769\n",
            "Epoch 115/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.8261\n",
            "Epoch 115: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3998 - accuracy: 0.8261 - val_loss: 0.5347 - val_accuracy: 0.7814\n",
            "Epoch 116/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3818 - accuracy: 0.8396\n",
            "Epoch 116: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3819 - accuracy: 0.8395 - val_loss: 0.5286 - val_accuracy: 0.7799\n",
            "Epoch 117/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3909 - accuracy: 0.8351\n",
            "Epoch 117: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3903 - accuracy: 0.8357 - val_loss: 0.5291 - val_accuracy: 0.7725\n",
            "Epoch 118/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8313\n",
            "Epoch 118: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3928 - accuracy: 0.8312 - val_loss: 0.5308 - val_accuracy: 0.7695\n",
            "Epoch 119/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8330\n",
            "Epoch 119: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3841 - accuracy: 0.8331 - val_loss: 0.5308 - val_accuracy: 0.7740\n",
            "Epoch 120/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3807 - accuracy: 0.8379\n",
            "Epoch 120: loss did not improve from 0.38008\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3803 - accuracy: 0.8383 - val_loss: 0.5302 - val_accuracy: 0.7725\n",
            "Epoch 121/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.8394\n",
            "Epoch 121: loss improved from 0.38008 to 0.37709, saving model to ./model_PID__121_loss_0.377_vloss_0.527_acc_0.840_vacc_0.771.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3771 - accuracy: 0.8395 - val_loss: 0.5274 - val_accuracy: 0.7710\n",
            "Epoch 122/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3965 - accuracy: 0.8351\n",
            "Epoch 122: loss did not improve from 0.37709\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3978 - accuracy: 0.8338 - val_loss: 0.5260 - val_accuracy: 0.7754\n",
            "Epoch 123/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.8409\n",
            "Epoch 123: loss did not improve from 0.37709\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3821 - accuracy: 0.8395 - val_loss: 0.5245 - val_accuracy: 0.7814\n",
            "Epoch 124/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3968 - accuracy: 0.8363\n",
            "Epoch 124: loss did not improve from 0.37709\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3963 - accuracy: 0.8363 - val_loss: 0.5291 - val_accuracy: 0.7769\n",
            "Epoch 125/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.8441\n",
            "Epoch 125: loss did not improve from 0.37709\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3864 - accuracy: 0.8453 - val_loss: 0.5295 - val_accuracy: 0.7799\n",
            "Epoch 126/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8442\n",
            "Epoch 126: loss improved from 0.37709 to 0.37664, saving model to ./model_PID__126_loss_0.377_vloss_0.531_acc_0.845_vacc_0.780.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3766 - accuracy: 0.8447 - val_loss: 0.5306 - val_accuracy: 0.7799\n",
            "Epoch 127/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8435\n",
            "Epoch 127: loss did not improve from 0.37664\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3794 - accuracy: 0.8440 - val_loss: 0.5332 - val_accuracy: 0.7829\n",
            "Epoch 128/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3811 - accuracy: 0.8385\n",
            "Epoch 128: loss did not improve from 0.37664\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3809 - accuracy: 0.8383 - val_loss: 0.5297 - val_accuracy: 0.7829\n",
            "Epoch 129/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.8357\n",
            "Epoch 129: loss did not improve from 0.37664\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3819 - accuracy: 0.8350 - val_loss: 0.5342 - val_accuracy: 0.7859\n",
            "Epoch 130/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.4006 - accuracy: 0.8324\n",
            "Epoch 130: loss did not improve from 0.37664\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.4017 - accuracy: 0.8312 - val_loss: 0.5305 - val_accuracy: 0.7859\n",
            "Epoch 131/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8349\n",
            "Epoch 131: loss improved from 0.37664 to 0.37333, saving model to ./model_PID__131_loss_0.373_vloss_0.532_acc_0.835_vacc_0.787.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3733 - accuracy: 0.8350 - val_loss: 0.5321 - val_accuracy: 0.7874\n",
            "Epoch 132/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8458\n",
            "Epoch 132: loss did not improve from 0.37333\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3746 - accuracy: 0.8460 - val_loss: 0.5365 - val_accuracy: 0.7859\n",
            "Epoch 133/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8333\n",
            "Epoch 133: loss did not improve from 0.37333\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3800 - accuracy: 0.8344 - val_loss: 0.5397 - val_accuracy: 0.7859\n",
            "Epoch 134/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3791 - accuracy: 0.8365\n",
            "Epoch 134: loss did not improve from 0.37333\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3779 - accuracy: 0.8370 - val_loss: 0.5376 - val_accuracy: 0.7889\n",
            "Epoch 135/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.8418\n",
            "Epoch 135: loss did not improve from 0.37333\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3805 - accuracy: 0.8421 - val_loss: 0.5378 - val_accuracy: 0.7844\n",
            "Epoch 136/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3711 - accuracy: 0.8444\n",
            "Epoch 136: loss improved from 0.37333 to 0.37143, saving model to ./model_PID__136_loss_0.371_vloss_0.536_acc_0.844_vacc_0.784.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3714 - accuracy: 0.8440 - val_loss: 0.5363 - val_accuracy: 0.7844\n",
            "Epoch 137/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8448\n",
            "Epoch 137: loss did not improve from 0.37143\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3723 - accuracy: 0.8460 - val_loss: 0.5336 - val_accuracy: 0.7904\n",
            "Epoch 138/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.8415\n",
            "Epoch 138: loss did not improve from 0.37143\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3764 - accuracy: 0.8415 - val_loss: 0.5284 - val_accuracy: 0.7889\n",
            "Epoch 139/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3712 - accuracy: 0.8463\n",
            "Epoch 139: loss did not improve from 0.37143\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3723 - accuracy: 0.8447 - val_loss: 0.5240 - val_accuracy: 0.7889\n",
            "Epoch 140/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3724 - accuracy: 0.8369\n",
            "Epoch 140: loss did not improve from 0.37143\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3715 - accuracy: 0.8376 - val_loss: 0.5238 - val_accuracy: 0.7874\n",
            "Epoch 141/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3685 - accuracy: 0.8380\n",
            "Epoch 141: loss improved from 0.37143 to 0.36849, saving model to ./model_PID__141_loss_0.368_vloss_0.527_acc_0.838_vacc_0.787.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3685 - accuracy: 0.8383 - val_loss: 0.5271 - val_accuracy: 0.7874\n",
            "Epoch 142/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3619 - accuracy: 0.8417\n",
            "Epoch 142: loss improved from 0.36849 to 0.36226, saving model to ./model_PID__142_loss_0.362_vloss_0.527_acc_0.841_vacc_0.787.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3623 - accuracy: 0.8415 - val_loss: 0.5268 - val_accuracy: 0.7874\n",
            "Epoch 143/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8400\n",
            "Epoch 143: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3834 - accuracy: 0.8389 - val_loss: 0.5272 - val_accuracy: 0.7874\n",
            "Epoch 144/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.8365\n",
            "Epoch 144: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3780 - accuracy: 0.8370 - val_loss: 0.5310 - val_accuracy: 0.7859\n",
            "Epoch 145/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3721 - accuracy: 0.8458\n",
            "Epoch 145: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3721 - accuracy: 0.8460 - val_loss: 0.5273 - val_accuracy: 0.7859\n",
            "Epoch 146/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3723 - accuracy: 0.8481\n",
            "Epoch 146: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3710 - accuracy: 0.8485 - val_loss: 0.5274 - val_accuracy: 0.7889\n",
            "Epoch 147/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3638 - accuracy: 0.8429\n",
            "Epoch 147: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3652 - accuracy: 0.8421 - val_loss: 0.5261 - val_accuracy: 0.7889\n",
            "Epoch 148/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.8522\n",
            "Epoch 148: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3758 - accuracy: 0.8530 - val_loss: 0.5289 - val_accuracy: 0.7889\n",
            "Epoch 149/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8420\n",
            "Epoch 149: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3766 - accuracy: 0.8421 - val_loss: 0.5293 - val_accuracy: 0.7889\n",
            "Epoch 150/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.8419\n",
            "Epoch 150: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3790 - accuracy: 0.8415 - val_loss: 0.5265 - val_accuracy: 0.7904\n",
            "Epoch 151/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8400\n",
            "Epoch 151: loss did not improve from 0.36226\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3791 - accuracy: 0.8408 - val_loss: 0.5228 - val_accuracy: 0.7889\n",
            "Epoch 152/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8448\n",
            "Epoch 152: loss improved from 0.36226 to 0.35370, saving model to ./model_PID__152_loss_0.354_vloss_0.526_acc_0.846_vacc_0.789.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3537 - accuracy: 0.8460 - val_loss: 0.5258 - val_accuracy: 0.7889\n",
            "Epoch 153/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3610 - accuracy: 0.8403\n",
            "Epoch 153: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3607 - accuracy: 0.8402 - val_loss: 0.5277 - val_accuracy: 0.7874\n",
            "Epoch 154/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8443\n",
            "Epoch 154: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3666 - accuracy: 0.8447 - val_loss: 0.5261 - val_accuracy: 0.7904\n",
            "Epoch 155/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8450\n",
            "Epoch 155: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3633 - accuracy: 0.8447 - val_loss: 0.5256 - val_accuracy: 0.7904\n",
            "Epoch 156/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8430\n",
            "Epoch 156: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3648 - accuracy: 0.8440 - val_loss: 0.5237 - val_accuracy: 0.7889\n",
            "Epoch 157/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3660 - accuracy: 0.8332\n",
            "Epoch 157: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3652 - accuracy: 0.8338 - val_loss: 0.5298 - val_accuracy: 0.7889\n",
            "Epoch 158/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3544 - accuracy: 0.8453\n",
            "Epoch 158: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3542 - accuracy: 0.8453 - val_loss: 0.5293 - val_accuracy: 0.7874\n",
            "Epoch 159/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8458\n",
            "Epoch 159: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3774 - accuracy: 0.8460 - val_loss: 0.5260 - val_accuracy: 0.7829\n",
            "Epoch 160/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3629 - accuracy: 0.8494\n",
            "Epoch 160: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3645 - accuracy: 0.8485 - val_loss: 0.5288 - val_accuracy: 0.7844\n",
            "Epoch 161/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8439\n",
            "Epoch 161: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3632 - accuracy: 0.8440 - val_loss: 0.5275 - val_accuracy: 0.7844\n",
            "Epoch 162/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3676 - accuracy: 0.8454\n",
            "Epoch 162: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3675 - accuracy: 0.8453 - val_loss: 0.5267 - val_accuracy: 0.7859\n",
            "Epoch 163/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8486\n",
            "Epoch 163: loss did not improve from 0.35370\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3775 - accuracy: 0.8492 - val_loss: 0.5268 - val_accuracy: 0.7889\n",
            "Epoch 164/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8467\n",
            "Epoch 164: loss improved from 0.35370 to 0.34893, saving model to ./model_PID__164_loss_0.349_vloss_0.528_acc_0.846_vacc_0.787.hdf5\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3489 - accuracy: 0.8460 - val_loss: 0.5279 - val_accuracy: 0.7874\n",
            "Epoch 165/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8438\n",
            "Epoch 165: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3688 - accuracy: 0.8427 - val_loss: 0.5298 - val_accuracy: 0.7829\n",
            "Epoch 166/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8415\n",
            "Epoch 166: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3598 - accuracy: 0.8421 - val_loss: 0.5283 - val_accuracy: 0.7844\n",
            "Epoch 167/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3719 - accuracy: 0.8461\n",
            "Epoch 167: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3709 - accuracy: 0.8460 - val_loss: 0.5301 - val_accuracy: 0.7844\n",
            "Epoch 168/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8545\n",
            "Epoch 168: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3503 - accuracy: 0.8543 - val_loss: 0.5319 - val_accuracy: 0.7859\n",
            "Epoch 169/500\n",
            "1538/1558 [============================>.] - ETA: 0s - loss: 0.3804 - accuracy: 0.8420\n",
            "Epoch 169: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3820 - accuracy: 0.8408 - val_loss: 0.5274 - val_accuracy: 0.7874\n",
            "Epoch 170/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3757 - accuracy: 0.8337\n",
            "Epoch 170: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 7s 5ms/step - loss: 0.3763 - accuracy: 0.8331 - val_loss: 0.5235 - val_accuracy: 0.7859\n",
            "Epoch 171/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8452\n",
            "Epoch 171: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3499 - accuracy: 0.8453 - val_loss: 0.5256 - val_accuracy: 0.7859\n",
            "Epoch 172/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3601 - accuracy: 0.8424\n",
            "Epoch 172: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3597 - accuracy: 0.8427 - val_loss: 0.5322 - val_accuracy: 0.7844\n",
            "Epoch 173/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8472\n",
            "Epoch 173: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3648 - accuracy: 0.8479 - val_loss: 0.5292 - val_accuracy: 0.7874\n",
            "Epoch 174/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3581 - accuracy: 0.8469\n",
            "Epoch 174: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3568 - accuracy: 0.8479 - val_loss: 0.5276 - val_accuracy: 0.7859\n",
            "Epoch 175/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3548 - accuracy: 0.8481\n",
            "Epoch 175: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3573 - accuracy: 0.8479 - val_loss: 0.5330 - val_accuracy: 0.7919\n",
            "Epoch 176/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3549 - accuracy: 0.8462\n",
            "Epoch 176: loss did not improve from 0.34893\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3532 - accuracy: 0.8472 - val_loss: 0.5383 - val_accuracy: 0.7904\n",
            "Epoch 177/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8432\n",
            "Epoch 177: loss improved from 0.34893 to 0.34880, saving model to ./model_PID__177_loss_0.349_vloss_0.537_acc_0.843_vacc_0.792.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3488 - accuracy: 0.8427 - val_loss: 0.5372 - val_accuracy: 0.7919\n",
            "Epoch 178/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8448\n",
            "Epoch 178: loss did not improve from 0.34880\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3714 - accuracy: 0.8460 - val_loss: 0.5313 - val_accuracy: 0.7934\n",
            "Epoch 179/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8542\n",
            "Epoch 179: loss did not improve from 0.34880\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3519 - accuracy: 0.8543 - val_loss: 0.5344 - val_accuracy: 0.7889\n",
            "Epoch 180/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8527\n",
            "Epoch 180: loss did not improve from 0.34880\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3596 - accuracy: 0.8524 - val_loss: 0.5327 - val_accuracy: 0.7904\n",
            "Epoch 181/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8477\n",
            "Epoch 181: loss did not improve from 0.34880\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3588 - accuracy: 0.8479 - val_loss: 0.5295 - val_accuracy: 0.7904\n",
            "Epoch 182/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8480\n",
            "Epoch 182: loss did not improve from 0.34880\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3615 - accuracy: 0.8485 - val_loss: 0.5292 - val_accuracy: 0.7919\n",
            "Epoch 183/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8466\n",
            "Epoch 183: loss did not improve from 0.34880\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3602 - accuracy: 0.8466 - val_loss: 0.5251 - val_accuracy: 0.7919\n",
            "Epoch 184/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3656 - accuracy: 0.8500\n",
            "Epoch 184: loss did not improve from 0.34880\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3648 - accuracy: 0.8504 - val_loss: 0.5206 - val_accuracy: 0.7889\n",
            "Epoch 185/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8420\n",
            "Epoch 185: loss improved from 0.34880 to 0.34418, saving model to ./model_PID__185_loss_0.344_vloss_0.522_acc_0.843_vacc_0.789.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3442 - accuracy: 0.8427 - val_loss: 0.5220 - val_accuracy: 0.7889\n",
            "Epoch 186/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3518 - accuracy: 0.8546\n",
            "Epoch 186: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3523 - accuracy: 0.8543 - val_loss: 0.5263 - val_accuracy: 0.7889\n",
            "Epoch 187/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3466 - accuracy: 0.8478\n",
            "Epoch 187: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3466 - accuracy: 0.8479 - val_loss: 0.5195 - val_accuracy: 0.7904\n",
            "Epoch 188/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8521\n",
            "Epoch 188: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3460 - accuracy: 0.8524 - val_loss: 0.5240 - val_accuracy: 0.7874\n",
            "Epoch 189/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3690 - accuracy: 0.8466\n",
            "Epoch 189: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3669 - accuracy: 0.8472 - val_loss: 0.5190 - val_accuracy: 0.7874\n",
            "Epoch 190/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.8498\n",
            "Epoch 190: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3572 - accuracy: 0.8498 - val_loss: 0.5282 - val_accuracy: 0.7874\n",
            "Epoch 191/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8546\n",
            "Epoch 191: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3488 - accuracy: 0.8543 - val_loss: 0.5251 - val_accuracy: 0.7874\n",
            "Epoch 192/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3503 - accuracy: 0.8543\n",
            "Epoch 192: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3490 - accuracy: 0.8549 - val_loss: 0.5189 - val_accuracy: 0.7874\n",
            "Epoch 193/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8540\n",
            "Epoch 193: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3507 - accuracy: 0.8537 - val_loss: 0.5188 - val_accuracy: 0.7874\n",
            "Epoch 194/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8447\n",
            "Epoch 194: loss did not improve from 0.34418\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3641 - accuracy: 0.8440 - val_loss: 0.5173 - val_accuracy: 0.7859\n",
            "Epoch 195/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3435 - accuracy: 0.8473\n",
            "Epoch 195: loss improved from 0.34418 to 0.34281, saving model to ./model_PID__195_loss_0.343_vloss_0.523_acc_0.848_vacc_0.786.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3428 - accuracy: 0.8479 - val_loss: 0.5234 - val_accuracy: 0.7859\n",
            "Epoch 196/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8547\n",
            "Epoch 196: loss did not improve from 0.34281\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3452 - accuracy: 0.8543 - val_loss: 0.5165 - val_accuracy: 0.7859\n",
            "Epoch 197/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3593 - accuracy: 0.8558\n",
            "Epoch 197: loss did not improve from 0.34281\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3588 - accuracy: 0.8562 - val_loss: 0.5189 - val_accuracy: 0.7859\n",
            "Epoch 198/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8528\n",
            "Epoch 198: loss did not improve from 0.34281\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3455 - accuracy: 0.8524 - val_loss: 0.5150 - val_accuracy: 0.7874\n",
            "Epoch 199/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.8551\n",
            "Epoch 199: loss improved from 0.34281 to 0.33468, saving model to ./model_PID__199_loss_0.335_vloss_0.514_acc_0.856_vacc_0.786.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3347 - accuracy: 0.8556 - val_loss: 0.5143 - val_accuracy: 0.7859\n",
            "Epoch 200/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3503 - accuracy: 0.8532\n",
            "Epoch 200: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3496 - accuracy: 0.8537 - val_loss: 0.5186 - val_accuracy: 0.7859\n",
            "Epoch 201/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3754 - accuracy: 0.8431\n",
            "Epoch 201: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3744 - accuracy: 0.8440 - val_loss: 0.5105 - val_accuracy: 0.7889\n",
            "Epoch 202/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8449\n",
            "Epoch 202: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3581 - accuracy: 0.8453 - val_loss: 0.5102 - val_accuracy: 0.7889\n",
            "Epoch 203/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3573 - accuracy: 0.8510\n",
            "Epoch 203: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3573 - accuracy: 0.8511 - val_loss: 0.5114 - val_accuracy: 0.7889\n",
            "Epoch 204/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8483\n",
            "Epoch 204: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3576 - accuracy: 0.8485 - val_loss: 0.5090 - val_accuracy: 0.7889\n",
            "Epoch 205/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8472\n",
            "Epoch 205: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3483 - accuracy: 0.8479 - val_loss: 0.5157 - val_accuracy: 0.7919\n",
            "Epoch 206/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8510\n",
            "Epoch 206: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3487 - accuracy: 0.8504 - val_loss: 0.5172 - val_accuracy: 0.7919\n",
            "Epoch 207/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.8612\n",
            "Epoch 207: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3392 - accuracy: 0.8614 - val_loss: 0.5197 - val_accuracy: 0.7904\n",
            "Epoch 208/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8514\n",
            "Epoch 208: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3509 - accuracy: 0.8504 - val_loss: 0.5059 - val_accuracy: 0.7889\n",
            "Epoch 209/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3535 - accuracy: 0.8503\n",
            "Epoch 209: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3530 - accuracy: 0.8504 - val_loss: 0.5042 - val_accuracy: 0.7889\n",
            "Epoch 210/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8562\n",
            "Epoch 210: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3356 - accuracy: 0.8562 - val_loss: 0.5025 - val_accuracy: 0.7904\n",
            "Epoch 211/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8489\n",
            "Epoch 211: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3615 - accuracy: 0.8492 - val_loss: 0.5007 - val_accuracy: 0.7859\n",
            "Epoch 212/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8520\n",
            "Epoch 212: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3576 - accuracy: 0.8511 - val_loss: 0.5027 - val_accuracy: 0.7874\n",
            "Epoch 213/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3590 - accuracy: 0.8514\n",
            "Epoch 213: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3587 - accuracy: 0.8517 - val_loss: 0.5076 - val_accuracy: 0.7874\n",
            "Epoch 214/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.8536\n",
            "Epoch 214: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3431 - accuracy: 0.8537 - val_loss: 0.5093 - val_accuracy: 0.7874\n",
            "Epoch 215/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3547 - accuracy: 0.8535\n",
            "Epoch 215: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3545 - accuracy: 0.8537 - val_loss: 0.5107 - val_accuracy: 0.7889\n",
            "Epoch 216/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8527\n",
            "Epoch 216: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3581 - accuracy: 0.8530 - val_loss: 0.5102 - val_accuracy: 0.7904\n",
            "Epoch 217/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3619 - accuracy: 0.8518\n",
            "Epoch 217: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3627 - accuracy: 0.8517 - val_loss: 0.5048 - val_accuracy: 0.7874\n",
            "Epoch 218/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.8552\n",
            "Epoch 218: loss did not improve from 0.33468\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3451 - accuracy: 0.8549 - val_loss: 0.5113 - val_accuracy: 0.7889\n",
            "Epoch 219/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8539\n",
            "Epoch 219: loss improved from 0.33468 to 0.32672, saving model to ./model_PID__219_loss_0.327_vloss_0.509_acc_0.854_vacc_0.790.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3267 - accuracy: 0.8543 - val_loss: 0.5087 - val_accuracy: 0.7904\n",
            "Epoch 220/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.8671\n",
            "Epoch 220: loss did not improve from 0.32672\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3322 - accuracy: 0.8652 - val_loss: 0.5045 - val_accuracy: 0.7904\n",
            "Epoch 221/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3406 - accuracy: 0.8517\n",
            "Epoch 221: loss did not improve from 0.32672\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3392 - accuracy: 0.8524 - val_loss: 0.5049 - val_accuracy: 0.7904\n",
            "Epoch 222/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3646 - accuracy: 0.8520\n",
            "Epoch 222: loss did not improve from 0.32672\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3665 - accuracy: 0.8498 - val_loss: 0.5027 - val_accuracy: 0.7904\n",
            "Epoch 223/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8596\n",
            "Epoch 223: loss improved from 0.32672 to 0.32429, saving model to ./model_PID__223_loss_0.324_vloss_0.503_acc_0.858_vacc_0.790.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3243 - accuracy: 0.8582 - val_loss: 0.5030 - val_accuracy: 0.7904\n",
            "Epoch 224/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.8567\n",
            "Epoch 224: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3427 - accuracy: 0.8549 - val_loss: 0.5039 - val_accuracy: 0.7919\n",
            "Epoch 225/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.8511\n",
            "Epoch 225: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3627 - accuracy: 0.8511 - val_loss: 0.5029 - val_accuracy: 0.7934\n",
            "Epoch 226/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3377 - accuracy: 0.8524\n",
            "Epoch 226: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3368 - accuracy: 0.8530 - val_loss: 0.4993 - val_accuracy: 0.7934\n",
            "Epoch 227/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3519 - accuracy: 0.8558\n",
            "Epoch 227: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3528 - accuracy: 0.8549 - val_loss: 0.5027 - val_accuracy: 0.7934\n",
            "Epoch 228/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3417 - accuracy: 0.8582\n",
            "Epoch 228: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3410 - accuracy: 0.8588 - val_loss: 0.5026 - val_accuracy: 0.7904\n",
            "Epoch 229/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8566\n",
            "Epoch 229: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3461 - accuracy: 0.8549 - val_loss: 0.5048 - val_accuracy: 0.7904\n",
            "Epoch 230/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3350 - accuracy: 0.8559\n",
            "Epoch 230: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3351 - accuracy: 0.8556 - val_loss: 0.4973 - val_accuracy: 0.7904\n",
            "Epoch 231/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3398 - accuracy: 0.8508\n",
            "Epoch 231: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3391 - accuracy: 0.8511 - val_loss: 0.5060 - val_accuracy: 0.7919\n",
            "Epoch 232/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.8585\n",
            "Epoch 232: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3359 - accuracy: 0.8582 - val_loss: 0.5038 - val_accuracy: 0.7904\n",
            "Epoch 233/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8479\n",
            "Epoch 233: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3443 - accuracy: 0.8485 - val_loss: 0.5042 - val_accuracy: 0.7889\n",
            "Epoch 234/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8591\n",
            "Epoch 234: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3333 - accuracy: 0.8588 - val_loss: 0.5055 - val_accuracy: 0.7874\n",
            "Epoch 235/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3493 - accuracy: 0.8561\n",
            "Epoch 235: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3492 - accuracy: 0.8562 - val_loss: 0.5043 - val_accuracy: 0.7859\n",
            "Epoch 236/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8582\n",
            "Epoch 236: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3474 - accuracy: 0.8582 - val_loss: 0.5021 - val_accuracy: 0.7874\n",
            "Epoch 237/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8571\n",
            "Epoch 237: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3446 - accuracy: 0.8575 - val_loss: 0.4977 - val_accuracy: 0.7889\n",
            "Epoch 238/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8567\n",
            "Epoch 238: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3277 - accuracy: 0.8575 - val_loss: 0.5012 - val_accuracy: 0.7904\n",
            "Epoch 239/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8561\n",
            "Epoch 239: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3360 - accuracy: 0.8562 - val_loss: 0.5016 - val_accuracy: 0.7934\n",
            "Epoch 240/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.8593\n",
            "Epoch 240: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3417 - accuracy: 0.8588 - val_loss: 0.4994 - val_accuracy: 0.7949\n",
            "Epoch 241/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3526 - accuracy: 0.8580\n",
            "Epoch 241: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3526 - accuracy: 0.8582 - val_loss: 0.5068 - val_accuracy: 0.7949\n",
            "Epoch 242/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3435 - accuracy: 0.8578\n",
            "Epoch 242: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3430 - accuracy: 0.8582 - val_loss: 0.5048 - val_accuracy: 0.7949\n",
            "Epoch 243/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.8602\n",
            "Epoch 243: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3369 - accuracy: 0.8601 - val_loss: 0.5041 - val_accuracy: 0.7919\n",
            "Epoch 244/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.8549\n",
            "Epoch 244: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3303 - accuracy: 0.8549 - val_loss: 0.5019 - val_accuracy: 0.7964\n",
            "Epoch 245/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3348 - accuracy: 0.8592\n",
            "Epoch 245: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3355 - accuracy: 0.8588 - val_loss: 0.4994 - val_accuracy: 0.7919\n",
            "Epoch 246/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8608\n",
            "Epoch 246: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3388 - accuracy: 0.8614 - val_loss: 0.4915 - val_accuracy: 0.7919\n",
            "Epoch 247/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8550\n",
            "Epoch 247: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3286 - accuracy: 0.8556 - val_loss: 0.4962 - val_accuracy: 0.7919\n",
            "Epoch 248/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.8583\n",
            "Epoch 248: loss did not improve from 0.32429\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3479 - accuracy: 0.8588 - val_loss: 0.4946 - val_accuracy: 0.7919\n",
            "Epoch 249/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8639\n",
            "Epoch 249: loss improved from 0.32429 to 0.31848, saving model to ./model_PID__249_loss_0.318_vloss_0.494_acc_0.863_vacc_0.792.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3185 - accuracy: 0.8633 - val_loss: 0.4937 - val_accuracy: 0.7919\n",
            "Epoch 250/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8548\n",
            "Epoch 250: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3534 - accuracy: 0.8543 - val_loss: 0.4856 - val_accuracy: 0.7904\n",
            "Epoch 251/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8503\n",
            "Epoch 251: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3409 - accuracy: 0.8498 - val_loss: 0.4923 - val_accuracy: 0.7964\n",
            "Epoch 252/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3420 - accuracy: 0.8593\n",
            "Epoch 252: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3416 - accuracy: 0.8594 - val_loss: 0.4885 - val_accuracy: 0.7964\n",
            "Epoch 253/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8595\n",
            "Epoch 253: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3427 - accuracy: 0.8601 - val_loss: 0.4931 - val_accuracy: 0.7949\n",
            "Epoch 254/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8513\n",
            "Epoch 254: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3449 - accuracy: 0.8524 - val_loss: 0.4966 - val_accuracy: 0.7979\n",
            "Epoch 255/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8586\n",
            "Epoch 255: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3398 - accuracy: 0.8582 - val_loss: 0.4974 - val_accuracy: 0.7979\n",
            "Epoch 256/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8524\n",
            "Epoch 256: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3336 - accuracy: 0.8524 - val_loss: 0.4987 - val_accuracy: 0.8009\n",
            "Epoch 257/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8567\n",
            "Epoch 257: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3261 - accuracy: 0.8569 - val_loss: 0.4980 - val_accuracy: 0.7994\n",
            "Epoch 258/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.8585\n",
            "Epoch 258: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3383 - accuracy: 0.8588 - val_loss: 0.4981 - val_accuracy: 0.7994\n",
            "Epoch 259/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8568\n",
            "Epoch 259: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3264 - accuracy: 0.8569 - val_loss: 0.4914 - val_accuracy: 0.7979\n",
            "Epoch 260/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8618\n",
            "Epoch 260: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3276 - accuracy: 0.8614 - val_loss: 0.4965 - val_accuracy: 0.7994\n",
            "Epoch 261/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3348 - accuracy: 0.8614\n",
            "Epoch 261: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3377 - accuracy: 0.8607 - val_loss: 0.4968 - val_accuracy: 0.7994\n",
            "Epoch 262/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.8556\n",
            "Epoch 262: loss did not improve from 0.31848\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3358 - accuracy: 0.8556 - val_loss: 0.4908 - val_accuracy: 0.7994\n",
            "Epoch 263/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8634\n",
            "Epoch 263: loss improved from 0.31848 to 0.31478, saving model to ./model_PID__263_loss_0.315_vloss_0.490_acc_0.864_vacc_0.796.hdf5\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3148 - accuracy: 0.8639 - val_loss: 0.4900 - val_accuracy: 0.7964\n",
            "Epoch 264/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8597\n",
            "Epoch 264: loss did not improve from 0.31478\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3245 - accuracy: 0.8601 - val_loss: 0.4941 - val_accuracy: 0.7964\n",
            "Epoch 265/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8650\n",
            "Epoch 265: loss improved from 0.31478 to 0.30777, saving model to ./model_PID__265_loss_0.308_vloss_0.492_acc_0.865_vacc_0.796.hdf5\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3078 - accuracy: 0.8652 - val_loss: 0.4918 - val_accuracy: 0.7964\n",
            "Epoch 266/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3470 - accuracy: 0.8576\n",
            "Epoch 266: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3461 - accuracy: 0.8582 - val_loss: 0.4822 - val_accuracy: 0.7949\n",
            "Epoch 267/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8588\n",
            "Epoch 267: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3350 - accuracy: 0.8575 - val_loss: 0.4810 - val_accuracy: 0.7979\n",
            "Epoch 268/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8641\n",
            "Epoch 268: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3261 - accuracy: 0.8639 - val_loss: 0.4812 - val_accuracy: 0.7979\n",
            "Epoch 269/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8521\n",
            "Epoch 269: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3290 - accuracy: 0.8524 - val_loss: 0.4761 - val_accuracy: 0.7994\n",
            "Epoch 270/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8490\n",
            "Epoch 270: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3569 - accuracy: 0.8492 - val_loss: 0.4852 - val_accuracy: 0.7979\n",
            "Epoch 271/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.8581\n",
            "Epoch 271: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3394 - accuracy: 0.8575 - val_loss: 0.4861 - val_accuracy: 0.7979\n",
            "Epoch 272/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.8586\n",
            "Epoch 272: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3162 - accuracy: 0.8588 - val_loss: 0.4835 - val_accuracy: 0.7964\n",
            "Epoch 273/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3097 - accuracy: 0.8595\n",
            "Epoch 273: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3115 - accuracy: 0.8588 - val_loss: 0.4871 - val_accuracy: 0.7994\n",
            "Epoch 274/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8638\n",
            "Epoch 274: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3150 - accuracy: 0.8639 - val_loss: 0.4854 - val_accuracy: 0.7979\n",
            "Epoch 275/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3267 - accuracy: 0.8556\n",
            "Epoch 275: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3277 - accuracy: 0.8549 - val_loss: 0.4802 - val_accuracy: 0.8009\n",
            "Epoch 276/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8525\n",
            "Epoch 276: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3520 - accuracy: 0.8530 - val_loss: 0.4841 - val_accuracy: 0.7979\n",
            "Epoch 277/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3334 - accuracy: 0.8614\n",
            "Epoch 277: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3334 - accuracy: 0.8614 - val_loss: 0.4872 - val_accuracy: 0.7979\n",
            "Epoch 278/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.8593\n",
            "Epoch 278: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3290 - accuracy: 0.8588 - val_loss: 0.4874 - val_accuracy: 0.7979\n",
            "Epoch 279/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3164 - accuracy: 0.8638\n",
            "Epoch 279: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3164 - accuracy: 0.8639 - val_loss: 0.4885 - val_accuracy: 0.7979\n",
            "Epoch 280/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8668\n",
            "Epoch 280: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3220 - accuracy: 0.8665 - val_loss: 0.4901 - val_accuracy: 0.7979\n",
            "Epoch 281/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.8562\n",
            "Epoch 281: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3272 - accuracy: 0.8575 - val_loss: 0.4904 - val_accuracy: 0.7979\n",
            "Epoch 282/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8627\n",
            "Epoch 282: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3134 - accuracy: 0.8633 - val_loss: 0.4857 - val_accuracy: 0.7979\n",
            "Epoch 283/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3322 - accuracy: 0.8571\n",
            "Epoch 283: loss did not improve from 0.30777\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3325 - accuracy: 0.8569 - val_loss: 0.4833 - val_accuracy: 0.7979\n",
            "Epoch 284/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8597\n",
            "Epoch 284: loss improved from 0.30777 to 0.30532, saving model to ./model_PID__284_loss_0.305_vloss_0.486_acc_0.860_vacc_0.799.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3053 - accuracy: 0.8601 - val_loss: 0.4864 - val_accuracy: 0.7994\n",
            "Epoch 285/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8632\n",
            "Epoch 285: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3069 - accuracy: 0.8633 - val_loss: 0.4873 - val_accuracy: 0.7979\n",
            "Epoch 286/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8569\n",
            "Epoch 286: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3366 - accuracy: 0.8575 - val_loss: 0.4846 - val_accuracy: 0.7994\n",
            "Epoch 287/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8606\n",
            "Epoch 287: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3250 - accuracy: 0.8607 - val_loss: 0.4837 - val_accuracy: 0.7994\n",
            "Epoch 288/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8600\n",
            "Epoch 288: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 8s 5ms/step - loss: 0.3287 - accuracy: 0.8601 - val_loss: 0.4805 - val_accuracy: 0.8039\n",
            "Epoch 289/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8572\n",
            "Epoch 289: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3215 - accuracy: 0.8562 - val_loss: 0.4828 - val_accuracy: 0.8009\n",
            "Epoch 290/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8612\n",
            "Epoch 290: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3063 - accuracy: 0.8614 - val_loss: 0.4950 - val_accuracy: 0.8039\n",
            "Epoch 291/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8656\n",
            "Epoch 291: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3107 - accuracy: 0.8659 - val_loss: 0.4922 - val_accuracy: 0.8024\n",
            "Epoch 292/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.8572\n",
            "Epoch 292: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3345 - accuracy: 0.8575 - val_loss: 0.5014 - val_accuracy: 0.7979\n",
            "Epoch 293/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8597\n",
            "Epoch 293: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3357 - accuracy: 0.8601 - val_loss: 0.4870 - val_accuracy: 0.7994\n",
            "Epoch 294/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3276 - accuracy: 0.8658\n",
            "Epoch 294: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 5s 3ms/step - loss: 0.3279 - accuracy: 0.8652 - val_loss: 0.4781 - val_accuracy: 0.7994\n",
            "Epoch 295/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.8622\n",
            "Epoch 295: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3160 - accuracy: 0.8620 - val_loss: 0.4824 - val_accuracy: 0.8039\n",
            "Epoch 296/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3147 - accuracy: 0.8607\n",
            "Epoch 296: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3144 - accuracy: 0.8607 - val_loss: 0.4804 - val_accuracy: 0.7994\n",
            "Epoch 297/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8627\n",
            "Epoch 297: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3219 - accuracy: 0.8626 - val_loss: 0.4884 - val_accuracy: 0.8009\n",
            "Epoch 298/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3285 - accuracy: 0.8651\n",
            "Epoch 298: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 5s 4ms/step - loss: 0.3270 - accuracy: 0.8665 - val_loss: 0.4822 - val_accuracy: 0.7964\n",
            "Epoch 299/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8655\n",
            "Epoch 299: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3191 - accuracy: 0.8665 - val_loss: 0.4752 - val_accuracy: 0.7994\n",
            "Epoch 300/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8594\n",
            "Epoch 300: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3278 - accuracy: 0.8594 - val_loss: 0.4776 - val_accuracy: 0.7979\n",
            "Epoch 301/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8607\n",
            "Epoch 301: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3175 - accuracy: 0.8601 - val_loss: 0.4744 - val_accuracy: 0.8009\n",
            "Epoch 302/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8638\n",
            "Epoch 302: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3097 - accuracy: 0.8639 - val_loss: 0.4773 - val_accuracy: 0.8009\n",
            "Epoch 303/500\n",
            "1539/1558 [============================>.] - ETA: 0s - loss: 0.3189 - accuracy: 0.8642\n",
            "Epoch 303: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3177 - accuracy: 0.8652 - val_loss: 0.4802 - val_accuracy: 0.8009\n",
            "Epoch 304/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3366 - accuracy: 0.8646\n",
            "Epoch 304: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3373 - accuracy: 0.8639 - val_loss: 0.4736 - val_accuracy: 0.8039\n",
            "Epoch 305/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8650\n",
            "Epoch 305: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3108 - accuracy: 0.8652 - val_loss: 0.4756 - val_accuracy: 0.7994\n",
            "Epoch 306/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.8589\n",
            "Epoch 306: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3224 - accuracy: 0.8594 - val_loss: 0.4817 - val_accuracy: 0.8009\n",
            "Epoch 307/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8661\n",
            "Epoch 307: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3153 - accuracy: 0.8652 - val_loss: 0.4756 - val_accuracy: 0.8024\n",
            "Epoch 308/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8680\n",
            "Epoch 308: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3159 - accuracy: 0.8678 - val_loss: 0.4776 - val_accuracy: 0.8009\n",
            "Epoch 309/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.8662\n",
            "Epoch 309: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3137 - accuracy: 0.8665 - val_loss: 0.4801 - val_accuracy: 0.8009\n",
            "Epoch 310/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.8631\n",
            "Epoch 310: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3147 - accuracy: 0.8639 - val_loss: 0.4736 - val_accuracy: 0.8039\n",
            "Epoch 311/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.8614\n",
            "Epoch 311: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3260 - accuracy: 0.8614 - val_loss: 0.4809 - val_accuracy: 0.8009\n",
            "Epoch 312/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8721\n",
            "Epoch 312: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3056 - accuracy: 0.8703 - val_loss: 0.4812 - val_accuracy: 0.8054\n",
            "Epoch 313/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.8521\n",
            "Epoch 313: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3286 - accuracy: 0.8511 - val_loss: 0.4774 - val_accuracy: 0.8024\n",
            "Epoch 314/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8604\n",
            "Epoch 314: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3223 - accuracy: 0.8601 - val_loss: 0.4769 - val_accuracy: 0.8054\n",
            "Epoch 315/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3161 - accuracy: 0.8684\n",
            "Epoch 315: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3178 - accuracy: 0.8671 - val_loss: 0.4819 - val_accuracy: 0.8009\n",
            "Epoch 316/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8698\n",
            "Epoch 316: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3126 - accuracy: 0.8697 - val_loss: 0.4829 - val_accuracy: 0.8039\n",
            "Epoch 317/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8634\n",
            "Epoch 317: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3219 - accuracy: 0.8633 - val_loss: 0.4814 - val_accuracy: 0.8069\n",
            "Epoch 318/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3313 - accuracy: 0.8596\n",
            "Epoch 318: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3310 - accuracy: 0.8594 - val_loss: 0.4783 - val_accuracy: 0.8009\n",
            "Epoch 319/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.8699\n",
            "Epoch 319: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3053 - accuracy: 0.8703 - val_loss: 0.4803 - val_accuracy: 0.8024\n",
            "Epoch 320/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8623\n",
            "Epoch 320: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3106 - accuracy: 0.8626 - val_loss: 0.4783 - val_accuracy: 0.8024\n",
            "Epoch 321/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3335 - accuracy: 0.8643\n",
            "Epoch 321: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3327 - accuracy: 0.8646 - val_loss: 0.4688 - val_accuracy: 0.8069\n",
            "Epoch 322/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8580\n",
            "Epoch 322: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3129 - accuracy: 0.8588 - val_loss: 0.4593 - val_accuracy: 0.8069\n",
            "Epoch 323/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.8588\n",
            "Epoch 323: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3382 - accuracy: 0.8588 - val_loss: 0.4623 - val_accuracy: 0.8069\n",
            "Epoch 324/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8644\n",
            "Epoch 324: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3352 - accuracy: 0.8646 - val_loss: 0.4641 - val_accuracy: 0.8054\n",
            "Epoch 325/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8718\n",
            "Epoch 325: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3058 - accuracy: 0.8716 - val_loss: 0.4686 - val_accuracy: 0.8084\n",
            "Epoch 326/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8585\n",
            "Epoch 326: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3286 - accuracy: 0.8588 - val_loss: 0.4652 - val_accuracy: 0.8069\n",
            "Epoch 327/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8679\n",
            "Epoch 327: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3202 - accuracy: 0.8684 - val_loss: 0.4674 - val_accuracy: 0.8069\n",
            "Epoch 328/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8664\n",
            "Epoch 328: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3165 - accuracy: 0.8659 - val_loss: 0.4713 - val_accuracy: 0.8069\n",
            "Epoch 329/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8584\n",
            "Epoch 329: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3197 - accuracy: 0.8588 - val_loss: 0.4668 - val_accuracy: 0.8084\n",
            "Epoch 330/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8683\n",
            "Epoch 330: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3087 - accuracy: 0.8684 - val_loss: 0.4639 - val_accuracy: 0.8054\n",
            "Epoch 331/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.8613\n",
            "Epoch 331: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3423 - accuracy: 0.8620 - val_loss: 0.4618 - val_accuracy: 0.8084\n",
            "Epoch 332/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8682\n",
            "Epoch 332: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3158 - accuracy: 0.8684 - val_loss: 0.4600 - val_accuracy: 0.8114\n",
            "Epoch 333/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8690\n",
            "Epoch 333: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 7s 5ms/step - loss: 0.3113 - accuracy: 0.8691 - val_loss: 0.4681 - val_accuracy: 0.8099\n",
            "Epoch 334/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8640\n",
            "Epoch 334: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3245 - accuracy: 0.8633 - val_loss: 0.4629 - val_accuracy: 0.8084\n",
            "Epoch 335/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy: 0.8634\n",
            "Epoch 335: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3301 - accuracy: 0.8633 - val_loss: 0.4729 - val_accuracy: 0.8069\n",
            "Epoch 336/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8648\n",
            "Epoch 336: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3169 - accuracy: 0.8639 - val_loss: 0.4677 - val_accuracy: 0.8069\n",
            "Epoch 337/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8618\n",
            "Epoch 337: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3400 - accuracy: 0.8620 - val_loss: 0.4643 - val_accuracy: 0.8099\n",
            "Epoch 338/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8683\n",
            "Epoch 338: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3183 - accuracy: 0.8684 - val_loss: 0.4651 - val_accuracy: 0.8099\n",
            "Epoch 339/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.8633\n",
            "Epoch 339: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3256 - accuracy: 0.8633 - val_loss: 0.4687 - val_accuracy: 0.8099\n",
            "Epoch 340/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8604\n",
            "Epoch 340: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3304 - accuracy: 0.8601 - val_loss: 0.4634 - val_accuracy: 0.8084\n",
            "Epoch 341/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8682\n",
            "Epoch 341: loss did not improve from 0.30532\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3098 - accuracy: 0.8691 - val_loss: 0.4725 - val_accuracy: 0.8099\n",
            "Epoch 342/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8673\n",
            "Epoch 342: loss improved from 0.30532 to 0.30014, saving model to ./model_PID__342_loss_0.300_vloss_0.473_acc_0.868_vacc_0.811.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3001 - accuracy: 0.8678 - val_loss: 0.4727 - val_accuracy: 0.8114\n",
            "Epoch 343/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.8645\n",
            "Epoch 343: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3138 - accuracy: 0.8639 - val_loss: 0.4721 - val_accuracy: 0.8069\n",
            "Epoch 344/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8619\n",
            "Epoch 344: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3219 - accuracy: 0.8620 - val_loss: 0.4627 - val_accuracy: 0.8084\n",
            "Epoch 345/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8774\n",
            "Epoch 345: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3055 - accuracy: 0.8774 - val_loss: 0.4687 - val_accuracy: 0.8099\n",
            "Epoch 346/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.8701\n",
            "Epoch 346: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3197 - accuracy: 0.8691 - val_loss: 0.4622 - val_accuracy: 0.8069\n",
            "Epoch 347/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8644\n",
            "Epoch 347: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3101 - accuracy: 0.8646 - val_loss: 0.4701 - val_accuracy: 0.8084\n",
            "Epoch 348/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8649\n",
            "Epoch 348: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3117 - accuracy: 0.8659 - val_loss: 0.4716 - val_accuracy: 0.8099\n",
            "Epoch 349/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.8605\n",
            "Epoch 349: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3236 - accuracy: 0.8614 - val_loss: 0.4662 - val_accuracy: 0.8114\n",
            "Epoch 350/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8695\n",
            "Epoch 350: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3197 - accuracy: 0.8697 - val_loss: 0.4678 - val_accuracy: 0.8114\n",
            "Epoch 351/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3287 - accuracy: 0.8709\n",
            "Epoch 351: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3287 - accuracy: 0.8710 - val_loss: 0.4624 - val_accuracy: 0.8099\n",
            "Epoch 352/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8692\n",
            "Epoch 352: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3070 - accuracy: 0.8697 - val_loss: 0.4690 - val_accuracy: 0.8129\n",
            "Epoch 353/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8667\n",
            "Epoch 353: loss did not improve from 0.30014\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3212 - accuracy: 0.8665 - val_loss: 0.4655 - val_accuracy: 0.8144\n",
            "Epoch 354/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8727\n",
            "Epoch 354: loss improved from 0.30014 to 0.29517, saving model to ./model_PID__354_loss_0.295_vloss_0.468_acc_0.872_vacc_0.814.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2952 - accuracy: 0.8723 - val_loss: 0.4680 - val_accuracy: 0.8144\n",
            "Epoch 355/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8707\n",
            "Epoch 355: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3113 - accuracy: 0.8697 - val_loss: 0.4603 - val_accuracy: 0.8129\n",
            "Epoch 356/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8633\n",
            "Epoch 356: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3265 - accuracy: 0.8633 - val_loss: 0.4726 - val_accuracy: 0.8129\n",
            "Epoch 357/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8667\n",
            "Epoch 357: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3271 - accuracy: 0.8665 - val_loss: 0.4730 - val_accuracy: 0.8144\n",
            "Epoch 358/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8677\n",
            "Epoch 358: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3133 - accuracy: 0.8671 - val_loss: 0.4764 - val_accuracy: 0.8144\n",
            "Epoch 359/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8628\n",
            "Epoch 359: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3404 - accuracy: 0.8633 - val_loss: 0.4670 - val_accuracy: 0.8114\n",
            "Epoch 360/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8632\n",
            "Epoch 360: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3172 - accuracy: 0.8639 - val_loss: 0.4706 - val_accuracy: 0.8159\n",
            "Epoch 361/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3366 - accuracy: 0.8637\n",
            "Epoch 361: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3363 - accuracy: 0.8639 - val_loss: 0.4632 - val_accuracy: 0.8114\n",
            "Epoch 362/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8760\n",
            "Epoch 362: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3000 - accuracy: 0.8761 - val_loss: 0.4587 - val_accuracy: 0.8114\n",
            "Epoch 363/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.8666\n",
            "Epoch 363: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3133 - accuracy: 0.8659 - val_loss: 0.4595 - val_accuracy: 0.8114\n",
            "Epoch 364/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8691\n",
            "Epoch 364: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2976 - accuracy: 0.8691 - val_loss: 0.4581 - val_accuracy: 0.8099\n",
            "Epoch 365/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.8699\n",
            "Epoch 365: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3120 - accuracy: 0.8691 - val_loss: 0.4585 - val_accuracy: 0.8099\n",
            "Epoch 366/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3204 - accuracy: 0.8682\n",
            "Epoch 366: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3198 - accuracy: 0.8684 - val_loss: 0.4630 - val_accuracy: 0.8114\n",
            "Epoch 367/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8715\n",
            "Epoch 367: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2958 - accuracy: 0.8723 - val_loss: 0.4671 - val_accuracy: 0.8114\n",
            "Epoch 368/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8657\n",
            "Epoch 368: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3218 - accuracy: 0.8659 - val_loss: 0.4699 - val_accuracy: 0.8114\n",
            "Epoch 369/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8676\n",
            "Epoch 369: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3217 - accuracy: 0.8671 - val_loss: 0.4732 - val_accuracy: 0.8129\n",
            "Epoch 370/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8682\n",
            "Epoch 370: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3117 - accuracy: 0.8691 - val_loss: 0.4723 - val_accuracy: 0.8114\n",
            "Epoch 371/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3177 - accuracy: 0.8661\n",
            "Epoch 371: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3178 - accuracy: 0.8659 - val_loss: 0.4683 - val_accuracy: 0.8144\n",
            "Epoch 372/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.8656\n",
            "Epoch 372: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3174 - accuracy: 0.8659 - val_loss: 0.4667 - val_accuracy: 0.8129\n",
            "Epoch 373/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8673\n",
            "Epoch 373: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3066 - accuracy: 0.8684 - val_loss: 0.4653 - val_accuracy: 0.8114\n",
            "Epoch 374/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8680\n",
            "Epoch 374: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3210 - accuracy: 0.8684 - val_loss: 0.4704 - val_accuracy: 0.8114\n",
            "Epoch 375/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8671\n",
            "Epoch 375: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3224 - accuracy: 0.8652 - val_loss: 0.4618 - val_accuracy: 0.8069\n",
            "Epoch 376/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8691\n",
            "Epoch 376: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3250 - accuracy: 0.8691 - val_loss: 0.4650 - val_accuracy: 0.8114\n",
            "Epoch 377/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8748\n",
            "Epoch 377: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2977 - accuracy: 0.8736 - val_loss: 0.4609 - val_accuracy: 0.8099\n",
            "Epoch 378/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8696\n",
            "Epoch 378: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3098 - accuracy: 0.8697 - val_loss: 0.4618 - val_accuracy: 0.8099\n",
            "Epoch 379/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.8684\n",
            "Epoch 379: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3129 - accuracy: 0.8691 - val_loss: 0.4609 - val_accuracy: 0.8099\n",
            "Epoch 380/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8662\n",
            "Epoch 380: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3146 - accuracy: 0.8652 - val_loss: 0.4649 - val_accuracy: 0.8144\n",
            "Epoch 381/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.8733\n",
            "Epoch 381: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3165 - accuracy: 0.8736 - val_loss: 0.4635 - val_accuracy: 0.8144\n",
            "Epoch 382/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.8699\n",
            "Epoch 382: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3144 - accuracy: 0.8703 - val_loss: 0.4644 - val_accuracy: 0.8144\n",
            "Epoch 383/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.8638\n",
            "Epoch 383: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3292 - accuracy: 0.8633 - val_loss: 0.4624 - val_accuracy: 0.8129\n",
            "Epoch 384/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8729\n",
            "Epoch 384: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3006 - accuracy: 0.8723 - val_loss: 0.4632 - val_accuracy: 0.8129\n",
            "Epoch 385/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8718\n",
            "Epoch 385: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3065 - accuracy: 0.8710 - val_loss: 0.4630 - val_accuracy: 0.8144\n",
            "Epoch 386/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.8735\n",
            "Epoch 386: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2994 - accuracy: 0.8729 - val_loss: 0.4618 - val_accuracy: 0.8159\n",
            "Epoch 387/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8718\n",
            "Epoch 387: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3094 - accuracy: 0.8716 - val_loss: 0.4629 - val_accuracy: 0.8204\n",
            "Epoch 388/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8703\n",
            "Epoch 388: loss did not improve from 0.29517\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3176 - accuracy: 0.8710 - val_loss: 0.4578 - val_accuracy: 0.8159\n",
            "Epoch 389/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.8744\n",
            "Epoch 389: loss improved from 0.29517 to 0.28537, saving model to ./model_PID__389_loss_0.285_vloss_0.455_acc_0.872_vacc_0.816.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2854 - accuracy: 0.8723 - val_loss: 0.4552 - val_accuracy: 0.8159\n",
            "Epoch 390/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8678\n",
            "Epoch 390: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3053 - accuracy: 0.8678 - val_loss: 0.4562 - val_accuracy: 0.8144\n",
            "Epoch 391/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8665\n",
            "Epoch 391: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3193 - accuracy: 0.8659 - val_loss: 0.4495 - val_accuracy: 0.8099\n",
            "Epoch 392/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3160 - accuracy: 0.8708\n",
            "Epoch 392: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3158 - accuracy: 0.8710 - val_loss: 0.4563 - val_accuracy: 0.8159\n",
            "Epoch 393/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3117 - accuracy: 0.8686\n",
            "Epoch 393: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3106 - accuracy: 0.8691 - val_loss: 0.4558 - val_accuracy: 0.8129\n",
            "Epoch 394/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.8665\n",
            "Epoch 394: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3197 - accuracy: 0.8665 - val_loss: 0.4539 - val_accuracy: 0.8159\n",
            "Epoch 395/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.2997 - accuracy: 0.8695\n",
            "Epoch 395: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.2993 - accuracy: 0.8697 - val_loss: 0.4582 - val_accuracy: 0.8174\n",
            "Epoch 396/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.8735\n",
            "Epoch 396: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3144 - accuracy: 0.8736 - val_loss: 0.4545 - val_accuracy: 0.8159\n",
            "Epoch 397/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8717\n",
            "Epoch 397: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3173 - accuracy: 0.8723 - val_loss: 0.4543 - val_accuracy: 0.8114\n",
            "Epoch 398/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8709\n",
            "Epoch 398: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3344 - accuracy: 0.8703 - val_loss: 0.4522 - val_accuracy: 0.8114\n",
            "Epoch 399/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8613\n",
            "Epoch 399: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3270 - accuracy: 0.8614 - val_loss: 0.4534 - val_accuracy: 0.8159\n",
            "Epoch 400/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8683\n",
            "Epoch 400: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3024 - accuracy: 0.8684 - val_loss: 0.4551 - val_accuracy: 0.8159\n",
            "Epoch 401/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8691\n",
            "Epoch 401: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.2924 - accuracy: 0.8691 - val_loss: 0.4633 - val_accuracy: 0.8159\n",
            "Epoch 402/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8690\n",
            "Epoch 402: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3233 - accuracy: 0.8691 - val_loss: 0.4576 - val_accuracy: 0.8099\n",
            "Epoch 403/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8676\n",
            "Epoch 403: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3184 - accuracy: 0.8678 - val_loss: 0.4641 - val_accuracy: 0.8099\n",
            "Epoch 404/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.8696\n",
            "Epoch 404: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3110 - accuracy: 0.8697 - val_loss: 0.4644 - val_accuracy: 0.8114\n",
            "Epoch 405/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8643\n",
            "Epoch 405: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3058 - accuracy: 0.8652 - val_loss: 0.4615 - val_accuracy: 0.8069\n",
            "Epoch 406/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8670\n",
            "Epoch 406: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2998 - accuracy: 0.8665 - val_loss: 0.4655 - val_accuracy: 0.8099\n",
            "Epoch 407/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8686\n",
            "Epoch 407: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3083 - accuracy: 0.8678 - val_loss: 0.4672 - val_accuracy: 0.8129\n",
            "Epoch 408/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8642\n",
            "Epoch 408: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3036 - accuracy: 0.8646 - val_loss: 0.4622 - val_accuracy: 0.8129\n",
            "Epoch 409/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8723\n",
            "Epoch 409: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3241 - accuracy: 0.8723 - val_loss: 0.4590 - val_accuracy: 0.8114\n",
            "Epoch 410/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8688\n",
            "Epoch 410: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3037 - accuracy: 0.8697 - val_loss: 0.4600 - val_accuracy: 0.8114\n",
            "Epoch 411/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.8678\n",
            "Epoch 411: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3021 - accuracy: 0.8671 - val_loss: 0.4522 - val_accuracy: 0.8099\n",
            "Epoch 412/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8658\n",
            "Epoch 412: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3049 - accuracy: 0.8659 - val_loss: 0.4445 - val_accuracy: 0.8129\n",
            "Epoch 413/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.8752\n",
            "Epoch 413: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2870 - accuracy: 0.8748 - val_loss: 0.4484 - val_accuracy: 0.8114\n",
            "Epoch 414/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8727\n",
            "Epoch 414: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3021 - accuracy: 0.8729 - val_loss: 0.4528 - val_accuracy: 0.8144\n",
            "Epoch 415/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.8739\n",
            "Epoch 415: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2986 - accuracy: 0.8742 - val_loss: 0.4469 - val_accuracy: 0.8114\n",
            "Epoch 416/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8635\n",
            "Epoch 416: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3065 - accuracy: 0.8626 - val_loss: 0.4434 - val_accuracy: 0.8144\n",
            "Epoch 417/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8708\n",
            "Epoch 417: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3099 - accuracy: 0.8716 - val_loss: 0.4430 - val_accuracy: 0.8159\n",
            "Epoch 418/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8719\n",
            "Epoch 418: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3017 - accuracy: 0.8723 - val_loss: 0.4465 - val_accuracy: 0.8144\n",
            "Epoch 419/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8735\n",
            "Epoch 419: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3085 - accuracy: 0.8729 - val_loss: 0.4481 - val_accuracy: 0.8159\n",
            "Epoch 420/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8714\n",
            "Epoch 420: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3123 - accuracy: 0.8716 - val_loss: 0.4490 - val_accuracy: 0.8114\n",
            "Epoch 421/500\n",
            "1551/1558 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8575\n",
            "Epoch 421: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3203 - accuracy: 0.8569 - val_loss: 0.4461 - val_accuracy: 0.8144\n",
            "Epoch 422/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8714\n",
            "Epoch 422: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2901 - accuracy: 0.8723 - val_loss: 0.4422 - val_accuracy: 0.8099\n",
            "Epoch 423/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8779\n",
            "Epoch 423: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3170 - accuracy: 0.8780 - val_loss: 0.4363 - val_accuracy: 0.8114\n",
            "Epoch 424/500\n",
            "1540/1558 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8734\n",
            "Epoch 424: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2991 - accuracy: 0.8742 - val_loss: 0.4411 - val_accuracy: 0.8129\n",
            "Epoch 425/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8690\n",
            "Epoch 425: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2994 - accuracy: 0.8684 - val_loss: 0.4443 - val_accuracy: 0.8114\n",
            "Epoch 426/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8739\n",
            "Epoch 426: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2958 - accuracy: 0.8742 - val_loss: 0.4466 - val_accuracy: 0.8144\n",
            "Epoch 427/500\n",
            "1541/1558 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.8709\n",
            "Epoch 427: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3154 - accuracy: 0.8723 - val_loss: 0.4417 - val_accuracy: 0.8129\n",
            "Epoch 428/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8727\n",
            "Epoch 428: loss did not improve from 0.28537\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2937 - accuracy: 0.8723 - val_loss: 0.4460 - val_accuracy: 0.8129\n",
            "Epoch 429/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.8764\n",
            "Epoch 429: loss improved from 0.28537 to 0.27917, saving model to ./model_PID__429_loss_0.279_vloss_0.450_acc_0.877_vacc_0.820.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2792 - accuracy: 0.8774 - val_loss: 0.4500 - val_accuracy: 0.8204\n",
            "Epoch 430/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8761\n",
            "Epoch 430: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3004 - accuracy: 0.8761 - val_loss: 0.4468 - val_accuracy: 0.8144\n",
            "Epoch 431/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.8607\n",
            "Epoch 431: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3168 - accuracy: 0.8607 - val_loss: 0.4383 - val_accuracy: 0.8144\n",
            "Epoch 432/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8730\n",
            "Epoch 432: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2881 - accuracy: 0.8742 - val_loss: 0.4438 - val_accuracy: 0.8144\n",
            "Epoch 433/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8738\n",
            "Epoch 433: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2880 - accuracy: 0.8748 - val_loss: 0.4444 - val_accuracy: 0.8174\n",
            "Epoch 434/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8691\n",
            "Epoch 434: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3120 - accuracy: 0.8691 - val_loss: 0.4401 - val_accuracy: 0.8129\n",
            "Epoch 435/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.8744\n",
            "Epoch 435: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2868 - accuracy: 0.8748 - val_loss: 0.4435 - val_accuracy: 0.8159\n",
            "Epoch 436/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8750\n",
            "Epoch 436: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2998 - accuracy: 0.8742 - val_loss: 0.4515 - val_accuracy: 0.8204\n",
            "Epoch 437/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8728\n",
            "Epoch 437: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3084 - accuracy: 0.8729 - val_loss: 0.4459 - val_accuracy: 0.8129\n",
            "Epoch 438/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8760\n",
            "Epoch 438: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3078 - accuracy: 0.8748 - val_loss: 0.4421 - val_accuracy: 0.8129\n",
            "Epoch 439/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.8639\n",
            "Epoch 439: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3178 - accuracy: 0.8646 - val_loss: 0.4382 - val_accuracy: 0.8114\n",
            "Epoch 440/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8714\n",
            "Epoch 440: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2858 - accuracy: 0.8710 - val_loss: 0.4423 - val_accuracy: 0.8114\n",
            "Epoch 441/500\n",
            "1542/1558 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8664\n",
            "Epoch 441: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3197 - accuracy: 0.8659 - val_loss: 0.4462 - val_accuracy: 0.8144\n",
            "Epoch 442/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8739\n",
            "Epoch 442: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3075 - accuracy: 0.8736 - val_loss: 0.4474 - val_accuracy: 0.8129\n",
            "Epoch 443/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8744\n",
            "Epoch 443: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2946 - accuracy: 0.8742 - val_loss: 0.4486 - val_accuracy: 0.8204\n",
            "Epoch 444/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8728\n",
            "Epoch 444: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3245 - accuracy: 0.8723 - val_loss: 0.4443 - val_accuracy: 0.8204\n",
            "Epoch 445/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8723\n",
            "Epoch 445: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.3012 - accuracy: 0.8710 - val_loss: 0.4429 - val_accuracy: 0.8189\n",
            "Epoch 446/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8686\n",
            "Epoch 446: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3175 - accuracy: 0.8678 - val_loss: 0.4369 - val_accuracy: 0.8144\n",
            "Epoch 447/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8714\n",
            "Epoch 447: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2975 - accuracy: 0.8716 - val_loss: 0.4451 - val_accuracy: 0.8189\n",
            "Epoch 448/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.8727\n",
            "Epoch 448: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2921 - accuracy: 0.8723 - val_loss: 0.4535 - val_accuracy: 0.8189\n",
            "Epoch 449/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8668\n",
            "Epoch 449: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3214 - accuracy: 0.8671 - val_loss: 0.4521 - val_accuracy: 0.8159\n",
            "Epoch 450/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8817\n",
            "Epoch 450: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2981 - accuracy: 0.8813 - val_loss: 0.4448 - val_accuracy: 0.8144\n",
            "Epoch 451/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.2824 - accuracy: 0.8737\n",
            "Epoch 451: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2833 - accuracy: 0.8729 - val_loss: 0.4436 - val_accuracy: 0.8159\n",
            "Epoch 452/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.8742\n",
            "Epoch 452: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2878 - accuracy: 0.8742 - val_loss: 0.4408 - val_accuracy: 0.8159\n",
            "Epoch 453/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8757\n",
            "Epoch 453: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2907 - accuracy: 0.8755 - val_loss: 0.4422 - val_accuracy: 0.8174\n",
            "Epoch 454/500\n",
            "1544/1558 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8756\n",
            "Epoch 454: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2976 - accuracy: 0.8742 - val_loss: 0.4421 - val_accuracy: 0.8204\n",
            "Epoch 455/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8782\n",
            "Epoch 455: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2997 - accuracy: 0.8768 - val_loss: 0.4495 - val_accuracy: 0.8219\n",
            "Epoch 456/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8686\n",
            "Epoch 456: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3067 - accuracy: 0.8691 - val_loss: 0.4459 - val_accuracy: 0.8189\n",
            "Epoch 457/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3190 - accuracy: 0.8681\n",
            "Epoch 457: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3173 - accuracy: 0.8691 - val_loss: 0.4443 - val_accuracy: 0.8204\n",
            "Epoch 458/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8683\n",
            "Epoch 458: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3054 - accuracy: 0.8684 - val_loss: 0.4391 - val_accuracy: 0.8219\n",
            "Epoch 459/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.8697\n",
            "Epoch 459: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3051 - accuracy: 0.8697 - val_loss: 0.4397 - val_accuracy: 0.8174\n",
            "Epoch 460/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8732\n",
            "Epoch 460: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2952 - accuracy: 0.8729 - val_loss: 0.4437 - val_accuracy: 0.8204\n",
            "Epoch 461/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.8736\n",
            "Epoch 461: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2911 - accuracy: 0.8723 - val_loss: 0.4486 - val_accuracy: 0.8174\n",
            "Epoch 462/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8731\n",
            "Epoch 462: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3041 - accuracy: 0.8729 - val_loss: 0.4485 - val_accuracy: 0.8189\n",
            "Epoch 463/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8747\n",
            "Epoch 463: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2936 - accuracy: 0.8742 - val_loss: 0.4539 - val_accuracy: 0.8204\n",
            "Epoch 464/500\n",
            "1546/1558 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8745\n",
            "Epoch 464: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3035 - accuracy: 0.8742 - val_loss: 0.4550 - val_accuracy: 0.8159\n",
            "Epoch 465/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8725\n",
            "Epoch 465: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2920 - accuracy: 0.8729 - val_loss: 0.4498 - val_accuracy: 0.8159\n",
            "Epoch 466/500\n",
            "1548/1558 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.8695\n",
            "Epoch 466: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3039 - accuracy: 0.8691 - val_loss: 0.4434 - val_accuracy: 0.8144\n",
            "Epoch 467/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8739\n",
            "Epoch 467: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 7s 4ms/step - loss: 0.2945 - accuracy: 0.8742 - val_loss: 0.4479 - val_accuracy: 0.8189\n",
            "Epoch 468/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8706\n",
            "Epoch 468: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2970 - accuracy: 0.8710 - val_loss: 0.4476 - val_accuracy: 0.8189\n",
            "Epoch 469/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8660\n",
            "Epoch 469: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3130 - accuracy: 0.8652 - val_loss: 0.4420 - val_accuracy: 0.8204\n",
            "Epoch 470/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.8733\n",
            "Epoch 470: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2893 - accuracy: 0.8729 - val_loss: 0.4406 - val_accuracy: 0.8174\n",
            "Epoch 471/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.8776\n",
            "Epoch 471: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3122 - accuracy: 0.8774 - val_loss: 0.4469 - val_accuracy: 0.8204\n",
            "Epoch 472/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8712\n",
            "Epoch 472: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3103 - accuracy: 0.8710 - val_loss: 0.4445 - val_accuracy: 0.8204\n",
            "Epoch 473/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8727\n",
            "Epoch 473: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3014 - accuracy: 0.8729 - val_loss: 0.4464 - val_accuracy: 0.8204\n",
            "Epoch 474/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8668\n",
            "Epoch 474: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3047 - accuracy: 0.8671 - val_loss: 0.4420 - val_accuracy: 0.8204\n",
            "Epoch 475/500\n",
            "1545/1558 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8744\n",
            "Epoch 475: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3003 - accuracy: 0.8742 - val_loss: 0.4383 - val_accuracy: 0.8204\n",
            "Epoch 476/500\n",
            "1553/1558 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8712\n",
            "Epoch 476: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3054 - accuracy: 0.8710 - val_loss: 0.4392 - val_accuracy: 0.8189\n",
            "Epoch 477/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8708\n",
            "Epoch 477: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3101 - accuracy: 0.8703 - val_loss: 0.4409 - val_accuracy: 0.8234\n",
            "Epoch 478/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8801\n",
            "Epoch 478: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3006 - accuracy: 0.8787 - val_loss: 0.4409 - val_accuracy: 0.8204\n",
            "Epoch 479/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8784\n",
            "Epoch 479: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2910 - accuracy: 0.8787 - val_loss: 0.4487 - val_accuracy: 0.8219\n",
            "Epoch 480/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8803\n",
            "Epoch 480: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2854 - accuracy: 0.8806 - val_loss: 0.4456 - val_accuracy: 0.8204\n",
            "Epoch 481/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8767\n",
            "Epoch 481: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2954 - accuracy: 0.8768 - val_loss: 0.4463 - val_accuracy: 0.8204\n",
            "Epoch 482/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8701\n",
            "Epoch 482: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3128 - accuracy: 0.8703 - val_loss: 0.4453 - val_accuracy: 0.8219\n",
            "Epoch 483/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8708\n",
            "Epoch 483: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3193 - accuracy: 0.8703 - val_loss: 0.4436 - val_accuracy: 0.8204\n",
            "Epoch 484/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.8723\n",
            "Epoch 484: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2946 - accuracy: 0.8723 - val_loss: 0.4406 - val_accuracy: 0.8174\n",
            "Epoch 485/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.8823\n",
            "Epoch 485: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2871 - accuracy: 0.8825 - val_loss: 0.4443 - val_accuracy: 0.8174\n",
            "Epoch 486/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.8724\n",
            "Epoch 486: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3126 - accuracy: 0.8729 - val_loss: 0.4378 - val_accuracy: 0.8174\n",
            "Epoch 487/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.8772\n",
            "Epoch 487: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2880 - accuracy: 0.8780 - val_loss: 0.4313 - val_accuracy: 0.8174\n",
            "Epoch 488/500\n",
            "1550/1558 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8716\n",
            "Epoch 488: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 7s 5ms/step - loss: 0.3027 - accuracy: 0.8723 - val_loss: 0.4334 - val_accuracy: 0.8174\n",
            "Epoch 489/500\n",
            "1552/1558 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8750\n",
            "Epoch 489: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2933 - accuracy: 0.8748 - val_loss: 0.4333 - val_accuracy: 0.8159\n",
            "Epoch 490/500\n",
            "1558/1558 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.8723\n",
            "Epoch 490: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3004 - accuracy: 0.8723 - val_loss: 0.4346 - val_accuracy: 0.8174\n",
            "Epoch 491/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.8734\n",
            "Epoch 491: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2894 - accuracy: 0.8736 - val_loss: 0.4398 - val_accuracy: 0.8219\n",
            "Epoch 492/500\n",
            "1549/1558 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8715\n",
            "Epoch 492: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2933 - accuracy: 0.8716 - val_loss: 0.4380 - val_accuracy: 0.8219\n",
            "Epoch 493/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8709\n",
            "Epoch 493: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3009 - accuracy: 0.8710 - val_loss: 0.4321 - val_accuracy: 0.8204\n",
            "Epoch 494/500\n",
            "1556/1558 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8779\n",
            "Epoch 494: loss did not improve from 0.27917\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3026 - accuracy: 0.8780 - val_loss: 0.4329 - val_accuracy: 0.8159\n",
            "Epoch 495/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.8756\n",
            "Epoch 495: loss improved from 0.27917 to 0.27894, saving model to ./model_PID__495_loss_0.279_vloss_0.441_acc_0.876_vacc_0.822.hdf5\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2789 - accuracy: 0.8761 - val_loss: 0.4406 - val_accuracy: 0.8219\n",
            "Epoch 496/500\n",
            "1554/1558 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8707\n",
            "Epoch 496: loss did not improve from 0.27894\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2918 - accuracy: 0.8703 - val_loss: 0.4366 - val_accuracy: 0.8174\n",
            "Epoch 497/500\n",
            "1547/1558 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8707\n",
            "Epoch 497: loss did not improve from 0.27894\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.3119 - accuracy: 0.8716 - val_loss: 0.4358 - val_accuracy: 0.8174\n",
            "Epoch 498/500\n",
            "1543/1558 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8795\n",
            "Epoch 498: loss did not improve from 0.27894\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2814 - accuracy: 0.8793 - val_loss: 0.4375 - val_accuracy: 0.8204\n",
            "Epoch 499/500\n",
            "1555/1558 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.8765\n",
            "Epoch 499: loss did not improve from 0.27894\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2852 - accuracy: 0.8768 - val_loss: 0.4474 - val_accuracy: 0.8159\n",
            "Epoch 500/500\n",
            "1557/1558 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8735\n",
            "Epoch 500: loss did not improve from 0.27894\n",
            "1558/1558 [==============================] - 6s 4ms/step - loss: 0.2955 - accuracy: 0.8736 - val_loss: 0.4433 - val_accuracy: 0.8159\n"
          ]
        }
      ],
      "source": [
        "if __learning__: \n",
        "    history = model.fit(X_train, y_train, epochs=500, batch_size=_batch_size_, validation_data=(X_test, y_test),verbose=1,callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgzklVywoNmk"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwcWQ94IpDFu",
        "outputId": "6632c80a-a04c-4866-870f-0ed07eb15c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_display_object_visualization.html#sphx-glr-auto-examples-miscellaneous-plot-display-object-visualization-py"
      ],
      "metadata": {
        "id": "H0c0Fkd2cWRj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "zctwrl1AcTZ0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bina_transformer=Binarizer(threshold=0.5)\n",
        "y_pred_transform=bina_transformer.fit_transform(y_pred)"
      ],
      "metadata": {
        "id": "hxZwDiKYhA5H"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "eCqcqNJl79G5"
      },
      "outputs": [],
      "source": [
        "cm=confusion_matrix(y_test,y_pred_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Z69kCq3T-pMo",
        "outputId": "ad037cf8-372d-4acc-8f50-3e1fb6007eda"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nZ0rmkNsBGnl",
        "outputId": "3ed2116c-eece-441c-88e2-936062333135"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRV53nv8e+DpIOQQIh5FsI2HvBsK9iG6zqO43hIYpJ4jkOMnVW3SZzb3qS5121yU9dpk5u6TVfTum2cBkGJg2MntheJpySNHbcCDARsjPFQigSI2ViMQmh67h97C46EhiOkfbaO9u+z1lna09nn2RK8z9n7nczdERGR5BoSdwAiIhIvJQIRkYRTIhARSTglAhGRhFMiEBFJuPy4A+itsWPHenl5edxhiIjklN/97nfvufu4zvblXCIoLy9nzZo1cYchIpJTzGxLV/v0aEhEJOGUCEREEk6JQEQk4ZQIREQSTolARCThIksEZrbQzPaY2YYu9puZfc/MNpnZejO7JKpYRESka1HeESwCru9m/w3AzPB1H/DPEcYiIiJdiKwfgbu/Ymbl3RwyD/g3D8bBXmlmpWY2yd13RhWTiMhA5u4cbWqhrr6JuiONvH+kkbr6RuqONFJX38Q154zngqml/f65cXYomwJsS1uvDbedlAjM7D6CuwbKysqyEpyISF+4O0caW8JCvLFd4b6/vpH307ad+NnIsebWLs85bsTQQZcIMubujwKPAlRUVGgmHRHJKnfn0LFm9h9pCgvwxpMK8Lr6tkK+6fjPxpbOC3UzKB1WwKjiFKOKUkwpHcZ5k0sYXZyitCjF6OKC8GeKUUUFjCpKMXJYAfl50TzNjzMRbAempa1PDbeJiESmtdU51NAcFNz14bfzI03hz84K92Bfc2vn30GHGIwqSlFaVMDo4hRlo4u4cGppWMifKOyPF+5FKUqGFZA3xLJ85V2LMxEsA+43s8eBy4ADqh8Qkd5obXUONjS1K8A7K9z31zcd315X30RLF4V63hBjVNGJAnzG2GIunZ46XoCfVLgXpRhRmM+QAVSon4rIEoGZLQU+CIw1s1rgz4ECAHf/F+A54EZgE1AP3BNVLCIy8LW0OgeONp14hp5WgLd9Qz9euNcH+/bXN9JFmU5BXluhnmJUcQEzxw8/UZAXtX1Lb1+4jxiaj1luF+qnIspWQ3f2sN+BL0b1+SISn+aWVvYfPVERml5B2vYMvV0lan0jB4424V0U6qn8IYxOe/xyzsQSRhUXtCvoOxbuxam8RBbqpyInKotFJD5NLa1h5WhT+0rScDm9cG/7Jn+wobnL8w3NHxJWggYF+OTSYScqSdMeu6QX8EUq1COlRCCSIMeaW9gfFuLBt/JOCvcOLWIOHeu6UC9K5bUrsMtGFx1/zHKicD/xTX5UUYphqbwsXrFkQolAJEc1NLW0/6aeVoC/36Htetu+I40tXZ5v+ND84wV2aVFQUdquCWNxW6HeVsgXUFigQn0wUCIQGQCONra0qxTtWIDXpX2Lb3sMc7Sp60J9xND84BFLcYoxw1PMHD/8pPbpbYX+6KIUI4sKGJqvQj2plAhE+pG7U9944pv6iaaMJ7dPT/8m39DUdW/SksL849/SJ5QUcvbEks7bp4eFe+mwFKl8DSwsmVMiEOmCu3P4WPOJVi4dOhilfzs/XrjXN9HYxRABZjByWMHxZ+aTSwuZFfYmTW+7nl64l0bYm1SkjRKBJIK7c7ChucsCvKvOR00tXfcmLS060SZ92ugiLpg6sl1Ho46dj0YOsN6kIm2UCCTntPUmravvvPPR8UcxaY9e9tc3dTlEQNCb9EQ79PKxRVxcVHpS56O29dHFKUoKC3K+N6lIGyUCiVVLq3PwaHrv0RPP0d+vbzw+yFd6YV/XTW/S/CHWrgA/I62StLPOR229SVWoS5IpEUi/aW5p5cDRpuPPyk/0Hk1vq96+sN/fXW/SvCHtCu2zJo443nO0XQuYtAJ+eEKHCBDpCyUC6VZLq7Nuax3vHT7W4VHMyW3XDxxt6vI8bb1J2wrwcyaXhAV4Wk/S4vadj9SbVCQ7lAikW0tW1PDgzze22zasIK9dO/Spo4oYXXRy+/T0wl29SUUGLiUC6VJLq1O5vIYLp5XyrU+ed7xwV29SkcFFiUC69NLbe9iyr56vXncW504eGXc4IhIR9VSRLlUur2bSyEKuO3di3KGISISUCKRT7+4+RNWmfcy/YjoF6tkqMqjpf7h0qrKqhsKCIdz5gbK4QxGRiCkRyEn21zfy9LpaPnnxFEYVp+IOR0QipkQgJ1m6ahsNTa3cPac87lBEJAuUCKSd5pZWlqyoYc7pYzh7Yknc4YhIFigRSDu/3LibHQcauGfujLhDEZEsUSKQdiqrqikbXcSHzh4fdygikiVKBHLchu0HWF1Tx91zyjVuvkiCKBHIcQurqilO5XFrxdS4QxGRLFIiEAD2HjrGL17fyS2XTqWksCDucEQki5QIBIAfv7qVxhY1GRVJIiUCobG5lR+9uoWrzxrHaeOGxx2OiGSZEoHw7Bs72HvomJqMiiSUEkHCuTuVVTWcPq6YK2eOjTscEYmBEkHCrd1ax/raAyyYO0PTQooklBJBwi2sqqGkMJ+bL5kSdygiEpNIE4GZXW9m75jZJjN7oJP9ZWb2kpmtM7P1ZnZjlPFIezsPHOWFDbu4Y3YZRSlNVieSVJElAjPLAx4BbgBmAXea2awOh30deMLdLwbuAP4pqnjkZEtWbMHdmX/59LhDEZEYRXlHMBvY5O6b3b0ReByY1+EYB9qGuBwJ7IgwHknT0NTC0lVbuXbWBKaNLoo7HBGJUZSJYAqwLW29NtyW7kHgM2ZWCzwHfKmzE5nZfWa2xszW7N27N4pYE+eZddupq29Sk1ERib2y+E5gkbtPBW4ElpjZSTG5+6PuXuHuFePGjct6kINNW5PRcyaVcNmM0XGHIyIxizIRbAempa1PDbel+xzwBIC7rwAKATVmj9iKzft4Z/ch7plbriajIhJpIlgNzDSzGWaWIqgMXtbhmK3ANQBmdg5BItCzn4hVVtUwujjFTRdOjjsUERkAIksE7t4M3A+8CLxF0DroTTN7yMxuCg/7CvD7ZvY6sBRY4O4eVUwCW/fV8+u3dvPp2WUUFuTFHY6IDACRNh539+cIKoHTt30jbXkjMDfKGKS9xStqyDNj/hVqMioigbgriyWLDh9r5onV27jx/ElMKCmMOxwRGSCUCBLkqbW1HDrWzD1zy+MORUQGECWChGhtdRZV1XDRtFIuLhsVdzgiMoAoESTEb/9rL5vfO6K7ARE5iRJBQlRW1TB+xFBuOG9S3KGIyACjRJAAm/Yc5pV39zL/8umk8vUnF5H2VCokwOLlNaTyh/Dpy8riDkVEBiAlgkHuwNEmfra2lnkXTmbM8KFxhyMiA5ASwSD3xOpt1De2sECVxCLSBSWCQayl1Vm8oobZM0Zz7uSRcYcjIgNUxonAzDR7SY751cbd1NYd5V7dDYhIN3pMBGY2x8w2Am+H6xeamaaUzAGLllczpXQY186aGHcoIjKAZXJH8HfAdcA+AHd/Hfi9KIOSvntr50FWbn6fu+dMJ2+I5hwQka5l9GjI3bd12NQSQSzSjyqrqhlWkMftFWoyKiLdy2QY6m1mNgdwMysA/ohgfgEZoPYdPsYzr+3g1kunMrKoIO5wRGSAy+SO4A+BLxJMPL8duAj4QpRBSd8sXbWVxuZWjSskIhnJ5I7gLHe/K32Dmc0FqqIJSfqiqaWVJSu3cOXMsZwxfkTc4YhIDsjkjuAfMtwmA8DzG3ax++Ax7p07I+5QRCRHdHlHYGZXAHOAcWb25bRdJYAmux2gKquqmTG2mKvOHBd3KCKSI7q7I0gBwwmSxYi010HgluhDk956bdt+1m3dz91XTGeImoyKSIa6vCNw998CvzWzRe6+JYsxySmqrKpmxNB8bqmYFncoIpJDMqksrjezh4FzgeMznrv7hyKLSnpt98EGnl2/k89eUc7woZn8WUVEAplUFj9GMLzEDOAvgBpgdYQxySl4bOUWWtxZMKc87lBEJMdkkgjGuPsPgSZ3/6273wvobmAAaWhq4bFXt3LN2RMoG6OxAUWkdzJ5htAU/txpZh8FdgCjowtJeuvnr+9g35FGdSATkVOSSSL4SzMbCXyFoP9ACfDHkUYlGXN3KqtqOGvCCOacPibucEQkB/WYCNz9F+HiAeBqON6zWAaAVdXvs3HnQb79qfMxU5NREem97jqU5QG3EYwx9IK7bzCzjwF/BgwDLs5OiNKdRctrKC0q4BMXTYk7FBHJUd3dEfwQmAasAr5nZjuACuABd38mG8FJ92rr6nnxzV38wVWnMyylzt4icmq6SwQVwAXu3mpmhcAu4HR335ed0KQnS1ZswcyYf/n0uEMRkRzWXfPRRndvBXD3BmBzb5OAmV1vZu+Y2SYze6CLY24zs41m9qaZ/bg350+y+sZmlq7ayvXnTmRy6bC4wxGRHNbdHcHZZrY+XDbg9HDdAHf3C7o7cVjH8AhwLVALrDazZe6+Me2YmcCfAnPdvc7MxvfhWhLlqbXbOdjQrCajItJn3SWCc/p47tnAJnffDGBmjwPzgI1px/w+8Ii71wG4+54+fmYiuDuLltdw/pSRXDp9VNzhiEiO627Qub4ONDcFSJ/ruBa4rMMxZwKYWRXB0NYPuvsLHU9kZvcB9wGUlWkO3v/c9B6b9hzmu7ddqCajItJnGU1eH6F8YCbwQeBO4AdmVtrxIHd/1N0r3L1i3DiNs19ZVcPY4UP56AWT4g5FRAaBKBPBdoLmp22mhtvS1QLL3L3J3auBdwkSg3Sh+r0j/ObtPdx1WRlD89VkVET6LqNEYGbDzOysXp57NTDTzGaYWQq4A1jW4ZhnCO4GMLOxBI+KNvfycxJl8fIaCvKMuy7XIzIR6R89JgIz+zjwGvBCuH6RmXUs0E/i7s3A/cCLwFvAE+7+ppk9ZGY3hYe9COwzs43AS8BX1U+ha4camnhyzTY+fsFkxo8o7PkNIiIZyGTQuQcJWgC9DODur5lZRjOju/tzwHMdtn0jbdmBL4cv6cGTa2o50tjCPZqYXkT6USaPhprc/UCHbR5FMNK1llZn8YoaLp0+ivOnjow7HBEZRDJJBG+a2aeBPDObaWb/ACyPOC7p4KW397BlX706kIlIv8skEXyJYL7iY8CPCYaj1nwEWVa5vJpJIwu57tyJcYciIoNMJnUEZ7v714CvRR2MdO7d3Yeo2rSP/339WRTkxd31Q0QGm0xKlb81s7fM7Jtmdl7kEclJKqtqKCwYwp0fUJNREel/PSYCd7+aYGayvcD3zewNM/t65JEJAPvrG3l6XS2fvHgKo4pTcYcjIoNQRs8Z3H2Xu38P+EOCPgXf6OEt0k+WrtpGQ1Mrd88pjzsUERmkMulQdo6ZPWhmbxBMXr+cYLgIiVhzSytLVtQw5/QxnD2xJO5wRGSQyqSyeCHwE+A6d98RcTyS5pcbd7PjQAN/MU9VMyISnR4TgbtfkY1A5GSVVdWUjS7iQ2drvh4RiU6XicDMnnD328JHQuk9iTOaoUz6ZsP2A6yuqeP/fmwWeUM054CIRKe7O4I/Cn9+LBuBSHsLq6opTuVxa4WqY0QkWl1WFrv7znDxC+6+Jf0FfCE74SXT3kPH+MXrO7nl0qmUFBbEHY6IDHKZNB+9tpNtN/R3IHLCj1/dSmOLmoyKSHZ0V0fweYJv/qeZ2fq0XSOAqqgDS6rG5lZ+9OoWrj5rHKeNGx53OCKSAN3VEfwYeB74NvBA2vZD7v5+pFEl2LNv7GDvoWOac0BEsqa7RODuXmNmX+y4w8xGKxn0P3ensqqG08cVc+XMsXGHIyIJ0dMdwceA3xE0H01vw+jAaRHGlUhrt9axvvYA3/zEeZipyaiIZEeXicDdPxb+1DOKLFlYVUNJYT43XzIl7lBEJEEyGWtorpkVh8ufMbPvmpnGQ+5nOw8c5YUNu7hjdhlFqUxG/hAR6R+ZNB/9Z6DezC4EvgL8N7Ak0qgSaMmKLbg78y+fHncoIpIwmSSCZnd3YB7wj+7+CEETUuknDU0tLF21lWtnTWDa6KK4wxGRhMnkGcQhM/tTYD5wpZkNAdTdtR89s247dfVNajIqIrHI5I7gdoKJ6+91910EcxE8HGlUCdLWZPScSSVcNmN03OGISAJlMlXlLuAxYKSZfQxocPd/izyyhFixeR/v7D7EPXPL1WRURGKRSauh24BVwK3AbcCrZnZL1IElRWVVDaOLU9x04eS4QxGRhMqkjuBrwAfcfQ+AmY0Dfg38NMrAkmDrvnp+/dZuvvjBMygsyIs7HBFJqEzqCIa0JYHQvgzfJz1YvKKGPDPmX6EmoyISn0zuCF4wsxeBpeH67cBz0YWUDIePNfPE6m3ceP4kJpQUxh2OiCRYJnMWf9XMPgX8j3DTo+7+dLRhDX5Pra3l0LFm7plbHncoIpJw3c1HMBP4G+B04A3gT9x9e7YCG8xaW51FVTVcNK2Ui8tGxR2OiCRcd8/6FwK/AG4mGIH0H3p7cjO73szeMbNNZvZAN8fdbGZuZhW9/Yxc9Nv/2svm947obkBEBoTuHg2NcPcfhMvvmNna3pzYzPKARwimuqwFVpvZMnff2OG4EcAfAa/25vy5rLKqhvEjhnLDeZPiDkVEpNs7gkIzu9jMLjGzS4BhHdZ7MhvY5O6b3b0ReJxgvKKOvgl8B2jodfQ5aNOew7zy7l7mXz6dVL4aX4lI/Lq7I9gJfDdtfVfaugMf6uHcU4Btaeu1wGXpB4QJZZq7P2tmX+3qRGZ2H3AfQFlZbo+AvXh5Dan8IXz6sty+DhEZPLqbmObqKD84HLzuu8CCno5190eBRwEqKio8yriidOBoEz9bW8u8CyczZvjQuMMREQGi7Ri2HZiWtj413NZmBHAe8LKZ1QCXA8sGc4XxE6u3Ud/YwgJVEovIABJlIlgNzDSzGWaWAu4AlrXtdPcD7j7W3cvdvRxYCdzk7msijCk2La3O4hU1zJ4xmnMnj4w7HBGR4yJLBO7eDNwPvAi8BTzh7m+a2UNmdlNUnztQ/WrjbmrrjnKv7gZEZIDpsWexBWMj3wWc5u4PhfMVT3T3VT29192fo8NwFO7+jS6O/WBGEeeoRcurmVI6jGtnTYw7FBGRdjK5I/gn4ArgznD9EEH/AMnQWzsPsnLz+9w9Zzp5QzTngIgMLJkMOneZu19iZusA3L0ufOYvGaqsqmZYQR63V6jJqIgMPJncETSFvYQdjs9H0BppVIPIvsPHeOa1HXzqkimMLNJUzyIy8GSSCL4HPA2MN7O/Av4T+FakUQ0iS1dtpbG5VeMKiciAlckw1I+Z2e+AawADPuHub0Ue2SDQ1NLKkpVbuHLmWM4YPyLucEREOpXJnMVlQD3wc4J+AEfCbdKD5zfsYvfBY9w7d0bcoYiIdCmTyuJnCeoHDCgEZgDvAOdGGNegUFlVzYyxxVx15ri4QxER6VImj4bOT18PB4r7QmQRDRKvbdvPuq37efDjsxiiJqMiMoD1umexu6+lwyiicrLKqmpGDM3nloppPR8sIhKjTHoWfzltdQhwCbAjsogGgd0HG3h2/U4+e0U5w4dm8vRNRCQ+mZRS6c1dmgnqDH4WTTiDw2Mrt9DizoI55XGHIiLSo24TQdiRbIS7/0mW4sl5DU0tPPbqVq45ewJlY4riDkdEpEdd1hGYWb67twBzsxhPzvv56zvYd6RRHchEJGd0d0ewiqA+4DUzWwY8CRxp2+nuT0UcW85xdyqrajhrwgjmnD4m7nBERDKSSR1BIbCPYI7itv4EDigRdLCq+n027jzItz91PsHo3SIiA193iWB82GJoAycSQJucnTc4SouW11BaVMAnLpoSdygiIhnrLhHkAcNpnwDaKBF0UFtXz4tv7uIPrjqdYam8uMMREclYd4lgp7s/lLVIctySFVswM+ZfPj3uUEREeqW7nsV6yJ2h+sZmlq7ayvXnTmRy6bC4wxER6ZXuEsE1WYsixz21djsHG5rVZFREclKXicDd389mILnK3Vm0vIbzp4zk0umj4g5HRKTXej3onLT3n5veY9Oew9wzt1xNRkUkJykR9FFlVQ1jhw/loxdMijsUEZFTokTQB9XvHeE3b+/hrsvKGJqvJqMikpuUCPpg8fIaCvKMuy7XzJ0ikruUCE7RoYYmnlyzjY9fMJnxIwrjDkdE5JQpEZyiJ9fUcqSxhXs0Mb2I5DglglPQ0uosXlHDpdNHcf7UkXGHIyLSJ0oEp+Clt/ewZV+9OpCJyKCgRHAKKpdXM2lkIdedOzHuUERE+izSRGBm15vZO2a2ycwe6GT/l81so5mtN7N/N7MBP2Lbu7sPUbVpH/OvmE5BnvKoiOS+yEqycL7jR4AbgFnAnWY2q8Nh64AKd78A+Cnw11HF018qq2oYmj+EOz+gJqMiMjhE+ZV2NrDJ3Te7eyPwODAv/QB3f8nd68PVlcDUCOPps/31jTy9rpZPXjyFUcWpuMMREekXUSaCKcC2tPXacFtXPgc839kOM7vPzNaY2Zq9e/f2Y4i9s3TVNhqaWlmgSmIRGUQGxENuM/sMUAE83Nl+d3/U3SvcvWLcuHHZDS7U3NLKkhU1zDl9DGdPLIklBhGRKESZCLYD09LWp4bb2jGzDwNfA25y92MRxtMnv9y4mx0HGtSBTEQGnSgTwWpgppnNMLMUcAewLP0AM7sY+D5BEtgTYSx9VllVTdnoIj509vi4QxER6VeRJQJ3bwbuB14E3gKecPc3zewhM7spPOxhYDjwpJm9ZmbLujhdrDZsP8DqmjrunlNO3hDNOSAig0t3k9f3mbs/BzzXYds30pY/HOXn95eFVdUUp/K4tWJAN2oSETklA6KyeCDbe+gYv3h9J7dcOpWSwoK4wxER6XdKBD147NUtNLa0cvec8rhDERGJhBJBNxqbW/nRyq1cfdY4Ths3PO5wREQioUTQjWff2MF7h4+pyaiIDGpKBF1wdyqrajh9XDFXzhwbdzgiIpFRIujC2q11rK89wIK5MzBTk1ERGbyUCLqwsKqGksJ8br6ku+GRRERynxJBJ3YeOMoLG3Zxx+wyilKRdrUQEYmdEkEnlqzYgrsz//IBP0+OiEifKRF00NDUwtJVW7l21gSmjS6KOxwRkcgpEXTwzLrt1NU3qcmoiCSGEkGatiaj50wq4bIZo+MOR0QkK5QI0qzYvI93dh/inrnlajIqIomhRJCmsqqG0cUpbrpwctyhiIhkjRJBaOu+en791m4+PbuMwoK8uMMREckaJYLQ4hU15Jkx/wo1GRWRZFEiAA4fa+aJ1du48fxJTCgpjDscEZGsUiIAnlpby6FjzdwztzzuUEREsi7xiaC11VlUVcNF00q5uGxU3OGIiGRd4hPBb/9rL5vfO6K7ARFJrMQngsqqGsaPGMoN502KOxQRkVgkOhFs2nOYV97dy/zLp5PKT/SvQkQSLNGl3+LlNaTyh/Dpy8riDkVEJDaJTQQHjjbxs7W1zLtwMmOGD407HBGR2CQ2ETyxehv1jS0sUCWxiCRcIhNBS6uzeEUNs2eM5tzJI+MOR0QkVolMBL/auJvauqPcq7sBEZFkJoJFy6uZUjqMa2dNjDsUEZHYJS4RvLXzICs3v8/dc6aTN0RzDoiIJC4RVFZVM6wgj9sr1GRURAQSlgj2HT7GM6/t4FOXTGFkUUHc4YiIDAiRJgIzu97M3jGzTWb2QCf7h5rZT8L9r5pZeZTxLF21lcbmVo0rJCKSJrJEYGZ5wCPADcAs4E4zm9XhsM8Bde5+BvB3wHeiiqeppZUlK7dw5cyxnDF+RFQfIyKSc6K8I5gNbHL3ze7eCDwOzOtwzDxgcbj8U+Aai2jW+Oc37GL3wWPcO3dGFKcXEclZUSaCKcC2tPXacFunx7h7M3AAGNPxRGZ2n5mtMbM1e/fuPaVgilN5XDtrAledOe6U3i8iMljlxx1AJtz9UeBRgIqKCj+Vc1xzzgSuOWdCv8YlIjIYRHlHsB2YlrY+NdzW6TFmlg+MBPZFGJOIiHQQZSJYDcw0sxlmlgLuAJZ1OGYZcHe4fAvwG3c/pW/8IiJyaiJ7NOTuzWZ2P/AikAcsdPc3zewhYI27LwN+CCwxs03A+wTJQkREsijSOgJ3fw54rsO2b6QtNwC3RhmDiIh0L1E9i0VE5GRKBCIiCadEICKScEoEIiIJZ7nWWtPM9gJbTvHtY4H3+jGcXKBrTgZdczL05Zqnu3unQyvkXCLoCzNb4+4VcceRTbrmZNA1J0NU16xHQyIiCadEICKScElLBI/GHUAMdM3JoGtOhkiuOVF1BCIicrKk3RGIiEgHSgQiIgk3KBOBmV1vZu+Y2SYze6CT/UPN7Cfh/lfNrDz7UfavDK75y2a20czWm9m/m9n0OOLsTz1dc9pxN5uZm1nONzXM5JrN7Lbwb/2mmf042zH2twz+bZeZ2Utmti78931jHHH2FzNbaGZ7zGxDF/vNzL4X/j7Wm9klff5Qdx9UL4Ihr/8bOA1IAa8Dszoc8wXgX8LlO4CfxB13Fq75aqAoXP58Eq45PG4E8AqwEqiIO+4s/J1nAuuAUeH6+LjjzsI1Pwp8PlyeBdTEHXcfr/n3gEuADV3svxF4HjDgcuDVvn7mYLwjmA1scvfN7t4IPA7M63DMPGBxuPxT4BozsyzG2N96vGZ3f8nd68PVlQQzxuWyTP7OAN8EvgM0ZDO4iGRyzb8PPOLudQDuvifLMfa3TK7ZgZJweSSwI4vx9Tt3f4VgfpauzAP+zQMrgVIzm9SXzxyMiWAKsC1tvTbc1ukx7t4MHADGZCW6aGRyzek+R/CNIpf1eM3hLfM0d382m4FFKJO/85nAmWZWZWYrzez6rEUXjUyu+UHgM2ZWSzD/yZeyE1psevv/vUc5MXm99B8z+wxQAVwVdyxRMrMhwHeBBTGHkm35BI+HPkhw1/eKmZ3v7vtjjSpadwKL3P1vzewKglkPz3P31rgDyxWD8Y5gOzAtbX1quK3TY8wsn+B2cl9WootGJteMmX0Y+Bpwk7sfy1JsUenpmkcA5wEvm1kNwbPUZTleYRf6mC4AAAVOSURBVJzJ37kWWObuTe5eDbxLkBhyVSbX/DngCQB3XwEUEgzONlhl9P+9NwZjIlgNzDSzGWaWIqgMXtbhmGXA3eHyLcBvPKyFyVE9XrOZXQx8nyAJ5PpzY+jhmt39gLuPdfdydy8nqBe5yd3XxBNuv8jk3/YzBHcDmNlYgkdFm7MZZD/L5Jq3AtcAmNk5BIlgb1ajzK5lwGfD1kOXAwfcfWdfTjjoHg25e7OZ3Q+8SNDiYKG7v2lmDwFr3H0Z8EOC28dNBJUyd8QXcd9leM0PA8OBJ8N68a3uflNsQfdRhtc8qGR4zS8CHzGzjUAL8FV3z9m73Qyv+SvAD8zsfxFUHC/I5S92ZraUIJmPDes9/hwoAHD3fyGoB7kR2ATUA/f0+TNz+PclIiL9YDA+GhIRkV5QIhARSTglAhGRhFMiEBFJOCUCEZGEUyKQAcnMWszstbRXeTfHHu6Hz1tkZtXhZ60Ne6j29hz/amazwuU/67BveV9jDM/T9nvZYGY/N7PSHo6/KNdH45ToqfmoDEhmdtjdh/f3sd2cYxHwC3f/qZl9BPgbd7+gD+frc0w9ndfMFgPvuvtfdXP8AoJRV+/v71hk8NAdgeQEMxsezqOw1szeMLOTRho1s0lm9kraN+Yrw+0fMbMV4XufNLOeCuhXgDPC9345PNcGM/vjcFuxmT1rZq+H228Pt79sZhVm9v+AYWEcj4X7Doc/Hzezj6bFvMjMbjGzPDN72MxWh2PM/0EGv5YVhIONmdns8BrXmdlyMzsr7In7EHB7GMvtYewLzWxVeGxnI7ZK0sQ99rZeenX2IugV+1r4epqgF3xJuG8sQa/Ktjvaw+HPrwBfC5fzCMYbGktQsBeH2/8P8I1OPm8RcEu4fCvwKnAp8AZQTNAr+03gYuBm4Adp7x0Z/nyZcM6DtpjSjmmL8ZPA4nA5RTCK5DDgPuDr4fahwBpgRidxHk67vieB68P1EiA/XP4w8LNweQHwj2nv/xbwmXC5lGAsouK4/956xfsadENMyKBx1N0valsxswLgW2b2e0ArwTfhCcCutPesBhaGxz7j7q+Z2VUEk5VUhUNrpAi+SXfmYTP7OsE4NZ8jGL/maXc/EsbwFHAl8ALwt2b2HYLHSf/Ri+t6Hvh7MxsKXA+84u5Hw8dRF5jZLeFxIwkGi6vu8P5hZvZaeP1vAb9KO36xmc0kGGahoIvP/whwk5n9SbheCJSF55KEUiKQXHEXMA641N2bLBhRtDD9AHd/JUwUHwUWmdl3gTrgV+5+Zwaf8VV3/2nbipld09lB7v6uBXMd3Aj8pZn9u7s/lMlFuHuDmb0MXAfcTjDRCgSzTX3J3V/s4RRH3f0iMysiGH/ni8D3CCbgecndPxlWrL/cxfsNuNnd38kkXkkG1RFIrhgJ7AmTwNXASXMuWzAP8253/wHwrwTT/a0E5ppZ2zP/YjM7M8PP/A/gE2ZWZGbFBI91/sPMJgP17v4jgsH8Opsztim8M+nMTwgGCmu7u4CgUP9823vM7MzwMzvlwWxz/xP4ip0YSr1tKOIFaYceInhE1uZF4EsW3h5ZMCqtJJwSgeSKx4AKM3sD+CzwdifHfBB43czWEXzb/nt330tQMC41s/UEj4XOzuQD3X0tQd3BKoI6g39193XA+cCq8BHNnwN/2cnbHwXWt1UWd/BLgomBfu3B9IsQJK6NwFoLJi3/Pj3csYexrCeYmOWvgW+H157+vpeAWW2VxQR3DgVhbG+G65Jwaj4qIpJwuiMQEUk4JQIRkYRTIhARSTglAhGRhFMiEBFJOCUCEZGEUyIQEUm4/w/10cH3ujJ2CAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fpr, tpr, _ = roc_curve( y_pred_transform,y_test,pos_label=1)\n",
        "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "auc = roc_auc_score(y_test, y_pred_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMRd5eGU9ASA",
        "outputId": "df08e4f3-95da-45e0-a504-d10a06749a58"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8230566748699643"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzlQR00e8Miq",
        "outputId": "2c5c0675-44fb-4cfc-a2cb-8bee9abd12bf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.       , 0.2565445, 1.       ])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(fpr,tpr)\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZRlw4piW7_uW",
        "outputId": "7929c1d9-666a-4708-d796-013189508432"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grafikon3(fx,desc1,txt1,desc2=\"\",txt2=\"\",desc3=\"\",txt3=\"\",ngraf=2,c1='rgba(35,128,132,0.8)', c2='rgba(193,99,99,0.8)',c3='rgba(193,99,99,0.8)',title=None):\n",
        "    '''\n",
        "    fx: dataFrame\n",
        "    desc1:column1\n",
        "    txt1: label1\n",
        "    desc2:column2\n",
        "    txt2: label2\n",
        "    ngraf: number of graph\n",
        "    c1: color1\n",
        "    c2: color2\n",
        "    title: graph title\n",
        "    '''\n",
        "    \n",
        "    #x_=[i for i in range(len(y_pred))]\n",
        "    if title==None:\n",
        "      title=txt1+\" \"+txt2\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    fig0 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "\n",
        "    if ngraf>=3:\n",
        "        fig0.add_trace(\n",
        "            go.Bar(x=fx.index, y=fx[desc3], marker_color='rgba(225, 20, 20,0.2)',  name=txt3, showlegend=True, ),\n",
        "              secondary_y=False,\n",
        "            #row=1, col=1\n",
        "        )\n",
        "\n",
        "\n",
        "    if ngraf>=2:\n",
        "        fig0.add_trace(\n",
        "            go.Scatter(x=fx.index, y=fx[desc2], name=txt2, line=dict(color=c2) ,showlegend=True  ),\n",
        "            secondary_y=False,\n",
        "            #row=1, col=1\n",
        "\n",
        "        )\n",
        "\n",
        "    fig0.add_trace(\n",
        "        go.Scatter(x=fx.index, y=fx[desc1], name=txt1, line=dict(color=c1) ,showlegend=True  ),\n",
        "        secondary_y=False,\n",
        "        #row=1, col=1\n",
        "\n",
        "    )\n",
        "\n",
        "    fig0.update_layout(\n",
        "        title=title,\n",
        "        autosize=False,\n",
        "        width=1200,\n",
        "        height=600,\n",
        "        \n",
        "        )\n",
        "\n",
        "    print(title)\n",
        "    fig0.update_yaxes(title_text=\"<b>\"+title+\"</b>\", secondary_y=False)\n",
        "    #fig0.update_yaxes(title_text=\"<b>Alarm státusz</b>\", secondary_y=True)\n",
        "    fig0.update_layout(paper_bgcolor='rgb(200,200,200)')\n",
        "    fig0.show()"
      ],
      "metadata": {
        "id": "qa-AQAZV0EPd"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_df=pd.DataFrame({\"epoch\":history.epoch, \"loss\":history.history[\"loss\"],\"val_loss\":history.history[\"val_loss\"]})"
      ],
      "metadata": {
        "id": "Uve0EfpV0Rkl"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grafikon3(history_df,\"loss\",\"Loss\",\"val_loss\",\"Val_Loss\",title=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "4ENvDCA-0U1g",
        "outputId": "4191bb10-88b1-4393-af3b-0ef8fdb051b6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Val_Loss\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"003b79c1-e5c2-4b3f-998f-aa5131c5becc\" class=\"plotly-graph-div\" style=\"height:600px; width:1200px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"003b79c1-e5c2-4b3f-998f-aa5131c5becc\")) {                    Plotly.newPlot(                        \"003b79c1-e5c2-4b3f-998f-aa5131c5becc\",                        [{\"line\":{\"color\":\"rgba(193,99,99,0.8)\"},\"name\":\"Val_Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499],\"y\":[0.6046006679534912,0.591455340385437,0.5881681442260742,0.5878205299377441,0.5862314105033875,0.5865735411643982,0.5869579315185547,0.5878915786743164,0.588835597038269,0.5871296525001526,0.5820785760879517,0.5799729824066162,0.5804007053375244,0.576602041721344,0.5762079954147339,0.5744374990463257,0.5736362934112549,0.5756533741950989,0.5720990300178528,0.5709088444709778,0.5694674253463745,0.5664798021316528,0.5662171840667725,0.5640878081321716,0.5613840818405151,0.5578567981719971,0.557555079460144,0.5593673586845398,0.5567107796669006,0.557360827922821,0.5543403029441833,0.5531986951828003,0.554991602897644,0.5573344826698303,0.5560569763183594,0.5525951981544495,0.5541914105415344,0.5552808046340942,0.5526449680328369,0.5497732162475586,0.5449038743972778,0.5440889596939087,0.541266918182373,0.5385922789573669,0.5386815071105957,0.5414527058601379,0.5377370119094849,0.5393060445785522,0.5399559140205383,0.5381510853767395,0.5361121296882629,0.5352689623832703,0.5362288951873779,0.5368424654006958,0.5374664664268494,0.5382716059684753,0.5367873907089233,0.5360457897186279,0.5368092656135559,0.5363130569458008,0.5336155891418457,0.5286301970481873,0.528939425945282,0.5324763059616089,0.5326864719390869,0.5342361330986023,0.5325799584388733,0.5314393639564514,0.5285373330116272,0.5301663279533386,0.5312963128089905,0.5331132411956787,0.5315757989883423,0.5345996618270874,0.5396817326545715,0.5369338393211365,0.5329100489616394,0.530480682849884,0.5301494598388672,0.5319961905479431,0.5312376022338867,0.5320451855659485,0.5296614766120911,0.5331254601478577,0.5309246182441711,0.5336303114891052,0.533101499080658,0.5333631634712219,0.5356085896492004,0.5350679755210876,0.5320868492126465,0.5347080230712891,0.5359524488449097,0.5384235978126526,0.5367947816848755,0.5341740250587463,0.5378865599632263,0.5345231294631958,0.5322679877281189,0.5318623781204224,0.5348528027534485,0.5294936299324036,0.5348144173622131,0.5375790596008301,0.5406267642974854,0.5403690934181213,0.5387054085731506,0.5371792912483215,0.5330019593238831,0.5340906977653503,0.5355757474899292,0.5356489419937134,0.5317511558532715,0.5326058268547058,0.5346763730049133,0.5286219716072083,0.5291160345077515,0.5307903289794922,0.5307722091674805,0.5302483439445496,0.5274022221565247,0.5260092616081238,0.5245312452316284,0.529148280620575,0.529492974281311,0.5306425094604492,0.5331503748893738,0.5296992063522339,0.5342286229133606,0.53053879737854,0.5321329832077026,0.5365297794342041,0.5397191643714905,0.5375580787658691,0.5377665758132935,0.5363039374351501,0.533567488193512,0.5283722281455994,0.523987352848053,0.5237991809844971,0.5271105766296387,0.5267556309700012,0.5272016525268555,0.5309850573539734,0.5272814035415649,0.5274118185043335,0.5261002779006958,0.5289306640625,0.5292948484420776,0.5265005826950073,0.5228466391563416,0.5257904529571533,0.5277196168899536,0.5260761976242065,0.5255946516990662,0.5236822366714478,0.5298069715499878,0.5292900800704956,0.5260469317436218,0.5287643074989319,0.5274559259414673,0.5267364382743835,0.5268021821975708,0.5279250741004944,0.5298388600349426,0.5283293724060059,0.5300524234771729,0.5319381356239319,0.5273672938346863,0.523506760597229,0.5256323218345642,0.5322409868240356,0.529226541519165,0.5276352167129517,0.5329965353012085,0.538325309753418,0.5371789336204529,0.531266987323761,0.5344184637069702,0.5327497720718384,0.5295336842536926,0.5291959643363953,0.5251479744911194,0.5206499099731445,0.5219697952270508,0.52630215883255,0.5194544196128845,0.5240191221237183,0.518968939781189,0.5281985402107239,0.525146484375,0.5188698768615723,0.5188038349151611,0.51727294921875,0.5234336853027344,0.5165136456489563,0.5189226269721985,0.5149782299995422,0.514264702796936,0.5186251997947693,0.5105462670326233,0.5102213621139526,0.5113937854766846,0.508998453617096,0.5156884789466858,0.5172268748283386,0.519671618938446,0.5059115290641785,0.5041699409484863,0.5025201439857483,0.5006506443023682,0.5026838779449463,0.5076178908348083,0.5093138217926025,0.5107066631317139,0.5101636052131653,0.5048152804374695,0.5112720131874084,0.5087277293205261,0.504489541053772,0.5048894286155701,0.5027313232421875,0.5030088424682617,0.5039076209068298,0.5028544664382935,0.4993149936199188,0.5026881694793701,0.5026398301124573,0.5048030614852905,0.49731069803237915,0.5059815645217896,0.5037950277328491,0.5041608214378357,0.505499541759491,0.5043224692344666,0.5021359324455261,0.4977041780948639,0.5012397766113281,0.5015701055526733,0.49941056966781616,0.5067711472511292,0.5047717690467834,0.5041167140007019,0.5019345283508301,0.49941277503967285,0.49152132868766785,0.4962218403816223,0.4946347177028656,0.4937288165092468,0.48562657833099365,0.4922516942024231,0.48851683735847473,0.49313175678253174,0.4965677857398987,0.49741578102111816,0.498731791973114,0.4980054795742035,0.4980883002281189,0.49143579602241516,0.4964664876461029,0.4968424439430237,0.49078163504600525,0.4900384843349457,0.4940808117389679,0.4918418824672699,0.48224520683288574,0.48102468252182007,0.48117563128471375,0.4760781526565552,0.48517799377441406,0.48609521985054016,0.4835452735424042,0.48705318570137024,0.48542702198028564,0.4802224636077881,0.4841481149196625,0.4872364401817322,0.48744621872901917,0.48851990699768066,0.490136057138443,0.4903500974178314,0.48571157455444336,0.48333269357681274,0.486436128616333,0.4873092472553253,0.48456600308418274,0.4837212562561035,0.48048314452171326,0.48284026980400085,0.4950219392776489,0.49219855666160583,0.5014156699180603,0.486972838640213,0.4780924916267395,0.4823713004589081,0.4804452657699585,0.48835164308547974,0.48216959834098816,0.4752487242221832,0.4775545001029968,0.4743500351905823,0.4773074984550476,0.480193167924881,0.47363898158073425,0.475615918636322,0.4816742539405823,0.47560644149780273,0.47756150364875793,0.4801178574562073,0.4735771119594574,0.480879545211792,0.4812309145927429,0.4773862361907959,0.4768983721733093,0.48185840249061584,0.4829194247722626,0.4814126491546631,0.4782682955265045,0.480340838432312,0.4782833158969879,0.4687865376472473,0.459264874458313,0.46230366826057434,0.46410274505615234,0.4685799479484558,0.46515125036239624,0.46743348240852356,0.4713391661643982,0.4668464958667755,0.4639360308647156,0.46182912588119507,0.4599824547767639,0.4681476950645447,0.46286505460739136,0.47288694977760315,0.46770092844963074,0.4643019139766693,0.46510049700737,0.4686509966850281,0.4633520841598511,0.47248560190200806,0.47274479269981384,0.47208231687545776,0.4627372622489929,0.4687153697013855,0.4621938467025757,0.47010543942451477,0.47155481576919556,0.46618032455444336,0.4677666425704956,0.4624134600162506,0.4689812660217285,0.465521901845932,0.46799537539482117,0.4603354036808014,0.47259384393692017,0.47298628091812134,0.47637322545051575,0.4670030474662781,0.470615416765213,0.46320292353630066,0.4586848318576813,0.4595206677913666,0.45809197425842285,0.45851942896842957,0.4630385935306549,0.46705952286720276,0.469937264919281,0.47317224740982056,0.472339391708374,0.4682715833187103,0.46667787432670593,0.4652708172798157,0.47042545676231384,0.4618341624736786,0.4649794399738312,0.46086516976356506,0.4618082046508789,0.46093904972076416,0.4649243950843811,0.4635258615016937,0.46436378359794617,0.4623834788799286,0.463199645280838,0.46302855014801025,0.4617522954940796,0.4629018306732178,0.45783302187919617,0.45524710416793823,0.45617133378982544,0.4495432376861572,0.4563176929950714,0.45578354597091675,0.453926146030426,0.4581608176231384,0.4544978439807892,0.4543456435203552,0.45216867327690125,0.453448623418808,0.4551151394844055,0.4632778465747833,0.45756223797798157,0.4640595316886902,0.4643711745738983,0.46146488189697266,0.46547722816467285,0.46720707416534424,0.4622141122817993,0.45902952551841736,0.4600493311882019,0.45223501324653625,0.444503515958786,0.448356568813324,0.4528319239616394,0.4469173550605774,0.4433670938014984,0.443034291267395,0.4464644491672516,0.4481446146965027,0.44897520542144775,0.4460989534854889,0.442209392786026,0.436309814453125,0.441080242395401,0.4442500174045563,0.44657212495803833,0.4417363107204437,0.44597917795181274,0.45000919699668884,0.44676733016967773,0.43833741545677185,0.443774938583374,0.44441547989845276,0.4400964081287384,0.443497896194458,0.4515261650085449,0.44593346118927,0.4420822858810425,0.4381667673587799,0.44233018159866333,0.44619178771972656,0.44735947251319885,0.44863730669021606,0.4443201422691345,0.4428820013999939,0.4368833005428314,0.44514232873916626,0.4535302221775055,0.45214301347732544,0.44484743475914,0.44360479712486267,0.44075340032577515,0.44221898913383484,0.4420846700668335,0.449507474899292,0.4459359645843506,0.44428330659866333,0.4391084313392639,0.4397425353527069,0.4436667859554291,0.44861963391304016,0.4485417902469635,0.45389798283576965,0.45503973960876465,0.4497740864753723,0.4433653652667999,0.4479334056377411,0.44758108258247375,0.44204843044281006,0.4405663311481476,0.44694581627845764,0.44454383850097656,0.446372389793396,0.4420159161090851,0.4382971227169037,0.43917348980903625,0.4409092366695404,0.44088464975357056,0.44870948791503906,0.44563034176826477,0.4463001787662506,0.4453403651714325,0.44364413619041443,0.4405600130558014,0.4443308711051941,0.43775707483291626,0.43130046129226685,0.4334322214126587,0.433274507522583,0.43456876277923584,0.43983954191207886,0.4379790425300598,0.43209171295166016,0.4328501224517822,0.4405744969844818,0.43656235933303833,0.4357830286026001,0.4374781548976898,0.4473942220211029,0.4432591497898102],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"line\":{\"color\":\"rgba(35,128,132,0.8)\"},\"name\":\"Loss\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499],\"y\":[0.6344687938690186,0.5876516103744507,0.5667210817337036,0.5394640564918518,0.5494144558906555,0.5168027281761169,0.5225605964660645,0.5003162026405334,0.5106493234634399,0.4992198348045349,0.5069264769554138,0.493829607963562,0.4830368161201477,0.5023992657661438,0.477823942899704,0.48950186371803284,0.47827112674713135,0.4714500308036804,0.49303191900253296,0.4725654423236847,0.4732919931411743,0.4599577784538269,0.4650672972202301,0.4652012884616852,0.4693244695663452,0.4638308584690094,0.46903425455093384,0.46639034152030945,0.4666959345340729,0.4491100311279297,0.46013474464416504,0.447426974773407,0.4454919695854187,0.44231945276260376,0.44205841422080994,0.4533705413341522,0.4426364004611969,0.43296125531196594,0.43772271275520325,0.44664186239242554,0.457732617855072,0.44182831048965454,0.4465356767177582,0.4404783546924591,0.4433515667915344,0.43953654170036316,0.43843549489974976,0.43302199244499207,0.436588853597641,0.4367278516292572,0.42866480350494385,0.4396459758281708,0.4310157001018524,0.42966532707214355,0.4378679096698761,0.4255044460296631,0.436576247215271,0.4220704138278961,0.42283502221107483,0.42748135328292847,0.42321765422821045,0.44996437430381775,0.419007807970047,0.4262543320655823,0.42091500759124756,0.42050185799598694,0.4225044250488281,0.4208875000476837,0.4235093295574188,0.41528457403182983,0.4127212464809418,0.4102783501148224,0.40829604864120483,0.4123799204826355,0.40273764729499817,0.4071824848651886,0.4160880744457245,0.41668811440467834,0.4204288721084595,0.40838944911956787,0.41005992889404297,0.4041244685649872,0.4097610116004944,0.40839970111846924,0.4060255289077759,0.40416499972343445,0.40585169196128845,0.39934465289115906,0.3960912227630615,0.4161188006401062,0.409690260887146,0.4003739058971405,0.407194584608078,0.4033936858177185,0.40393006801605225,0.3957761228084564,0.38608619570732117,0.39645275473594666,0.39612051844596863,0.40198883414268494,0.38288360834121704,0.3980821669101715,0.3915005624294281,0.3816649317741394,0.3882274329662323,0.4022732079029083,0.3957240879535675,0.395797997713089,0.38007852435112,0.40222829580307007,0.3828258216381073,0.392623633146286,0.3968619108200073,0.38315823674201965,0.3998333215713501,0.381926953792572,0.3902967572212219,0.39284756779670715,0.3841337561607361,0.3803434371948242,0.3770926594734192,0.39775288105010986,0.38206931948661804,0.39625945687294006,0.3863951861858368,0.376636266708374,0.3794316053390503,0.38087570667266846,0.3818729519844055,0.4017420709133148,0.37332913279533386,0.37459102272987366,0.3799728751182556,0.37794241309165955,0.3805136978626251,0.3714331388473511,0.37231767177581787,0.37641605734825134,0.37232306599617004,0.371472030878067,0.3684883117675781,0.3622625470161438,0.3833524286746979,0.3780464231967926,0.3721095025539398,0.3709721863269806,0.3651949167251587,0.3758135139942169,0.3765718340873718,0.37898892164230347,0.3791138827800751,0.3536956012248993,0.36074116826057434,0.3665512502193451,0.3632763922214508,0.36480712890625,0.36518120765686035,0.35422778129577637,0.3773881196975708,0.3645171523094177,0.3631751537322998,0.36746492981910706,0.37753504514694214,0.34892725944519043,0.3688201308250427,0.3597745895385742,0.37093642354011536,0.3502563238143921,0.38198694586753845,0.37634342908859253,0.3499472141265869,0.3596956431865692,0.36479994654655457,0.3568122386932373,0.35727983713150024,0.3531862497329712,0.3488033413887024,0.37136390805244446,0.35185348987579346,0.3595513701438904,0.3587562143802643,0.3615171015262604,0.36016717553138733,0.36475059390068054,0.3441782593727112,0.352272629737854,0.3465602695941925,0.3459819257259369,0.36692944169044495,0.35721176862716675,0.3487955927848816,0.34904223680496216,0.3506915867328644,0.3640681505203247,0.34280645847320557,0.3451794981956482,0.35877764225006104,0.34550532698631287,0.33468082547187805,0.34960123896598816,0.3744308650493622,0.35813960433006287,0.3573155999183655,0.3575602173805237,0.3482821583747864,0.34868288040161133,0.3391801118850708,0.3508801758289337,0.3530021905899048,0.3356182277202606,0.3614858090877533,0.35764631628990173,0.3586624264717102,0.343131959438324,0.3544721007347107,0.3581010699272156,0.36266085505485535,0.3451037108898163,0.32672059535980225,0.3321681320667267,0.33920684456825256,0.3665195107460022,0.32428932189941406,0.34273603558540344,0.3626854717731476,0.3367704153060913,0.352776437997818,0.34101560711860657,0.34613335132598877,0.3351004123687744,0.3391420543193817,0.33585667610168457,0.3443416655063629,0.3332999050617218,0.34917810559272766,0.3473556637763977,0.344559907913208,0.3276815712451935,0.3360080420970917,0.34165140986442566,0.352595716714859,0.3429516851902008,0.3369074761867523,0.33033233880996704,0.33546850085258484,0.3388429880142212,0.3285857141017914,0.3479035198688507,0.31847554445266724,0.35335397720336914,0.3409408628940582,0.3415560722351074,0.3427276313304901,0.3449081778526306,0.3397880792617798,0.33356761932373047,0.326144278049469,0.33828598260879517,0.32637086510658264,0.327602744102478,0.33774685859680176,0.33575302362442017,0.3147755265235901,0.3245193660259247,0.3077705502510071,0.34607478976249695,0.33498120307922363,0.3261280655860901,0.3289778530597687,0.3568856418132782,0.33941054344177246,0.3161754906177521,0.3115106523036957,0.31503981351852417,0.32767361402511597,0.35197338461875916,0.333440899848938,0.3289872109889984,0.31636711955070496,0.3219812214374542,0.3272058367729187,0.31335335969924927,0.332492470741272,0.3053201138973236,0.30689433217048645,0.3366420567035675,0.32503148913383484,0.32865241169929504,0.3215380907058716,0.30633237957954407,0.3106840252876282,0.334492564201355,0.33568960428237915,0.32790741324424744,0.31596410274505615,0.3144436478614807,0.3218589127063751,0.3270394206047058,0.3191015124320984,0.32784250378608704,0.31753233075141907,0.30967530608177185,0.317687064409256,0.33734357357025146,0.3107829988002777,0.3224063515663147,0.31525784730911255,0.3158549666404724,0.31370052695274353,0.314655065536499,0.3260388672351837,0.3056468367576599,0.32861003279685974,0.3222900331020355,0.3178327977657318,0.31264039874076843,0.32191070914268494,0.33104878664016724,0.3053313195705414,0.31059953570365906,0.33265069127082825,0.3128669857978821,0.3382188081741333,0.3352315127849579,0.3058059811592102,0.3286013603210449,0.3202078342437744,0.31653133034706116,0.31966647505760193,0.30867359042167664,0.34230145812034607,0.31582170724868774,0.3113264739513397,0.3244558274745941,0.33005136251449585,0.31690314412117004,0.33999571204185486,0.3182803690433502,0.3255949318408966,0.330364465713501,0.3097923994064331,0.3001372814178467,0.3138349652290344,0.32189151644706726,0.30548524856567383,0.3196984529495239,0.3101486563682556,0.3116665482521057,0.3235776126384735,0.31971973180770874,0.3287160098552704,0.30701297521591187,0.3212367594242096,0.29516732692718506,0.3112954795360565,0.3264930248260498,0.3270801901817322,0.31326228380203247,0.3404361605644226,0.3172038495540619,0.33633744716644287,0.3000035285949707,0.3133202791213989,0.29755595326423645,0.31195133924484253,0.31981533765792847,0.29580947756767273,0.3218354284763336,0.3216854929924011,0.3116545081138611,0.31778743863105774,0.31743863224983215,0.3065897226333618,0.32103580236434937,0.32238897681236267,0.325047105550766,0.2976824939250946,0.3097516596317291,0.31286486983299255,0.3146294355392456,0.31649476289749146,0.31441348791122437,0.3291754424571991,0.30057263374328613,0.306514710187912,0.29944393038749695,0.30942368507385254,0.3175966441631317,0.28536954522132874,0.30530714988708496,0.3193351626396179,0.3158018887042999,0.3106003403663635,0.3196777105331421,0.2992945909500122,0.31441089510917664,0.31732773780822754,0.33435171842575073,0.3269968330860138,0.30236539244651794,0.29235631227493286,0.32333993911743164,0.3183847665786743,0.3110227882862091,0.3057651221752167,0.2997530400753021,0.3083225190639496,0.30358242988586426,0.3241037428379059,0.3037163317203522,0.302071750164032,0.3048820197582245,0.28698837757110596,0.3020675778388977,0.2986459732055664,0.30653005838394165,0.30986592173576355,0.30173224210739136,0.308501660823822,0.3123142421245575,0.3203027844429016,0.29011258482933044,0.3169816732406616,0.29910311102867126,0.2993908226490021,0.29580456018447876,0.31543514132499695,0.2936776876449585,0.27916523814201355,0.30036476254463196,0.31681883335113525,0.28805550932884216,0.2879754900932312,0.312021940946579,0.2867819368839264,0.29976728558540344,0.3083777129650116,0.30781275033950806,0.31780269742012024,0.28578996658325195,0.31969159841537476,0.30745476484298706,0.2945610582828522,0.3244868218898773,0.30118221044540405,0.3175209164619446,0.2975454330444336,0.29214805364608765,0.3214153051376343,0.29811397194862366,0.2832861840724945,0.28776460886001587,0.2906889021396637,0.29758983850479126,0.2997322082519531,0.30665984749794006,0.3172765374183655,0.3053945004940033,0.3050554096698761,0.2952386140823364,0.2911408543586731,0.3040584623813629,0.29357853531837463,0.3034803867340088,0.29195961356163025,0.30394792556762695,0.2944868206977844,0.29699698090553284,0.3130154609680176,0.28933724761009216,0.3121509850025177,0.3103182315826416,0.30143558979034424,0.3047125041484833,0.3003093898296356,0.30539414286613464,0.3101213276386261,0.3006455600261688,0.2909757196903229,0.28539443016052246,0.2953508198261261,0.31275561451911926,0.31932759284973145,0.2946126461029053,0.287078857421875,0.3126361072063446,0.28804436326026917,0.3027368187904358,0.2932735085487366,0.3003821074962616,0.2893986403942108,0.2932657301425934,0.3009423017501831,0.30256029963493347,0.278942346572876,0.2917594909667969,0.3119010627269745,0.28135597705841064,0.2851691246032715,0.295529842376709],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.94]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"<b>Loss Val_Loss</b>\"}},\"yaxis2\":{\"anchor\":\"x\",\"overlaying\":\"y\",\"side\":\"right\"},\"title\":{\"text\":\"Loss Val_Loss\"},\"autosize\":false,\"width\":1200,\"height\":600,\"paper_bgcolor\":\"rgb(200,200,200)\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('003b79c1-e5c2-4b3f-998f-aa5131c5becc');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdeNK3579qMZoMnHfLWTmy",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18f72c05c3994f66a54bbaf3091308be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d1973648af446f38f74919bb5c41cef",
              "IPY_MODEL_7a61f7954c4c4c94a1b7eee21032e259"
            ],
            "layout": "IPY_MODEL_7e689a6d32bc4ee2be6ef277d9df2927"
          }
        },
        "2d1973648af446f38f74919bb5c41cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59d4e8bc6b154935bfb8a9ea4a0fd573",
            "placeholder": "​",
            "style": "IPY_MODEL_0e604228deab4c0b8b7e861b1e3a26ac",
            "value": "0.136 MB of 0.136 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "7a61f7954c4c4c94a1b7eee21032e259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd3e37ae408541e4adb2c670bfcf8564",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11d9a50a2c264df09b9140aeeb8bc5c0",
            "value": 1
          }
        },
        "7e689a6d32bc4ee2be6ef277d9df2927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d4e8bc6b154935bfb8a9ea4a0fd573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e604228deab4c0b8b7e861b1e3a26ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd3e37ae408541e4adb2c670bfcf8564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11d9a50a2c264df09b9140aeeb8bc5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}